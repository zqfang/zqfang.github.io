<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><meta http-equiv=x-ua-compatible content="IE=edge, chrome=1"><title>Expectation Maximization - My Computational Genomic Playground</title><meta name=Description content="My Computational Genomic Playground"><meta property="og:title" content="Expectation Maximization"><meta property="og:description" content="Maximum Likelihood Estimation Gaussian Mixture Model Expectation Maximization 1. Probability and likelihood likehood & maximum likehood 在非正式场合似然（likelihood）和概率（Probabilit"><meta property="og:type" content="article"><meta property="og:url" content="https://zqfang.github.io/2020-01-26-ml-em/"><meta property="og:image" content="https://zqfang.github.io/logo.png"><meta property="article:published_time" content="2020-01-26T00:00:00+00:00"><meta property="article:modified_time" content="2020-01-26T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://zqfang.github.io/logo.png"><meta name=twitter:title content="Expectation Maximization"><meta name=twitter:description content="Maximum Likelihood Estimation Gaussian Mixture Model Expectation Maximization 1. Probability and likelihood likehood & maximum likehood 在非正式场合似然（likelihood）和概率（Probabilit"><meta name=application-name content="Pleiades"><meta name=apple-mobile-web-app-title content="Pleiades"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://zqfang.github.io/2020-01-26-ml-em/><link rel=prev href=https://zqfang.github.io/2020-01-20-begin/><link rel=next href=https://zqfang.github.io/2020-01-26-ml-gradient/><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/normalize.css@8.0.1/normalize.min.css><link rel=stylesheet href=/css/style.min.1e2694bed152fa2922dbe909a441838ed693d88b1330f97485bfa8ed78da42df.css integrity="sha256-HiaUvtFS+iki2+kJpEGDjtaT2IsTMPl0hb+o7XjaQt8="><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@3.7.2/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Expectation Maximization","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/zqfang.github.io\/2020-01-26-ml-em\/"},"image":["https:\/\/zqfang.github.io\/images\/Apple-Devices-Preview.png"],"genre":"posts","keywords":"Expectation Maximization, Statistical Learning","wordcount":2904,"url":"https:\/\/zqfang.github.io\/2020-01-26-ml-em\/","datePublished":"2020-01-26T00:00:00+00:00","dateModified":"2020-01-26T00:00:00+00:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Organization","name":"xxxx","logo":"https:\/\/zqfang.github.io\/images\/avatar.png"},"author":{"@type":"Person","name":"zqfang"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem('theme')?localStorage.getItem('theme')==='dark':('auto'==='auto'?window.matchMedia('(prefers-color-scheme: dark)').matches:'auto'==='dark'))&&document.body.setAttribute('theme','dark');</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="My Computational Genomic Playground"><span class=header-title-pre><span>&#8711;</span></span>Pleiades <span class=header-title-post><i class="fas fa-terminal"></i></span></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=/publication/>Publication </a><a class=menu-item href=/portfolio/>Portfolio </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/about/ title=About>About </a><a class=menu-item href=https://github.com/zqfang title=GitHub rel="noopener noreffer" target=_blank><i class="fab fa-github fa-fw"></i></a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop><input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw"></i></a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw"></i></a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin"></i></span></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="My Computational Genomic Playground"><span class=header-title-pre><span>&#8711;</span></span>Pleiades <span class=header-title-post><i class="fas fa-terminal"></i></span></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw"></i></a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw"></i></a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin"></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/posts/>Posts</a><a class=menu-item href=/categories/>Categories</a><a class=menu-item href=/publication/>Publication</a><a class=menu-item href=/portfolio/>Portfolio</a><a class=menu-item href=/tags/>Tags</a><a class=menu-item href=/about/ title=About>About</a><a class=menu-item href=https://github.com/zqfang title=GitHub rel="noopener noreffer" target=_blank><i class="fab fa-github fa-fw"></i></a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animated flipInX">Expectation Maximization</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw"></i>zqfang</a></span>&nbsp;<span class=post-category>included in <a href=/categories/machine-learning/><i class="far fa-folder fa-fw"></i>Machine Learning</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2020-01-26>2020-01-26</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;2904 words&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;6 minutes&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#1-probability-and-likelihood>1. Probability and likelihood</a></li><li><a href=#2-maximum-likelihood-estimation>2. Maximum Likelihood Estimation</a></li><li><a href=#3-gaussian-mixture-model>3. Gaussian Mixture Model</a></li><li><a href=#4-expectation-maximization>4. Expectation Maximization</a><ul><li><a href=#0-理解em的前提>(0) 理解EM的前提</a></li></ul></li><li><a href=#5-求解gmm>5. 求解GMM</a><ul><li><a href=#1-gmm-e-step>(1) GMM E-Step:</a></li><li><a href=#2-gmm-m-step>(2) GMM M-Step:</a></li><li><a href=#3-求-pi>(3) 求 $\pi$:</a></li><li><a href=#4-计算mu>(4) 计算$\mu$</a></li><li><a href=#5-计算sigma>(5) 计算$\sigma$</a></li></ul></li><li><a href=#6-从kl散度角度解释em>6 从KL散度角度解释EM</a></li></ul></nav></div></div><div class=content id=content><p>Maximum Likelihood Estimation<br>Gaussian Mixture Model<br>Expectation Maximization</p><h2 id=1-probability-and-likelihood>1. Probability and likelihood</h2><p><a href=http://fangs.in/post/thinkstats/likelihood/ target=_blank rel="noopener noreffer">likehood & maximum likehood</a></p><p>在非正式场合似然（likelihood）和概率（Probability）几乎是一对同义词，但是在统计学中似然和概率却是两个不同的概念。</p><p><strong>概率</strong>: 在特定环境下某件事情发生的可能性，也就是结果没有产生之前依据环境所对应的参数来预测某件事情发生的可能性。</p><blockquote><p>比如抛硬币，抛之前我们不知道最后是哪一面朝上，但是根据硬币的性质我们可以推测任何一面朝上的可能性均为50%，这个概率只有在抛硬币之前才是有意义的，抛完硬币后的结果便是确定的；</p></blockquote><p><strong>似然</strong>: 刚好相反，是在确定的结果下去推测产生这个结果的可能环境（参数）。</p><blockquote><p>假设随机抛掷一枚硬币1,000次，结果500次人头朝上，500次数字朝上，那么两面朝上的概率均为50%。<span style=color:red>运用出现的结果来判断这个事情本身的性质（参数）</span>，也就是似然。</p></blockquote><p><strong>当结果和参数相互对应，似然和概率在数值上相等</strong>。 用 θ 表示环境对应的参数，x 表示结果，那么概率可以表示为：</p><p>$$P(x | \theta )$$</p><p>$p(x \vert θ)$ 是条件概率的表示方法。θ 是前置条件，理解为在 θ 的前提下，事件 x 发生的概率，相对应的似然可以表示为:</p><p>$$\mathcal{L}(\theta | x)$$</p><p>可以理解为已知结果为 x ，参数为 θ (似然函数里 θ 是变量，这里说的参数和变量是相对与概率而言的)对应的概率，即：</p><p>$$\mathcal{L}(\theta | x)=P(x | \theta)$$</p><p>两者在数值上相等，但是意义并不相同, $\mathcal{L}$ 是关于 θ 的函数，而 P 则是关于 x 的函数。</p><h2 id=2-maximum-likelihood-estimation>2. Maximum Likelihood Estimation</h2><p>单高斯模型 $x \sim \mathcal{N}(\mu, \Sigma)$, $x_{i} \in \mathcal{D}$, 那么对参数 $\mu$和 $\Sigma$ 进行估计，只需要最大化log-likelihood函数：</p><p>$$
\begin{aligned}
\log p(X) &=\sum_{i=1}^{N} \log \mathcal{N}\left(x_{i} | \mu, \Sigma\right) \cr
&=\sum_{i=1}^{N} \log \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{\left(x_{i}-\mu\right)^{2}}{2 \sigma^{2}}} \cr
&=\sum_{i=1}^{N} \log \frac{1}{\sqrt{2 \pi} \sigma}+\sum_{i=1}^{N}-\frac{\left(x_{i}-\mu\right)^{2}}{2 \sigma^{2}} \cr
&=-\frac{N}{2} \log 2 \pi-\frac{N}{2} \log \sigma^{2}-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{N}\left(x_{i}-\mu\right)^{2}
\end{aligned}
$$</p><p>求偏导数，得到参数估计：</p><p>$$
\begin{aligned}
\frac{\partial \log p(X)}{\partial \mu} &=\frac{1}{\sigma^{2}} \sum_{i=1}^{N}\left(x_{i}-\mu\right)=0 \cr
& \Rightarrow \mu=\frac{1}{N} \sum_{i=1}^{N} x_{i} \cr
\frac{\partial \log p(X)}{\partial \sigma^{2}} &=-\frac{N}{2 \sigma^{2}}+\frac{1}{2 \sigma^{4}} \sum_{i=1}^{N}\left(x_{i}-\mu\right)^{2}=0 \cr
& \Rightarrow \sigma^{2}=\frac{1}{N} \sum_{i=1}^{N}\left(x_{i}-\mu\right)^{2}
\end{aligned}
$$</p><h2 id=3-gaussian-mixture-model>3. Gaussian Mixture Model</h2><p>如果有K个高斯线性叠加:</p><p>$$
\begin{aligned}
p(x)=& \sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(x | \mu_{k}, \Sigma_{k}\right) \cr
& \text { s.t. } \sum_{k=1}^{K} \pi_{k}=1 \cr
& 0 \leq \pi_{k} \leq 1
\end{aligned}
$$</p><p>那么对数似然函数为</p><p>$$
\log p(X)=\sum_{i=1}^{N} \log \lbrace \sum_{k=1}^{K} \pi_{k} \mathcal{N} (x_{i} | \mu_{k}, \Sigma_{k}) \rbrace
$$</p><p>因为对数里有求和，因此无法无法直接通过最大似然估计方法进行参数估计。</p><p>其中，如果$\pi_{k}$是每个高斯出现的概率$p(k)$，则高斯混合模型分解为以$p(k)$获得一个高斯分布，然后在分布中获得$x$，因此$x$边缘概率分布为：</p><p>$$
p(x)=\sum_{k=1}^{K} p(k) p(x | k)
$$</p><p>后验概率$p(k\vert x)$表示$x$属于每个高斯的概率（离散值）:</p><p>$$
\begin{aligned}
p(k | x) &=\frac{p(x | k) p(k)}{\sum_{l} p(x | l) p(l)} \cr
&=\frac{\pi_{k} \mathcal{N}\left(x | \mu_{k}, \Sigma_{k}\right)}{\sum_{l} \pi_{l} \mathcal{N}\left(x | \mu_{l}, \Sigma_{l}\right)}
\end{aligned}
$$</p><h2 id=4-expectation-maximization>4. Expectation Maximization</h2><p>思想： 通过引入隐变量，运用迭代方法，求解混合高斯模型</p><p>$$
\theta^{(t+1)}=\underset{\theta}{\arg \max } \mathcal{L}(\theta ; X)
$$</p><p>引入隐变量Zi(状态i）， z服从多项分布，选择zi的概率为p(zi),则高斯混合模型为：</p><p>$$
\begin{aligned}
z_{i} & \sim \operatorname{Multinoimal}\left(\pi_{1}, \cdots, \pi_{k}\right) \cr
x_{i} | z_{i} & \sim \mathcal{N}\left(\mu_{z_{i}}, \Sigma_{z_{i}}\right)
\end{aligned}
$$</p><p>步骤：</p><ul><li>E-Step: 在现有$\theta^{(t)}$下最大化似然下界, 计算隐变量$z$的期望$Q\left(z_{i}\right)=p\left(z_{i} \vert x_{i}, \theta\right)$ 作为其下界</li><li>M-Step: 在上面$Q(z_{i})$下计算参数列表$\theta$来最大化似然</li></ul><h3 id=0-理解em的前提>(0) 理解EM的前提</h3><p><strong>凹凸函数</strong>:<br>$\forall_{x \in \mathbb{R}}, f^{\prime \prime}(x) \geq 0$,则$f$为凸函数。<br>当$x$为向量，如果其hessian矩阵 $H$ 是半正定的($H \geq 0$),则$f$为凸函数<br>如果$f^{\prime \prime}(x)>0$或者$H>0$, $f$是严格凸函数。<br>如果$f^{\prime \prime}(x)&lt;0$或者$H>0$, $f$是凹函数。</p><p><strong>Jensen 不等式</strong>:</p><ol><li>如果$f$为凸函数, 则$E[f(X)] \geq f(E [ X ])$。当且仅当$x$是常数时，$E[f(x)]=f(E[ x ])$。</li><li>如果$f$是凹函数, 则$E[f(X)] \leq f(E[ X ])$。</li></ol><p>引入隐变量后，变换<strong>对数似然函数</strong>:</p><p>$$
\begin{aligned}
\mathcal{L}(\theta ; X) &=\sum_{i=1}^{N} \log p\left(x_{i} | \theta\right) \cr
&=\sum_{i=1}^{N} \log \sum_{z_{i}} p\left(x_{i}, z_{i} | \theta\right) \cr
&=\sum_{i=1}^{N} \log \sum_{z_{i}} Q\left(z_{i}\right) \frac{p\left(x_{i}, z_{i} | \theta\right)}{Q\left(z_{i}\right)} \cr
& \geq \sum_{i=1}^{N} \sum_{z_{i}} Q\left(z_{i}\right) \log \frac{p\left(x_{i}, z_{i} | \theta\right)}{Q\left(z_{i}\right)}
\end{aligned}
$$</p><p><strong>推导</strong>:</p><ol><li>把式中的log函数体看成是一个整体，由于$\log (x)$的二阶导数为$-\frac{1}{x^2}$, 小于0，为凹函数。所以使用Jensen不等式时，应用第二条准则：$f(E [ X ] ) \geq E[f(x)]$。</li></ol><p>$$
f\left(E_{z_{i} \sim Q}\left[\frac{p\left(x_{i}, z_{i} | \theta\right)}{Q\left(z_{i}\right)}\right]\right) \geq E_{z_{i} \sim Q}\left[f\left(\frac{p\left(x_{i}, z_{i} | \theta\right)}{Q\left(z_{i}\right)}\right)\right]
$$</p><ol start=2><li><p>这里，$Q\left(z_{i}\right)$是$z_{i}$的函数， 且$\sum_{z_{i}} Q\left(z_{i}\right)=1$。</p></li><li><p>由数学期望$E_{x \sim p}[g(X)]=\sum_{x} g(x) p(x)$，上式可以理解为: $p(x)$对应$Q\left(z_{i}\right)$, g(x)对应$\log \frac{p\left(x_{i}, z_{i} \vert \theta\right)}{Q\left(z_{i}\right)}$表示$z_{i}$的函数。</p></li><li><p>似然函数: $\mathcal{L}(\theta) \geq \mathcal{J}(z,Q)$（$z$为隐含变量），那么我们可以通过不断的最大化$\mathcal{J}$的下界，来使得$\mathcal{L}(\theta)$不断提高，最终达到它的最大值。</p></li></ol><p>最大化$\mathcal{L}(\theta)$函数的下界，即让$g(x)$为常数c:</p><p>$$
\frac{p\left(x_{i}, z_{i} | \theta\right)}{Q\left(z_{i}\right)}=c
$$</p><p>Jensen不等式中说到，当自变量$X=E(X)$时，即为常数的时候，等式成立!</p><p>变换公式, 对所有$z$求和得:</p><p>$$
\begin{aligned}
p\left(x_{i}, z_{i} | \theta\right) &=c \cdot Q\left(z_{i}\right) \cr
\sum_{z_{i}} p\left(x_{i}, z_{i} | \theta\right) &=c \cdot \sum_{z_{i}} Q\left(z_{i}\right) \cr
c &=\sum_{z_{i}} p\left(x_{i}, z_{i} | \theta\right)
\end{aligned}
$$</p><p>其中，$\sum_{z_{i}} Q\left(z_{i}\right) = 1$, 也得：</p><p>$$
\begin{aligned}
Q\left(z_{i}\right) &=\frac{p\left(x_{i}, z_{i} | \theta\right)}{\sum_{z_{i}} p\left(x_{i}, z_{i} | \theta\right)} \cr
&=p\left(z_{i} | x_{i}, \theta\right)
\end{aligned}
$$</p><p>至此，我们推出了在固定参数θ后，使下界拉升的$Q(z)$的计算公式就是后验概率（条件概率），一并解决了$Q(z)$如何选择的问题。此步就是EM算法的<code>E-step</code>。</p><p>执行<code>E-Step</code>后与下界重合，此时似然变为：</p><p>$$
\mathcal{L}\left(\theta^{(t)} ; X\right)=\sum_{i=1}^{N} \sum_{z_{i}} Q^{(t)}\left(z_{i}\right) \log \frac{p\left(x_{i}, z_{i} | \theta^{(t)}\right)}{Q^{(t)}\left(z_{i}\right)}
$$</p><p>这时，对公式求导</p><p>$$
\theta^{(t+1)}=\underset{\theta}{\arg \max } \mathcal{L}(\theta ; X)
$$</p><p>得到 $t+1$ 步的似然函数 $\mathcal{L}\left(\theta^{(t+1)} ; X\right)$。<br>通过不断的迭代，可以得到使似然函数$\mathcal{L}(\theta)$最大化的参数 $\theta$，直至函数收敛。<br>只需要证明$\mathcal{L}\left(\theta^{(t+1)} ; X\right) \geq \mathcal{L}\left(\theta^{(t)} ; X\right)$, 则可证明EM的收敛性:</p><p>$$
\begin{aligned}
\mathcal{L}\left(\theta^{(t+1)} ; X\right) &=\sum_{i=1}^{N} \log \sum_{z_{i}} Q^{(t)}\left(z_{i}\right) \frac{p\left(x_{i}, z_{i} | \theta^{(t+1)}\right)}{Q^{(t)}\left(z_{i}\right)} \cr
& \geq \sum_{i=1}^{N} \sum_{z_{i}} Q^{(t)}\left(z_{i}\right) \log \frac{p\left(x_{i}, z_{i} | \theta^{(t+1)}\right)}{Q^{(t)}\left(z_{i}\right)} \cr
& \geq \sum_{i=1}^{N} \sum_{z_{i}} Q^{(t)}\left(z_{i}\right) \log \frac{p\left(x_{i}, z_{i} | \theta^{(t)}\right)}{Q^{(t)}\left(z_{i}\right)} \cr
&=\mathcal{L}\left(\theta^{(t)} ; X\right)
\end{aligned}
$$</p><h2 id=5-求解gmm>5. 求解GMM</h2><h3 id=1-gmm-e-step>(1) GMM E-Step:</h3><p>已知$\theta^{(t)}$, 求$Q^{(t+1)}\left(z_{i}\right)$:</p><p>$$
\begin{aligned}
Q^{(t+1)}\left(z_{i}\right) &=\frac{p\left(x_{i}, z_{i} | \theta^{(t)}\right)}{p\left(x_{i} | \theta^{(t)}\right)} \cr
&=\frac{p\left(x_{i}, z_{i} | \theta^{(t)}\right)}{\sum_{l \in z_{i}} p\left(x_{i}, l | \theta^{(t)}\right)} \cr
&=\frac{p\left(x_{i} | z_{i}, \theta^{(t)}\right) p\left(z_{i} | \theta^{(t)}\right)}{\sum_{l \in z_{i}} p\left(x_{i} | l, \theta^{(t)}\right) p\left(l | \theta^{(t)}\right)} \cr
&=\frac{\mathcal{N}\left(\mu_{z_{i}}, \Sigma_{z_{i}}\right) \pi_{z_{i}}}{\sum_{l \in z_{i}} \mathcal{N}\left(\mu_{l}, \Sigma_{l}\right) \pi_{l}}
\end{aligned}
$$</p><h3 id=2-gmm-m-step>(2) GMM M-Step:</h3><p>已知$Q^{(t+1)}\left(z_{i}\right)$, 求 $\theta^{(t+1)}$:</p><p>$$
\begin{aligned}
\mathcal{L}(\theta ; X) &=\sum_{i}^{N} \sum_{l}^{K} Q_{i}(l) \log \frac{p\left(x_{i}, l | \theta\right)}{Q_{i}(l)} \cr
&=\sum_{i}^{N} \sum_{l}^{K} Q_{i}(l) \log p\left(x_{i}, l | \theta\right)-\sum_{i}^{N} \sum_{l}^{K} Q_{i}(l) \log Q_{i}(l) \cr
&=\sum_{i}^{N} \sum_{l}^{K} Q_{i}(l) \log p\left(x_{i}, l | \theta\right)-\text {Constant } \cr
&=\sum_{i}^{N} \sum_{l}^{K} Q_{i}(l) \log \pi_{l} \mathcal{N}\left(\mu_{l}, \Sigma_{l}\right)-\text {Constant } \cr
&=\sum_{i}^{N} \sum_{l}^{K} Q_{i}(l) \log \pi_{l}+\sum_{i}^{N} \sum_{l}^{K} Q_{i}(l) \log \mathcal{N}\left(\mu_{l}, \Sigma_{l}\right)-\text {Constant}
\end{aligned}
$$</p><h3 id=3-求-pi>(3) 求 $\pi$:</h3><p>令 $\forall_{l \in{1, \cdots, K}}$</p><p>$$
\begin{aligned}
\frac{\partial \mathcal{L}(\theta ; X)}{\partial \pi_{l}} &=0 \cr
\text { s.t. } \sum_{l}^{K} \pi_{l} &= 1
\end{aligned}
$$</p><p>拉格朗日乘法约束</p><p>$$
\begin{cases}\begin{aligned}
L_{\pi_{l}} &=\frac{\partial \mathcal{L}(\theta ; X)}{\partial \pi_{l}}+\lambda(\sum_{l}^{K} \pi_{l}-1)=0 \cr
L_{\lambda} &=\sum_{l}^{K} \pi_{l}-1=0
\end{aligned}\end{cases}
$$</p><p>求导：</p><p>$$
\begin{cases}\begin{array}{c}
\frac{1}{\pi_{1}} \sum_{i}^{N} Q_{i}(1)-\lambda=0 \cr
\vdots \cr
\frac{1}{\pi_{l}} \sum_{i}^{N} Q_{i}(l)-\lambda=0
\end{array}\end{cases}
$$</p><p>相加得：</p><p>$$
\sum_{l}^{K} \sum_{i}^{N} Q_{i}(l)=\lambda \sum_{l}^{K} \pi_{l}=\lambda
$$</p><p>由 $Q_{i}(l)=p\left(l \vert x_{i}, \theta\right)$, 得</p><p>$$
\begin{aligned}
\sum_{l}^{K} \sum_{i}^{N} Q_{i}(l) &=\sum_{i}^{N} \sum_{l}^{K} Q_{i}(l) \cr
&=\sum_{i}^{N} \sum_{l}^{K} p\left(l | x_{i}, \theta\right) \cr
&=\sum_{i}^{N} 1 \cr
&=N
\end{aligned}
$$</p><p>则</p><p>$$
\begin{aligned}
\pi_{l} &=\frac{1}{\lambda} \sum_{i}^{N} Q_{i}(l) \cr
&=\frac{1}{N} \sum_{i}^{N} Q_{i}(l) \cr
&=\frac{1}{N} \sum_{i}^{N} p\left(l | x_{i}, \theta\right)
\end{aligned}
$$</p><h3 id=4-计算mu>(4) 计算$\mu$</h3><p>$$
\begin{aligned}
&\sum_{i}^{N} \sum_{l}^{K} Q_{i}(l) \log \mathcal{N}\left(\mu_{l}, \Sigma_{l}\right)\cr
&=\sum_{i}^{N} \sum_{l}^{K} Q_{i}(l) \log \frac{1}{\sqrt{2 \pi} \sigma_{l}} e^{-\frac{\left(x_{i}-\mu_{l}\right)^{2}}{2 \sigma_{l}^{2}}}\cr
&=\sum_{i}^{N} \sum_{l}^{K} Q_{i}(l) \lbrace -\frac{1}{2} \log 2 \pi-\frac{1}{2} \log \sigma_{l}^{2}-\frac{\left(x_{i}-\mu_{l}\right)^{2}}{2 \sigma_{l}^{2}}\rbrace
\end{aligned}
$$</p><p>求偏导：</p><p>$$
\begin{aligned}
\frac{\partial \mathcal{L}(\theta ; X)}{\partial \mu_{l}} &=\sum_{i}^{N} Q_{i}(l) \frac{x_{i}-\mu_{l}}{\sigma^{2}} \cr
&=0
\end{aligned}
$$</p><p>得$\mu$：</p><p>$$
\mu_{l}=\frac{\sum_{i}^{N} Q_{i}(l) x_{i}}{\sum_{i}^{N} Q_{i}(l)}
$$</p><h3 id=5-计算sigma>(5) 计算$\sigma$</h3><p>$$
\begin{aligned}
\frac{\partial \mathcal{L}(\theta ; X)}{\partial \sigma_{l}^{2}} &=\sum_{i}^{N} Q_{i}(l) \bigg\lbrace -\frac{1}{2 \sigma_{l}^{2}}+\frac{\left(x_{i}-\mu_{l}\right)^{2}}{2 \sigma_{l}^{4}} \bigg\rbrace \cr
&=0
\end{aligned}
$$</p><p>得到</p><p>$$
\sigma_{l}=\frac{\sum_{i}^{N} Q_{i}(l)\left(x_{i}-\mu_{l}\right)^{2}}{\sum_{i}^{N} Q_{i}(l)}
$$</p><h2 id=6-从kl散度角度解释em>6 从KL散度角度解释EM</h2><p>$$
\begin{aligned}
K L(q | p) &=\sum_{z} q(z) \log \frac{q(z)}{p(z | x, \theta)} \cr
&=\sum_{z} q(z) \log \frac{q(z) p(x | \theta)}{p(z, x | \theta)} \cr
&=-\sum_{z} q(z) \log \frac{p(z, x | \theta)}{q(z)}+\sum_{z} q(z) \log p(x | \theta) \cr
&=-\sum_{z} q(z) \log \frac{p(z, x | \theta)}{q(z)}+\log p(x | \theta) \sum_{z} q(z) \cr
&=-\sum_{z} q(z) \log \frac{p(z, x | \theta)}{q(z)}+\log p(x | \theta) \cr
\log p(x | \theta) &=K L(q | p)+\sum_{z} q(z) \log \frac{p(z, x | \theta)}{q(z)} \cr
&=K L(q | p)+\mathcal{L}(q, \theta)
\end{aligned}
$$</p><p>参考：</p><p><a href="https://www.youtube.com/watch?v=Bq5s80ZCmC0&list=PLyAft-JyjIYpno8IfZZS0mnxD5TYZ6BIc" target=_blank rel="noopener noreffer">徐亦达-机器学习-EM</a></p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2020-01-26</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://zqfang.github.io/2020-01-26-ml-em/ data-title="Expectation Maximization" data-hashtags="Expectation Maximization,Statistical Learning"><i class="fab fa-twitter fa-fw"></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://zqfang.github.io/2020-01-26-ml-em/ data-hashtag="Expectation Maximization"><i class="fab fa-facebook-square fa-fw"></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://zqfang.github.io/2020-01-26-ml-em/ data-title="Expectation Maximization"><i class="fab fa-hacker-news fa-fw"></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://zqfang.github.io/2020-01-26-ml-em/ data-title="Expectation Maximization"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@2.14.0/icons/line.svg></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://zqfang.github.io/2020-01-26-ml-em/ data-title="Expectation Maximization"><i class="fab fa-weibo fa-fw"></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/expectation-maximization/>Expectation Maximization</a>,&nbsp;<a href=/tags/statistical-learning/>Statistical Learning</a></section><section><span><a href=javascript:void(0); onclick=window.history.back();>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/2020-01-20-begin/ class=prev rel=prev title="Cheatsheet for command line"><i class="fas fa-angle-left fa-fw"></i>Cheatsheet for command line</a>
<a href=/2020-01-26-ml-gradient/ class=next rel=next title="Derivative, Gradient, Jacobian, Hessian, Laplacian">Derivative, Gradient, Jacobian, Hessian, Laplacian<i class="fas fa-angle-right fa-fw"></i></a></div></div><div id=comments><div id=disqus_thread class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://disqus.com/?ref_noscript>Disqus</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2020 - 2021</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank>zqfang</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw"></i></a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw"></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.css><script type=text/javascript src=https://bioninja-1.disqus.com/embed.js defer></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/smooth-scroll@16.1.3/dist/smooth-scroll.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.37.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lunr@2.3.8/lunr.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.2.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.4.0/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":50},"comment":{},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"lunr"}};</script><script type=text/javascript src=/js/theme.min.f51938f3065a40ee841bcb558e4330e31fd26c0ea55343fff8770b88b0319a3c.js integrity="sha256-9Rk48wZaQO6EG8tVjkMw4x/SbA6lU0P/+HcLiLAxmjw="></script></body></html>