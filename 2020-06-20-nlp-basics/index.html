<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><meta http-equiv=x-ua-compatible content="IE=edge, chrome=1"><title>NLP: A short guide for biologist - My Computational Genomic Playground</title><meta name=Description content="My Computational Genomic Playground"><meta property="og:title" content="NLP: A short guide for biologist"><meta property="og:description" content="NLP Basics for the newbies like me
Languwage model Models that assigns probabilities to sequences of words are called languwage models.
Count-based Representation 1. one-hot representation 2. BoW: Bag of words Blow describes the occurrence of words within a document. including
 A Vocabulary of known words A measure of the presence of known words, e."><meta property="og:type" content="article"><meta property="og:url" content="https://zqfang.github.io/2020-06-20-nlp-basics/"><meta property="og:image" content="https://zqfang.github.io/logo.png"><meta property="article:published_time" content="2020-06-20T00:00:00+00:00"><meta property="article:modified_time" content="2020-06-20T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://zqfang.github.io/logo.png"><meta name=twitter:title content="NLP: A short guide for biologist"><meta name=twitter:description content="NLP Basics for the newbies like me
Languwage model Models that assigns probabilities to sequences of words are called languwage models.
Count-based Representation 1. one-hot representation 2. BoW: Bag of words Blow describes the occurrence of words within a document. including
 A Vocabulary of known words A measure of the presence of known words, e."><meta name=application-name content="Pleiades"><meta name=apple-mobile-web-app-title content="Pleiades"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://zqfang.github.io/2020-06-20-nlp-basics/><link rel=prev href=https://zqfang.github.io/2020-06-14-stats-effectsize/><link rel=next href=https://zqfang.github.io/2020-06-22-stats-survival-analysis/><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/normalize.css@8.0.1/normalize.min.css><link rel=stylesheet href=/css/style.min.1e2694bed152fa2922dbe909a441838ed693d88b1330f97485bfa8ed78da42df.css integrity="sha256-HiaUvtFS+iki2+kJpEGDjtaT2IsTMPl0hb+o7XjaQt8="><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@3.7.2/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"NLP: A short guide for biologist","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/zqfang.github.io\/2020-06-20-nlp-basics\/"},"image":["https:\/\/zqfang.github.io\/images\/Apple-Devices-Preview.png"],"genre":"posts","keywords":"Deep Learning, NLP","wordcount":1129,"url":"https:\/\/zqfang.github.io\/2020-06-20-nlp-basics\/","datePublished":"2020-06-20T00:00:00+00:00","dateModified":"2020-06-20T00:00:00+00:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Organization","name":"xxxx","logo":"https:\/\/zqfang.github.io\/images\/avatar.png"},"author":{"@type":"Person","name":"zqfang"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem('theme')?localStorage.getItem('theme')==='dark':('auto'==='auto'?window.matchMedia('(prefers-color-scheme: dark)').matches:'auto'==='dark'))&&document.body.setAttribute('theme','dark');</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="My Computational Genomic Playground"><span class=header-title-pre><span>&#8711;</span></span>Pleiades <span class=header-title-post><i class="fas fa-terminal"></i></span></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=/publication/>Publication </a><a class=menu-item href=/portfolio/>Portfolio </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/about/ title=About>About </a><a class=menu-item href=https://github.com/zqfang title=GitHub rel="noopener noreffer" target=_blank><i class="fab fa-github fa-fw"></i></a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop><input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw"></i></a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw"></i></a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin"></i></span></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="My Computational Genomic Playground"><span class=header-title-pre><span>&#8711;</span></span>Pleiades <span class=header-title-post><i class="fas fa-terminal"></i></span></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw"></i></a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw"></i></a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin"></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/posts/>Posts</a><a class=menu-item href=/categories/>Categories</a><a class=menu-item href=/publication/>Publication</a><a class=menu-item href=/portfolio/>Portfolio</a><a class=menu-item href=/tags/>Tags</a><a class=menu-item href=/about/ title=About>About</a><a class=menu-item href=https://github.com/zqfang title=GitHub rel="noopener noreffer" target=_blank><i class="fab fa-github fa-fw"></i></a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animated flipInX">NLP: A short guide for biologist</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw"></i>zqfang</a></span>&nbsp;<span class=post-category>included in <a href=/categories/nature-language-processing/><i class="far fa-folder fa-fw"></i>Nature Language Processing</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2020-06-20>2020-06-20</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;1129 words&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;6 minutes&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#languwage-model>Languwage model</a></li><li><a href=#count-based-representation>Count-based Representation</a><ul><li><a href=#1-one-hot-representation>1. one-hot representation</a></li><li><a href=#2-bow-bag-of-words>2. BoW: Bag of words</a></li><li><a href=#3-tf-or-tf-idf-representation-term-frequency-inverse-document-frequency>3. TF or TF-IDF representation: Term Frequency Inverse Document Frequency</a></li><li><a href=#4-positive-pointwise-mutual-information-ppmi>4. Positive pointwise Mutual Information (PPMI)</a></li></ul></li><li><a href=#word-embedding>Word Embedding</a><ul><li><a href=#word2vec>word2vec</a></li><li><a href=#glove>GloVe</a></li></ul></li><li><a href=#concepts>Concepts</a><ul><li><a href=#corpora-tokens-and-types>Corpora, Tokens, and Types</a></li><li><a href=#unigram-bigrams-trigrams---n-grams>Unigram, Bigrams, Trigrams, &mldr; , N-grams</a></li><li><a href=#lemmas-and-stems>Lemmas and Stems</a></li><li><a href=#word-senses-and-semantics>Word Senses and Semantics</a></li><li><a href=#categorizing-words-pos-tagging>Categorizing Words: POS Tagging</a></li><li><a href=#categorizing-spans-chunking-and-named-entity-recognition>Categorizing Spans: Chunking and Named Entity Recognition</a></li><li><a href=#coreference>Coreference</a></li><li><a href=#minimumn-edit-distance>Minimumn Edit distance</a></li><li><a href=#perplexity>Perplexity</a></li><li><a href=#entropy>Entropy</a></li><li><a href=#cross-entropy>Cross-entropy</a></li><li><a href=#the-relation-of-perplexity-and-cross-entropy>The relation of perplexity and cross-entropy</a></li></ul></li><li><a href=#statistical-testing>statistical testing</a></li></ul></nav></div></div><div class=content id=content><p>NLP Basics for the newbies like me</p><h2 id=languwage-model>Languwage model</h2><p>Models that <strong>assigns probabilities to sequences of words</strong> are called languwage models.</p><h2 id=count-based-representation>Count-based Representation</h2><h3 id=1-one-hot-representation>1. one-hot representation</h3><h3 id=2-bow-bag-of-words>2. BoW: Bag of words</h3><p>Blow describes the <code>occurrence of words</code> within a document. including</p><ul><li>A Vocabulary of known words</li><li>A measure of the presence of known words, e.g. count</li></ul><h3 id=3-tf-or-tf-idf-representation-term-frequency-inverse-document-frequency>3. TF or TF-IDF representation: Term Frequency Inverse Document Frequency</h3><ul><li>TF: the sum of the one-hot representation of a phrase, sentence or document&rsquo;s constituent words</li></ul><p>$$
TF (w) = \frac { \text{ Number of the term w appears in the document }} { \text{Number of terms in the document}}
$$</p><ul><li>IDF: penalizes common tokens and rewards rare tokens</li></ul><p>$$
IDF(w) = \log \frac{\text{Number of documents}}{\text{Number of documents with term w}}
$$</p><ul><li>TF-IDF: $TF(w) \times IDF(w)$</li></ul><h3 id=4-positive-pointwise-mutual-information-ppmi>4. Positive pointwise Mutual Information (PPMI)</h3><p>An alternative weighting function to tf-idf</p><p>PPMI is a measure of how often two events x and y occur, compared with what we would expect if they were independent:</p><p>$$
I(x,y) = \log_2 \frac{P(x, y)}{{P(x)}{P(y)}}
$$</p><p>The pointwise mutual information between a target word w and a context word c is then defined as:</p><p>$$
PMI(w,c) = \log_2 \frac{P(w, c)}{{P(w)}{P(c)}}
$$</p><p>PMI is a useful tool whenever we need to find words that are strongly associated.</p><p><code>PPMI</code> replaces all negative PMI values with zeros:</p><p>$$
\operatorname{PPMI}(w,c) = \max (\log_2 \frac{P(w, c)}{{P(w)}{P(c)}}, 0 )
$$</p><p>However, <strong>PMI has the problem</strong>: very rare words tend to have very high PMI values. So, use a different function $P_{\alpha}(c)$ that raise the probability of the context word to the power of $\alpha$:</p><p>$$
\operatorname{PPMI_{\alpha}}(w,c) = \max (\log_2 \frac{P(w, c)}{{P(w)}{P_{\alpha}(c)}}, 0 )
$$</p><h2 id=word-embedding>Word Embedding</h2><h3 id=word2vec>word2vec</h3><p>The intuition here is taht we could just use running text as implicitly supervised training data for such a classifer: a word $s$ that occurs near the target word $apricot$ acts as gold &lsquo;correct answer&rsquo; to the question &ldquo;Is word $w$ likely to show up near $apricot$?&rdquo;</p><p>This advoids the need for any sort of hand-labeled superivsion signal.</p><p>Alogrithm:</p><ul><li><code>Skip-gram with negative sampling</code>, aslo called SGNS</li><li>Skip-gram trains a probabilistic classifier that given a test targe word $t$ and its context window of $k$ workds $c_{1:k}$, assigns a probability based on how similar this contxt window is to the target word.</li></ul><p>$$
\begin{aligned}
P\left(+\mid t, c_{1: k}\right) &=\prod_{i=1}^{k} \frac{1}{1+e^{-t \cdot c_{i}}} \cr
\log P\left(+\mid t, c_{1: k}\right) &=\sum_{i=1}^{k} \log \frac{1}{1+e^{-t \cdot c_{i}}}
\end{aligned}
$$</p><ul><li>Note: similarity between embeddings (Cosine)</li></ul><p>$$
Similarity(t,c) \approx t \cdot c
$$</p><ul><li>Skip-gram makes the strong assumption that all contxt words are independent</li></ul><p>The intuition of skip-gram is:</p><ol><li>Treat the target word and a neighboring context word as positive examples;</li><li>Randomly sample other words in the lexicon to get negative samles;</li><li>Use logistic regression to train a classifer to distinguid those two case;</li><li>use the regression weights as the embeddings.</li></ol><h3 id=glove>GloVe</h3><h2 id=concepts>Concepts</h2><h3 id=corpora-tokens-and-types>Corpora, Tokens, and Types</h3><ol><li>corpus (plural: corpora): a text dataset</li><li>tokens (English): words and numeric sequences separated by white-spaces characters or punctuation</li><li>instance or data point: the text along with its metatdata</li><li>dataset: a collection of instances</li><li>types: unique tokens present in a corpus.</li><li>vocabulary or lexicon: the set of all types in a corpus</li></ol><p><img class=lazyload src=/svg/loading.min.svg data-src=/images/nlp/corpus.png data-srcset="/images/nlp/corpus.png, /images/nlp/corpus.png 1.5x, /images/nlp/corpus.png 2x" data-sizes=auto alt=/images/nlp/corpus.png title=corpus></p><h3 id=unigram-bigrams-trigrams---n-grams>Unigram, Bigrams, Trigrams, &mldr; , N-grams</h3><p>N-grams are fixed-length consecutive token sequence occurring in the text</p><h3 id=lemmas-and-stems>Lemmas and Stems</h3><p><code>Lemmas</code> are the root forms of words. e.g. the root form of the word fly, can be inflected into other words &ndash; flow, flew, flies, flown, flowing &mldr;</p><p>Lemmas, also called <code>citation form</code>.</p><p>Stemming: use handcrafted rules to strip endings of words to reduce them to a common form called <code>stems</code></p><h3 id=word-senses-and-semantics>Word Senses and Semantics</h3><p>Senses: the different meanings of a word</p><h3 id=categorizing-words-pos-tagging>Categorizing Words: POS Tagging</h3><p>Part-of-speech (POS): also known as word classes, or syntactic categories</p><p>POS divided into two broad supercateogries:</p><ul><li><code>closed class</code> types: <code>function words</code> <em>like of, it, and</em>, or <em>you</em></li><li><code>open class</code> types: nouns, verbs, adjectives, adverbs</li></ul><p>POS tagging: labeling individual words or tokens</p><p>Common algorithms to do tagging</p><ul><li>HMM: Hidden Markov Models</li><li>MEMM: maximum Entropy Markov Models</li></ul><h3 id=categorizing-spans-chunking-and-named-entity-recognition>Categorizing Spans: Chunking and Named Entity Recognition</h3><p>a span of text: a contiguous multi-token boundary.</p><p><code>chunking</code> or <code>shallow parsing</code>: identify the noun phrases (NP) and verb pharses (VP) in a span of text.</p><p>A <code>named entity</code> is a string mention of a real-world concept like a person, location, organization, drug name, et. al.</p><h3 id=coreference>Coreference</h3><p>The task of deciding whether two strings refer to same entity</p><h3 id=minimumn-edit-distance>Minimumn Edit distance</h3><p>A way to quantify string similarity.</p><h3 id=perplexity>Perplexity</h3><p>The <strong>perplexity</strong> (sometimes called PP) of a language model on a test set is the inverse probability of the test set, normalised by the number of words.</p><p>$$\begin{aligned}
\mathrm{PP}(W) &=P(w_{1} w_{2} \ldots w_{N})^{-\frac{1}{N}} \cr &=\sqrt[N]{\frac{1}{P(w_{1} w_{2} \ldots w_{N})}} \cr
&= \sqrt[N]{\prod_{i=1}^{N} \frac{1}{P(w_{i} \mid w_{1} \ldots w_{i-1})}}
\end{aligned}
$$</p><p>Another way to hink about perplexity: as <strong>the weighted average branching factor</strong> of a language.</p><h3 id=entropy>Entropy</h3><p>Entropy is a measure of information. The entropy of the random variable X is:</p><p>$$
H(X)=-\sum_{x \in \chi} p(x) \log _{2} p(x)
$$</p><p>the log can be computed in any base. If we use log base 2, the resulting value of entropy will mesured in <strong>bits</strong>.</p><p>One intuitive way to think about entorpy is as a <strong>lower bound</strong> on the number of bits it would take to encode a certain desision or piece of information in the optimal coding scheme.</p><h3 id=cross-entropy>Cross-entropy</h3><p>The cross-entropy is useful when we don&rsquo;t know the actual probability distribution p that generated some data.</p><p>The cross-entropy of m (a model of p) on p is defined by</p><p>$$
H(p,m) = \lim_{n \rightarrow \infty} - \frac{1}{n}\sum_{W \in L} p (w_1,\cdots,w_n) \log m (w_1, cdots, w_n)
$$</p><p>the cross-entropy $H(p,m)$ is an upper bound on the entropy $H(p)$. For any model m:</p><p>$$
H(p) \leq H(p,m)
$$</p><p>The more accurate m is, the closer the cross-entropy $H(p,m)$ will be to the true entropy $H(p)$.</p><h3 id=the-relation-of-perplexity-and-cross-entropy>The relation of perplexity and cross-entropy</h3><p>The approximation to the cross-entropy of a model $M = P(w_i | W_{i-N+1} \cdots W_{i-1})$ on a sequence of words W is</p><p>$$
H(W) = - \frac{1}{N} \log P(w_1 w_2 \cdots w_N)
$$</p><p>The perplexity of a model P on a seqence of words W is defined as exp of this cross-entropy</p><p>$$
\begin{aligned}
\operatorname{Perplexity}(W) &=2^{H(W)} \cr
&=P\left(w_{1} w_{2} \ldots w_{N}\right)^{-\frac{1}{N}} \cr
&=\sqrt[N]{\frac{1}{P\left(w_{1} w_{2} \ldots w_{N}\right)}} \cr
&=\sqrt[N]{\prod_{i=1}^{N} \frac{1}{P\left(w_{i} \mid w_{1} \ldots w_{i-1}\right)}}
\end{aligned}
$$</p><h2 id=statistical-testing>statistical testing</h2><p>The approach to computing p-values(x) in NLP is to use non-parametric tests. e.g.</p><ul><li>bootstrap test</li><li>approximate randomization</li></ul><p><strong>bootstrapping</strong> refers to repeated drawing large numbers of smaller samples with replacement from an orignial larger sample.</p><ul><li>the intuition of the bootstrap test is that we can create many virtual test sets from an observed test set by repeated sampling from it.</li><li>the method only maks the assumption that sample is representative of the population</li></ul></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2020-06-20</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://zqfang.github.io/2020-06-20-nlp-basics/ data-title="NLP: A short guide for biologist" data-hashtags="Deep Learning,NLP"><i class="fab fa-twitter fa-fw"></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://zqfang.github.io/2020-06-20-nlp-basics/ data-hashtag="Deep Learning"><i class="fab fa-facebook-square fa-fw"></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://zqfang.github.io/2020-06-20-nlp-basics/ data-title="NLP: A short guide for biologist"><i class="fab fa-hacker-news fa-fw"></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://zqfang.github.io/2020-06-20-nlp-basics/ data-title="NLP: A short guide for biologist"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@2.14.0/icons/line.svg></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://zqfang.github.io/2020-06-20-nlp-basics/ data-title="NLP: A short guide for biologist"><i class="fab fa-weibo fa-fw"></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/deep-learning/>Deep Learning</a>,&nbsp;<a href=/tags/nlp/>NLP</a></section><section><span><a href=javascript:void(0); onclick=window.history.back();>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/2020-06-14-stats-effectsize/ class=prev rel=prev title="Power, Effect size, Sample size"><i class="fas fa-angle-left fa-fw"></i>Power, Effect size, Sample size</a>
<a href=/2020-06-22-stats-survival-analysis/ class=next rel=next title="Survival Analysis">Survival Analysis<i class="fas fa-angle-right fa-fw"></i></a></div></div><div id=comments><div id=disqus_thread class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://disqus.com/?ref_noscript>Disqus</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2020</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank>zqfang</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw"></i></a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw"></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.css><script type=text/javascript src=https://bioninja-1.disqus.com/embed.js defer></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/smooth-scroll@16.1.3/dist/smooth-scroll.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.37.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lunr@2.3.8/lunr.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.2.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.4.0/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":50},"comment":{},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"lunr"}};</script><script type=text/javascript src=/js/theme.min.f51938f3065a40ee841bcb558e4330e31fd26c0ea55343fff8770b88b0319a3c.js integrity="sha256-9Rk48wZaQO6EG8tVjkMw4x/SbA6lU0P/+HcLiLAxmjw="></script></body></html>