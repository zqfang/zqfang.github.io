<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><meta http-equiv=x-ua-compatible content="IE=edge, chrome=1"><title>NLP: RNN and Transformers - My Computational Genomic Playground</title><meta name=Description content="My Computational Genomic Playground"><meta property="og:title" content="NLP: RNN and Transformers"><meta property="og:description" content="Backpropagation Through Time Long Short-Term Memory Delete information from the context that is no longer needed: Forget Gate f $$ f_t = \sigma (U_f h_{t-1} + W_f X_t) $$ $$ k_t = c_{t-1} \odot f_t $$ Compute the actual information we need to extract from the previous hidden stat and"><meta property="og:type" content="article"><meta property="og:url" content="https://zqfang.github.io/2020-07-31-nlp-rnn/"><meta property="og:image" content="https://zqfang.github.io/logo.png"><meta property="article:published_time" content="2020-07-31T00:00:00+00:00"><meta property="article:modified_time" content="2020-07-31T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://zqfang.github.io/logo.png"><meta name=twitter:title content="NLP: RNN and Transformers"><meta name=twitter:description content="Backpropagation Through Time Long Short-Term Memory Delete information from the context that is no longer needed: Forget Gate f $$ f_t = \sigma (U_f h_{t-1} + W_f X_t) $$ $$ k_t = c_{t-1} \odot f_t $$ Compute the actual information we need to extract from the previous hidden stat and"><meta name=application-name content="Pleiades"><meta name=apple-mobile-web-app-title content="Pleiades"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://zqfang.github.io/2020-07-31-nlp-rnn/><link rel=prev href=https://zqfang.github.io/2020-07-26-dl-gnn/><link rel=next href=https://zqfang.github.io/2020-08-01-nlp-constituency-and-depenency-parsing/><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/normalize.css@8.0.1/normalize.min.css><link rel=stylesheet href=/css/style.min.1e2694bed152fa2922dbe909a441838ed693d88b1330f97485bfa8ed78da42df.css integrity="sha256-HiaUvtFS+iki2+kJpEGDjtaT2IsTMPl0hb+o7XjaQt8="><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@3.7.2/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"NLP: RNN and Transformers","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/zqfang.github.io\/2020-07-31-nlp-rnn\/"},"image":["https:\/\/zqfang.github.io\/images\/Apple-Devices-Preview.png"],"genre":"posts","keywords":"Deep Learning, NLP","wordcount":1611,"url":"https:\/\/zqfang.github.io\/2020-07-31-nlp-rnn\/","datePublished":"2020-07-31T00:00:00+00:00","dateModified":"2020-07-31T00:00:00+00:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Organization","name":"xxxx","logo":"https:\/\/zqfang.github.io\/images\/avatar.png"},"author":{"@type":"Person","name":"zqfang"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem('theme')?localStorage.getItem('theme')==='dark':('auto'==='auto'?window.matchMedia('(prefers-color-scheme: dark)').matches:'auto'==='dark'))&&document.body.setAttribute('theme','dark');</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="My Computational Genomic Playground"><span class=header-title-pre><span>&#8711;</span></span>Pleiades <span class=header-title-post><i class="fas fa-terminal"></i></span></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=/publication/>Publication </a><a class=menu-item href=/portfolio/>Portfolio </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/about/ title=About>About </a><a class=menu-item href=https://github.com/zqfang title=GitHub rel="noopener noreffer" target=_blank><i class="fab fa-github fa-fw"></i></a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop><input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw"></i></a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw"></i></a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin"></i></span></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="My Computational Genomic Playground"><span class=header-title-pre><span>&#8711;</span></span>Pleiades <span class=header-title-post><i class="fas fa-terminal"></i></span></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw"></i></a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw"></i></a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin"></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/posts/>Posts</a><a class=menu-item href=/categories/>Categories</a><a class=menu-item href=/publication/>Publication</a><a class=menu-item href=/portfolio/>Portfolio</a><a class=menu-item href=/tags/>Tags</a><a class=menu-item href=/about/ title=About>About</a><a class=menu-item href=https://github.com/zqfang title=GitHub rel="noopener noreffer" target=_blank><i class="fab fa-github fa-fw"></i></a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animated flipInX">NLP: RNN and Transformers</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw"></i>zqfang</a></span>&nbsp;<span class=post-category>included in <a href=/categories/nature-language-processing/><i class="far fa-folder fa-fw"></i>Nature Language Processing</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2020-07-31>2020-07-31</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;1611 words&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;4 minutes&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#backpropagation-through-time>Backpropagation Through Time</a></li><li><a href=#long-short-term-memory>Long Short-Term Memory</a></li><li><a href=#gated-recurrent-units>Gated Recurrent Units</a><ul><li><a href=#example-text-classification>Example: Text Classification</a></li></ul></li><li><a href=#attention-mechanism>Attention mechanism</a></li><li><a href=#self-attention>Self-Attention</a></li><li><a href=#transformer>Transformer</a><ul><li><a href=#positoinal-encoding>Positoinal encoding</a></li><li><a href=#encoder>Encoder:</a></li><li><a href=#decoder>Decoder</a></li><li><a href=#positionwisefeedforwardlayer>PositionwiseFeedforwardLayer:</a></li></ul></li><li><a href=#reference>Reference</a></li></ul></nav></div></div><div class=content id=content><h2 id=backpropagation-through-time>Backpropagation Through Time</h2><h2 id=long-short-term-memory>Long Short-Term Memory</h2><ol><li>Delete information from the context that is no longer needed: <code>Forget Gate</code> f</li></ol><p>$$
f_t = \sigma (U_f h_{t-1} + W_f X_t)
$$</p><p>$$
k_t = c_{t-1} \odot f_t
$$</p><ol start=2><li>Compute the actual information we need to extract from the previous hidden stat and current inputs</li></ol><p>$$
g_t = \tanh (U_g h_{t-1} + W_g x_t)
$$</p><ol start=3><li>Select the information to add to the current context: <code>Add Gate</code> i</li></ol><p>$$
i_t = \sigma (U_i h_{t-1} + W_i X_t)
$$</p><p>$$
j_t = g_{t} \odot i_t
$$</p><ol start=4><li>Get new context vector</li></ol><p>$$
c_t = j_t + k_t
$$</p><ol start=5><li><code>Output Gate</code> o: decide what information is required for the current hiddent state (as opposed to what information need to be preseved for future decicions)</li></ol><p>$$
o_t = \sigma (U_o h_{t-1} + W_o x_t)
$$</p><p>$$
h_t = o_t \odot \tanh (c_t)
$$</p><h2 id=gated-recurrent-units>Gated Recurrent Units</h2><p>GRU ease the tranning burden by dispensing with the use of a separate context vector, and by reducing the number of gates to 2:</p><ol><li>a reset gate, $r$: decide which aspects of the previous hidden state are relevant to the current context and what can be ignored.</li></ol><p>$$
r_t = \sigma (U_r h_{t-1} + W_r x_t)
$$</p><p>Then computing an intermediate representation for the new hidden stat at time $t$</p><p>$$
\tilde h_t = \tanh (U(r_t \odot h_{t-1}) + Wx_t)
$$</p><ol start=2><li>an update gate, $z$: detemine which aspects of the new intermedicate representation will be used directly and which aspects of the previous stat need to be preseverd for future use</li></ol><p>$$
z_t = \sigma (U_z h_{t-1} + W_z x_t)
$$</p><p>$$
h_t = (1- z_t)h_{t-1} + z_t \tilde h_t
$$</p><h3 id=example-text-classification>Example: Text Classification</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>torch</span>
<span class=kn>import</span> <span class=nn>torch.nn</span> <span class=kn>as</span> <span class=nn>nn</span>

<span class=k>class</span> <span class=nc>RNN</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>,</span> <span class=n>num_classes</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>(</span><span class=n>RNN</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>num_layers</span> <span class=o>=</span> <span class=n>num_layers</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span> <span class=o>=</span> <span class=n>hidden_size</span>
        <span class=c1>## MARK: work with nn.RNN, nn.GRU, nn.LSTM</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>rnn</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>GRU</span><span class=p>(</span><span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>,</span> <span class=n>batch_first</span><span class=o>=</span><span class=bp>True</span><span class=p>)</span> 
        <span class=c1># Batch x Seq_len x embeding_size (input_size)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>num_classes</span><span class=p>)</span>
    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>inputs</span><span class=p>):</span>

        <span class=c1>## MARK: init hidden state (h0)</span>
        <span class=n>hidden</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>num_layers</span><span class=p>,</span> <span class=n>inputs</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>)</span>
        <span class=c1># if LSTM, need init cell state (c0)</span>
        <span class=c1># cell = torch.zeros(self.num_layers, inputs.size(0), self.hidden_size)</span>

        <span class=n>out</span><span class=p>,</span> <span class=n>hidden</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>rnn</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>hidden</span><span class=p>)</span> <span class=c1># out: B x S x hidden_size</span>
        <span class=c1># out, (hidden, cell) = self.rnn(inputs, (hidden, cell))</span>

        <span class=c1>## Mark: only need last output for sentence classification</span>
        <span class=n>out</span> <span class=o>=</span> <span class=n>out</span><span class=p>[:,</span><span class=o>-</span><span class=mi>1</span><span class=p>,:]</span> <span class=c1># out: B x hidden_size</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>out</span>
</code></pre></td></tr></table></div></div><h2 id=attention-mechanism>Attention mechanism</h2><p>Consider Encoder to Deconder Network, a decoder</p><p>$$
h_i^d = g(\hat y_{i-1}, h_{i-1}^d, c_i)
$$</p><ol><li>computing context vector $c_i$: a vector of scores that capture the relevance of each encoder hidden state to the decoder state captured in $h_{i-1}^d$. That&rsquo;s, at each state $i$ during decoding, we&rsquo;ll compute $score(h_{i-1}^d, h_j^e)$ for each encoder state $j$. Recall similarity</li></ol><p>$$
score(h_{i-1}^d, h_j^e) = h_{i-1}^d \cdot h_j^e
$$</p><ol start=2><li>make a more robust similarity score by adding a learnable weights, $W_s$:</li></ol><p>$$
score(h_{i-1}^d, h_j^e) = h_{i-1}^d W_s h_j^e
$$</p><ol start=3><li>normalize the scores</li></ol><p>$$
\begin{aligned}
\alpha_{ij} &= \operatorname{softmax} (score(h_{i-1}^d, h_j^e)) \cr
&= \frac {\exp (score(h_{i-1}^d, h_j^e))} {\sum_k score(h_{i-1}^d, h_j^e)}
\end{aligned}
$$</p><ol start=4><li>finally, give $\alpha$,</li></ol><p>$$
c_i = \sum_j \alpha_{ij}h_j^e
$$</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/images/nlp/encoder-decoder.png data-srcset="/images/nlp/encoder-decoder.png, /images/nlp/encoder-decoder.png 1.5x, /images/nlp/encoder-decoder.png 2x" data-sizes=auto alt=/images/nlp/encoder-decoder.png title=attention></p><h2 id=self-attention>Self-Attention</h2><ol><li><p>create 3 vectors from each of the encoders&rsquo; input vector, a <code>Query</code> vector, a <code>Key</code> vector and a <code>Value</code> vector, then multiplying the embedding (of word) <code>X</code></p><p><img class=lazyload src=/svg/loading.min.svg data-src=/images/nlp/self-attention-1.png data-srcset="/images/nlp/self-attention-1.png, /images/nlp/self-attention-1.png 1.5x, /images/nlp/self-attention-1.png 2x" data-sizes=auto alt=/images/nlp/self-attention-1.png title=attention1></p></li><li><p>calculate a score by taking the dot product of the <code>Query</code> with the <code>Key</code></p></li><li><p>divide the scores by the square root of the dimension of the key vector (a more stable gradients), then pass the result grought a softmax.</p></li><li><p>multiply each <code>Value</code> vector by the softmax score</p></li><li><p>sum up the weighted value vectors</p></li></ol><p><img class=lazyload src=/svg/loading.min.svg data-src=/images/nlp/self-attention-2.png data-srcset="/images/nlp/self-attention-2.png, /images/nlp/self-attention-2.png 1.5x, /images/nlp/self-attention-2.png 2x" data-sizes=auto alt=/images/nlp/self-attention-2.png title=attention2></p><ol start=6><li><strong>multi-head attention</strong>: to focus on different region and give &ldquo;representation subspace&rdquo;</li></ol><p><img class=lazyload src=/svg/loading.min.svg data-src=/images/nlp/self-attention-3.png data-srcset="/images/nlp/self-attention-3.png, /images/nlp/self-attention-3.png 1.5x, /images/nlp/self-attention-3.png 2x" data-sizes=auto alt=/images/nlp/self-attention-3.png title=multiheads></p><h2 id=transformer>Transformer</h2><p>A transformer of two stacked encoder and decoder looks like this</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/images/nlp/transformers.png data-srcset="/images/nlp/transformers.png, /images/nlp/transformers.png 1.5x, /images/nlp/transformers.png 2x" data-sizes=auto alt=/images/nlp/transformers.png title=transformer></p><h3 id=positoinal-encoding>Positoinal encoding</h3><p>Transformer use <code>positoinal encoding</code> vector to representing the order of the sequence. It follows a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence.</p><p>let $t$ be the position in an input sentence, $\overrightarrow{p_{t}} \in R^d$ be the encoding, $d$ be the encoding dimension, then</p><p>$$
\overrightarrow{p_{t}}^{(i)}=f(t)^{(i)}:= \begin{cases}
\sin (\omega_{k} \cdot t), & \text { if } i=2 k \cr
\cos (\omega_{k} \cdot t), & \text { if } i=2 k+1
\end{cases}
$$</p><p>where</p><p>$$
\omega_{k}=\frac{1}{10000^{2 k / d}}
$$</p><p>image that the positional embeding look like this:</p><p>$$
\overrightarrow{p_{t}}=\left[\begin{array}{c}
\sin \left(\omega_{1} \cdot t\right) \cr
\cos \left(\omega_{1} \cdot t\right) \cr
\sin \left(\omega_{2} \cdot t\right) \cr
\cos \left(\omega_{2} \cdot t\right) \cr
\vdots \cr
\sin \left(\omega_{d / 2} \cdot t\right) \cr
\cos \left(\omega_{d / 2} \cdot t\right)
\end{array}\right]_{d \times 1}
$$</p><p><strong>Word embeding + Positional encoding</strong>:<br>For every word $\omega_{t}$ in a sentence, calculating the correspondent embedding which is fed to the model is as follows:</p><p>$$
\psi^{\prime}\left(w_{t}\right)=\psi\left(w_{t}\right)+\overrightarrow{p_{t}}
$$</p><p>To make this summation possible, keep
$$d_{\text{word embed}} = d_{\text {pos embed}}$$</p><h3 id=encoder>Encoder:</h3><ul><li><p>Self-attention: 计算的是src或trg自身的词与词之间的依赖关系 (之前教程的Attention则是计算src的词与trg的词之间的依赖关系)</p><ul><li>每个input token转成w2v</li><li>用w2v乘以三个权重矩阵(Wq,Wk,Wv)得到三个(Query,Key,Value)向量, q,k,v
用该位置token的q乘以自己以及其他token的k, 得到self-attention分数值</li><li>分数值除以一个常数(default 8), 让梯度更稳定, 然后放入softmax, 得到自己与其他每个token的权重</li><li>所有位置的权重乘以v并相加, 得到self-attention在该位置的输出 Z</li></ul><p>$$A(Q,K,V)= \operatorname{softmax} ( \frac{QK^{T}}{ \sqrt d_{k}})V=Z$$</p></li><li><p>Multi-Headed Attention - 扩展了模型专注于不同位置的能力:</p><ul><li>把上述Self-attention的过程做8次, 即开始就初始化8组权重矩阵(Wq,Wk,Wv), 得到8个Q,K,V矩阵, 通过上述计算最后得到8个 Z</li><li>将8个Z合并, 并乘以另一权重矩阵Wo, 最终得到一个Z矩阵</li></ul></li><li><p>Positional Encoding - 表示序列的顺序将src的position放入到embedding layer</p></li><li><p>Layer Normalization - 解决多层神经网络训练困难的问题，通过将前一层的信息无差的传递到下一层, 使特征的平均值为0, 标准差为1, 更容易训练</p></li></ul><h3 id=decoder>Decoder</h3><p>与 Encoder 相似, 但比Encoder多了一层Multi-Headed Attention
一层是src或trg自身的词与词之间的依赖关系, 另一层是是计算src的词与trg的词之间的依赖关系</p><ul><li>Mask<ul><li>Padding Mask - Encoder和Decoder都会用到, 大小与batch size对齐后序列一致, 的部分为0, 其余为1</li><li>Sequence/Subsequent Mask - Decoder会用到, 为了使其看不到未来的信息(使Decoder输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 和 t 之后的输出), 通过下三角矩阵解决, 且下三角矩阵应与decoder的padding mask 结合</li></ul></li></ul><h3 id=positionwisefeedforwardlayer>PositionwiseFeedforwardLayer:</h3><ol><li>after attetnion operation, apply a fc layer first to transformed from hid_dim to pf_dim (512 to 2048)</li><li>apply relu activation function (In BERT, use glue activation function)</li><li>apply dropout</li><li>apply another fc layer to transformed from pf_dim to hid_dim</li></ol><h2 id=reference>Reference</h2><p><a href=https://www.michaelphi.com/illustrated-guide-to-lstms-and-grus-a-step-by-step-explanation/ target=_blank rel="noopener noreffer">GRU and LSTM</a><br><a href=https://jalammar.github.io/illustrated-transformer/ target=_blank rel="noopener noreffer">transformer</a><br><a href=https://charon.me/posts/pytorch/pytorch_seq2seq_6/ target=_blank rel="noopener noreffer">transformer code breakdown: pytorch</a><br><a href=https://kazemnejad.com/blog/transformer_architecture_positional_encoding/ target=_blank rel="noopener noreffer">what and why postional encoding</a></p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2020-07-31</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://zqfang.github.io/2020-07-31-nlp-rnn/ data-title="NLP: RNN and Transformers" data-hashtags="Deep Learning,NLP"><i class="fab fa-twitter fa-fw"></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://zqfang.github.io/2020-07-31-nlp-rnn/ data-hashtag="Deep Learning"><i class="fab fa-facebook-square fa-fw"></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://zqfang.github.io/2020-07-31-nlp-rnn/ data-title="NLP: RNN and Transformers"><i class="fab fa-hacker-news fa-fw"></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://zqfang.github.io/2020-07-31-nlp-rnn/ data-title="NLP: RNN and Transformers"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@2.14.0/icons/line.svg></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://zqfang.github.io/2020-07-31-nlp-rnn/ data-title="NLP: RNN and Transformers"><i class="fab fa-weibo fa-fw"></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/deep-learning/>Deep Learning</a>,&nbsp;<a href=/tags/nlp/>NLP</a></section><section><span><a href=javascript:void(0); onclick=window.history.back();>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/2020-07-26-dl-gnn/ class=prev rel=prev title="Graph: GNN review"><i class="fas fa-angle-left fa-fw"></i>Graph: GNN review</a>
<a href=/2020-08-01-nlp-constituency-and-depenency-parsing/ class=next rel=next title="NLP: Parse trees">NLP: Parse trees<i class="fas fa-angle-right fa-fw"></i></a></div></div><div id=comments><div id=disqus_thread class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://disqus.com/?ref_noscript>Disqus</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2020 - 2021</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank>zqfang</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw"></i></a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw"></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.css><script type=text/javascript src=https://bioninja-1.disqus.com/embed.js defer></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/smooth-scroll@16.1.3/dist/smooth-scroll.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.37.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lunr@2.3.8/lunr.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.2.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.4.0/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":50},"comment":{},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"lunr"}};</script><script type=text/javascript src=/js/theme.min.f51938f3065a40ee841bcb558e4330e31fd26c0ea55343fff8770b88b0319a3c.js integrity="sha256-9Rk48wZaQO6EG8tVjkMw4x/SbA6lU0P/+HcLiLAxmjw="></script></body></html>