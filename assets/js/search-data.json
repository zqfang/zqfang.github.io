{
  
    
        "post0": {
            "title": "Boosting",
            "content": "æå‡ï¼ˆBoostingï¼‰æ–¹æ³•ï¼š é€šè¿‡æ”¹å˜è®­ç»ƒæ ·æœ¬çš„æƒé‡ï¼ˆæ¦‚ç‡åˆ†å¸ƒï¼‰ï¼Œå­¦ä¹ è¿‡ä¸ªåˆ†ç±»å™¨ï¼Œå¹¶å°†è¿™äº›åˆ†ç±»å™¨çº¿æ€§ç»„åˆï¼Œæé«˜åˆ†ç±»æ€§èƒ½ã€‚ . 1. AdaBoost . AdaBoosté€šè¿‡æé«˜è¢«å‰ä¸€è½®å¼±åˆ†ç±»å™¨é”™è¯¯åˆ†ç±»æ ·æœ¬çš„æƒå€¼ï¼Œä»è€Œé™ä½è¢«æ­£ç¡®åˆ†ç±»æ ·æœ¬çš„æƒå€¼ï¼Œå¹¶é‡‡å–ç”²é†›å¤šæ•°è¡¨å†³çš„æ–¹æ³•è¾¾åˆ°åˆ†ç±»ç›®çš„ã€‚ . è¾“å…¥ï¼šè®­ç»ƒæ•°æ®é›†T={(x1,y1),(x2,y2),â‹¯â€‰,(xN,yN)}T= {(x_1, y_1), (x_2, y_2), cdots, (x_N, y_N) }T={(x1â€‹,y1â€‹),(x2â€‹,y2â€‹),â‹¯,(xNâ€‹,yNâ€‹)}, Y={âˆ’1,+1} mathcal{Y} = {-1,+1 }Y={âˆ’1,+1}; è¾“å‡ºï¼šåˆ†ç±»å™¨G(x)G(x)G(x) . 1). åˆå§‹åŒ–è®­ç»ƒæ•°æ®æƒå€¼åˆ†å¸ƒ . D1=(w11,â‹¯â€‰,w1i,â‹¯â€‰,w1N),w1i=1N,i=1,2,â‹¯â€‰,ND_1 = (w_{11}, cdots, w_{1i}, cdots, w_{1N}), w_{1i} = frac{1}{N}, i = 1,2, cdots,ND1â€‹=(w11â€‹,â‹¯,w1iâ€‹,â‹¯,w1Nâ€‹),w1iâ€‹=N1â€‹,i=1,2,â‹¯,N . 2) å¯¹ m=1ï¼Œ2ï¼Œâ‹¯â€‰,Mm = 1ï¼Œ2ï¼Œ cdots, Mm=1ï¼Œ2ï¼Œâ‹¯,M a.å¯¹æƒå€¼åˆ†å¸ƒDmD_mDmâ€‹çš„è®­ç»ƒæ•°æ®é›†å­¦ä¹ ï¼Œå¾—åˆ°åŸºæœ¬åˆ†ç±»å™¨ . Gm(x):Xâ†’{âˆ’1,+1}G_{m}(x): mathcal{X} rightarrow {-1,+1 }Gmâ€‹(x):Xâ†’{âˆ’1,+1} . b.è®¡ç®—G(x)G(x)G(x)åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šçš„åˆ†ç±»è¯¯å·®ç‡ . em=âˆ‘i=1NP(Gm(xi)â‰ yi)=âˆ‘i=1NwmiI(Gm(xi)â‰ yi)e_{m}= sum_{i=1}^{N} P left(G_{m} left(x_{i} right) neq y_{i} right)= sum_{i=1}^{N} w_{m i} I left(G_{m} left(x_{i} right) neq y_{i} right)emâ€‹=i=1âˆ‘Nâ€‹P(Gmâ€‹(xiâ€‹)î€ â€‹=yiâ€‹)=i=1âˆ‘Nâ€‹wmiâ€‹I(Gmâ€‹(xiâ€‹)î€ â€‹=yiâ€‹) . c. è®¡ç®—G(x)G(x)G(x)çš„ç³»æ•° . Î±m=12logâ¡1âˆ’emem alpha_{m}= frac{1}{2} log frac{1-e_{m}}{e_{m}}Î±mâ€‹=21â€‹logemâ€‹1âˆ’emâ€‹â€‹ . d. æ›´æ–°è®­ç»ƒæ•°æ®çš„æƒå€¼åˆ†å¸ƒ . Dm+1=(wm+1,1,â‹¯â€‰,wm+1,i,â‹¯â€‰,wm+1,N)wm+1,i=wmiZmexpâ¡(âˆ’Î±myiGm(xi)),i=1,2,â‹¯â€‰,N begin{array}{c} D_{m+1}= left(w_{m+1,1}, cdots, w_{m+1, i}, cdots, w_{m+1, N} right) w_{m+1, i}= frac{w_{m i}}{Z_{m}} exp left(- alpha_{m} y_{i} G_{m} left(x_{i} right) right), quad i=1,2, cdots, N end{array}Dm+1â€‹=(wm+1,1â€‹,â‹¯,wm+1,iâ€‹,â‹¯,wm+1,Nâ€‹)wm+1,iâ€‹=Zmâ€‹wmiâ€‹â€‹exp(âˆ’Î±mâ€‹yiâ€‹Gmâ€‹(xiâ€‹)),i=1,2,â‹¯,Nâ€‹ . å…¶ä¸­ï¼Œ . Zm=âˆ‘i=1Nwmiexpâ¡(âˆ’Î±myiGm(xi))Z_{m}= sum_{i=1}^{N} w_{m i} exp left(- alpha_{m} y_{i} G_{m} left(x_{i} right) right)Zmâ€‹=i=1âˆ‘Nâ€‹wmiâ€‹exp(âˆ’Î±mâ€‹yiâ€‹Gmâ€‹(xiâ€‹)) . 3ï¼‰æ„å»ºåŸºæœ¬çº¿æ€§åˆ†ç±»å™¨ç»„åˆ . f(x)=âˆ‘m=1MÎ±mGm(x)f(x)= sum_{m=1}^{M} alpha_{m} G_{m}(x)f(x)=m=1âˆ‘Mâ€‹Î±mâ€‹Gmâ€‹(x) . å¾—åˆ°æœ€ç»ˆåˆ†ç±»å™¨ . G(x)=signâ¡(f(x))=signâ¡(âˆ‘m=1MÎ±mGm(x)) begin{aligned} G(x) &amp;= operatorname{sign}(f(x)) &amp;= operatorname{sign} left( sum_{m=1}^{M} alpha_{m} G_{m}(x) right) end{aligned}G(x)â€‹=sign(f(x))=sign(m=1âˆ‘Mâ€‹Î±mâ€‹Gmâ€‹(x))â€‹ . 1.1 AdaBoostç®—æ³•è¯¯å·® . AdaBoostç®—æ³•æœ€ç»ˆåˆ†ç±»å™¨è®­ç»ƒè¯¯å·®ç•Œä¸º . 1Nâˆ‘i=1NI(G(xi)â‰ yi)â©½1Nâˆ‘iexpâ¡(âˆ’yif(xi))=âˆmZm frac{1}{N} sum_{i=1}^{N} I left(G left(x_{i} right) neq y_{i} right) leqslant frac{1}{N} sum_{i} exp left(-y_{i} f left(x_{i} right) right)= prod_{m} Z_{m}N1â€‹i=1âˆ‘Nâ€‹I(G(xiâ€‹)î€ â€‹=yiâ€‹)â©½N1â€‹iâˆ‘â€‹exp(âˆ’yiâ€‹f(xiâ€‹))=mâˆâ€‹Zmâ€‹ . è¿™ä¸€å®šç†è¯´æ˜ï¼Œæ¯ä¸€è½®é€‰å–é€‚å½“çš„GmG_mGmâ€‹ä½¿ZmZ_mZmâ€‹æœ€å°ï¼Œä»è€Œä½¿è®­ç»ƒè¯¯å·®ä¸‹é™æœ€å¿«ã€‚ . å¯¹äºäºŒåˆ†ç±»é—®é¢˜ï¼š . âˆm=1MZm=âˆm=1M[2em(1âˆ’em)]=âˆm=1M(1âˆ’4Î³m2)â©½expâ¡(âˆ’2âˆ‘m=1MÎ³m2) begin{aligned} prod_{m=1}^{M} Z_{m} &amp;= prod_{m=1}^{M}[2 sqrt{e_{m} left(1-e_{m} right)}] &amp;= prod_{m=1}^{M} sqrt{ left(1-4 gamma_{m}^{2} right)} &amp; leqslant exp left(-2 sum_{m=1}^{M} gamma_{m}^{2} right) end{aligned}m=1âˆMâ€‹Zmâ€‹â€‹=m=1âˆMâ€‹[2emâ€‹(1âˆ’emâ€‹) . â€‹]=m=1âˆMâ€‹(1âˆ’4Î³m2â€‹) . â€‹â©½exp(âˆ’2m=1âˆ‘Mâ€‹Î³m2â€‹)â€‹ . å…¶ä¸­ï¼Œ Î³m=12âˆ’em gamma_{m}= frac{1}{2}-e_{m}Î³mâ€‹=21â€‹âˆ’emâ€‹ . 1.2 AdaBoostç®—æ³•è§£é‡Š . AdaBooostå¯ä»¥è®¤ä¸ºï¼šæ¨¡å‹ä¸ºåŠ æ³•æ¨¡å‹ï¼ŒæŸå¤±å‡½æ•°ä¸ºæŒ‡æ•°å‡½æ•°ï¼Œå­¦ä¹ ç®—æ³•ä¸ºå‰å‘åˆ†å¸ƒç®—æ³•çš„äºŒåˆ†ç±»å­¦ä¹ æ–¹æ³• . 1.2.1 å‰å‘åˆ†æ­¥ç®—æ³• . è€ƒè™‘åŠ æ³•æ¨¡å‹ï¼ˆadditive modelï¼‰ . f(x)=âˆ‘m=1MÎ²mb(x;Î³m)f(x)= sum_{m=1}^{M} beta_{m} b left(x ; gamma_{m} right)f(x)=m=1âˆ‘Mâ€‹Î²mâ€‹b(x;Î³mâ€‹) . å…¶ä¸­ï¼Œb(x;Î³m)b(x; gamma_m)b(x;Î³mâ€‹)ä¸ºåŸºå‡½æ•°ï¼Œgammamgamma_mgammamâ€‹ä¸ºå‚æ•°ï¼Œ Î²m beta_mÎ²mâ€‹ä¸ºç³»æ•°ã€‚ . åœ¨ç»™å®šè®­ç»ƒé›†å’ŒæŸå¤±å‡½æ•°L(y,f(x))L(y,f(x))L(y,f(x))çš„æ¡ä»¶ä¸‹ï¼Œå­¦ä¹ åŠ æ³•æ¨¡å‹f(x)f(x)f(x)æˆä¸ºç»éªŒé£é™©æå°åŒ–ï¼ˆæŸå¤±å‡½æ•°æå°åŒ–ï¼‰é—®é¢˜ï¼š . minâ¡Î²m,Î³mâˆ‘i=1NL(yi,âˆ‘m=1MÎ²mb(xi;Î³m)) min _{ beta_{m}, gamma_{m}} sum_{i=1}^{N} L left(y_{i}, sum_{m=1}^{M} beta_{m} b left(x_{i} ; gamma_{m} right) right)Î²mâ€‹,Î³mâ€‹minâ€‹i=1âˆ‘Nâ€‹L(yiâ€‹,m=1âˆ‘Mâ€‹Î²mâ€‹b(xiâ€‹;Î³mâ€‹)) . å‰å‘åˆ†å¸ƒç®—æ³•æ€æƒ³æ˜¯ï¼š ä»å‰å‘åï¼Œæ¯ä¸€æ­¥åªå­¦ä¸€ä¸ªåŸºå‡½æ•°åŠå…¶ç³»æ•°ï¼Œé€æ­¥é€¼è¿‘ä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼Œè¾¾åˆ°ä¼˜åŒ–æ­¥éª¤ç®€åŒ–çš„ç›®çš„ã€‚ . å› æ­¤ï¼Œæ¯ä¸€æ­¥åªéœ€ä¼˜åŒ–å¦‚ä¸‹æŸå¤±å‡½æ•°ï¼š . minâ¡Î²,Î³âˆ‘i=1NL(yi,Î²b(xi;Î³)) min _{ beta, gamma} sum_{i=1}^{N} L left(y_{i}, beta b left(x_{i} ; gamma right) right)Î²,Î³minâ€‹i=1âˆ‘Nâ€‹L(yiâ€‹,Î²b(xiâ€‹;Î³)) . ç®—æ³•æ­¥éª¤ . è¾“å…¥ï¼šè®­ç»ƒæ•°æ®é›†T={(x1,y1),(x2,y2),â‹¯â€‰,(xN,yN)}T= {(x_1, y_1), (x_2, y_2), cdots, (x_N, y_N) }T={(x1â€‹,y1â€‹),(x2â€‹,y2â€‹),â‹¯,(xNâ€‹,yNâ€‹)}, æŸå¤±å‡½æ•°L(y,f(x))L(y,f(x))L(y,f(x));åŸºå‡½æ•°é›†{b(x;Î³} {b(x; gamma }{b(x;Î³}; è¾“å‡ºï¼šåŠ æ³•æ¨¡å‹f(x)f(x)f(x) . 1ï¼‰åˆå§‹åŒ–f0(x)=0f_0(x) = 0f0â€‹(x)=0 2) å¯¹m=1,2,â‹¯â€‰,Mm = 1,2, cdots, Mm=1,2,â‹¯,M a.æå°åŒ–æŸå¤±å‡½æ•° . (Î²m,Î³m)=argâ¡minâ¡Î²,Î³âˆ‘i=1NL(yi,fmâˆ’1(xi)+Î²b(xi;Î³)) left( beta_{m}, gamma_{m} right)= arg min _{ beta, gamma} sum_{i=1}^{N} L left(y_{i}, f_{m-1} left(x_{i} right)+ beta b left(x_{i} ; gamma right) right)(Î²mâ€‹,Î³mâ€‹)=argÎ²,Î³minâ€‹i=1âˆ‘Nâ€‹L(yiâ€‹,fmâˆ’1â€‹(xiâ€‹)+Î²b(xiâ€‹;Î³)) . å¾—åˆ°å‚æ•°Î²m beta_mÎ²mâ€‹, Î³m gamma_mÎ³mâ€‹ã€‚ . b.æ›´æ–° . fm(x)=fmâˆ’1(x)+Î²mb(x;Î³m)f_{m}(x)=f_{m-1}(x)+ beta_{m} b left(x ; gamma_{m} right)fmâ€‹(x)=fmâˆ’1â€‹(x)+Î²mâ€‹b(x;Î³mâ€‹) . 3ï¼‰å¾—åˆ°åŠ æ³•æ¨¡å‹ . f(x)=fM(x)=âˆ‘m=1MÎ²mb(x;Î³m)f(x)=f_{M}(x)= sum_{m=1}^{M} beta_{m} b left(x ; gamma_{m} right)f(x)=fMâ€‹(x)=m=1âˆ‘Mâ€‹Î²mâ€‹b(x;Î³mâ€‹) . 2. Boosting Tree . æå‡æ ‘ğŸŒ²æ˜¯ä»¥å†³ç­–æ ‘ä¸ºåŸºæœ¬åˆ†ç±»å™¨çš„æå‡æ–¹æ³• . 2.1 æå‡æ ‘æ¨¡å‹ . é‡‡ç”¨åŠ æ³•æ¨¡å‹ï¼ˆåŸºå‡½æ•°çš„çº¿æ€§ç»„åˆï¼‰ä¸å‰å‘åˆ†å¸ƒç®—æ³•ï¼š . fM(x)=âˆ‘m=1MT(x;Î˜m)f_{M}(x)= sum_{m=1}^{M} T left(x ; Theta_{m} right)fMâ€‹(x)=m=1âˆ‘Mâ€‹T(x;Î˜mâ€‹) . å…¶ä¸­ T(x;Î˜m)T left(x ; Theta_{m} right)T(x;Î˜mâ€‹)è¡¨ç¤ºå†³ç­–æ ‘ï¼ŒÎ˜m Theta_{m}Î˜mâ€‹å†³ç­–æ ‘å‚æ•°ï¼Œ MMMä¸ºæ ‘çš„ä¸ªæ•° . 2.2 æå‡æ ‘ç®—æ³• . é‡‡ç”¨åŠ æ³•æ¨¡å‹å’Œå‰å‘åˆ†å¸ƒç®—æ³•å®ç°å­¦ä¹ ä¼˜åŒ–çš„è¿‡ç¨‹ã€‚ . é¦–å…ˆç¡®å®šæå‡æ ‘f0(x)=0f_{0}(x)=0f0â€‹(x)=0ï¼Œ ç¬¬mmmæ­¥çš„æ¨¡å‹æ˜¯ . fm(x)=fmâˆ’1(x)+T(x;Î˜m)f_{m}(x)=f_{m-1}(x)+T left(x ; Theta_{m} right)fmâ€‹(x)=fmâˆ’1â€‹(x)+T(x;Î˜mâ€‹) . å…¶ä¸­ï¼Œ fmâˆ’1(x)f_{m-1}(x)fmâˆ’1â€‹(x)ä¸ºå½“å‰æ¨¡å‹ï¼Œé€šè¿‡ç»éªŒé£é™©æå°åŒ–ç¡®å®šä¸‹ä¸€åˆ»å†³ç­–æ ‘çš„å‚æ•°Î˜m Theta_{m}Î˜mâ€‹ï¼š . Î˜^m=argâ¡minâ¡Î˜mâˆ‘i=1NL(yi,fmâˆ’1(xi)+T(xi;Î˜m)) hat{ Theta}_{m}= arg min _{ Theta_{m}} sum_{i=1}^{N} L left(y_{i}, f_{m-1} left(x_{i} right)+T left(x_{i} ; Theta_{m} right) right)Î˜^mâ€‹=argÎ˜mâ€‹minâ€‹i=1âˆ‘Nâ€‹L(yiâ€‹,fmâˆ’1â€‹(xiâ€‹)+T(xiâ€‹;Î˜mâ€‹)) . 2.2.1 å›å½’é—®é¢˜æå‡æ ‘ . è®­ç»ƒæ•°æ®é›†T={(x1,y1),(x2,y2),â‹¯â€‰,(xN,yN)}T= {(x_1, y_1), (x_2, y_2), cdots, (x_N, y_N) }T={(x1â€‹,y1â€‹),(x2â€‹,y2â€‹),â‹¯,(xNâ€‹,yNâ€‹)}, xiâˆˆXâŠ†Rnx_{i} in mathcal{X} subseteq mathbf{R}^{n}xiâ€‹âˆˆXâŠ†Rn,X mathcal{X}Xä¸ºè¾“å…¥ç©ºé—´ï¼Œ YâŠ†R mathcal{Y} subseteq mathbf{R}YâŠ†R; . å°†è¾“å…¥ç©ºé—´åˆ’åˆ†ä¸ºJJJä¸ªäº’ä¸ç›¸äº¤çš„åŒºåŸŸR1ï¼ŒR2,â‹¯â€‰,RJR1ï¼ŒR2, cdots, R_JR1ï¼ŒR2,â‹¯,RJâ€‹ï¼Œ å¹¶ä¸”æ¯ä¸ªåŒºåŸŸä¸Šç¡®å®šè¾“å‡ºçš„å¸¸é‡cjc_jcjâ€‹ï¼Œé‚£ä¹ˆæ ‘å¯ä»¥è¡¨ç¤ºä¸ºï¼š . T(x;Î˜)=âˆ‘j=1JcjI(xâˆˆRj)T(x ; Theta)= sum_{j=1}^{J} c_{j} I left(x in R_{j} right)T(x;Î˜)=j=1âˆ‘Jâ€‹cjâ€‹I(xâˆˆRjâ€‹) . å…¶ä¸­ï¼Œ . Î˜={(R1,c1),(R2,c2),â‹¯â€‰,(RJ,cJ)} Theta= left { left(R_{1}, c_{1} right), left(R_{2}, c_{2} right), cdots, left(R_{J}, c_{J} right) right }Î˜={(R1â€‹,c1â€‹),(R2â€‹,c2â€‹),â‹¯,(RJâ€‹,cJâ€‹)} è¡¨ç¤ºæ ‘çš„å´ä¸åˆ’åˆ†å’Œå„ä¸ªå–æ‚¦æ˜¯é‚£ä¸ªçš„å¸¸æ•°ã€‚ . é‡‡ç”¨ä¸€ä¸‹å‰å‘åˆ†å¸ƒç®—æ³• . f0(x)=0fm(x)=fmâˆ’1(x)+T(x;Î˜m),m=1,2,â‹¯â€‰,MfM(x)=âˆ‘m=1MT(x;Î˜m) begin{aligned} &amp;f_{0}(x)=0 &amp; begin{array}{l} f_{m}(x)=f_{m-1}(x)+T left(x ; Theta_{m} right), quad m=1,2, cdots, M f_{M}(x)= sum_{m=1}^{M} T left(x ; Theta_{m} right) end{array} end{aligned}â€‹f0â€‹(x)=0fmâ€‹(x)=fmâˆ’1â€‹(x)+T(x;Î˜mâ€‹),m=1,2,â‹¯,MfMâ€‹(x)=âˆ‘m=1Mâ€‹T(x;Î˜mâ€‹)â€‹â€‹ . æ±‚è§£Î˜^m hat{ Theta}_{m}Î˜^mâ€‹ï¼Œ è‹¥ç”¨å¹³æ–¹è¯¯å·®æŸå¤±å‡½æ•°ï¼š . L(y,f(x))=(yâˆ’f(x))2L(y, f(x))=(y-f(x))^{2}L(y,f(x))=(yâˆ’f(x))2 . åˆ™æŸå¤±å‡½æ•°ä¸ºï¼š . L(y,fmâˆ’1(x)+T(x;Î˜m))=[yâˆ’fmâˆ’1(x)âˆ’T(x;Î˜m)]2=[râˆ’T(x;Î˜m)]2 begin{aligned} L left(y, f_{m-1}(x)+T left(x ; Theta_{m} right) right) &amp;= left[y-f_{m-1}(x)-T left(x ; Theta_{m} right) right]^{2} &amp;= left[r-T left(x ; Theta_{m} right) right]^{2} end{aligned}L(y,fmâˆ’1â€‹(x)+T(x;Î˜mâ€‹))â€‹=[yâˆ’fmâˆ’1â€‹(x)âˆ’T(x;Î˜mâ€‹)]2=[râˆ’T(x;Î˜mâ€‹)]2â€‹ . è¿™é‡Œï¼Œ . r=yâˆ’fmâˆ’1(x)r=y-f_{m-1}(x)r=yâˆ’fmâˆ’1â€‹(x) . æ˜¯å½“å‰æ¨¡å‹æ‹Ÿåˆæ•°æ®çš„æ®‹å·®ï¼ˆresidualï¼‰ã€‚å› æ­¤å¯¹äºå›å½’é—®é¢˜æå‡æ ‘ï¼Œåªéœ€æ‹Ÿåˆå½“å‰æ¨¡å‹æ®‹å·®ã€‚å¾—åˆ°T(x;Î˜m)T left(x ; Theta_{m} right)T(x;Î˜mâ€‹)ï¼Œæ›´æ–°æ¨¡å‹ï¼Œå¾—åˆ°fm(x)f_m(x)fmâ€‹(x)ã€‚ . 3. æ¢¯åº¦æå‡ . å½“æŸå¤±å‡½æ•°ä¸æ˜¯ç®€å•çš„å¹³æ–¹æŸå¤±ã€æŒ‡æ•°æŸå¤±æ—¶ï¼Œæå‡æ ‘çš„ä¼˜åŒ–å°±å¾ˆéš¾ã€‚æ¢¯åº¦æå‡ç®—æ³•åˆ©ç”¨æœ€é€Ÿä¸‹é™æ³•çš„è¿‘ä¼¼æ–¹æ³•ï¼Œè®¡ç®—æŸå¤±å‡½æ•°çš„è´Ÿæ¢¯åº¦åœ¨å½“å‰æ¨¡å‹çš„å€¼ . âˆ’[âˆ‚L(y,f(xi))âˆ‚f(xi)]f(x)=fmâˆ’1(x)- left[ frac{ partial L left(y, f left(x_{i} right) right)}{ partial f left(x_{i} right)} right]_{f(x)=f_{m-1}(x)}âˆ’[âˆ‚f(xiâ€‹)âˆ‚L(y,f(xiâ€‹))â€‹]f(x)=fmâˆ’1â€‹(x)â€‹ . å¹¶å°†å…¶ä½œä¸ºå›å½’é—®é¢˜æå‡æ ‘ç®—æ³•ä¸­çš„æ®‹å·®è¿‘ä¼¼å€¼ï¼Œæ‹Ÿåˆä¸€ä¸ªå›å½’æ ‘ã€‚ . è¾“å…¥ï¼š è®­ç»ƒæ•°æ®é›†T={(x1,y1),(x2,y2),â‹¯â€‰,(xN,yN)}T= {(x_1, y_1), (x_2, y_2), cdots, (x_N, y_N) }T={(x1â€‹,y1â€‹),(x2â€‹,y2â€‹),â‹¯,(xNâ€‹,yNâ€‹)}, xiâˆˆXâŠ†Rnx_{i} in mathcal{X} subseteq mathbf{R}^{n}xiâ€‹âˆˆXâŠ†Rn,X mathcal{X}Xä¸ºè¾“å…¥ç©ºé—´ï¼Œ YâŠ†R mathcal{Y} subseteq mathbf{R}YâŠ†R; æŸå¤±å‡½æ•°L(y,f(x))L(y,f(x))L(y,f(x)) è¾“å‡ºï¼š å›å½’æ ‘f^(x) hat f(x)f^â€‹(x) . 1ï¼‰åˆå§‹åŒ– . f0(x)=argâ¡minâ¡câˆ‘i=1NL(yi,c)f_{0}(x)= arg min _{c} sum_{i=1}^{N} L left(y_{i}, c right)f0â€‹(x)=argcminâ€‹i=1âˆ‘Nâ€‹L(yiâ€‹,c) . 2) å¯¹ m=1ï¼Œ2ï¼Œâ‹¯â€‰,Mm=1ï¼Œ2ï¼Œ cdots, Mm=1ï¼Œ2ï¼Œâ‹¯,M a. å¯¹i=1ï¼Œ2ï¼Œâ‹¯â€‰,Ni=1ï¼Œ2ï¼Œ cdots, Ni=1ï¼Œ2ï¼Œâ‹¯,Nè®¡ç®— . rmi=âˆ’[âˆ‚L(yi,f(xi))âˆ‚f(xi)]f(x)=fmâˆ’1(x)r_{m i}=- left[ frac{ partial L left(y_{i}, f left(x_{i} right) right)}{ partial f left(x_{i} right)} right]_{f(x)=f_{m-1}(x)}rmiâ€‹=âˆ’[âˆ‚f(xiâ€‹)âˆ‚L(yiâ€‹,f(xiâ€‹))â€‹]f(x)=fmâˆ’1â€‹(x)â€‹ . b) å¯¹rmir_{mi}rmiâ€‹æ‹Ÿåˆä¸€ä¸ªå›å½’æ ‘ï¼Œå¾—åˆ°ç¬¬mmmé¢—æ ‘çš„èŠ‚ç‚¹åŒºåŸŸRmjR_{mj}Rmjâ€‹ . c) å¯¹j=1,2,â‹¯â€‰,Jj=1,2, cdots, Jj=1,2,â‹¯,J, è®¡ç®— . cmj=argâ¡minâ¡câˆ‘xiâˆˆRmjL(yi,fmâˆ’1(xi)+c)c_{m j}= arg min _{c} sum_{x_{i} in R_{m j}} L left(y_{i}, f_{m-1} left(x_{i} right)+c right)cmjâ€‹=argcminâ€‹xiâ€‹âˆˆRmjâ€‹âˆ‘â€‹L(yiâ€‹,fmâˆ’1â€‹(xiâ€‹)+c) . d)æ›´æ–° . fm(x)=fmâˆ’1(x)+âˆ‘j=1JcmjI(xâˆˆRmj)f_{m}(x)=f_{m-1}(x)+ sum_{j=1}^{J} c_{m j} I left(x in R_{m j} right)fmâ€‹(x)=fmâˆ’1â€‹(x)+j=1âˆ‘Jâ€‹cmjâ€‹I(xâˆˆRmjâ€‹) . 3)å¾—åˆ°å›å½’æ ‘ . f^(x)=fM(x)=âˆ‘m=1Mâˆ‘j=1JcmjI(xâˆˆRmj) hat{f}(x)=f_{M}(x)= sum_{m=1}^{M} sum_{j=1}^{J} c_{m j} I left(x in R_{m j} right)f^â€‹(x)=fMâ€‹(x)=m=1âˆ‘Mâ€‹j=1âˆ‘Jâ€‹cmjâ€‹I(xâˆˆRmjâ€‹) . å‚è€ƒï¼š æèˆªã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹ .",
            "url": "https://zqfang.github.io/machine%20learning/2020/05/05/ML-Boosting.html",
            "relUrl": "/machine%20learning/2020/05/05/ML-Boosting.html",
            "date": " â€¢ May 5, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Hidden Markov Model (HMM)",
            "content": "éšé©¬å¯å¤«æ¨¡å‹ï¼ˆHMMï¼‰æè¿°éšè—çš„é©¬å¯å¤«é“¾éšæœºç”Ÿæˆè§‚æµ‹åºåˆ—çš„è¿‡ç¨‹ï¼Œå±äºç”Ÿæˆæ¨¡å‹ã€‚ HMMåœ¨è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€ç”Ÿç‰©ä¿¡æ¯ã€æ¨¡å¼è¯†åˆ«ç­‰é¢†åŸŸç”±å¹¿æ³›åº”ç”¨ã€‚ . 1. HMMçš„å®šä¹‰ . éšé©¬å¯å¤«æ¨¡å‹æ˜¯å…³äºæ—¶åºçš„æ¦‚ç‡æ¨¡å‹ï¼Œ æè¿°ç”±ä¸€ä¸ªéšè—çš„é©¬å¯å¤«é“¾éšæœºç”Ÿæˆä¸å¯è§‚æµ‹çš„çŠ¶æ€ï¼Œå†ç”±å„ä¸ªçŠ¶æ€ç”Ÿæˆä¸€ä¸ªè§‚æµ‹ï¼Œä»è€Œäº§ç”Ÿè§‚æµ‹éšæœºåºåˆ—çš„è¿‡ç¨‹ã€‚ . ç®€è€Œè¨€ä¹‹ï¼Œéšé©¬å¯å¤«é“¾éšæœºæˆçŠ¶æ€åºåˆ—ï¼ˆstate sequenceï¼‰ï¼Œè€Œæ¯ä¸ªçŠ¶æ€ç”Ÿæˆè§‚æµ‹ï¼Œäº§ç”Ÿè§‚æµ‹åºåˆ—ï¼ˆobservation sequenceï¼‰ã€‚åºåˆ—çš„ä¸€ä¸ªä½ç½®å¯ä»¥çœ‹ä½œä¸€ä¸ªæ—¶åˆ»ã€‚ . ä»¤QQQ è¡¨ç¤ºæ‰€æœ‰å¯èƒ½çŠ¶æ€çš„é›†åˆï¼šQ={q1,q2,â‹¯â€‰,qN}Q = { q_1, q_2, cdots, q_N }Q={q1â€‹,q2â€‹,â‹¯,qNâ€‹}; ä»¤VVV è¡¨ç¤ºæ‰€æœ‰å¯èƒ½çš„è§‚æµ‹é›†åˆï¼šV={v1,v2,â‹¯â€‰,vM}V = {v_1, v_2, cdots, v_M }V={v1â€‹,v2â€‹,â‹¯,vMâ€‹}; ä»¤III è¡¨ç¤ºé•¿åº¦ä¸ºTçš„çŠ¶æ€åºåˆ—ï¼š I=(i1,i2,â‹¯â€‰,iT)I = (i_1, i_2, cdots, i_T)I=(i1â€‹,i2â€‹,â‹¯,iTâ€‹); ä»¤OOO è¡¨ç¤ºå¯¹åº”çš„æ˜¯è§‚æµ‹åºåˆ—ï¼š O=(o1,o2,â‹¯â€‰,oT)O = (o_1, o_2, cdots, o_T)O=(o1â€‹,o2â€‹,â‹¯,oTâ€‹). . ä»¤AAAæ˜¯è½¬ç§»æ¦‚ç‡çŸ©é˜µï¼š . A=[aij]NÃ—NA = [a_{ij}]_{N times N}A=[aijâ€‹]NÃ—Nâ€‹ . å…¶ä¸­ï¼Œ . aij=P(it+1=qjâˆ£it=qj),i=1,2,â‹¯â€‰,N;j=1,2,â‹¯â€‰,Na_{ij} = P(i_{t+1} = q_j | i_t = q_j), i=1,2, cdots, N; j = 1,2, cdots, Naijâ€‹=P(it+1â€‹=qjâ€‹âˆ£itâ€‹=qjâ€‹),i=1,2,â‹¯,N;j=1,2,â‹¯,N . æ˜¯åœ¨æ—¶åˆ»tttå¤„äºçŠ¶æ€qiq_iqiâ€‹çš„æ¡ä»¶ä¸‹ç”Ÿæˆè§‚æµ‹t+1t +1t+1è½¬ç§»åˆ°çŠ¶æ€qjq_jqjâ€‹çš„æ¦‚ç‡ã€‚ . ä»¤BBBæ˜¯è§‚æµ‹æ¦‚ç‡çŸ©é˜µï¼š . B=[bj(k)]NÃ—MB = [b_j(k)]_{N times M}B=[bjâ€‹(k)]NÃ—Mâ€‹ . å…¶ä¸­ï¼Œ . bj(k)=P(ot=vkâˆ£it=qj),k=1,2,â‹¯â€‰,M;j=1,2,â‹¯â€‰,Nb_j(k) = P(o_t = v_k | i_t = q_j), k=1,2, cdots, M; j=1,2, cdots, Nbjâ€‹(k)=P(otâ€‹=vkâ€‹âˆ£itâ€‹=qjâ€‹),k=1,2,â‹¯,M;j=1,2,â‹¯,N . æ˜¯åœ¨æ—¶åˆ»tttå¤„äºçŠ¶æ€qjq_jqjâ€‹çš„æ¡ä»¶ä¸‹ç”Ÿæˆè§‚æµ‹vkv_kvkâ€‹ çš„æ¦‚ç‡ã€‚ . ä»¤Ï€ piÏ€æ˜¯åˆå§‹çŠ¶æ€æ¦‚ç‡å‘é‡ï¼š . Ï€=(Ï€i) pi = ( pi_i)Ï€=(Ï€iâ€‹) . å…¶ä¸­ï¼Œ . Ï€i=P(i1=qi),i=1,2,â‹¯â€‰,N pi_{i} = P(i_1 = q_i),i=1,2, cdots, NÏ€iâ€‹=P(i1â€‹=qiâ€‹),i=1,2,â‹¯,N . æ˜¯æ—¶åˆ»t=1å¤„äºçŠ¶æ€qiq_iqiâ€‹çš„æ¦‚ç‡. . éšé©¬å¯å¤«æ¨¡å‹Î» lambdaÎ»ç”±Ï€ piÏ€ï¼Œ AAAï¼ŒBBBå†³å®šã€‚ . Î»=(A,B,Ï€) lambda = (A, B, pi)Î»=(A,B,Ï€) . å…¶ä¸­ï¼ŒÏ€ piÏ€å’ŒAAAå†³å®šçŠ¶æ€åºåˆ—ï¼ŒBBBå†³å®šè§‚æµ‹åºåˆ—ã€‚ . éšé©¬å¯å¤«æ¨¡å‹çš„ä¸¤ä¸ªåŸºæœ¬å‡è®¾ . é½æ¬¡é©¬å¯å¤«æ€§ éšé©¬å¯å¤«é“¾åœ¨ä»»æ„æ—¶åˆ»tçš„çŠ¶æ€å‰ä¸€æ—¶åˆ»çŠ¶æ€ï¼Œä¸å…¶ä»–æ—¶åˆ»çš„éšçŠ¶æ€å’Œè§‚æµ‹æ— å…³ï¼Œ ä¹Ÿä¸æ—¶åˆ»tæ— å…³ï¼š | . | P(itâˆ£itâˆ’1,Otâˆ’1,â‹¯â€‰,i1,o1)=P(itâˆ£itâˆ’1),t=1,2,â‹¯â€‰,TP(i_t | i_{t-1}, O_{t-1}, cdots, i_1, o_1) = P(i_t | i_{t-1}), t = 1,2, cdots,TP(itâ€‹âˆ£itâˆ’1â€‹,Otâˆ’1â€‹,â‹¯,i1â€‹,o1â€‹)=P(itâ€‹âˆ£itâˆ’1â€‹),t=1,2,â‹¯,T . è§‚æµ‹ç‹¬ç«‹æ€§ ä»»æ„æ—¶åˆ»çš„è§‚æµ‹åªä¾èµ–æ”¹æ—¶åˆ»çš„é©¬å¯å¤«é“¾çŠ¶æ€ï¼Œä¸å…¶ä»–è§‚æµ‹å’ŒçŠ¶æ€æ— å…³: | . P(otâˆ£iT,OT,iTâˆ’1,oTâˆ’1â‹¯â€‰,it+1,Ot+1,itâˆ’1,Otâˆ’1,i1,o1)=P(otâˆ£it)P(o_t | i_{T}, O_{T}, i_{T-1}, o_{T-1} cdots, i_{t+1}, O_{t+1}, i_{t-1}, O_{t-1}, i_1, o_1) = P(o_t | i_{t})P(otâ€‹âˆ£iTâ€‹,OTâ€‹,iTâˆ’1â€‹,oTâˆ’1â€‹â‹¯,it+1â€‹,Ot+1â€‹,itâˆ’1â€‹,Otâˆ’1â€‹,i1â€‹,o1â€‹)=P(otâ€‹âˆ£itâ€‹) | 2. HMMçš„3ä¸ªåŸºæœ¬é—®é¢˜ . æ¦‚ç‡è®¡ç®—ï¼šç»™å®šæ¨¡å‹Î»=(A,B,Ï€) lambda = (A, B, pi)Î»=(A,B,Ï€)å’Œè§‚æµ‹åºåˆ— O=(o1,o2,â‹¯â€‰,oT)O = (o_1, o_2, cdots, o_T)O=(o1â€‹,o2â€‹,â‹¯,oTâ€‹)ï¼Œ æ±‚æ¦‚ç‡P(Oâˆ£Î»)P(O | lambda)P(Oâˆ£Î») . å­¦ä¹ : å·²çŸ¥è§‚æµ‹åºåˆ—O=(o1,o2,â‹¯â€‰,oT)O = (o_1, o_2, cdots, o_T)O=(o1â€‹,o2â€‹,â‹¯,oTâ€‹)ï¼Œä¼°è®¡æ¨¡å‹å‚æ•°Î»=(A,B,Ï€) lambda = (A, B, pi)Î»=(A,B,Ï€)ï¼Œ ä½¿æ¦‚ç‡P(Oâˆ£Î»)P(O vert lambda)P(Oâˆ£Î»)æœ€å¤§ï¼ˆç”¨æå¤§ä¼¼ç„¶ä¼°è®¡ï¼‰ã€‚ . é¢„æµ‹ï¼šç»™å®šæ¨¡å‹Î»=(A,B,Ï€) lambda = (A, B, pi)Î»=(A,B,Ï€)å’Œè§‚æµ‹åºåˆ— O=(o1,o2,â‹¯â€‰,oT)O = (o_1, o_2, cdots, o_T)O=(o1â€‹,o2â€‹,â‹¯,oTâ€‹)ï¼Œæ±‚æ¡ä»¶æ¦‚ç‡P(Iâˆ£O)P(I | O)P(Iâˆ£O)æœ€å¤§çš„çŠ¶æ€åºåˆ— I=(i1,i2,â‹¯â€‰,iT)I = (i_1, i_2, cdots, i_T)I=(i1â€‹,i2â€‹,â‹¯,iTâ€‹). . 2.1 æ¦‚ç‡è®¡ç®—å‰å‘ï¼ˆforwardï¼‰å’Œåå‘ï¼ˆbackwardï¼‰ç®—æ³• . 2.1.1 å‰å‘ç®—æ³• . ç»™å®šæ¨¡å‹Î» lambdaÎ»ï¼Œå½“æ—¶åˆ»tttæ—¶ï¼ŒçŠ¶æ€ä¸ºqiq_iqiâ€‹ï¼Œéƒ¨åˆ†è§‚æµ‹åºåˆ—ä¸ºo1,o2,â‹¯â€‰,oto_1, o_2, cdots, o_to1â€‹,o2â€‹,â‹¯,otâ€‹ï¼Œè®°ï¼š . Î±t(i)=P(o1,o2,â‹¯â€‰,ot,it=qiâˆ£Î») alpha_{t}(i) = P(o_1, o_2, cdots, o_t, i_t = q_i | lambda)Î±tâ€‹(i)=P(o1â€‹,o2â€‹,â‹¯,otâ€‹,itâ€‹=qiâ€‹âˆ£Î») . è¾“å…¥ï¼š éšé©¬å¯å¤«æ¨¡å‹ Î» lambdaÎ»ï¼Œ è§‚æµ‹åºåˆ—OOO; è¾“å‡ºï¼š è§‚æµ‹åºåˆ—æ¦‚ç‡P(Oâˆ£Î»)P(O | lambda)P(Oâˆ£Î») . ï¼ˆ1ï¼‰åˆå€¼ . Î±1(i)=Ï€ibi(o1),i=1,2,â‹¯â€‰,N alpha_{1}(i)= pi_{i} b_{i} left(o_{1} right), quad i=1,2, cdots, NÎ±1â€‹(i)=Ï€iâ€‹biâ€‹(o1â€‹),i=1,2,â‹¯,N . ï¼ˆ2ï¼‰é€’æ¨ å¯¹ t=1,2,â‹¯â€‰,Tâˆ’1,t = 1,2, cdots, T-1,t=1,2,â‹¯,Tâˆ’1, . Î±t+1(i)=[âˆ‘j=1NÎ±t(j)aji]bi(ot+1),i=1,2,â‹¯â€‰,N alpha_{t+1}(i)= left[ sum_{j=1}^{N} alpha_{t}(j) a_{j i} right] b_{i} left(o_{t+1} right), quad i=1,2, cdots, NÎ±t+1â€‹(i)=[j=1âˆ‘Nâ€‹Î±tâ€‹(j)ajiâ€‹]biâ€‹(ot+1â€‹),i=1,2,â‹¯,N . ï¼ˆ3ï¼‰ç»ˆæ­¢ . P(Oâˆ£Î»)=âˆ‘i=1NÎ±T(i)P(O | lambda)= sum_{i=1}^{N} alpha_{T}(i)P(Oâˆ£Î»)=i=1âˆ‘Nâ€‹Î±Tâ€‹(i) . 2.1.2 åå‘ç®—æ³• . ç»™å®šæ¨¡å‹Î» lambdaÎ»ï¼Œå½“æ—¶åˆ»tttæ—¶ï¼ŒçŠ¶æ€ä¸ºqiq_iqiâ€‹ï¼Œéƒ¨åˆ†è§‚æµ‹åºåˆ—ä¸ºo1,o2,â‹¯â€‰,oto_1, o_2, cdots, o_to1â€‹,o2â€‹,â‹¯,otâ€‹ï¼Œè®°ï¼š . Î²t(i)=P(ot+1,ot+2,â‹¯â€‰,oTâˆ£it=qi,Î») beta_{t}(i) = P(o_{t+1}, o_{t+2}, cdots, o_T | i_t = q_i, lambda)Î²tâ€‹(i)=P(ot+1â€‹,ot+2â€‹,â‹¯,oTâ€‹âˆ£itâ€‹=qiâ€‹,Î») . è¾“å…¥ï¼š éšé©¬å¯å¤«æ¨¡å‹ Î» lambdaÎ»ï¼Œ è§‚æµ‹åºåˆ—OOO; è¾“å‡ºï¼š è§‚æµ‹åºåˆ—æ¦‚ç‡P(Oâˆ£Î»)P(O | lambda)P(Oâˆ£Î») . ï¼ˆ1ï¼‰åˆå§‹ ä»¤æœ€ç»ˆæ—¶åˆ»æ‰€æœ‰çŠ¶æ€qiq_iqiâ€‹ . Î²T(i)=1,i=1,2,â‹¯â€‰,N beta_T(i) = 1, i=1,2, cdots, NÎ²Tâ€‹(i)=1,i=1,2,â‹¯,N . ï¼ˆ2ï¼‰é€’æ¨ å¯¹t=Tâˆ’1,Tâˆ’2,â‹¯â€‰,1t=T-1, T-2, cdots, 1t=Tâˆ’1,Tâˆ’2,â‹¯,1 . Î²t(i)=âˆ‘j=1Naijbj(ot+1)Î²t+1(j),i=1,2,â‹¯â€‰,N beta_{t}(i)= sum_{j=1}^{N} a_{i j} b_{j} left(o_{t+1} right) beta_{t+1}(j), quad i=1,2, cdots, NÎ²tâ€‹(i)=j=1âˆ‘Nâ€‹aijâ€‹bjâ€‹(ot+1â€‹)Î²t+1â€‹(j),i=1,2,â‹¯,N . ï¼ˆ3ï¼‰ç»ˆæ­¢ . P(Oâˆ£Î»)=âˆ‘i=1NÏ€ibi(o1)Î²1(i)P(O | lambda)= sum_{i=1}^{N} pi_{i} b_{i} left(o_{1} right) beta_{1}(i)P(Oâˆ£Î»)=i=1âˆ‘Nâ€‹Ï€iâ€‹biâ€‹(o1â€‹)Î²1â€‹(i) . åˆ©ç”¨å‰åå‘æ¦‚ç‡å®šä¹‰ï¼Œå¯ä»¥å°†è§‚æµ‹åºåˆ—æ¦‚ç‡P(Oâˆ£Î»)P(O vert lambda)P(Oâˆ£Î»)ç»Ÿä¸€å†™æˆ . P(Oâˆ£Î»)=âˆ‘i=1Nâˆ‘j=1NÎ±t(i)aijbj(ot+1)Î²t+1(j),t=1,2,â‹¯â€‰,Tâˆ’1P(O | lambda)= sum_{i=1}^{N} sum_{j=1}^{N} alpha_{t}(i) a_{i j} b_{j} left(o_{t+1} right) beta_{t+1}(j), quad t=1,2, cdots, T-1P(Oâˆ£Î»)=i=1âˆ‘Nâ€‹j=1âˆ‘Nâ€‹Î±tâ€‹(i)aijâ€‹bjâ€‹(ot+1â€‹)Î²t+1â€‹(j),t=1,2,â‹¯,Tâˆ’1 . 2.2 å­¦ä¹ é—®é¢˜çš„EMç®—æ³• . å®è´¨ä¸Šæ±‚ä¸€ä¸ªéšå˜é‡çš„æ¦‚ç‡æ¨¡å‹çš„å‚æ•°ä¼°è®¡ï¼š . P(Oâˆ£Î»)=âˆ‘IP(Oâˆ£I,Î»)P(Iâˆ£Î»)P(O | lambda)= sum_{I} P(O | I, lambda) P(I | lambda)P(Oâˆ£Î»)=Iâˆ‘â€‹P(Oâˆ£I,Î»)P(Iâˆ£Î») . å‚æ•°ä¼°è®¡ç”±EMç®—æ³•å®ç°: (å¾…ç»­) . è¾“å…¥ï¼šè§‚æµ‹æ•°æ®O=(o1,o2,â‹¯â€‰,oTO = (o_1, o_2, cdots, o_TO=(o1â€‹,o2â€‹,â‹¯,oTâ€‹; è¾“å‡ºï¼šéšé©¬å¯å¤«æ¨¡å‹å‚æ•° . ï¼ˆ1ï¼‰åˆå§‹åŒ– å¯¹ n=0ï¼Œ é€‰å–aij(0)a_{ij}^{(0)}aij(0)â€‹,bj(k)(0)b_{j}(k)^{(0)}bjâ€‹(k)(0),Ï€i(0) pi_{i}^{(0)}Ï€i(0)â€‹,å¾—åˆ°æ¨¡å‹Î»=(A(0),B(0),Ï€(0)) lambda = (A^{(0)}, B^{(0)}, pi^{(0)})Î»=(A(0),B(0),Ï€(0)) ï¼ˆ2ï¼‰é€’æ¨ å¯¹n=1,2,â‹¯â€‰,n=1,2, cdots,n=1,2,â‹¯, . aij(n+1)=âˆ‘t=1Tâˆ’1Î¾t(i,j)âˆ‘t=1Tâˆ’1Î³t(i)a_{i j}^{(n+1)}= frac{ sum_{t=1}^{T-1} xi_{t}(i, j)}{ sum_{t=1}^{T-1} gamma_{t}(i)}aij(n+1)â€‹=âˆ‘t=1Tâˆ’1â€‹Î³tâ€‹(i)âˆ‘t=1Tâˆ’1â€‹Î¾tâ€‹(i,j)â€‹ . bj(k)(n+1)=âˆ‘t=1,ot=vkTÎ³t(j)âˆ‘t=1TÎ³t(j)b_{j}(k)^{(n+1)}= frac{ sum_{t=1, o_{t}=v_{k}}^{T} gamma_{t}(j)}{ sum_{t=1}^{T} gamma_{t}(j)}bjâ€‹(k)(n+1)=âˆ‘t=1Tâ€‹Î³tâ€‹(j)âˆ‘t=1,otâ€‹=vkâ€‹Tâ€‹Î³tâ€‹(j)â€‹ . Ï€i(n+1)=Î³1(i) pi_{i}^{(n+1)}= gamma_{1}(i)Ï€i(n+1)â€‹=Î³1â€‹(i) . å…¶ä¸­ï¼Œæ—¶åˆ»tttå¤„äºqiq_iqiâ€‹ï¼Œä¸”æ—¶åˆ»t+1t+1t+1å¤„äºçŠ¶æ€qjq_jqjâ€‹çš„æ¦‚ç‡, è®° . Î¾t(i,j)=P(it=qi,it+1=qjâˆ£O,Î») xi_{t}(i, j)=P left(i_{t}=q_{i}, i_{t+1}=q_{j} | O, lambda right)Î¾tâ€‹(i,j)=P(itâ€‹=qiâ€‹,it+1â€‹=qjâ€‹âˆ£O,Î») . é‚£ä¹ˆ . Î¾t(i,j)=P(it=qi,it+1=qj,Oâˆ£Î»)P(Oâˆ£Î»)=P(it=qi,it+1=qj,Oâˆ£Î»)âˆ‘i=1Nâˆ‘j=1NP(it=qi,it+1=qj,Oâˆ£Î») xi_{t}(i, j)= frac{P left(i_{t}=q_{i}, i_{t+1}=q_{j}, O | lambda right)}{P(O | lambda)}= frac{P left(i_{t}=q_{i}, i_{t+1}=q_{j}, O | lambda right)}{ sum_{i=1}^{N} sum_{j=1}^{N} P left(i_{t}=q_{i}, i_{t+1}=q_{j}, O | lambda right)}Î¾tâ€‹(i,j)=P(Oâˆ£Î»)P(itâ€‹=qiâ€‹,it+1â€‹=qjâ€‹,Oâˆ£Î»)â€‹=âˆ‘i=1Nâ€‹âˆ‘j=1Nâ€‹P(itâ€‹=qiâ€‹,it+1â€‹=qjâ€‹,Oâˆ£Î»)P(itâ€‹=qiâ€‹,it+1â€‹=qjâ€‹,Oâˆ£Î»)â€‹ . å’Œæ—¶åˆ»tttå¤„äºqiq_iqiâ€‹çš„æ¦‚ç‡ï¼Œ æœ‰ . Î³t(i)=P(it=qiâˆ£O,Î»)=P(it=qi,Oâˆ£Î»)P(Oâˆ£Î») gamma_{t}(i)=P left(i_{t}=q_{i} | O, lambda right)= frac{P left(i_{t}=q_{i}, O | lambda right)}{P(O | lambda)}Î³tâ€‹(i)=P(itâ€‹=qiâ€‹âˆ£O,Î»)=P(Oâˆ£Î»)P(itâ€‹=qiâ€‹,Oâˆ£Î»)â€‹ . ï¼ˆ3ï¼‰ç»ˆæ­¢ . å¾—åˆ°æ¨¡å‹å‚æ•°Î»(n+1)=(A(n+1),B(n+1),Ï€(n+1)) lambda^{(n+1)} = (A^{(n+1)}, B^{(n+1)}, pi^{(n+1)})Î»(n+1)=(A(n+1),B(n+1),Ï€(n+1)) . 2.3 é¢„æµ‹ç®—æ³• . åŒ…æ‹¬è¿‘ä¼¼ç®—æ³•å’Œç»´ç‰¹æ¯”ç®—æ³•ï¼ˆViterbi algorithmï¼‰ . 2.3.1 è¿‘ä¼¼ç®—æ³• . åœ¨æ¯ä¸ªæ—¶åˆ»tttï¼Œ é€‰æ‹©åœ¨è¯¥æ—¶åˆ»æœ€å¯èƒ½å‡ºç°çš„çŠ¶æ€itâˆ—i^*_titâˆ—â€‹ï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªçŠ¶æ€åºåˆ—Iâˆ—=(iiâˆ—,iiâˆ—,â‹¯â€‰,iTâˆ—)I^* = (i^*_i, i^*_i, cdots, i^*_T)Iâˆ—=(iiâˆ—â€‹,iiâˆ—â€‹,â‹¯,iTâˆ—â€‹)ï¼Œå°†å®ƒæœ€ä¸ºé¢„æµ‹ç»“æœã€‚ . ç»™å®šæ¨¡å‹Î» lambdaÎ»å’Œè§‚æµ‹åºåˆ—OOOï¼Œ åœ¨æ—¶åˆ»tttå¤„äºçŠ¶æ€qiq_iqiâ€‹çš„æ¦‚ç‡Î³t(i) gamma_t(i)Î³tâ€‹(i)æ˜¯ . Î³t(i)=Î±t(i)Î²t(i)P(Oâˆ£Î»)=Î±t(i)Î²t(i)âˆ‘j=1NÎ±t(j)Î²t(j) gamma_{t}(i)= frac{ alpha_{t}(i) beta_{t}(i)}{P(O | lambda)}= frac{ alpha_{t}(i) beta_{t}(i)}{ sum_{j=1}^{N} alpha_{t}(j) beta_{t}(j)}Î³tâ€‹(i)=P(Oâˆ£Î»)Î±tâ€‹(i)Î²tâ€‹(i)â€‹=âˆ‘j=1Nâ€‹Î±tâ€‹(j)Î²tâ€‹(j)Î±tâ€‹(i)Î²tâ€‹(i)â€‹ . è€Œæ¯ä¸€æ—¶åˆ»tttæœ€æœ‰å¯èƒ½çš„çŠ¶æ€itâˆ—i_{t}^{*}itâˆ—â€‹æ˜¯ . itâˆ—=argâ¡maxâ¡1â©½iâ©½N[Î³t(i)],t=1,2,â‹¯â€‰,Ti_{t}^{*}= arg max _{1 leqslant i leqslant N} left[ gamma_{t}(i) right], quad t=1,2, cdots, Titâˆ—â€‹=arg1â©½iâ©½Nmaxâ€‹[Î³tâ€‹(i)],t=1,2,â‹¯,T . ä»è€Œå¾—åˆ°çŠ¶æ€åºåˆ—Iâˆ—=(iiâˆ—,iiâˆ—,â‹¯â€‰,iTâˆ—)I^* = (i^*_i, i^*_i, cdots, i^*_T)Iâˆ—=(iiâˆ—â€‹,iiâˆ—â€‹,â‹¯,iTâˆ—â€‹) . ç¼ºç‚¹ï¼š ä¸èƒ½ä¿è¯é¢„æµ‹çŠ¶æ€åºåˆ—æ•´ä½“æ˜¯æœ€æœ‰å¯èƒ½çš„çŠ¶æ€åºåˆ—ï¼Œå› ä¸ºé¢„æµ‹çš„çŠ¶æ€åºåˆ—å®é™…å¯èƒ½ç”±ä¸å‘ç”Ÿçš„éƒ¨åˆ†ã€‚ . 2.3.2 ç»´ç‰¹æ¯”ç®—æ³• . å®è´¨æ˜¯è¿ç”¨åŠ¨æ€è§„åˆ’æ±‚æ¦‚ç‡æœ€å¤§è·¯å¾„ï¼Œä»è€Œè§£å†³HMMçš„é¢„æµ‹é—®é¢˜ . åªéœ€ä»æ—¶åˆ»t=1t=1t=1å¼€å§‹ï¼Œé€’æ¨åœ°è®¡ç®—åœ¨æ—¶åˆ»tttçŠ¶æ€ä¸ºqiq_iqiâ€‹çš„å„æ¡éƒ¨åˆ†è·¯å¾„çš„æœ€å¤§æ¦‚ç‡ï¼Œç›´è‡³å¾—åˆ°æ—¶åˆ»t=Tt = Tt=TçŠ¶æ€ä¸ºiiiçš„å„æ¡è·¯å¾„çš„æœ€å¤§æ¦‚ç‡ã€‚æ—¶åˆ» t=Tt = Tt=T çš„æœ€å¤§æ¦‚ç‡å³ä¸ºæœ€ä¼˜è·¯å¾„çš„æ¦‚ç‡Pâˆ—P^*Pâˆ—, æœ€ä¼˜è·¯å¾„çš„ç»ˆç»“ç‚¹iTâˆ—i^*_TiTâˆ—â€‹ä¹ŸåŒæ—¶å¾—åˆ°ã€‚ä¹‹åï¼Œä¸ºäº†æ‰¾å‡ºæœ€ä¼˜è·¯å¾„çš„å„ä¸ªç»“ç‚¹ï¼Œä»ç»ˆç»“ç‚¹iTâˆ—i^*_TiTâˆ—â€‹å¼€å§‹ï¼Œç”±åå‘å‰é€æ­¥æ±‚å¾—ç»“ç‚¹ iTâˆ’1âˆ—,â‹¯â€‰,i1âˆ—i^*_{T-1}, cdots, i^*_1iTâˆ’1âˆ—â€‹,â‹¯,i1âˆ—â€‹ï¼Œå¾—åˆ°æœ€ä¼˜è·¯å¾„ Iâˆ—=(iiâˆ—,iiâˆ—,â‹¯â€‰,iTâˆ—)I^* = (i^*_i, i^*_i, cdots, i^*_T)Iâˆ—=(iiâˆ—â€‹,iiâˆ—â€‹,â‹¯,iTâˆ—â€‹)ã€‚è¿™å°±æ˜¯ç»´ç‰¹æ¯”ç®—æ³•ã€‚ . å®šä¹‰åœ¨æ—¶åˆ»tçŠ¶æ€içš„æ‰€æœ‰å•ä¸ªè·¯å¾„ä¸­æ¦‚ç‡æœ€å¤§å€¼ä¸º . Î´t(i)=maxâ¡i1,i2,â‹¯â€‰,itâˆ’1P(it=i,itâˆ’1,â‹¯â€‰,i1,ot,â‹¯â€‰,o1âˆ£Î»),i=1,2,â‹¯â€‰,N delta_{t}(i)= max _{i_{1}, i_{2}, cdots, i_{t-1}} P left(i_{t}=i, i_{t-1}, cdots, i_{1}, o_{t}, cdots, o_{1} | lambda right), quad i=1,2, cdots, NÎ´tâ€‹(i)=i1â€‹,i2â€‹,â‹¯,itâˆ’1â€‹maxâ€‹P(itâ€‹=i,itâˆ’1â€‹,â‹¯,i1â€‹,otâ€‹,â‹¯,o1â€‹âˆ£Î»),i=1,2,â‹¯,N . å› æ­¤ . Î´t+1(i)=maxâ¡i1,i2,â‹¯â€‰,itP(it+1=i,it,â‹¯â€‰,i1,ot+1,â‹¯â€‰,o1âˆ£Î»)=maxâ¡1â©½jâ©½N[Î´t(j)aji]bi(ot+1),i=1,2,â‹¯â€‰,N;t=1,2,â‹¯â€‰,Tâˆ’1 begin{aligned} delta_{t+1}(i) &amp;= max _{i_{1}, i_{2}, cdots, i_{t}} P left(i_{t+1}=i, i_{t}, cdots, i_{1}, o_{t+1}, cdots, o_{1} | lambda right) &amp;= max _{1 leqslant j leqslant N} left[ delta_{t}(j) a_{j i} right] b_{i} left(o_{t+1} right), quad i=1,2, cdots, N ; quad t=1,2, cdots, T-1 end{aligned}Î´t+1â€‹(i)â€‹=i1â€‹,i2â€‹,â‹¯,itâ€‹maxâ€‹P(it+1â€‹=i,itâ€‹,â‹¯,i1â€‹,ot+1â€‹,â‹¯,o1â€‹âˆ£Î»)=1â©½jâ©½Nmaxâ€‹[Î´tâ€‹(j)ajiâ€‹]biâ€‹(ot+1â€‹),i=1,2,â‹¯,N;t=1,2,â‹¯,Tâˆ’1â€‹ . å®šä¹‰åœ¨æ—¶åˆ»tçŠ¶æ€içš„æ‰€æœ‰å•ä¸ªè·¯å¾„ä¸­æ¦‚ç‡æœ€å¤§è·¯å¾„çš„ç¬¬t-1Â ä¸ªèŠ‚ç‚¹ä¸º . Î¨t(i)=argâ¡maxâ¡1â©½jâ©½N[Î´tâˆ’1(j)aji],i=1,2,â‹¯â€‰,N Psi_{t}(i)= arg max _{1 leqslant j leqslant N} left[ delta_{t-1}(j) a_{j i} right], quad i=1,2, cdots, NÎ¨tâ€‹(i)=arg1â©½jâ©½Nmaxâ€‹[Î´tâˆ’1â€‹(j)ajiâ€‹],i=1,2,â‹¯,N . è¾“å…¥ï¼š éšé©¬å¯å¤«æ¨¡å‹ Î» lambdaÎ»ï¼Œ è§‚æµ‹åºåˆ—OOO; è¾“å‡ºï¼š æœ€ä¼˜è·¯å¾„Iâˆ—=(iiâˆ—,iiâˆ—,â‹¯â€‰,iTâˆ—)I^* = (i^*_i, i^*_i, cdots, i^*_T)Iâˆ—=(iiâˆ—â€‹,iiâˆ—â€‹,â‹¯,iTâˆ—â€‹) . ï¼ˆ1ï¼‰åˆå§‹åŒ– . Î´1(i)=Ï€ibi(o1),i=1,2,â‹¯â€‰,NÎ¨1(i)=0,i=1,2,â‹¯â€‰,N begin{array}{c} delta_{1}(i)= pi_{i} b_{i} left(o_{1} right), quad i=1,2, cdots, N Psi_{1}(i)=0, quad i=1,2, cdots, N end{array}Î´1â€‹(i)=Ï€iâ€‹biâ€‹(o1â€‹),i=1,2,â‹¯,NÎ¨1â€‹(i)=0,i=1,2,â‹¯,Nâ€‹ . ï¼ˆ2ï¼‰é€’æ¨ . Î´t(i)=maxâ¡1â©½jâ©½N[Î´tâˆ’1(j)aji]bi(ot),i=1,2,â‹¯â€‰,NÎ¨t(i)=argâ¡maxâ¡1â©½jâ©½N[Î´tâˆ’1(j)aji],i=1,2,â‹¯â€‰,N begin{array}{c} delta_{t}(i)= max _{1 leqslant j leqslant N} left[ delta_{t-1}(j) a_{j i} right] b_{i} left(o_{t} right), quad i=1,2, cdots, N Psi_{t}(i)= arg max _{1 leqslant j leqslant N} left[ delta_{t-1}(j) a_{j i} right], quad i=1,2, cdots, N end{array}Î´tâ€‹(i)=max1â©½jâ©½Nâ€‹[Î´tâˆ’1â€‹(j)ajiâ€‹]biâ€‹(otâ€‹),i=1,2,â‹¯,NÎ¨tâ€‹(i)=argmax1â©½jâ©½Nâ€‹[Î´tâˆ’1â€‹(j)ajiâ€‹],i=1,2,â‹¯,Nâ€‹ . ï¼ˆ3ï¼‰ç»ˆæ­¢ . Pâˆ—=maxâ¡1â©½iâ©½NÎ´T(i)iTâˆ—=argâ¡maxâ¡1â©½iâ©½N[Î´T(i)] begin{array}{c} P^* = max _{1 leqslant i leqslant N} delta_T(i) i^*_T = arg max _{1 leqslant i leqslant N} [ delta_T(i)] end{array}Pâˆ—=max1â©½iâ©½Nâ€‹Î´Tâ€‹(i)iTâˆ—â€‹=argmax1â©½iâ©½Nâ€‹[Î´Tâ€‹(i)]â€‹ . ï¼ˆ4ï¼‰æœ€ä¼˜è·¯å¾„å›æº¯ å¯¹t=Tâˆ’1,Tâˆ’2,â‹¯â€‰,1t=T-1, T-2, cdots, 1t=Tâˆ’1,Tâˆ’2,â‹¯,1 . itâˆ—=Î¨t+1(it+1âˆ—)i^*_t = Psi_{t+1}(i^*_{t+1})itâˆ—â€‹=Î¨t+1â€‹(it+1âˆ—â€‹) . å‚è€ƒï¼š æèˆªã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹ .",
            "url": "https://zqfang.github.io/machine%20learning/2020/05/03/ML-HMM.html",
            "relUrl": "/machine%20learning/2020/05/03/ML-HMM.html",
            "date": " â€¢ May 3, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Latent semantic analysis (LSA)",
            "content": "æ½œåœ¨è¯­ä¹‰åˆ†æï¼ˆLSAï¼‰æ˜¯ä¸€ç§éç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºæ–‡æœ¬è¯é¢˜åˆ†æã€‚å…¶ç‰¹ç‚¹æ˜¯é€šè¿‡çŸ©é˜µåˆ†è§£å‘ç°æ–‡æœ¬äºå•è¯ä¹‹é—´çš„åŸºäºè¯é¢˜çš„è¯­ä¹‰å…³ç³»ã€‚ . æ½œåœ¨è¯­ä¹‰åˆ†ææ˜¯ä¸€ç§éæ¦‚ç‡è¯é¢˜åˆ†ææ¨¡å‹ã€‚æ­¥éª¤ä¸ºï¼š . å…ˆå°†æ–‡æœ¬é›†åˆè¡¨ç¤ºä¸ºå•è¯-æ–‡æœ¬çŸ©é˜µ | å¯¹å•è¯-æ–‡æœ¬çŸ©é˜µè¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰æˆ–éè´ŸçŸ©é˜µåˆ†è§£ï¼ˆNMFï¼‰ï¼Œä»è€Œå¾—åˆ°è¯é¢˜å‘é‡ç©ºé—´ï¼Œä»¥åŠæ–‡æœ¬åœ¨è¯é¢˜å‘é‡ç©ºé—´çš„è¡¨ç¤º | 1. å•è¯å‘é‡ç©ºé—´ . ç»™å®šç”±nä¸ªæ–‡æœ¬çš„é›†åˆ D={d1,d2,â‹¯â€‰,dn}D = {d_1, d_2, cdots, d_n }D={d1â€‹,d2â€‹,â‹¯,dnâ€‹} ,ä»¥åŠæ‰€æœ‰æ–‡æœ¬ä¸­å•è¯é›† W={w1,w2,â‹¯â€‰,wm}W = {w_1, w_2, cdots, w_m }W={w1â€‹,w2â€‹,â‹¯,wmâ€‹}, åˆ™å•è¯åœ¨æ–‡æœ¬ä¸­å‡ºç°çš„æ•°æ®ç”¨å•è¯-æ–‡æœ¬çŸ©é˜µ(word-document matrix)è¡¨ç¤º, è®°ä¸ºï¼š . X=[x11x12â‹¯x1nx21x22â‹¯x2nâ‹®â‹®â‹®xm1xm2â‹¯xmn]X= left[ begin{array}{cccc} x_{11} &amp; x_{12} &amp; cdots &amp; x_{1 n} x_{21} &amp; x_{22} &amp; cdots &amp; x_{2 n} vdots &amp; vdots &amp; &amp; vdots x_{m 1} &amp; x_{m 2} &amp; cdots &amp; x_{m n} end{array} right]X=â£â¢â¢â¢â¢â¡â€‹x11â€‹x21â€‹â‹®xm1â€‹â€‹x12â€‹x22â€‹â‹®xm2â€‹â€‹â‹¯â‹¯â‹¯â€‹x1nâ€‹x2nâ€‹â‹®xmnâ€‹â€‹â¦â¥â¥â¥â¥â¤â€‹ . å…¶ä¸­ï¼Œ xijx_{ij}xijâ€‹è¡¨ç¤ºå•è¯wiw_iwiâ€‹åœ¨æ–‡æœ¬djd_jdjâ€‹ä¸­å‡ºç°çš„é¢‘æ•°æˆ–æƒå€¼ã€‚ç”±äºå•è¯ç§ç±»å¤šï¼Œè€Œæ¯ä¸ªæ–‡æœ¬ä¸­å•è¯ç§ç±»é€šå¸¸å°‘ï¼Œæ‰€ä»¥ä¸ºå•è¯-æ–‡æœ¬çŸ©é˜µæ˜¯ç¨€ç–çŸ©é˜µã€‚ . æƒå€¼å¸¸ç”¨å•è¯é¢‘ç‡-é€†æ–‡æœ¬é¢‘ç‡(term frequeny-inverse document frequency, TF-IDF)è¡¨ç¤º: . TFIDFij=tfijtfâ‹…jlogâ¡dfdfi,i=1,2,â‹¯â€‰,m;j=1,2,â‹¯â€‰,n mathrm{TFIDF}_{i j}= frac{ mathrm{tf}_{i j}}{ mathrm{tf}_{ cdot j}} log frac{ mathrm{df}}{ mathrm{df}_{i}}, quad i=1,2, cdots, m ; quad j=1,2, cdots, nTFIDFijâ€‹=tfâ‹…jâ€‹tfijâ€‹â€‹logdfiâ€‹dfâ€‹,i=1,2,â‹¯,m;j=1,2,â‹¯,n . å…¶ä¸­ï¼Œ tfij mathrm{tf}_{ij}tfijâ€‹ æ˜¯å•è¯wiw_iwiâ€‹åœ¨æ–‡æœ¬djd_jdjâ€‹ä¸­çš„é¢‘æ•°; tfâ‹…j mathrm{tf}_{ cdot j}tfâ‹…jâ€‹ æ˜¯djd_jdjâ€‹ä¸­å‡ºç°çš„æ‰€æœ‰å•è¯é¢‘æ•°ä¹‹å’Œ; dfi mathrm{df}_{i}dfiâ€‹ æ˜¯å«æœ‰å•è¯wiw_iwiâ€‹çš„æ–‡æœ¬æ•°; df mathrm{df}df æ˜¯æ–‡æœ¬é›†åˆDDDçš„å…¨éƒ¨æ–‡æœ¬æ•°ã€‚ . å•è¯-æ–‡æœ¬çŸ©é˜µçš„ç¬¬jåˆ—å‘é‡xjx_jxjâ€‹è¡¨ç¤ºæ–‡æœ¬djd_jdjâ€‹ï¼š . xj=[x1jx2jâ‹®xmj],j=1,2,â‹¯â€‰,nx_{j}= left[ begin{array}{c} x_{1 j} x_{2 j} vdots x_{m j} end{array} right], quad j=1,2, cdots, nxjâ€‹=â£â¢â¢â¢â¢â¡â€‹x1jâ€‹x2jâ€‹â‹®xmjâ€‹â€‹â¦â¥â¥â¥â¥â¤â€‹,j=1,2,â‹¯,n . ä¸¤ä¸ªå•è¯å‘é‡å†…ç§¯æˆ–è€…æ ‡å‡†åŒ–å†…ç§¯è¡¨ç¤ºå¯¹åº”æ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œ å› æ­¤æ–‡æœ¬did_idiâ€‹ä¸djd_jdjâ€‹çš„ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼š . xiâ‹…xj,xiâ‹…xjâˆ¥xiâˆ¥âˆ¥xjâˆ¥x_{i} cdot x_{j}, quad frac{x_{i} cdot x_{j}}{ left |x_{i} right | left |x_{j} right |}xiâ€‹â‹…xjâ€‹,âˆ¥xiâ€‹âˆ¥âˆ¥xjâ€‹âˆ¥xiâ€‹â‹…xjâ€‹â€‹ . å•è¯å‘é‡ç©ºé—´æ¨¡å‹çš„ä¼˜ç¼ºç‚¹ . ä¼˜ç‚¹ï¼š ç®€å•ï¼Œè®¡ç®—é«˜æ•ˆ | ç¼ºç‚¹ï¼š ä¸€è¯å¤šä¹‰(polysemy)å’Œå¤šè¯ä¸€ä¹‰(synonymy)ä¸èƒ½å¾ˆå¥½å¤„ç†ï¼Œ å­˜åœ¨ç›¸ä¼¼åº¦è®¡ç®—ä¸å‡†ç¡®çš„é—®é¢˜ | . 2. è¯é¢˜ï¼ˆtopicï¼‰å‘é‡ç©ºé—´ . è¯é¢˜æ²¡æœ‰ä¸¥æ ¼çš„å®šä¹‰ï¼Œæ˜¯æŒ‡æ–‡æœ¬è®¨è®ºçš„å†…å®¹æˆ–ä¸»é¢˜ã€‚è€ŒåŸºäºè¯é¢˜çš„æ¨¡å‹æ˜¯ä¸ºäº†è§£å†³å•è¯å‘é‡ä¸èƒ½å¾ˆå¥½å¤„ç†ä¸€è¯å¤šä¹‰(polysemy)å’Œå¤šè¯ä¸€ä¹‰(synonymy)çš„é—®é¢˜ã€‚ . ç»™å®šç”±nä¸ªæ–‡æœ¬çš„é›†åˆ D={d1,d2,â‹¯â€‰,dn}D = {d_1, d_2, cdots, d_n }D={d1â€‹,d2â€‹,â‹¯,dnâ€‹}, ä»¥åŠæ‰€æœ‰æ–‡æœ¬ä¸­å•è¯é›† W={w1,w2,â‹¯â€‰,wm}W = {w_1, w_2, cdots, w_m }W={w1â€‹,w2â€‹,â‹¯,wmâ€‹}, åˆ™å•è¯åœ¨æ–‡æœ¬ä¸­å‡ºç°çš„æ•°æ®ç”¨å•è¯-æ–‡æœ¬çŸ©é˜µ(word-document matrix)è¡¨ç¤º, è®°ä¸ºï¼š . X=[x11x12â‹¯x1nx21x22â‹¯x2nâ‹®â‹®â‹®xm1xm2â‹¯xmn]X= left[ begin{array}{cccc} x_{11} &amp; x_{12} &amp; cdots &amp; x_{1 n} x_{21} &amp; x_{22} &amp; cdots &amp; x_{2 n} vdots &amp; vdots &amp; &amp; vdots x_{m 1} &amp; x_{m 2} &amp; cdots &amp; x_{m n} end{array} right]X=â£â¢â¢â¢â¢â¡â€‹x11â€‹x21â€‹â‹®xm1â€‹â€‹x12â€‹x22â€‹â‹®xm2â€‹â€‹â‹¯â‹¯â‹¯â€‹x1nâ€‹x2nâ€‹â‹®xmnâ€‹â€‹â¦â¥â¥â¥â¥â¤â€‹ . å‡è®¾æ‰€æœ‰æ–‡æœ¬å«æœ‰kä¸ªè¯é¢˜ï¼Œ æ¯ä¸ªè¯é¢˜lç”±ä¸€ä¸ªå®šä¹‰åœ¨å•è¯é›†åˆWå‚»å§‘å¨˜çš„mç»´å‘é‡ï¼š . tl=[t1lt2lâ‹®tml],l=1,2,â‹¯â€‰,kt_{l}= left[ begin{array}{c} t_{1 l} t_{2 l} vdots t_{m l} end{array} right], quad l=1,2, cdots, ktlâ€‹=â£â¢â¢â¢â¢â¡â€‹t1lâ€‹t2lâ€‹â‹®tmlâ€‹â€‹â¦â¥â¥â¥â¥â¤â€‹,l=1,2,â‹¯,k . åˆ™æœ‰è¯é¢˜å‘é‡çŸ©é˜µTï¼š . T=[t11t12â‹¯t1kt21t22â‹¯t2kâ‹®â‹®â‹®tm1tm2â‹¯tmk]T= left[ begin{array}{cccc} t_{11} &amp; t_{12} &amp; cdots &amp; t_{1 k} t_{21} &amp; t_{22} &amp; cdots &amp; t_{2 k} vdots &amp; vdots &amp; &amp; vdots t_{m 1} &amp; t_{m 2} &amp; cdots &amp; t_{m k} end{array} right]T=â£â¢â¢â¢â¢â¡â€‹t11â€‹t21â€‹â‹®tm1â€‹â€‹t12â€‹t22â€‹â‹®tm2â€‹â€‹â‹¯â‹¯â‹¯â€‹t1kâ€‹t2kâ€‹â‹®tmkâ€‹â€‹â¦â¥â¥â¥â¥â¤â€‹ . 3. æ–‡æœ¬åœ¨è¯é¢˜å‘é‡ç©ºé—´çš„è¡¨ç¤º . æ–‡æœ¬djd_jdjâ€‹åœ¨å•è¯å‘é‡ç©ºé—´ç”¨xjx_jxjâ€‹è¡¨ç¤ºï¼Œå°†xjx_jxjâ€‹æŠ•å½±åˆ°è¯é¢˜å‘é‡ç©ºé—´TTTï¼Œå¾—åˆ°è¯é¢˜å‘é‡ç©ºé—´yjy_jyjâ€‹ï¼Œyjy_jyjâ€‹æ˜¯ä¸ªkç»´å‘é‡: . yj=[y1jy2jâ‹®ykj],j=1,2,â‹¯â€‰,ny_{j}= left[ begin{array}{c} y_{1 j} y_{2 j} vdots y_{k j} end{array} right], quad j=1,2, cdots, nyjâ€‹=â£â¢â¢â¢â¢â¡â€‹y1jâ€‹y2jâ€‹â‹®ykjâ€‹â€‹â¦â¥â¥â¥â¥â¤â€‹,j=1,2,â‹¯,n ã€€ . å› æ­¤ï¼Œè¯é¢˜-æ–‡æœ¬çŸ©é˜µYYYï¼š . Y=[y11y12â‹¯y1ny21y22â‹¯y2nâ‹®â‹®â‹®yk1yk2â‹¯ykn]Y= left[ begin{array}{cccc} y_{11} &amp; y_{12} &amp; cdots &amp; y_{1 n} y_{21} &amp; y_{22} &amp; cdots &amp; y_{2 n} vdots &amp; vdots &amp; &amp; vdots y_{k 1} &amp; y_{k 2} &amp; cdots &amp; y_{k n} end{array} right]Y=â£â¢â¢â¢â¢â¡â€‹y11â€‹y21â€‹â‹®yk1â€‹â€‹y12â€‹y22â€‹â‹®yk2â€‹â€‹â‹¯â‹¯â‹¯â€‹y1nâ€‹y2nâ€‹â‹®yknâ€‹â€‹â¦â¥â¥â¥â¥â¤â€‹ . 4. å•è¯å‘é‡ç©ºé—´åˆ°è¯é¢˜å‘é‡ç©ºé—´è½¬æ¢ . å› æ­¤ï¼Œå•è¯å‘é‡ç©ºé—´çš„æ–‡æœ¬å‘é‡xjx_jxjâ€‹ï¼Œå¯ä»¥ç”¨è¯é¢˜å‘é‡ç©ºé—´yjy_jyjâ€‹è¿‘ä¼¼è¡¨ç¤ºï¼Œå³ä»¥kä¸ªè¯é¢˜å‘é‡ä»¥yjy_jyjâ€‹ä¸ºç³»æ•°çš„çº¿æ€§ç»„åˆï¼š . xjâ‰ˆy1jt1+y2jt2+â‹¯+ykjtk,j=1,2,â‹¯â€‰,nx_{j} approx y_{1 j} t_{1}+y_{2 j} t_{2}+ cdots+y_{k j} t_{k}, quad j=1,2, cdots, nxjâ€‹â‰ˆy1jâ€‹t1â€‹+y2jâ€‹t2â€‹+â‹¯+ykjâ€‹tkâ€‹,j=1,2,â‹¯,n . çŸ©é˜µå½¢å¼ï¼š . Xâ‰ˆTYX approx TYXâ‰ˆTY . è¿™å°±æ˜¯æ½œåœ¨è¯­ä¹‰åˆ†æã€‚ . å‚è€ƒï¼š æèˆªã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹ .",
            "url": "https://zqfang.github.io/machine%20learning/2020/04/30/ML-LSA.html",
            "relUrl": "/machine%20learning/2020/04/30/ML-LSA.html",
            "date": " â€¢ Apr 30, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Conditional random field (CRF)",
            "content": "CRFæ¡ä»¶éšæœºåœºï¼Œå¯åº”ç”¨äºæ ‡æ³¨é—®é¢˜ . æ¦‚ç‡æ— å‘å›¾æ¨¡å‹Probabilistic undirected graphical model(Markov random field) æ˜¯ä¸€ä¸ªå¯ä»¥ç”±æ— å‘å›¾è¡¨ç¤ºçš„è”åˆæ¦‚ç‡åˆ†å¸ƒ . 0. æ¦‚ç‡å›¾æ¨¡å‹åˆ†ç±» . æ¦‚ç‡å›¾æ¨¡å‹ï¼ˆprobabilistic graphical model, PGMï¼‰ï¼Œæ˜¯ä¸€ç§å­¦ä¹ ä»»åŠ¡çš„æ¡†æ¶æè¿°ï¼Œå®ƒå°†å­¦ä¹ ä»»åŠ¡å½’ç»“ä¸ºè®¡ç®—å˜é‡çš„æ¦‚ç‡åˆ†å¸ƒã€‚ . æŒ‰ç…§æ¦‚ç‡å›¾ä¸­å˜é‡å…³ç³»çš„ä¸åŒï¼Œæ¦‚ç‡å›¾æ¨¡å‹å¯ä»¥å¤§è‡´åˆ†ä¸ºä¸¤ç±»ï¼š . è´å¶æ–¯ç½‘ç»œï¼šæœ‰å‘å›¾æ¨¡å‹ï¼Œä½¿ç”¨æœ‰å‘æ— ç¯å›¾è¡¨è¾¾å…³ç³»ï¼ˆé€šå¸¸ï¼Œå˜é‡é—´å­˜åœ¨æ˜¾å¼çš„å› æœå…³ç³»ï¼‰ | é©¬å°”ç§‘å¤«ç½‘ç»œï¼šæ— å‘å›¾æ¨¡å‹ï¼Œä½¿ç”¨æ— å›¾è¡¨è¾¾å…³ç³»ï¼ˆé€šå¸¸ï¼Œå˜é‡é—´å­˜æœ‰å…³ç³»ï¼Œä½†æ˜¯éš¾ä»¥æ˜¾å¼è¡¨è¾¾ï¼‰ | åŒæ—¶å­˜æœ‰æœ‰å‘è¾¹å’Œæ— å‘è¾¹çš„æ¨¡å‹ï¼Œå¦‚æ¡ä»¶éšæœºåœºï¼ˆconditional random fieldï¼‰å’Œé“¾å›¾ï¼ˆchain graphï¼‰ï¼Œå•ç‹¬çœ‹åšä¸€ç±»å±€éƒ¨æœ‰å‘æ¨¡å‹ã€‚ | è´å¶æ–¯ç½‘ç»œ . å¯ä»¥åˆ†ä¸ºé™æ€è´å¶æ–¯ç½‘ç»œå’ŒåŠ¨æ€è´å¶æ–¯ç½‘ç»œã€‚ç›¸æ¯”äºé™æ€è´å¶æ–¯ç½‘ç»œï¼ŒåŠ¨æ€ï¼ˆdynamicï¼‰è´å¶æ–¯ç½‘ç»œä¸»è¦ç”¨äºæ—¶åºæ•°æ®å»ºæ¨¡ï¼ˆå¦‚è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è½¨è¿¹æ•°æ®æŒ–æ˜ç­‰ï¼‰ã€‚å…¶ä¸­ï¼Œä¸€ç§ç»“æ„æœ€ç®€å•çš„åŠ¨æ€è´å¶æ–¯ç½‘ç»œå°±æ˜¯éšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆhidden markov model, HMMï¼‰ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œè´å¶æ–¯ç½‘ç»œä¸­æ¯ä¸€ä¸ªç»“ç‚¹éƒ½å¯¹åº”äºä¸€ä¸ªå…ˆéªŒæ¦‚ç‡åˆ†å¸ƒæˆ–è€…æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼Œå› æ­¤æ•´ä½“çš„è”åˆåˆ†å¸ƒå¯ä»¥ç›´æ¥åˆ†è§£ä¸ºæ‰€æœ‰å•ä¸ªç»“ç‚¹æ‰€å¯¹åº”çš„åˆ†å¸ƒçš„ä¹˜ç§¯ã€‚ | . é©¬å°”å¯å¤«ç½‘ . ç”±äºå˜é‡ä¹‹é—´æ²¡æœ‰æ˜ç¡®çš„å› æœå…³ç³»ï¼Œå®ƒçš„è”åˆæ¦‚ç‡åˆ†å¸ƒé€šå¸¸ä¼šè¡¨è¾¾ä¸ºä¸€ç³»åˆ—åŠ¿å‡½æ•°ï¼ˆpotential functionï¼‰çš„ä¹˜ç§¯ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œè¿™äº›ä¹˜ç§¯çš„ç§¯åˆ†å¹¶ä¸ç­‰äº1ï¼Œå› æ­¤ï¼Œè¿˜è¦å¯¹å…¶è¿›è¡Œå½’ä¸€åŒ–æ‰èƒ½å½¢æˆä¸€ä¸ªæœ‰æ•ˆçš„æ¦‚ç‡åˆ†å¸ƒâ€”â€”è¿™ä¸€ç‚¹å¾€å¾€åœ¨å®é™…åº”ç”¨ä¸­ç»™å‚æ•°ä¼°è®¡é€ æˆéå¸¸å¤§çš„å›°éš¾ã€‚ | . æŒ‰ç…§è¡¨ç¤ºçš„æŠ½è±¡çº§åˆ«ä¸åŒï¼Œæ¦‚ç‡å›¾æ¨¡å‹å¯ä»¥åˆ†ä¸ºï¼š . åŸºäºéšæœºå˜é‡çš„æ¦‚ç‡å›¾æ¨¡å‹ï¼Œå¦‚è´å¶æ–¯ç½‘ã€é©¬å°”å¯å¤«ç½‘ã€æ¡ä»¶éšæœºåœºå’Œé“¾å›¾ç­‰ | åŸºäºæ¨¡æ¿çš„æ¦‚ç‡å›¾æ¨¡å‹ï¼è¿™ç±»æ¨¡å‹æ ¹æ®åº”ç”¨åœºæ™¯ä¸åŒåˆå¯åˆ†ä¸ºä¸¤ç§ï¼š æš‚æ€æ¨¡å‹ï¼ŒåŒ…æ‹¬åŠ¨æ€è´å¶æ–¯ç½‘ï¼ˆDynamic Bayesian Network, DBNï¼‰å’ŒçŠ¶æ€è§‚æµ‹æ¨¡å‹ï¼Œå…¶ä¸­çŠ¶æ€è§‚æµ‹æ¨¡å‹åˆåŒ…æ‹¬çº¿æ€§åŠ¨æ€ç³»ç»Ÿï¼ˆLinear Dynamic System, LDSï¼‰å¦‚å¡å°”æ›¼æ»¤æ³¢å™¨ï¼Œè¿˜æœ‰éšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHidden Markov Model, HMMï¼‰ï¼› | å¯¹è±¡å…³ç³»é¢†åŸŸçš„æ¦‚ç‡å›¾æ¨¡å‹ï¼ŒåŒ…æ‹¬ç›˜æ¨¡å‹ï¼ˆPlate Modelï¼ŒPMï¼‰ã€æ¦‚ç‡å…³ç³»æ¨¡å‹ï¼ˆProbabilistic Relational Model, PRMï¼‰å’Œå…³ç³»é©¬å°”å¯å¤«ç½‘ï¼ˆRelational Markov Network, RMNï¼‰ã€‚ | . | . å‚è€ƒï¼šæ¦‚ç‡å›¾æ¨¡å‹æ€»è§ˆ . 1. æ¨¡å‹å®šä¹‰ . æ¦‚ç‡å›¾æ¨¡å‹ï¼šç”±å›¾ï¼ˆGraphï¼‰è¡¨ç¤ºçš„æ¦‚ç‡åˆ†å¸ƒã€‚ . ä»¤æ— å‘å›¾ G = (V, E) è¡¨ç¤ºè”åˆæ¦‚ç‡åˆ†å¸ƒP(Y)ï¼Œå³Gä¸­ï¼Œ . èŠ‚ç‚¹vâˆˆVv in VvâˆˆV è¡¨ç¤ºéšæœºå˜é‡Yv,Y=(Yv)vâˆˆVY_{v}, Y= left(Y_{v} right)_{v in V}Yvâ€‹,Y=(Yvâ€‹)vâˆˆVâ€‹ï¼› | è¾¹eâˆˆEe in EeâˆˆEè¡¨ç¤ºéšæœºå˜é‡ä¹‹é—´çš„æ¦‚ç‡ä¾èµ–å…³ç³» | . æ— å‘å›¾è¡¨ç¤ºçš„éšæœºå˜é‡å­˜åœ¨: . æˆå¯¹é©¬å¯å¤«æ€§ pariwise Markov property æŒ‡ä»»æ„ä¸¤ä¸ªæ²¡æœ‰è¾¹è¿æ¥çš„èŠ‚ç‚¹ï¼Œåœ¨ç»™å®šéšæœºå˜é‡ç»„ï¼ˆå…¶ä»–æ‰€æœ‰èŠ‚ç‚¹ï¼‰æ¡ä»¶ä¸‹ï¼Œè¯¥ä¸¤èŠ‚ç‚¹æ˜¯æ¡ä»¶ç‹¬ç«‹ | . | å±€éƒ¨é©¬å¯å¤«æ€§ local Markov property | å…¨å±€é©¬å¯å¤«æ€§ global Markov property | . æ¦‚ç‡æ— å‘å›¾æ¨¡å‹ï¼šæ— å‘å›¾ G=(V,E)G = (V, E)G=(V,E) è¡¨ç¤ºè”åˆæ¦‚ç‡åˆ†å¸ƒ P(Y)P(Y)P(Y)ï¼Œå¦‚æœè”åˆæ¦‚ç‡åˆ†å¸ƒ P(Y)P(Y)P(Y) æ»¡è¶³æˆå¯¹ã€å±€éƒ¨æˆ–å…¨å±€é©¬å¯å¤«æ€§ï¼Œå°±ç§°æ­¤è”åˆæ¦‚ç‡åˆ†å¸ƒ P(Y)P(Y)P(Y) ä¸ºæ¦‚ç‡æ— å‘å›¾æ¨¡å‹ï¼Œæˆ–é©¬å¯å¤«éšæœºåœº . å›¢ï¼ˆcliqueï¼‰ï¼šå›¾Gä¸­ä»»ä½•ä¸¤ä¸ªèŠ‚ç‚¹å‡æœ‰è¾¹è¿æ¥çš„èŠ‚ç‚¹å­é›† æœ€å¤§å›¢ï¼ˆmaximal cliqueï¼‰ï¼šå›¢Cä¸­ä¸èƒ½å†åŠ ä»»ä½•ä¸€ä¸ªèŠ‚ç‚¹ä½¿å®ƒæˆä¸ºæ›´å¤§çš„å›¢ï¼Œåˆ™ç§°æœ€å¤§å›¢ . 2. æ¡ä»¶éšæœºåœº . æ¡ä»¶éšæœºåœºæŒ‡ç»™å®šéšæœºå˜é‡Xæ¡ä»¶ä¸‹ï¼Œ éšæœºå˜é‡Yçš„é©¬å¯å¤«éšæœºåœºã€‚ . 2.1 æ¡ä»¶éšæœºåœºï¼š . è‹¥éšæœºå˜é‡YYYæ„æˆä¸€ä¸ªç”±æ— å‘å›¾G=(V,E)G = (V, E)G=(V,E)è¡¨ç¤ºçš„é©¬å¯å¤«éšæœºåœºï¼Œå³ . P(Yvâˆ£X,Yw,wâ‰ v)=P(Yvâˆ£X,Yw,wâˆ¼v)P left(Y_{v} | X, Y_{w}, w neq v right)=P left(Y_{v} | X, Y_{w}, w sim v right)P(Yvâ€‹âˆ£X,Ywâ€‹,wî€ â€‹=v)=P(Yvâ€‹âˆ£X,Ywâ€‹,wâˆ¼v) . å¯¹äºä»»æ„èŠ‚ç‚¹vvvæˆç«‹ï¼Œ åˆ™ç§°æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒP(Yâˆ£X)P(Y vert X)P(Yâˆ£X)ä¸ºæ¡ä»¶éšæœºåœºã€‚å…¶ä¸­wâˆ¼vw sim vwâˆ¼vè¡¨ç¤ºåœ¨å›¾G=(V,E)G = (V, E)G=(V,E)ä¸­ä¸èŠ‚ç‚¹vvvæœ‰è¾¹è¿æ¥çš„æ‰€æœ‰èŠ‚ç‚¹wwwï¼Œ wâ‰ vw neq vwî€ â€‹=vè¡¨ç¤ºèŠ‚ç‚¹vä»¥å¤–çš„æ‰€æœ‰èŠ‚ç‚¹ã€‚ . 2.2 çº¿æ€§é“¾æ¡ä»¶éšæœºåœºï¼ˆ linear chain conditional random fieldï¼‰ . çº¿æ€§é“¾æ¡ä»¶éšæœºåœºä¹Ÿæ˜¯å¯¹æ•°çº¿æ€§æ¨¡å‹(log linear model)ï¼Œå®šä¹‰ä¸ºï¼š . P(Yiâˆ£X,Y1,â‹¯â€‰,Yiâˆ’1,Yi+1,â‹¯â€‰,Yn)=P(Yiâˆ£X,Yiâˆ’1,Yi+1)i=1,2,â‹¯â€‰,nP left(Y_{i} | X, Y_{1}, cdots, Y_{i-1}, Y_{i+1}, cdots, Y_{n} right)=P left(Y_{i} | X, Y_{i-1}, Y_{i+1} right) i = 1,2, cdots, nP(Yiâ€‹âˆ£X,Y1â€‹,â‹¯,Yiâˆ’1â€‹,Yi+1â€‹,â‹¯,Ynâ€‹)=P(Yiâ€‹âˆ£X,Yiâˆ’1â€‹,Yi+1â€‹)i=1,2,â‹¯,n . åœ¨æ¡ä»¶æ¦‚ç‡æ¨¡å‹P(Yâˆ£X)P(Y vert X)P(Yâˆ£X)ä¸­ï¼Œ YYYæ˜¯è¾“å‡ºå˜é‡ï¼Œè¡¨ç¤ºæ ‡è®°åºåˆ—ï¼ˆçŠ¶æ€åºåˆ—ï¼Œå‚è§HMMï¼‰ï¼›XXXä½¿è¾“å…¥å˜é‡ï¼Œè¡¨ç¤ºéœ€è¦æ ‡æ³¨çš„è§‚æµ‹åºåˆ—ã€‚åˆ©ç”¨è®­ç»ƒé›†ï¼Œé€šè¿‡æå¤§ä¼¼ç„¶ä¼°è®¡æˆ–æ­£åˆ™åŒ–çš„æå¤§ä¼¼ç„¶ä¼°è®¡å¾—åˆ°æ¡ä»¶æ¦‚ç‡æ¨¡å‹P^(Yâˆ£X) hat{P}(Y vert X)P^(Yâˆ£X);é¢„æµ‹æ—¶ï¼Œå¯¹äºç»™å®šè¾“å…¥åºåˆ—xxxï¼Œæ±‚æ¡ä»¶æ¦‚ç‡P^(Yâˆ£X) hat{P}(Y vert X)P^(Yâˆ£X)æœ€å¤§çš„è¾“å‡ºåºåˆ—y^ hat{y}y^â€‹ã€‚ . 2.3 æ¡ä»¶éšæœºåœºçš„å‚æ•°åŒ–å½¢å¼ . è®¾P(Yâˆ£X)P(Y vert X)P(Yâˆ£X)ä¸ºçº¿æ€§é“¾æ¡ä»¶éšæœºåœºï¼ŒXå–å€¼ä¸ºxï¼Œ Yå–å€¼ä¸ºyçš„æ¡ä»¶æ¦‚ç‡å…·æœ‰å¦‚ä¸‹å½¢å¼ï¼š . P(yâˆ£x)=1Z(x)expâ¡(âˆ‘i,kÎ»ktk(yiâˆ’1,yi,x,i)+âˆ‘i,lÎ¼lsl(yi,x,i))P(y | x)= frac{1}{Z(x)} exp left( sum_{i, k} lambda_{k} t_{k} left(y_{i-1}, y_{i}, x, i right)+ sum_{i, l} mu_{l} s_{l} left(y_{i}, x, i right) right)P(yâˆ£x)=Z(x)1â€‹expââœâ›â€‹i,kâˆ‘â€‹Î»kâ€‹tkâ€‹(yiâˆ’1â€‹,yiâ€‹,x,i)+i,lâˆ‘â€‹Î¼lâ€‹slâ€‹(yiâ€‹,x,i)â âŸââ€‹ . å…¶ä¸­ï¼Œ . Z(x)=âˆ‘yexpâ¡(âˆ‘i,kÎ»ktk(yiâˆ’1,yi,x,i)+âˆ‘i,lÎ¼lsl(yi,x,i))Z(x)= sum_{y} exp left( sum_{i, k} lambda_{k} t_{k} left(y_{i-1}, y_{i}, x, i right)+ sum_{i, l} mu_{l} s_{l} left(y_{i}, x, i right) right)Z(x)=yâˆ‘â€‹expââœâ›â€‹i,kâˆ‘â€‹Î»kâ€‹tkâ€‹(yiâˆ’1â€‹,yiâ€‹,x,i)+i,lâˆ‘â€‹Î¼lâ€‹slâ€‹(yiâ€‹,x,i)â âŸââ€‹ . å¼ä¸­ï¼Œtkt_{k}tkâ€‹å’Œsls_{l}slâ€‹æ˜¯ç‰¹å¾å‡½æ•°, Î»k lambda_{k}Î»kâ€‹å’ŒÎ¼l mu_{l}Î¼lâ€‹æ˜¯å¯¹åº”çš„æƒå€¼ã€‚ Z(x)Z(x)Z(x)æ˜¯è§„èŒƒåŒ–å› å­ã€‚åœ¨æ‰€æœ‰å¯èƒ½è¾“å‡ºçš„åºåˆ—ä¸Šè¿›è¡Œæ±‚å’Œæ“ä½œã€‚ . å…³äºç‰¹å¾å‡½æ•°ï¼š . ä»¤tkt_{k}tkâ€‹æ˜¯å®šä¹‰åœ¨è¾¹ä¸Šçš„ç‰¹å¾å‡½æ•°ï¼Œç§°ä¸ºè½¬ç§»ç‰¹å¾ï¼Œä¾èµ–å½“å‰å’Œå‰ä¸€ä¸ªä½ç½® | ä»¤sls_{l}slâ€‹æ˜¯å®šä¹‰åœ¨èŠ‚ç‚¹ä¸Šçš„ç‰¹å¾å‡½æ•°ï¼Œç§°ä¸ºçŠ¶æ€ç‰¹å¾ï¼Œä¾èµ–å½“å‰ä½ç½® | ç‰¹å¾å‡½æ•°tkt_{k}tkâ€‹å’Œsls_{l}slâ€‹å–å€¼0æˆ–1ï¼›æ»¡è¶³æ¡ä»¶å–1ï¼Œåä¹‹0 | æ¡ä»¶éšæœºé•¿å®Œå…¨ç”±ç‰¹å¾å‡½æ•°tkt_{k}tkâ€‹å’Œsls_{l}slâ€‹ï¼Œ å’Œå¯¹åº”çš„æƒå€¼Î»k lambda_{k}Î»kâ€‹å’ŒÎ¼l mu_{l}Î¼lâ€‹ç¡®å®šã€‚ | . 2.4 æ¡ä»¶éšæœºåœºçš„çŸ©é˜µå½¢å¼ . å¯¹äºè§‚æµ‹åºåˆ—xçš„æ¯ä¸ªä½ç½®ï¼Œyåœ¨mä¸ªæ ‡è®°ä¸­å–å€¼ï¼Œå¯ä»¥å®šä¹‰ä¸€ä¸ªmé˜¶çš„çŸ©é˜µéšæœºå˜é‡ï¼š . Mi(x)=[Mi(yiâˆ’1,yiâˆ£x)]M_{i}(x) = [ M_{i}(y_{i-1}, y_{i}|x)]Miâ€‹(x)=[Miâ€‹(yiâˆ’1â€‹,yiâ€‹âˆ£x)] . çŸ©é˜µéšæœºå˜é‡å…ƒç´ ä¸º . Mi(yiâˆ’1,yiâˆ£x)=expâ¡(Wi(yiâˆ’1,yiâˆ£x))Wi(yiâˆ’1,yiâˆ£x)=âˆ‘k=1Kwkfk(yiâˆ’1,yi,x,i) begin{aligned} &amp;M_{i} left(y_{i-1}, y_{i} | x right)= exp left(W_{i} left(y_{i-1}, y_{i} | x right) right) &amp;W_{i} left(y_{i-1}, y_{i} | x right)= sum_{k=1}^{K} w_{k} f_{k} left(y_{i-1}, y_{i}, x, i right) end{aligned}â€‹Miâ€‹(yiâˆ’1â€‹,yiâ€‹âˆ£x)=exp(Wiâ€‹(yiâˆ’1â€‹,yiâ€‹âˆ£x))Wiâ€‹(yiâˆ’1â€‹,yiâ€‹âˆ£x)=k=1âˆ‘Kâ€‹wkâ€‹fkâ€‹(yiâˆ’1â€‹,yiâ€‹,x,i)â€‹ . è¿™é‡Œwkw_kwkâ€‹ä¸º . wk={Î»k,k=1,2,â‹¯â€‰,K1Î¼l,k=K1+l;l=1,2,â‹¯â€‰,K2w_{k}= left { begin{array}{ll} lambda_{k}, &amp; k=1,2, cdots, K_{1} mu_{l}, &amp; k=K_{1}+l ; l=1,2, cdots, K_{2} end{array} right.wkâ€‹={Î»kâ€‹,Î¼lâ€‹,â€‹k=1,2,â‹¯,K1â€‹k=K1â€‹+l;l=1,2,â‹¯,K2â€‹â€‹ . å’Œfkf_kfkâ€‹ä¸º . fk(yiâˆ’1,yi,x,i)={tk(yiâˆ’1,yi,x,i),k=1,2,â‹¯â€‰,K1sl(yi,x,i),k=K1+l;l=1,2,â‹¯â€‰,K2f_{k} left(y_{i-1}, y_{i}, x, i right)= left { begin{array}{ll} t_{k} left(y_{i-1}, y_{i}, x, i right), &amp; k=1,2, cdots, K_{1} s_{l} left(y_{i}, x, i right), &amp; k=K_{1}+l ; l=1,2, cdots, K_{2} end{array} right.fkâ€‹(yiâˆ’1â€‹,yiâ€‹,x,i)={tkâ€‹(yiâˆ’1â€‹,yiâ€‹,x,i),slâ€‹(yiâ€‹,x,i),â€‹k=1,2,â‹¯,K1â€‹k=K1â€‹+l;l=1,2,â‹¯,K2â€‹â€‹ . äºæ˜¯ï¼Œæ¡ä»¶æ¦‚ç‡Pw(yâˆ£x)P_{w}(y vert x)Pwâ€‹(yâˆ£x): . Pw(yâˆ£x)=1Zw(x)âˆi=1n+1Mi(yiâˆ’1,yiâˆ£x)P_{w}(y | x)= frac{1}{Z_{w}(x)} prod_{i=1}^{n+1} M_{i} left(y_{i-1}, y_{i} | x right)Pwâ€‹(yâˆ£x)=Zwâ€‹(x)1â€‹i=1âˆn+1â€‹Miâ€‹(yiâˆ’1â€‹,yiâ€‹âˆ£x) . å…¶ä¸­ï¼Œ . Zw(x)=[M1(x)M2(x)â‹¯Mn+1(x)]start,stopZ_{w}(x)= left[M_{1}(x) M_{2}(x) cdots M_{n+1}(x) right]_{ mathrm{start}, mathrm{stop}}Zwâ€‹(x)=[M1â€‹(x)M2â€‹(x)â‹¯Mn+1â€‹(x)]start,stopâ€‹ . æ³¨ï¼Œ y0=starty_{0} = mathrm{start}y0â€‹=startï¼Œè¡¨ç¤ºå¼€å§‹çŠ¶æ€ï¼› yn+1=stopy_{n+1} = mathrm{stop}yn+1â€‹=stopï¼Œ è¡¨ç¤ºç»ˆæ­¢çŠ¶æ€ . å‚è€ƒï¼š æèˆªã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹ .",
            "url": "https://zqfang.github.io/machine%20learning/2020/04/29/ML-CRF.html",
            "relUrl": "/machine%20learning/2020/04/29/ML-CRF.html",
            "date": " â€¢ Apr 29, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Convert Seurat Robj to Scanpy h5ad",
            "content": "It costed me a lot of time to convert seurat objects to scanpy. Itâ€™s not a pleasant experience. Finally, I solved it. . 1. Install Seurat v3.0.2, or python kernel will always died!!! . Donâ€™t know why latest seurat not work. . 2. Set the R version for rpy2 . # user defined R installation import os # path to your libR.so, only Seurat v3.0.2 works! # create a conda R env for seurat 3.0.2 first os.environ[&#39;R_HOME&#39;] = &#39;/home/fangzq/miniconda/envs/seurat/lib/R&#39; # path depends on where you installed Python. os.environ[&#39;R_USER&#39;] = &#39;/home/fangzq/miniconda/lib/python3.7/site-packages/rpy2&#39; . 3. Now, youâ€™er good to go . import scanpy as sc import glob . Install anndata2ri first . import anndata2ri from rpy2.robjects import r from rpy2.robjects.conversion import localconverter # activate rpy2 env anndata2ri.activate() . robjs = glob.glob(&quot;data/*Robj&quot;) . Convert to h5ad . r(&#39;library(Seurat)&#39;) for robj in robjs: r(f&#39;x&lt;-load(&quot;{robj}&quot;)&#39;) r(&#39;y=get(x)&#39;) r(&#39;rm(x)&#39;) r(&#39;DefaultAssay(y) &lt;- &quot;RNA&quot;&#39;) # get raw count matrix to save # seurat2 object # adata = r(&#39;as.SingleCellExperiment(UpdateSeuratObject(y))&#39;) adata = r(&#39;as.SingleCellExperiment(y)&#39;) adata.write_h5ad(filename=robj.replace(&quot;Robj&quot;,&quot;h5ad&quot;)) . 4. other way . seurat -&gt; loom -&gt; scanpy . Itâ€™s much easier, but I did not test. . save to loom format fist. pbmc.loom &lt;- as.loom(pbmc.seurat, filename = &quot;../output/pbmc3k.loom&quot;, verbose = FALSE) pbmc.loom . read into scanpy . pbmc3k = sc.read_loom(&quot;../output/pbmc3k.loom&quot;) . | use sceasy to save h5ad. |",
            "url": "https://zqfang.github.io/make%20bioinfo%20uncool%20again/2020/04/28/Seurat2Scanpy.html",
            "relUrl": "/make%20bioinfo%20uncool%20again/2020/04/28/Seurat2Scanpy.html",
            "date": " â€¢ Apr 28, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "How to do deep learning using custom Jupyter kernels on Sherlock",
            "content": "A recipe for interactive computing using custom Jupyter kernels on Stanfordâ€™s Sherlock. . Setting up custom conda environment on Sherlockâ€™s login node . 1. Download and install Miniconda . wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh # install bash Miniconda3-latest-Linux-x86_64.sh conda config --set always_yes yes . 2. Install jupyter notebook/lab and secure your notebooks with a password . # install the default py3 kernel for jupyter notebook conda install ipython jupyter notebook jupyterlab # add password jupyter notebook password . 3. (Optional) Add custom conda environment. i.e. fastai . conda create -n fastai ipython ipykernel # add the custom to Jupyter notebook conda activate fastai python -m ipykernel install --user --name fastai --display-name FastAI . you could also add R, Julia etc kernel. . 4. Install pytorch/tensorflow . You should select the existed cuda version which installed in Sherlock . conda install -c pytorch pytorch torchvision cudatoolkit=10.1 . tensorflow . conda install tensorflow-gpu cudatoolkit=10.1 . 5. Load gpu modules. Select the corresponding cuda version youâ€™ve just installed . # this is my version module load cuda/10.1.168 module load cudnn/7.6.4 module load nccl . 6. now, open ipython, run . import torch print(torch.cuda.is_avilable()) . if print out is True, then youâ€™er OK to use GPUs. . Follow these steps on your local machine . see details here. . 7. Download the forward repo . git clone https://github.com/vsoch/forward cd forward . 8. Generate your parameters . bash setup.sh . Select Sherlock partition: gpu . 9. SSH Credentials . bash hosts/sherlock_ssh.sh &gt;&gt; ~/.ssh/config . 10. create a sbatch script in forward/sbatches/sherlock and save as jupyter-gpu.sbatch . #!/bin/bash PORT=$1 NOTEBOOK_DIR=$2 if [ -z &quot;$NOTEBOOK_DIR&quot; ]; then cd $SCRATCH else cd $NOTEBOOK_DIR fi ## to compile libtorch C++ code, load these modules # module load gcc/7.3.0 # module load gdb # module load cmake # export CC=$(which gcc) # export CXX=$(which g++) # select cuda version you need module load cuda/10.1.168 module load cudnn/7.6.4 module load nccl # activate fastai env source activate fastai jupyter lab --no-browser --port=$PORT . 11. Start a session . The default working directory is $SCRATCH . bash start.sh jupyter-gpu . change the working directory . bash start.sh jupyter /path/to/dir . 12. open your browser in local machine and type . if your port is 51888, then . http://localhost:51888/ . here is my jupyter lab computing environment. Have fun! . fastai kernel . . Test GPUs . . 13. Resume a session . bash resume.sh jupyter-gpu # or bash resume.sh jupyter-gpu /path/to/dir . 14. Stop a session . bash end.sh jupyter-gpu # or bash end.sh jupyter-gpu /path/to/dir .",
            "url": "https://zqfang.github.io/machine%20learning/2020/02/10/ML-Sherlock.html",
            "relUrl": "/machine%20learning/2020/02/10/ML-Sherlock.html",
            "date": " â€¢ Feb 10, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Graph",
            "content": "Graphs . Some data structures to keep in my mind. . BinaryHeap: Complete binary tree MaxHeap: Parent &gt; Both Children | IndexMaxHeap | MinHeap: Parent &lt; Both Children | IndexMinHeap | Priority queue (MaxHeap) | . | BinarySearchTree Not always complete binary tree | Value: leftChild &lt; Parent &lt; rightChild | . | DenseGraph | SparseGraph | . Code snippets take from Play with Algorithm . 1. Dense Graph . #include &lt;iostream&gt; #include &lt;vector&gt; #include &lt;cassert&gt; using namespace std; // ç¨ å¯†å›¾ - é‚»æ¥çŸ©é˜µ class DenseGraph{ private: int n, m; bool directed; vector&lt;vector&lt;bool&gt;&gt; g; public: DenseGraph( int n , bool directed ){ this-&gt;n = n; this-&gt;m = 0; this-&gt;directed = directed; for( int i = 0 ; i &lt; n ; i ++ ) g.push_back( vector&lt;bool&gt;(n, false) ); } ~DenseGraph(){ } int V(){ return n;} int E(){ return m;} void addEdge( int v , int w ){ assert( v &gt;= 0 &amp;&amp; v &lt; n ); assert( w &gt;= 0 &amp;&amp; w &lt; n ); if( hasEdge( v , w ) ) return; g[v][w] = true; if( !directed ) g[w][v] = true; m ++; } bool hasEdge( int v , int w ){ assert( v &gt;= 0 &amp;&amp; v &lt; n ); assert( w &gt;= 0 &amp;&amp; w &lt; n ); return g[v][w]; } void show(){ for( int i = 0 ; i &lt; n ; i ++ ){ for( int j = 0 ; j &lt; n ; j ++ ) cout&lt;&lt;g[i][j]&lt;&lt;&quot; t&quot;; cout&lt;&lt;endl; } } class adjIterator{ private: DenseGraph &amp;G; int v; int index; public: adjIterator(DenseGraph &amp;graph, int v): G(graph){ this-&gt;v = v; this-&gt;index = -1; } int begin(){ index = -1; return next(); } int next(){ for( index += 1 ; index &lt; G.V() ; index ++ ) if( G.g[v][index] ) return index; return -1; } bool end(){ return index &gt;= G.V(); } }; }; . 2. Sparse Graph . #include &lt;iostream&gt; #include &lt;vector&gt; #include &lt;cassert&gt; using namespace std; // ç¨€ç–å›¾ - é‚»æ¥è¡¨ class SparseGraph{ private: int n, m; bool directed; vector&lt;vector&lt;int&gt;&gt; g; public: SparseGraph( int n , bool directed ){ this-&gt;n = n; this-&gt;m = 0; this-&gt;directed = directed; for( int i = 0 ; i &lt; n ; i ++ ) g.push_back( vector&lt;int&gt;() ); } ~SparseGraph(){ } int V(){ return n;} int E(){ return m;} void addEdge( int v, int w ){ assert( v &gt;= 0 &amp;&amp; v &lt; n ); assert( w &gt;= 0 &amp;&amp; w &lt; n ); g[v].push_back(w); if( v != w &amp;&amp; !directed ) g[w].push_back(v); m ++; } bool hasEdge( int v , int w ){ assert( v &gt;= 0 &amp;&amp; v &lt; n ); assert( w &gt;= 0 &amp;&amp; w &lt; n ); for( int i = 0 ; i &lt; g[v].size() ; i ++ ) if( g[v][i] == w ) return true; return false; } void show(){ for( int i = 0 ; i &lt; n ; i ++ ){ cout&lt;&lt;&quot;vertex &quot;&lt;&lt;i&lt;&lt;&quot;: t&quot;; for( int j = 0 ; j &lt; g[i].size() ; j ++ ) cout&lt;&lt;g[i][j]&lt;&lt;&quot; t&quot;; cout&lt;&lt;endl; } } class adjIterator{ private: SparseGraph &amp;G; int v; int index; public: adjIterator(SparseGraph &amp;graph, int v): G(graph){ this-&gt;v = v; this-&gt;index = 0; } int begin(){ index = 0; if( G.g[v].size() ) return G.g[v][index]; return -1; } int next(){ index ++; if( index &lt; G.g[v].size() ) return G.g[v][index]; return -1; } bool end(){ return index &gt;= G.g[v].size(); } }; }; .",
            "url": "https://zqfang.github.io/algorithm%20and%20data%20structure/2020/02/09/CS-graph.html",
            "relUrl": "/algorithm%20and%20data%20structure/2020/02/09/CS-graph.html",
            "date": " â€¢ Feb 9, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Sort",
            "content": "Sort algorithms Code snippets are taken from Play with Algorithm . Some algorithm to keep in my mind. . selectionSort | insertionSort | mergeSort | quickSort two way | three way | . | heapSort | . 1. insertionSort . #include &lt;iostream&gt; #include &lt;algorithm&gt; using namespace std; template&lt;typename T&gt; void insertionSort(T arr[], int n){ for( int i = 1 ; i &lt; n ; i ++ ) { T e = arr[i]; int j; for (j = i; j &gt; 0 &amp;&amp; arr[j-1] &gt; e; j--) arr[j] = arr[j-1]; arr[j] = e; } return; } // å¯¹arr[l...r]èŒƒå›´çš„æ•°ç»„è¿›è¡Œæ’å…¥æ’åº template&lt;typename T&gt; void insertionSort(T arr[], int l, int r){ for( int i = l+1 ; i &lt;= r ; i ++ ) { T e = arr[i]; int j; for (j = i; j &gt; l &amp;&amp; arr[j-1] &gt; e; j--) arr[j] = arr[j-1]; arr[j] = e; } return; } . 2. mergeSort . #include &lt;iostream&gt; #include &lt;algorithm&gt; #include &quot;InsertionSort.h&quot; using namespace std; template&lt;typename T&gt; void __merge(T arr[], int l, int mid, int r){ T aux[r-l+1]; for( int i = l ; i &lt;= r; i ++ ) aux[i-l] = arr[i]; int i = l, j = mid+1; for( int k = l ; k &lt;= r; k ++ ){ if( i &gt; mid ) { arr[k] = aux[j-l]; j ++;} else if( j &gt; r ){ arr[k] = aux[i-l]; i ++;} else if( aux[i-l] &lt; aux[j-l] ){ arr[k] = aux[i-l]; i ++;} else { arr[k] = aux[j-l]; j ++;} } } template&lt;typename T&gt; void __mergeSort(T arr[], int l, int r){ if( r - l &lt;= 15 ){ insertionSort(arr, l, r); return; } int mid = (l+r)/2; __mergeSort(arr, l, mid); __mergeSort(arr, mid+1, r); if( arr[mid] &gt; arr[mid+1] ) __merge(arr, l, mid, r); } template&lt;typename T&gt; void mergeSort(T arr[], int n){ __mergeSort( arr , 0 , n-1 ); } template &lt;typename T&gt; void mergeSortBU(T arr[], int n){ for( int i = 0 ; i &lt; n ; i += 16 ) insertionSort(arr,i,min(i+15,n-1)); for( int sz = 16; sz &lt;= n ; sz += sz ) for( int i = 0 ; i &lt; n - sz ; i += sz+sz ) if( arr[i+sz-1] &gt; arr[i+sz] ) __merge(arr, i, i+sz-1, min(i+sz+sz-1,n-1) ); } . 3. qucikSort . #include &lt;iostream&gt; #include &lt;ctime&gt; #include &lt;algorithm&gt; #include &quot;InsertionSort.h&quot; using namespace std; template &lt;typename T&gt; int _partition(T arr[], int l, int r){ // select a rand index in arr, and swap swap( arr[l] , arr[rand()%(r-l+1)+l] ); T v = arr[l]; int i = l+1, j = r; while( true ){ while( i &lt;= r &amp;&amp; arr[i] &lt; v ) i ++; while( j &gt;= l+1 &amp;&amp; arr[j] &gt; v ) j --; if( i &gt; j ) break; swap( arr[i] , arr[j] ); i ++; j --; } swap( arr[l] , arr[j]); return j; } template &lt;typename T&gt; void _quickSort(T arr[], int l, int r){ if( r - l &lt;= 15 ){ insertionSort(arr,l,r); return; } int p = _partition(arr, l, r); _quickSort(arr, l, p-1 ); _quickSort(arr, p+1, r); } template &lt;typename T&gt; void quickSort(T arr[], int n){ srand(time(NULL)); _quickSort(arr, 0, n-1); } template &lt;typename T&gt; void __quickSort3Ways(T arr[], int l, int r){ if( r - l &lt;= 15 ){ insertionSort(arr,l,r); return; } swap( arr[l], arr[rand()%(r-l+1)+l ] ); T v = arr[l]; int lt = l; // arr[l+1...lt] &lt; v int gt = r + 1; // arr[gt...r] &gt; v int i = l+1; // arr[lt+1...i) == v while( i &lt; gt ){ if( arr[i] &lt; v ){ swap( arr[i], arr[lt+1]); i ++; lt ++; } else if( arr[i] &gt; v ){ swap( arr[i], arr[gt-1]); gt --; } else{ // arr[i] == v i ++; } } swap( arr[l] , arr[lt] ); __quickSort3Ways(arr, l, lt-1); __quickSort3Ways(arr, gt, r); } template &lt;typename T&gt; void quickSort3Ways(T arr[], int n){ srand(time(NULL)); __quickSort3Ways( arr, 0, n-1); } . 4. heapSort . #include &quot;Heap.h&quot; using namespace std; template&lt;typename T&gt; void heapSort2(T arr[], int n){ MaxHeap&lt;T&gt; maxheap = MaxHeap&lt;T&gt;(arr,n); for( int i = n-1 ; i &gt;= 0 ; i-- ) arr[i] = maxheap.extractMax(); } template&lt;typename T&gt; void heapSort1(T arr[], int n){ MaxHeap&lt;T&gt; maxheap = MaxHeap&lt;T&gt;(n); for( int i = 0 ; i &lt; n ; i ++ ) maxheap.insert(arr[i]); for( int i = n-1 ; i &gt;= 0 ; i-- ) arr[i] = maxheap.extractMax(); } .",
            "url": "https://zqfang.github.io/algorithm%20and%20data%20structure/2020/02/08/CS-sort.html",
            "relUrl": "/algorithm%20and%20data%20structure/2020/02/08/CS-sort.html",
            "date": " â€¢ Feb 8, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Binary tree",
            "content": "Binary trees . Difference . Some data structures to keep in my mind. . BinaryHeap: Complete binary tree MaxHeap: Parent &gt; Both Children | IndexMaxHeap | MinHeap: Parent &lt; Both Children | IndexMinHeap | Priority queue (MaxHeap) | . | BinarySearchTree Not always complete binary tree | Value: leftChild &lt; Parent &lt; rightChild | . | DenseGraph | SparseGraph | . Code snippets take from Play with Algorithm . 1. MaxHeap . #include &lt;algorithm&gt; #include &lt;cassert&gt; using namespace std; template&lt;typename Item&gt; class MaxHeap{ private: Item *data; int count; int capacity; void shiftUp(int k){ while( k &gt; 1 &amp;&amp; data[k/2] &lt; data[k] ){ swap( data[k/2], data[k] ); k /= 2; } } void shiftDown(int k){ while( 2*k &lt;= count ){ int j = 2*k; // which child is larger, then swap if( j+1 &lt;= count &amp;&amp; data[j+1] &gt; data[j] ) j ++; if( data[k] &gt;= data[j] ) break; swap( data[k] , data[j] ); k = j; } } public: MaxHeap(int capacity){ data = new Item[capacity+1]; count = 0; this-&gt;capacity = capacity; } MaxHeap(Item arr[], int n){ data = new Item[n+1]; capacity = n; for( int i = 0 ; i &lt; n ; i ++ ) data[i+1] = arr[i]; count = n; for( int i = count/2 ; i &gt;= 1 ; i -- ) shiftDown(i); } ~MaxHeap(){ delete[] data; } int size(){ return count; } bool isEmpty(){ return count == 0; } void insert(Item item){ assert( count + 1 &lt;= capacity ); data[count+1] = item; shiftUp(count+1); count ++; } Item extractMax(){ assert( count &gt; 0 ); Item ret = data[1]; swap( data[1] , data[count] ); count --; shiftDown(1); return ret; } Item getMax(){ assert( count &gt; 0 ); return data[1]; } }; . 2. IndexMaxHeap . Need 3 vector: data, indexes, reverse . Code . #include &lt;algorithm&gt; #include &lt;cassert&gt; using namespace std; template&lt;typename Item&gt; class IndexMaxHeap{ private: Item *data; int *indexes; int *reverse; int count; int capacity; void shiftUp( int k ){ while( k &gt; 1 &amp;&amp; data[indexes[k/2]] &lt; data[indexes[k]] ){ swap( indexes[k/2] , indexes[k] ); reverse[indexes[k/2]] = k/2; reverse[indexes[k]] = k; k /= 2; } } void shiftDown( int k ){ while( 2*k &lt;= count ){ int j = 2*k; if( j + 1 &lt;= count &amp;&amp; data[indexes[j+1]] &gt; data[indexes[j]] ) j += 1; if( data[indexes[k]] &gt;= data[indexes[j]] ) break; swap( indexes[k] , indexes[j] ); reverse[indexes[k]] = k; reverse[indexes[j]] = j; k = j; } } public: IndexMaxHeap(int capacity){ data = new Item[capacity+1]; indexes = new int[capacity+1]; reverse = new int[capacity+1]; for( int i = 0 ; i &lt;= capacity ; i ++ ) reverse[i] = 0; count = 0; this-&gt;capacity = capacity; } ~IndexMaxHeap(){ delete[] data; delete[] indexes; delete[] reverse; } int size(){ return count; } bool isEmpty(){ return count == 0; } // ä¼ å…¥çš„iå¯¹ç”¨æˆ·è€Œè¨€,æ˜¯ä»0ç´¢å¼•çš„ void insert(int i, Item item){ assert( count + 1 &lt;= capacity ); assert( i + 1 &gt;= 1 &amp;&amp; i + 1 &lt;= capacity ); i += 1; data[i] = item; indexes[count+1] = i; reverse[i] = count+1; count++; shiftUp(count); } Item extractMax(){ assert( count &gt; 0 ); Item ret = data[indexes[1]]; swap( indexes[1] , indexes[count] ); reverse[indexes[count]] = 0; reverse[indexes[1]] = 1; count--; shiftDown(1); return ret; } int extractMaxIndex(){ assert( count &gt; 0 ); int ret = indexes[1] - 1; swap( indexes[1] , indexes[count] ); reverse[indexes[count]] = 0; reverse[indexes[1]] = 1; count--; shiftDown(1); return ret; } Item getMax(){ assert( count &gt; 0 ); return data[indexes[1]]; } int getMaxIndex(){ assert( count &gt; 0 ); return indexes[1]-1; } bool contain( int i ){ assert( i + 1 &gt;= 1 &amp;&amp; i + 1 &lt;= capacity ); return reverse[i+1] != 0; } Item getItem( int i ){ assert( contain(i) ); return data[i+1]; } void change( int i , Item newItem ){ assert( contain(i) ); i += 1; data[i] = newItem; // æ‰¾åˆ°indexes[j] = i, jè¡¨ç¤ºdata[i]åœ¨å †ä¸­çš„ä½ç½® // ä¹‹åshiftUp(j), å†shiftDown(j) // for( int j = 1 ; j &lt;= count ; j ++ ) // if( indexes[j] == i ){ // shiftUp(j); // shiftDown(j); // return; // } int j = reverse[i]; shiftUp( j ); shiftDown( j ); } // test reverse index bool testReverseIndex(){ int *copyIndexes = new int[count+1]; int *copyReverseIndexes = new int[count+1]; for( int i = 0 ; i &lt;= count ; i ++ ){ copyIndexes[i] = indexes[i]; copyReverseIndexes[i] = reverse[i]; } copyIndexes[0] = copyReverseIndexes[0] = 0; std::sort(copyIndexes, copyIndexes + count + 1); std::sort(copyReverseIndexes, copyReverseIndexes + count + 1); bool res = true; for( int i = 1 ; i &lt;= count ; i ++ ) if( copyIndexes[i-1] + 1 != copyIndexes[i] || copyReverseIndexes[i-1] + 1 != copyReverseIndexes[i] ) res = res || false; delete[] copyIndexes; delete[] copyReverseIndexes; if( !res ){ cout&lt;&lt;&quot;Error 1&quot;&lt;&lt;endl; return res; } for( int i = 1 ; i &lt;= count ; i ++ ) if( reverse[ indexes[i] ] != i ){ cout&lt;&lt;&quot;Error 2&quot;&lt;&lt;endl; return false; } return true; } }; . 3. BinarySearchTree . #include &lt;iostream&gt; #include &lt;queue&gt; #include &lt;cassert&gt; using namespace std; template &lt;typename Key, typename Value&gt; class BST{ private: struct Node{ Key key; Value value; Node *left; Node *right; Node(Key key, Value value){ this-&gt;key = key; this-&gt;value = value; this-&gt;left = this-&gt;right = NULL; } Node(Node *node){ this-&gt;key = node-&gt;key; this-&gt;value = node-&gt;value; this-&gt;left = node-&gt;left; this-&gt;right = node-&gt;right; } }; Node *root; int count; public: BST(){ root = NULL; count = 0; } ~BST(){ destroy( root ); } int size(){ return count; } bool isEmpty(){ return count == 0; } void insert(Key key, Value value){ root = insert(root, key, value); } bool contain(Key key){ return contain(root, key); } Value* search(Key key){ return search( root , key ); } // å‰åºéå† void preOrder(){ preOrder(root); } // ä¸­åºéå† void inOrder(){ inOrder(root); } // ååºéå† void postOrder(){ postOrder(root); } // å±‚åºéå† void levelOrder(){ queue&lt;Node*&gt; q; q.push(root); while( !q.empty() ){ Node *node = q.front(); q.pop(); cout&lt;&lt;node-&gt;key&lt;&lt;endl; if( node-&gt;left ) q.push( node-&gt;left ); if( node-&gt;right ) q.push( node-&gt;right ); } } // å¯»æ‰¾æœ€å°çš„é”®å€¼ Key minimum(){ assert( count != 0 ); Node* minNode = minimum( root ); return minNode-&gt;key; } // å¯»æ‰¾æœ€å¤§çš„é”®å€¼ Key maximum(){ assert( count != 0 ); Node* maxNode = maximum(root); return maxNode-&gt;key; } // ä»äºŒå‰æ ‘ä¸­åˆ é™¤æœ€å°å€¼æ‰€åœ¨èŠ‚ç‚¹ void removeMin(){ if( root ) root = removeMin( root ); } // ä»äºŒå‰æ ‘ä¸­åˆ é™¤æœ€å¤§å€¼æ‰€åœ¨èŠ‚ç‚¹ void removeMax(){ if( root ) root = removeMax( root ); } // ä»äºŒå‰æ ‘ä¸­åˆ é™¤é”®å€¼ä¸ºkeyçš„èŠ‚ç‚¹ void remove(Key key){ root = remove(root, key); } private: // å‘ä»¥nodeä¸ºæ ¹çš„äºŒå‰æœç´¢æ ‘ä¸­,æ’å…¥èŠ‚ç‚¹(key, value) // è¿”å›æ’å…¥æ–°èŠ‚ç‚¹åçš„äºŒå‰æœç´¢æ ‘çš„æ ¹ Node* insert(Node *node, Key key, Value value){ if( node == NULL ){ count ++; return new Node(key, value); } if( key == node-&gt;key ) node-&gt;value = value; else if( key &lt; node-&gt;key ) node-&gt;left = insert( node-&gt;left , key, value); else // key &gt; node-&gt;key node-&gt;right = insert( node-&gt;right, key, value); return node; } // æŸ¥çœ‹ä»¥nodeä¸ºæ ¹çš„äºŒå‰æœç´¢æ ‘ä¸­æ˜¯å¦åŒ…å«é”®å€¼ä¸ºkeyçš„èŠ‚ç‚¹ bool contain(Node* node, Key key){ if( node == NULL ) return false; if( key == node-&gt;key ) return true; else if( key &lt; node-&gt;key ) return contain( node-&gt;left , key ); else // key &gt; node-&gt;key return contain( node-&gt;right , key ); } // åœ¨ä»¥nodeä¸ºæ ¹çš„äºŒå‰æœç´¢æ ‘ä¸­æŸ¥æ‰¾keyæ‰€å¯¹åº”çš„value Value* search(Node* node, Key key){ if( node == NULL ) return NULL; if( key == node-&gt;key ) return &amp;(node-&gt;value); else if( key &lt; node-&gt;key ) return search( node-&gt;left , key ); else // key &gt; node-&gt;key return search( node-&gt;right, key ); } // å¯¹ä»¥nodeä¸ºæ ¹çš„äºŒå‰æœç´¢æ ‘è¿›è¡Œå‰åºéå† void preOrder(Node* node){ if( node != NULL ){ cout&lt;&lt;node-&gt;key&lt;&lt;endl; preOrder(node-&gt;left); preOrder(node-&gt;right); } } // å¯¹ä»¥nodeä¸ºæ ¹çš„äºŒå‰æœç´¢æ ‘è¿›è¡Œä¸­åºéå† void inOrder(Node* node){ if( node != NULL ){ inOrder(node-&gt;left); cout&lt;&lt;node-&gt;key&lt;&lt;endl; inOrder(node-&gt;right); } } // å¯¹ä»¥nodeä¸ºæ ¹çš„äºŒå‰æœç´¢æ ‘è¿›è¡Œååºéå† void postOrder(Node* node){ if( node != NULL ){ postOrder(node-&gt;left); postOrder(node-&gt;right); cout&lt;&lt;node-&gt;key&lt;&lt;endl; } } void destroy(Node* node){ if( node != NULL ){ destroy( node-&gt;left ); destroy( node-&gt;right ); delete node; count --; } } // åœ¨ä»¥nodeä¸ºæ ¹çš„äºŒå‰æœç´¢æ ‘ä¸­,è¿”å›æœ€å°é”®å€¼çš„èŠ‚ç‚¹ Node* minimum(Node* node){ if( node-&gt;left == NULL ) return node; return minimum(node-&gt;left); } // åœ¨ä»¥nodeä¸ºæ ¹çš„äºŒå‰æœç´¢æ ‘ä¸­,è¿”å›æœ€å¤§é”®å€¼çš„èŠ‚ç‚¹ Node* maximum(Node* node){ if( node-&gt;right == NULL ) return node; return maximum(node-&gt;right); } // åˆ é™¤æ‰ä»¥nodeä¸ºæ ¹çš„äºŒåˆ†æœç´¢æ ‘ä¸­çš„æœ€å°èŠ‚ç‚¹ // è¿”å›åˆ é™¤èŠ‚ç‚¹åæ–°çš„äºŒåˆ†æœç´¢æ ‘çš„æ ¹ Node* removeMin(Node* node){ if( node-&gt;left == NULL ){ Node* rightNode = node-&gt;right; delete node; count --; return rightNode; } node-&gt;left = removeMin(node-&gt;left); return node; } // åˆ é™¤æ‰ä»¥nodeä¸ºæ ¹çš„äºŒåˆ†æœç´¢æ ‘ä¸­çš„æœ€å¤§èŠ‚ç‚¹ // è¿”å›åˆ é™¤èŠ‚ç‚¹åæ–°çš„äºŒåˆ†æœç´¢æ ‘çš„æ ¹ Node* removeMax(Node* node){ if( node-&gt;right == NULL ){ Node* leftNode = node-&gt;left; delete node; count --; return leftNode; } node-&gt;right = removeMax(node-&gt;right); return node; } // åˆ é™¤æ‰ä»¥nodeä¸ºæ ¹çš„äºŒåˆ†æœç´¢æ ‘ä¸­é”®å€¼ä¸ºkeyçš„èŠ‚ç‚¹ // è¿”å›åˆ é™¤èŠ‚ç‚¹åæ–°çš„äºŒåˆ†æœç´¢æ ‘çš„æ ¹ Node* remove(Node* node, Key key){ if( node == NULL ) return NULL; if( key &lt; node-&gt;key ){ node-&gt;left = remove( node-&gt;left , key ); return node; } else if( key &gt; node-&gt;key ){ node-&gt;right = remove( node-&gt;right, key ); return node; } else{ // key == node-&gt;key if( node-&gt;left == NULL ){ Node *rightNode = node-&gt;right; delete node; count --; return rightNode; } if( node-&gt;right == NULL ){ Node *leftNode = node-&gt;left; delete node; count--; return leftNode; } // node-&gt;left != NULL &amp;&amp; node-&gt;right != NULL Node *successor = new Node(minimum(node-&gt;right)); count ++; successor-&gt;right = removeMin(node-&gt;right); successor-&gt;left = node-&gt;left; delete node; count --; return successor; } } }; .",
            "url": "https://zqfang.github.io/algorithm%20and%20data%20structure/2020/02/08/CS-binarytree.html",
            "relUrl": "/algorithm%20and%20data%20structure/2020/02/08/CS-binarytree.html",
            "date": " â€¢ Feb 8, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "What is Big O",
            "content": "What on earth is Big O? . Time complexity and space complexity . Time complexity . O(f(n)): number of commands need to execute. proportional to f(n). è¡¨ç¤ºè¿è¡Œç®—æ³•æ‰€éœ€è¦æ‰§è¡Œçš„æŒ‡ä»¤æ•°ï¼Œå’Œf(n)æˆæ­£ã€‚ . ä¸¥æ ¼æ¥è®²ï¼ŒO(f(n))è¡¨ç¤ºç®—æ³•æ‰§è¡Œçš„ä¸Šç•Œã€‚ä¸šç•Œé»˜è®¤ä¸ºç®—æ³•æ‰§è¡Œçš„æœ€ä½ä¸Šç•Œ(æœ€åæƒ…å†µï¼‰ã€‚ n represents the data scale æ•°æ®è§„æ¨¡ . when n is a large number, the constant is usually ignored. . algorithm n of cmd . Binary reserach $O(logn)$ | $a*logn$ | . Max/min in an array $O(n)$ | b*n | . merge sort $O(nlogn)$ | $c*nlogn$ | . select sort $O(n^2)$ | $d*n^2$ | . quick sort $O(nlogn)$ | e*nlogn | . adjacent graph | $O(V+E)$ | . Lazy Prim | $O(ElogE)$ | . Prim | $O(ElogV)$ | . Kruskal | $O(ElogE)$ | . Dijkstra | $O(ElogV)$ | . Bellman-Ford | $O(EV)$ | . minimum span tree Shortest path tree (Single source shortest path) . Space complexity . cmd complexity . new an array | $O(n)$ | . new 2d array | $O(n^2)$ | . new an constant space | $O(1)$ | . recursive function: the depth (n) of a recursive function, the extra space need $O(n)$. . Make sense of n . If you want to solve the problem in 1 second, then an algorithm of . complexity cmds n . $O(n^2)$ | could exec cmd n = $10^4$ | . $O(n)$ | could exec cmd n = $10^8$ | . $O(nlogn)$ | could exec cmd n = $10^7$ | . Example . binarySearch | find from n element find from $n/2$ element find from $n/4$ element ... find from 1 . Thatâ€™s, need how many steps of search when n = 1? $log_{2}n = O(logn)$. . int2string. Set num &gt; 0 | string int2string(int num) { string s=&quot;&quot;; while(num) { s += &#39;0&#39; + num%10; num /= 10; } reverse(s); // O(n) return s; } . That is, how many â€œ/10â€ steps when num = 0? $log_{10}n = O(logn)$. . Case: two nested for loop, not always $O(n^2)$ | void hello(int n){ for (int sz =1; sz &lt; n; sz ++ sz) // logn here for( int i=1; i &lt; n;; i++) //n cout&lt;&lt;&quot;hello, complex&quot; &lt;&lt;endl; } . So, should be $O(nlogn)$ . isPrime: $O( sqrt{n})$ // set n &gt; 1 bool isPrime(int n){ for( int x =2; x*x &lt;= n; x++){ if( n%x == 0) return false; return true; } } . | recursive function | single recursive function call int binarySearch(int arr[], int l, int r, int target) { if (l&gt;r) return -1; int mid = l +(r-l)/2; if (arr[mid] == target) return mid; else if (arr[mid] &gt; target) return binarySearch(arr, ;, mid-1, target); else return binarySearch(arr, mid+1, r, target); } . each step need O(1), so overall complexity depend on recursive exec depth. That is, if each function call needs time T, then time complexity: O(T*depth) -&gt; O(n). . | . Another example: recursion depth logn, them time complexity O(logn). . double pow( double x, int n){ assert(n &gt;=0); if (n==0) return 1.0; double t = pow(x, n/2); if( n%2) return x*t*t; return t*t; } . multi recursive exec how many exec step? int f(int n) { assert(n &gt;=0); if(n == 0) return 1; return f(n-1) + f(n); } . thatâ€™s, count how many nodes on a full binary tree. $2^{n+1} -1 = O(2^n)$ . | . how to think about this? . void mergeSort(int arr[]. int l. int r){ if (l &gt;=r) return; int mid = (l+r) /2; mergeSort(arr, l, mid); mergeSort(arr, mid+1, r); merge(arr, l, mid, r); } . For binary tree, complexity for each level O(n), while tree depth O(logn). Overall, O(nlogn) . Amortized time i.e. dynamic vector/stack/deque | template&lt;T&gt; class MyVector{ private: T* data; int capacity; int size; //O(n) void resize(int newCapacity){ assert(newCapacity &gt;= size); T* newData = new T[newCapacity]; for(int i = 0; i &lt; size; i++ ){ newData[i] = data[i]; } delete[] data; data = newData; capacity = newCapacity; } public: MyVector() { data = new T[10]; capacity = 10; size = 0; } ~MyVector() { delete[] data; } // Average: O(1) void push_back(T e){ //assert(size &lt; capacity) if (size == capacity) resize (2 *capacity); data[size++] = e; } // Average O(1) T pop_back(){ assert (size &gt;0); T ret = data[size-1]; size --; // note the denominator here. To advoid ossilation of space complexity if(size == capacity / 4) resize(capacity /2); return ret; } }; .",
            "url": "https://zqfang.github.io/algorithm%20and%20data%20structure/2020/02/01/CS-algo.html",
            "relUrl": "/algorithm%20and%20data%20structure/2020/02/01/CS-algo.html",
            "date": " â€¢ Feb 1, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Statistical Modeling and Inference",
            "content": "A breif review over the foundations of statistical inference . Statistical Models and Inference . statistical inference: a formal approach to characterizing a random phenomenon using observations, either by providing a description of a past phenomenon or by giving some predictions about future phenomenon of similar nature. . 1. Statistical Models . The first step in statistical inference is to specify a statistical model, under some simplifying assumptions (i.e. independence assumptions). . Hierarchical models: the probability distribution of one parameter is dependent on the values of other hierachical paramters (i.e. conditional independent). | . Steps: . Set assumptions (i.e. independent), parameter and model. Make explicit assumptions on the probability distributions. focus on parametric modeling, because of limited data | nonparametric not consider here, used for hypothesis testing or when sample size is very large. | . | Once the model is specified, then choose a method of inference, as well as an algorithm to obtain estimates. Most commonly use: Maximum likelihood inference | Bayesian inference | . | 2. Maximum likelihood inference . Quantifying confidence: the Fisher Information Matrix Newtonâ€™s algorithm Approximate Techniques . Monte Carlo Sampling for intractable likelihoods | Composite likelihood | . 3. Bayesian Inference . A statistical model describes the uncertainty about how the data was produced. The ultimate aim for statistical inference is to obtain information about the unknown parameter Î¸ thetaÎ¸ given the data D mathcal{D}D. . Frequentist: Î¸ thetaÎ¸ is fixed but unknown quantity. | Bayesian: use a fully probabilistic model and treat Î¸ thetaÎ¸ as a random quantity. To do so, chose an appropriate prior distribution P(Î¸) mathbb{P}( theta)P(Î¸), which reflects the knowledge (i.e. uncertainty) about Î¸ thetaÎ¸ prior to the experiment | the goal is to update the knowledge given the information contained in the data D mathcal{D}D. | the updated knowledge (i.e. reduced uncertainty) is encapsulated in the posterior distribution P(Î¸âˆ£D) mathbb{P}( theta vert mathcal{D})P(Î¸âˆ£D), which is calculated via Bayesâ€™theorem. | . | . P(Î¸âˆ£D)=P(Dâˆ£Î¸)P(Î¸)P(D) mathbb{P}( boldsymbol{ theta} | mathcal{D})= frac{ mathbb{P}( mathcal{D} | boldsymbol{ theta}) mathbb{P}( boldsymbol{ theta})}{ mathbb{P}( mathcal{D})}P(Î¸âˆ£D)=P(D)P(Dâˆ£Î¸)P(Î¸)â€‹ . The bayesian paradigm boils down to the slogan: posterior âˆ proptoâˆ likelihood Ã— timesÃ— prior . 3.1 Choice of prior distributions . Conjugate priors the prior and the posterior lie in the same class of distributions. | often chosen, because it leads to a well-known form of the posterior, which simplifies the calculations | . | choose a prior that contains as little information about the parameter as possible at first choice would, of course, be a locally uniform prior. Under a uniform prior we have P(Î¸âˆ£D)âˆL(Î¸) mathbb{P}( boldsymbol{ theta} vert boldsymbol{D}) propto mathcal{L}( boldsymbol{ theta})P(Î¸âˆ£D)âˆL(Î¸). | Jeffreyâ€™s prior, but often hard to come by | . | 3.2 Bayesian point estimates and confidence intervals . Bayesian point estimates: the posterior mean, mode and median . Î¸^=E[Î¸âˆ£D]=âˆ«Î¸P(Î¸âˆ£D)dÎ¸ hat{ theta}= mathbb{E}[ theta | D]= int theta mathbb{P}( theta | mathcal{D}) mathrm{d} thetaÎ¸^=E[Î¸âˆ£D]=âˆ«Î¸P(Î¸âˆ£D)dÎ¸ . confidence: highest posterior density (HPD) region . for a threshold value Ï€ piÏ€, the region CÎ±={Î¸:P(Î¸âˆ£D)&gt;Ï€} mathcal{C}_{ alpha}= { theta: mathbb{P}( theta vert mathcal{D})&gt; pi }CÎ±â€‹={Î¸:P(Î¸âˆ£D)&gt;Ï€}, we get . âˆ«CÎ±P(Î¸âˆ£D)dÎ¸=1âˆ’Î± int_{C_{ alpha}} mathbb{P}( theta | mathcal{D}) mathrm{d} theta=1- alphaâˆ«CÎ±â€‹â€‹P(Î¸âˆ£D)dÎ¸=1âˆ’Î± . This region CÎ± mathcal{C}_{ alpha}CÎ±â€‹ is the HPD region. . 3.3 Markov Chain Monte Carlo . A common challenge in Bayesian inference is that the integral . P(D)=âˆ«P(Dâˆ£Î¸)P(Î¸)dÎ¸ mathbb{P}(D)= int mathbb{P}(D | theta) mathbb{P}( theta) d_{ theta}P(D)=âˆ«P(Dâˆ£Î¸)P(Î¸)dÎ¸â€‹ . canâ€™t be solved analytically. . to be continuedâ€¦ . 3.4 Empirical Bayes for Latent Variable Problems . The first step is to infer point estimates for the parameters at higher levels by integrating out those at lower levels, and the infer posterior distributions for lower level parameters while setting those at a higher level to their point estimate. . 3.5 Approximate Bayesian Computation . Approximate Bayesian computation (ABC) is a class of simulation-based techniques to conduct Bayesian inference under models with intractable likelihoods . 3.6 Model selection . how to compare the several candidate models explaining the data D mathcal{D}D? the most commonly used methods: . likelihood ratio statistic | model posterior probabilities | Bayes factors | others: cross-validation, Akaikeâ€™s information criterion (AIC), Bayesian information criterion (BIC) | . 4. Naive Bayes and Bayesian estimation . Naive Bayes and Bayesian estimation are two different concepts! . Naive Bayes is a statistical learning method. For a give training set, learn the join probability distribution of P(X,Y)P(X,Y)P(X,Y). Based on this model, for a given input xxx, output a yyy with maximal posterior probability (Bayes theorem). . Set prior prob distribution: . P(Y=ck),k=1,2,â‹¯â€‰,KP left(Y=c_{k} right), quad k=1,2, cdots, KP(Y=ckâ€‹),k=1,2,â‹¯,K . Conditional prob distribution: . P(X=xâˆ£Y=ck)=P(X(1)=x(1),â‹¯â€‰,X(n)=x(n)âˆ£Y=ck),k=1,2,â‹¯â€‰,KP left(X=x | Y=c_{k} right)=P left(X^{(1)}=x^{(1)}, cdots, X^{(n)}=x^{(n)} | Y=c_{k} right), quad k=1,2, cdots, KP(X=xâˆ£Y=ckâ€‹)=P(X(1)=x(1),â‹¯,X(n)=x(n)âˆ£Y=ckâ€‹),k=1,2,â‹¯,K . Naive Bayes make a strong assumption that conditional prob distribution are all conditional independent, which is: . P(X=xâˆ£Y=ck)=P(X(1)=x(1),â‹¯â€‰,X(n)=x(n)âˆ£Y=ck)=âˆj=1nP(X(j)=x(j)âˆ£Y=ck) begin{aligned} P left(X=x | Y=c_{k} right) &amp;=P left(X^{(1)}=x^{(1)}, cdots, X^{(n)}=x^{(n)} | Y=c_{k} right) &amp;= prod_{j=1}^{n} P left(X^{(j)}=x^{(j)} | Y=c_{k} right) end{aligned}P(X=xâˆ£Y=ckâ€‹)â€‹=P(X(1)=x(1),â‹¯,X(n)=x(n)âˆ£Y=ckâ€‹)=j=1âˆnâ€‹P(X(j)=x(j)âˆ£Y=ckâ€‹)â€‹ . then, posterior prob is: . P(Y=ckâˆ£X=x)=P(X=xâˆ£Y=ck)P(Y=ck)âˆ‘kP(X=xâˆ£Y=ck)P(Y=ck)P left(Y=c_{k} | X=x right)= frac{P left(X=x | Y=c_{k} right) P left(Y=c_{k} right)}{ sum_{k} P left(X=x | Y=c_{k} right) P left(Y=c_{k} right)}P(Y=ckâ€‹âˆ£X=x)=âˆ‘kâ€‹P(X=xâˆ£Y=ckâ€‹)P(Y=ckâ€‹)P(X=xâˆ£Y=ckâ€‹)P(Y=ckâ€‹)â€‹ . If conditional probability of each input variable is not independent, then model become Baysian Network! . Naive Bayes Classifier is: . y=f(x)=argâ¡maxâ¡ckP(Y=ck)âˆjP(X(j)=x(j)âˆ£Y=ck)âˆ‘kP(Y=ck)âˆjP(X(j)=x(j)âˆ£Y=ck)y=f(x)= arg max _{c_{k}} frac{P left(Y=c_{k} right) prod_{j} P left(X^{(j)}=x^{(j)} | Y=c_{k} right) }{ sum_{k} P left(Y=c_{k} right) prod_{j} P left(X^{(j)}=x^{(j)} | Y=c_{k} right)}y=f(x)=argckâ€‹maxâ€‹âˆ‘kâ€‹P(Y=ckâ€‹)âˆjâ€‹P(X(j)=x(j)âˆ£Y=ckâ€‹)P(Y=ckâ€‹)âˆjâ€‹P(X(j)=x(j)âˆ£Y=ckâ€‹)â€‹ . and itâ€™s short form: . y=f(x)=argâ¡maxâ¡ckP(Y=ck)âˆjP(X(j)=x(j)âˆ£Y=ck)y=f(x)= arg max _{c_{k}} P left(Y=c_{k} right) prod_{j} P left(X^{(j)}=x^{(j)} | Y=c_{k} right)y=f(x)=argckâ€‹maxâ€‹P(Y=ckâ€‹)jâˆâ€‹P(X(j)=x(j)âˆ£Y=ckâ€‹) .",
            "url": "https://zqfang.github.io/machine%20learning/2020/01/30/ML-Bayes.html",
            "relUrl": "/machine%20learning/2020/01/30/ML-Bayes.html",
            "date": " â€¢ Jan 30, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Loss function for multi-label classification",
            "content": "Multi-label classification, tasks commonly be seen on health record data (multi symptoms). . Loss function design: . Multi binary cross-entropy each class has a binary output . | Label smoothing, another regularization technique . | Itâ€™s designed to make the model a little bit less certain of itâ€™s decision by changing a little bit its target: instead of wanting to predict 1 for the correct class and 0 for all the others, we ask it to predict 1-Îµ for the correct class and Îµ for all the others, with Îµ a (small) positive number and N the number of classes. This can be written as: . loss=(1âˆ’Îµ)ce(i)+Îµâˆ‘ce(j)/N text {loss}=(1- varepsilon) c e(i)+ varepsilon sum c e(j) / Nloss=(1âˆ’Îµ)ce(i)+Îµâˆ‘ce(j)/N . where ce(x) is cross-entropy of x (i.e. âˆ’log(px)), and i is the correct class. . finally, for multi-label loss function: . (1âˆ’Ïµ)âˆ‘i(âˆ’logâ¡pin)+ÏµNâˆ‘(âˆ’logâ¡pi)(1- epsilon) sum_{i} left(- frac{ log p_{i}}{n} right)+ frac{ epsilon}{N} sum left(- log p_{i} right)(1âˆ’Ïµ)iâˆ‘â€‹(âˆ’nlogpiâ€‹â€‹)+NÏµâ€‹âˆ‘(âˆ’logpiâ€‹) . See the fastai implementation here: LabelSmoothingCrossEntropy . about line 285: . class LabelSmoothingCrossEntropy(Module): y_int = True def __init__(self, eps:float=0.1, reduction=&#39;mean&#39;): self.eps,self.reduction = eps,reduction def forward(self, output, target): c = output.size()[-1] log_preds = F.log_softmax(output, dim=-1) if self.reduction==&#39;sum&#39;: loss = -log_preds.sum() else: loss = -log_preds.sum(dim=-1) if self.reduction==&#39;mean&#39;: loss = loss.mean() return loss*self.eps/c + (1-self.eps) * F.nll_loss(log_preds, target.long(), reduction=self.reduction) def activation(self, out): return F.softmax(out, dim=-1) def decodes(self, out): return out.argmax(dim=-1) .",
            "url": "https://zqfang.github.io/machine%20learning/2020/01/29/ML-Lossfunc.html",
            "relUrl": "/machine%20learning/2020/01/29/ML-Lossfunc.html",
            "date": " â€¢ Jan 29, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Mento Carlos",
            "content": "è’™ç‰¹å¡ç½—æ–¹æ³•ï¼Œåˆç§°ç»Ÿè®¡æ¨¡æ‹Ÿæ–¹æ³•(statistical simulation method), é€šè¿‡æ¦‚ç‡æ¨¡å‹çš„éšæœºæŠ½æ ·è¿›è¡Œè¿›è¡Œè¿‘ä¼¼æ•°å€¼è®¡ç®—çš„æ–¹æ³•ã€‚ . 1. è’™ç‰¹å¡ç½—æ–¹æ³•çš„æ ¸å¿ƒ . è’™ç‰¹å¡ç½—æ–¹æ³•çš„æ ¸å¿ƒæ˜¯éšæœºæŠ½æ ·(random sampling) . 2. Monte Carlo intergration: . è®¡ç®—h(x)ç§¯åˆ† . âˆ«Xh(x)dx int_{ mathcal{X}} h(x) mathrm{d} xâˆ«Xâ€‹h(x)dx . å°†h(x)åˆ†è§£æˆf(x)å’Œæ¦‚ç‡å¯†åº¦å‡½æ•°p(x)çš„ä¹˜ç§¯ï¼Œå³å‡½æ•°h(x)çš„ç§¯åˆ†å¯ä»¥è¡¨ç¤ºä¸ºå‡½æ•°f(x)å…³äºæ¦‚ç‡å¯†åº¦å‡½æ•°p(x)çš„æ•°å­¦æœŸæœ›ï¼š . âˆ«Xh(x)dx=âˆ«Xf(x)p(x)dx=Ep(x)[f(x)] int_{ mathcal{X}} h(x) mathrm{d} x= int_{ mathcal{X}} f(x) p(x) mathrm{d} x=E_{p(x)}[f(x)]âˆ«Xâ€‹h(x)dx=âˆ«Xâ€‹f(x)p(x)dx=Ep(x)â€‹[f(x)] . å› æ­¤ï¼Œå¯åˆ©ç”¨æ ·æœ¬å‡å€¼è®¡ç®—è¿‘ä¼¼ç§¯åˆ†ï¼š . âˆ«Xh(x)dx=Ep(x)[f(x)]â‰ˆ1nâˆ‘i=1nf(xi) int_{ mathcal{X}} h(x) mathrm{d} x=E_{p(x)}[f(x)] approx frac{1}{n} sum_{i=1}^{n} f left(x_{i} right)âˆ«Xâ€‹h(x)dx=Ep(x)â€‹[f(x)]â‰ˆn1â€‹i=1âˆ‘nâ€‹f(xiâ€‹) . å˜å½¢ . Ep(z)[f(z)]=âˆ«f(z)p(z)dz=âˆ«f(z)p(z)q(z)âŸnewf~(z)q(z)dzâ‰ˆ1Nâˆ‘n=1Nf(zi)p(zi)q(zi) begin{aligned} mathrm{E}_{p(z)}[f(z)] &amp;= int f(z) p(z) dz &amp;= int underbrace{f(z) frac{p(z)}{q(z)}}_{new tilde{f}(z)} q(z) dz &amp; approx frac{1}{N} sum_{n=1}^{N} f(z^{i}) frac{p(z^{i})}{q(z^{i})} end{aligned}Ep(z)â€‹[f(z)]â€‹=âˆ«f(z)p(z)dz=âˆ«newf~â€‹(z) . f(z)q(z)p(z)â€‹â€‹â€‹q(z)dzâ‰ˆN1â€‹n=1âˆ‘Nâ€‹f(zi)q(zi)p(zi)â€‹â€‹ . 3. Markov Chain Monte Carlo . Markov chain or markov process: . P(Xtâˆ£X0,X1,â‹¯â€‰,Xtâˆ’1)=P(Xtâˆ£Xtâˆ’1),t=1,2,â‹¯P left(X_{t} | X_{0}, X_{1}, cdots, X_{t-1} right)=P left(X_{t} | X_{t-1} right), quad t=1,2, cdotsP(Xtâ€‹âˆ£X0â€‹,X1â€‹,â‹¯,Xtâˆ’1â€‹)=P(Xtâ€‹âˆ£Xtâˆ’1â€‹),t=1,2,â‹¯ . å‚è€ƒï¼š æèˆªã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹ .",
            "url": "https://zqfang.github.io/machine%20learning/2020/01/26/ML-MonteCarlo.html",
            "relUrl": "/machine%20learning/2020/01/26/ML-MonteCarlo.html",
            "date": " â€¢ Jan 26, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Expectation Maximization",
            "content": "Maximum Likelihood Estimation Gaussian Mixture Model Expectation Maximization . 1. Probability and likelihood . likehood &amp; maximum likehood . åœ¨éæ­£å¼åœºåˆä¼¼ç„¶ï¼ˆlikelihoodï¼‰å’Œæ¦‚ç‡ï¼ˆProbabilityï¼‰å‡ ä¹æ˜¯ä¸€å¯¹åŒä¹‰è¯ï¼Œä½†æ˜¯åœ¨ç»Ÿè®¡å­¦ä¸­ä¼¼ç„¶å’Œæ¦‚ç‡å´æ˜¯ä¸¤ä¸ªä¸åŒçš„æ¦‚å¿µã€‚ . æ¦‚ç‡: åœ¨ç‰¹å®šç¯å¢ƒä¸‹æŸä»¶äº‹æƒ…å‘ç”Ÿçš„å¯èƒ½æ€§ï¼Œä¹Ÿå°±æ˜¯ç»“æœæ²¡æœ‰äº§ç”Ÿä¹‹å‰ä¾æ®ç¯å¢ƒæ‰€å¯¹åº”çš„å‚æ•°æ¥é¢„æµ‹æŸä»¶äº‹æƒ…å‘ç”Ÿçš„å¯èƒ½æ€§ã€‚ . æ¯”å¦‚æŠ›ç¡¬å¸ï¼ŒæŠ›ä¹‹å‰æˆ‘ä»¬ä¸çŸ¥é“æœ€åæ˜¯å“ªä¸€é¢æœä¸Šï¼Œä½†æ˜¯æ ¹æ®ç¡¬å¸çš„æ€§è´¨æˆ‘ä»¬å¯ä»¥æ¨æµ‹ä»»ä½•ä¸€é¢æœä¸Šçš„å¯èƒ½æ€§å‡ä¸º50%ï¼Œè¿™ä¸ªæ¦‚ç‡åªæœ‰åœ¨æŠ›ç¡¬å¸ä¹‹å‰æ‰æ˜¯æœ‰æ„ä¹‰çš„ï¼ŒæŠ›å®Œç¡¬å¸åçš„ç»“æœä¾¿æ˜¯ç¡®å®šçš„ï¼› . ä¼¼ç„¶: åˆšå¥½ç›¸åï¼Œæ˜¯åœ¨ç¡®å®šçš„ç»“æœä¸‹å»æ¨æµ‹äº§ç”Ÿè¿™ä¸ªç»“æœçš„å¯èƒ½ç¯å¢ƒï¼ˆå‚æ•°ï¼‰ã€‚ . å‡è®¾éšæœºæŠ›æ·ä¸€æšç¡¬å¸1,000æ¬¡ï¼Œç»“æœ500æ¬¡äººå¤´æœä¸Šï¼Œ500æ¬¡æ•°å­—æœä¸Šï¼Œé‚£ä¹ˆä¸¤é¢æœä¸Šçš„æ¦‚ç‡å‡ä¸º50%ã€‚è¿ç”¨å‡ºç°çš„ç»“æœæ¥åˆ¤æ–­è¿™ä¸ªäº‹æƒ…æœ¬èº«çš„æ€§è´¨ï¼ˆå‚æ•°ï¼‰ï¼Œä¹Ÿå°±æ˜¯ä¼¼ç„¶ã€‚ . å½“ç»“æœå’Œå‚æ•°ç›¸äº’å¯¹åº”ï¼Œä¼¼ç„¶å’Œæ¦‚ç‡åœ¨æ•°å€¼ä¸Šç›¸ç­‰ã€‚ ç”¨ Î¸ è¡¨ç¤ºç¯å¢ƒå¯¹åº”çš„å‚æ•°ï¼Œx è¡¨ç¤ºç»“æœï¼Œé‚£ä¹ˆæ¦‚ç‡å¯ä»¥è¡¨ç¤ºä¸ºï¼š . P(xâˆ£Î¸)P(x | theta )P(xâˆ£Î¸) . p(x vertÎ¸) æ˜¯æ¡ä»¶æ¦‚ç‡çš„è¡¨ç¤ºæ–¹æ³•ã€‚Î¸ æ˜¯å‰ç½®æ¡ä»¶ï¼Œç†è§£ä¸ºåœ¨ Î¸ çš„å‰æä¸‹ï¼Œäº‹ä»¶ x å‘ç”Ÿçš„æ¦‚ç‡ï¼Œç›¸å¯¹åº”çš„ä¼¼ç„¶å¯ä»¥è¡¨ç¤ºä¸º: . L(Î¸âˆ£x) mathcal{L}( theta | x)L(Î¸âˆ£x) . å¯ä»¥ç†è§£ä¸ºå·²çŸ¥ç»“æœä¸º x ï¼Œå‚æ•°ä¸º Î¸ (ä¼¼ç„¶å‡½æ•°é‡Œ Î¸ æ˜¯å˜é‡ï¼Œè¿™é‡Œè¯´çš„å‚æ•°å’Œå˜é‡æ˜¯ç›¸å¯¹ä¸æ¦‚ç‡è€Œè¨€çš„)å¯¹åº”çš„æ¦‚ç‡ï¼Œå³ï¼š . L(Î¸âˆ£x)=P(xâˆ£Î¸) mathcal{L}( theta | x)=P(x | theta)L(Î¸âˆ£x)=P(xâˆ£Î¸) . ä¸¤è€…åœ¨æ•°å€¼ä¸Šç›¸ç­‰ï¼Œä½†æ˜¯æ„ä¹‰å¹¶ä¸ç›¸åŒï¼ŒL mathcal{L}L æ˜¯å…³äº Î¸ çš„å‡½æ•°ï¼Œè€Œ P åˆ™æ˜¯å…³äº x çš„å‡½æ•°ã€‚ . 2. Maximum Likelihood Estimation . å•é«˜æ–¯æ¨¡å‹ xâˆ¼N(Î¼,Î£)x sim mathcal{N}( mu, Sigma)xâˆ¼N(Î¼,Î£), xiâˆˆDx_{i} in mathcal{D}xiâ€‹âˆˆD, é‚£ä¹ˆå¯¹å‚æ•° Î¼ muÎ¼å’Œ Î£ SigmaÎ£ è¿›è¡Œä¼°è®¡ï¼Œåªéœ€è¦æœ€å¤§åŒ–log-likelihoodå‡½æ•°ï¼š . logâ¡p(X)=âˆ‘i=1Nlogâ¡N(xiâˆ£Î¼,Î£)=âˆ‘i=1Nlogâ¡12Ï€Ïƒeâˆ’(xiâˆ’Î¼)22Ïƒ2=âˆ‘i=1Nlogâ¡12Ï€Ïƒ+âˆ‘i=1Nâˆ’(xiâˆ’Î¼)22Ïƒ2=âˆ’N2logâ¡2Ï€âˆ’N2logâ¡Ïƒ2âˆ’12Ïƒ2âˆ‘i=1N(xiâˆ’Î¼)2 begin{aligned} log p(X) &amp;= sum_{i=1}^{N} log mathcal{N} left(x_{i} | mu, Sigma right) &amp;= sum_{i=1}^{N} log frac{1}{ sqrt{2 pi} sigma} e^{- frac{ left(x_{i}- mu right)^{2}}{2 sigma^{2}}} &amp;= sum_{i=1}^{N} log frac{1}{ sqrt{2 pi} sigma}+ sum_{i=1}^{N}- frac{ left(x_{i}- mu right)^{2}}{2 sigma^{2}} &amp;=- frac{N}{2} log 2 pi- frac{N}{2} log sigma^{2}- frac{1}{2 sigma^{2}} sum_{i=1}^{N} left(x_{i}- mu right)^{2} end{aligned}logp(X)â€‹=i=1âˆ‘Nâ€‹logN(xiâ€‹âˆ£Î¼,Î£)=i=1âˆ‘Nâ€‹log2Ï€ . â€‹Ïƒ1â€‹eâˆ’2Ïƒ2(xiâ€‹âˆ’Î¼)2â€‹=i=1âˆ‘Nâ€‹log2Ï€ . â€‹Ïƒ1â€‹+i=1âˆ‘Nâ€‹âˆ’2Ïƒ2(xiâ€‹âˆ’Î¼)2â€‹=âˆ’2Nâ€‹log2Ï€âˆ’2Nâ€‹logÏƒ2âˆ’2Ïƒ21â€‹i=1âˆ‘Nâ€‹(xiâ€‹âˆ’Î¼)2â€‹ . æ±‚åå¯¼æ•°ï¼Œå¾—åˆ°å‚æ•°ä¼°è®¡ï¼š . âˆ‚logâ¡p(X)âˆ‚Î¼=1Ïƒ2âˆ‘i=1N(xiâˆ’Î¼)=0â‡’Î¼=1Nâˆ‘i=1Nxiâˆ‚logâ¡p(X)âˆ‚Ïƒ2=âˆ’N2Ïƒ2+12Ïƒ4âˆ‘i=1N(xiâˆ’Î¼)2=0â‡’Ïƒ2=1Nâˆ‘i=1N(xiâˆ’Î¼)2 begin{aligned} frac{ partial log p(X)}{ partial mu} &amp;= frac{1}{ sigma^{2}} sum_{i=1}^{N} left(x_{i}- mu right)=0 &amp; Rightarrow mu= frac{1}{N} sum_{i=1}^{N} x_{i} frac{ partial log p(X)}{ partial sigma^{2}} &amp;=- frac{N}{2 sigma^{2}}+ frac{1}{2 sigma^{4}} sum_{i=1}^{N} left(x_{i}- mu right)^{2}=0 &amp; Rightarrow sigma^{2}= frac{1}{N} sum_{i=1}^{N} left(x_{i}- mu right)^{2} end{aligned}âˆ‚Î¼âˆ‚logp(X)â€‹âˆ‚Ïƒ2âˆ‚logp(X)â€‹â€‹=Ïƒ21â€‹i=1âˆ‘Nâ€‹(xiâ€‹âˆ’Î¼)=0â‡’Î¼=N1â€‹i=1âˆ‘Nâ€‹xiâ€‹=âˆ’2Ïƒ2Nâ€‹+2Ïƒ41â€‹i=1âˆ‘Nâ€‹(xiâ€‹âˆ’Î¼)2=0â‡’Ïƒ2=N1â€‹i=1âˆ‘Nâ€‹(xiâ€‹âˆ’Î¼)2â€‹ . 3. Gaussian Mixture Model . å¦‚æœæœ‰Kä¸ªé«˜æ–¯çº¿æ€§å åŠ : . p(x)=âˆ‘k=1KÏ€kN(xâˆ£Î¼k,Î£k)Â s.t.Â âˆ‘k=1KÏ€k=10â‰¤Ï€kâ‰¤1 begin{aligned} p(x)=&amp; sum_{k=1}^{K} pi_{k} mathcal{N} left(x | mu_{k}, Sigma_{k} right) &amp; text { s.t. } sum_{k=1}^{K} pi_{k}=1 &amp; 0 leq pi_{k} leq 1 end{aligned}p(x)=â€‹k=1âˆ‘Kâ€‹Ï€kâ€‹N(xâˆ£Î¼kâ€‹,Î£kâ€‹)Â s.t.Â k=1âˆ‘Kâ€‹Ï€kâ€‹=10â‰¤Ï€kâ€‹â‰¤1â€‹ . é‚£ä¹ˆå¯¹æ•°ä¼¼ç„¶å‡½æ•°ä¸º . logâ¡p(X)=âˆ‘i=1Nlogâ¡{âˆ‘k=1KÏ€kN(xiâˆ£Î¼k,Î£k)} log p(X)= sum_{i=1}^{N} log left { sum_{k=1}^{K} pi_{k} mathcal{N} left(x_{i} | mu_{k}, Sigma_{k} right) right }logp(X)=i=1âˆ‘Nâ€‹log{k=1âˆ‘Kâ€‹Ï€kâ€‹N(xiâ€‹âˆ£Î¼kâ€‹,Î£kâ€‹)} . å› ä¸ºå¯¹æ•°é‡Œæœ‰æ±‚å’Œï¼Œå› æ­¤æ— æ³•æ— æ³•ç›´æ¥é€šè¿‡æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ–¹æ³•è¿›è¡Œå‚æ•°ä¼°è®¡ã€‚ . å…¶ä¸­ï¼Œå¦‚æœÏ€k pi_{k}Ï€kâ€‹æ˜¯æ¯ä¸ªé«˜æ–¯å‡ºç°çš„æ¦‚ç‡p(k)p(k)p(k)ï¼Œåˆ™é«˜æ–¯æ··åˆæ¨¡å‹åˆ†è§£ä¸ºä»¥p(k)p(k)p(k)è·å¾—ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œç„¶ååœ¨åˆ†å¸ƒä¸­è·å¾—xxxï¼Œå› æ­¤xxxè¾¹ç¼˜æ¦‚ç‡åˆ†å¸ƒä¸ºï¼š . p(x)=âˆ‘k=1Kp(k)p(xâˆ£k)p(x)= sum_{k=1}^{K} p(k) p(x | k)p(x)=k=1âˆ‘Kâ€‹p(k)p(xâˆ£k) . åéªŒæ¦‚ç‡p(kâˆ£x)p(k vert x)p(kâˆ£x)è¡¨ç¤ºxxxå±äºæ¯ä¸ªé«˜æ–¯çš„æ¦‚ç‡ï¼ˆç¦»æ•£å€¼ï¼‰: . p(kâˆ£x)=p(xâˆ£k)p(k)âˆ‘lp(xâˆ£l)p(l)=Ï€kN(xâˆ£Î¼k,Î£k)âˆ‘lÏ€lN(xâˆ£Î¼l,Î£l) begin{aligned} p(k | x) &amp;= frac{p(x | k) p(k)}{ sum_{l} p(x | l) p(l)} &amp;= frac{ pi_{k} mathcal{N} left(x | mu_{k}, Sigma_{k} right)}{ sum_{l} pi_{l} mathcal{N} left(x | mu_{l}, Sigma_{l} right)} end{aligned}p(kâˆ£x)â€‹=âˆ‘lâ€‹p(xâˆ£l)p(l)p(xâˆ£k)p(k)â€‹=âˆ‘lâ€‹Ï€lâ€‹N(xâˆ£Î¼lâ€‹,Î£lâ€‹)Ï€kâ€‹N(xâˆ£Î¼kâ€‹,Î£kâ€‹)â€‹â€‹ . 4. Expectation Maximization . æ€æƒ³ï¼š é€šè¿‡å¼•å…¥éšå˜é‡ï¼Œè¿ç”¨è¿­ä»£æ–¹æ³•ï¼Œæ±‚è§£æ··åˆé«˜æ–¯æ¨¡å‹ . Î¸(t+1)=argâ¡maxâ¡Î¸L(Î¸;X) theta^{(t+1)}= underset{ theta}{ arg max } mathcal{L}( theta ; X)Î¸(t+1)=Î¸argmaxâ€‹L(Î¸;X) . å¼•å…¥éšå˜é‡Zi(çŠ¶æ€iï¼‰ï¼Œ zæœä»å¤šé¡¹åˆ†å¸ƒï¼Œé€‰æ‹©ziçš„æ¦‚ç‡ä¸ºp(zi),åˆ™é«˜æ–¯æ··åˆæ¨¡å‹ä¸ºï¼š . ziâˆ¼Multinoimalâ¡(Ï€1,â‹¯â€‰,Ï€k)xiâˆ£ziâˆ¼N(Î¼zi,Î£zi) begin{aligned} z_{i} &amp; sim operatorname{Multinoimal} left( pi_{1}, cdots, pi_{k} right) x_{i} | z_{i} &amp; sim mathcal{N} left( mu_{z_{i}}, Sigma_{z_{i}} right) end{aligned}ziâ€‹xiâ€‹âˆ£ziâ€‹â€‹âˆ¼Multinoimal(Ï€1â€‹,â‹¯,Ï€kâ€‹)âˆ¼N(Î¼ziâ€‹â€‹,Î£ziâ€‹â€‹)â€‹ . æ­¥éª¤ï¼š . E-Step: åœ¨ç°æœ‰Î¸(t) theta^{(t)}Î¸(t)ä¸‹æœ€å¤§åŒ–ä¼¼ç„¶ä¸‹ç•Œ, è®¡ç®—éšå˜é‡zçš„æœŸæœ›Q(zi)=p(ziâˆ£xi,Î¸)Q left(z_{i} right)=p left(z_{i} vert x_{i}, theta right)Q(ziâ€‹)=p(ziâ€‹âˆ£xiâ€‹,Î¸) ä½œä¸ºå…¶ä¸‹ç•Œ | M-Step: åœ¨ä¸Šé¢Q(zi)Q(z_{i})Q(ziâ€‹)ä¸‹è®¡ç®—å‚æ•°åˆ—è¡¨Î¸ thetaÎ¸æ¥æœ€å¤§åŒ–ä¼¼ç„¶ | . (0) ç†è§£EMçš„å‰æ . å‡¹å‡¸å‡½æ•°: âˆ€xâˆˆR,fâ€²â€²(x)â‰¥0 forall_{x in mathbb{R}}, f^{ prime prime}(x) geq 0âˆ€xâˆˆRâ€‹,fâ€²â€²(x)â‰¥0,åˆ™fffä¸ºå‡¸å‡½æ•°ã€‚ å½“xä¸ºå‘é‡ï¼Œå¦‚æœå…¶hessiançŸ©é˜µHæ˜¯åŠæ­£å®šçš„(Hâ‰¥0H geq 0Hâ‰¥0),åˆ™fffä¸ºå‡¸å‡½æ•° å¦‚æœfâ€²â€²(x)&gt;0f^{ prime prime}(x)&gt;0fâ€²â€²(x)&gt;0æˆ–è€…H&gt;0H&gt;0H&gt;0, fffæ˜¯ä¸¥æ ¼å‡¸å‡½æ•°ã€‚ å¦‚æœfâ€²â€²(x)&lt;0f^{ prime prime}(x)&lt;0fâ€²â€²(x)&lt;0æˆ–è€…H&gt;0H&gt;0H&gt;0, fffæ˜¯å‡¹å‡½æ•°ã€‚ . Jensen ä¸ç­‰å¼: . å¦‚æœfffä¸ºå‡¸å‡½æ•°, åˆ™E[f(X)]â‰¥f(E[X])E[f(X)] geq f(E[X])E[f(X)]â‰¥f(E[X])ã€‚å½“ä¸”ä»…å½“xæ˜¯å¸¸æ•°æ—¶ï¼ŒE[f(x)]=f(E[x])E[f(x)]=f(E[x])E[f(x)]=f(E[x])ã€‚ | å¦‚æœfffæ˜¯å‡¹å‡½æ•°, åˆ™E[f(X)]â‰¤f(E[X])E[f(X)] leq f(E[X])E[f(X)]â‰¤f(E[X])ã€‚ | å¼•å…¥éšå˜é‡åï¼Œå˜æ¢å¯¹æ•°ä¼¼ç„¶å‡½æ•°: . L(Î¸;X)=âˆ‘i=1Nlogâ¡p(xiâˆ£Î¸)=âˆ‘i=1Nlogâ¡âˆ‘zip(xi,ziâˆ£Î¸)=âˆ‘i=1Nlogâ¡âˆ‘ziQ(zi)p(xi,ziâˆ£Î¸)Q(zi)â‰¥âˆ‘i=1Nâˆ‘ziQ(zi)logâ¡p(xi,ziâˆ£Î¸)Q(zi) begin{aligned} mathcal{L}( theta ; X) &amp;= sum_{i=1}^{N} log p left(x_{i} | theta right) &amp;= sum_{i=1}^{N} log sum_{z_{i}} p left(x_{i}, z_{i} | theta right) &amp;= sum_{i=1}^{N} log sum_{z_{i}} Q left(z_{i} right) frac{p left(x_{i}, z_{i} | theta right)}{Q left(z_{i} right)} &amp; geq sum_{i=1}^{N} sum_{z_{i}} Q left(z_{i} right) log frac{p left(x_{i}, z_{i} | theta right)}{Q left(z_{i} right)} end{aligned}L(Î¸;X)â€‹=i=1âˆ‘Nâ€‹logp(xiâ€‹âˆ£Î¸)=i=1âˆ‘Nâ€‹logziâ€‹âˆ‘â€‹p(xiâ€‹,ziâ€‹âˆ£Î¸)=i=1âˆ‘Nâ€‹logziâ€‹âˆ‘â€‹Q(ziâ€‹)Q(ziâ€‹)p(xiâ€‹,ziâ€‹âˆ£Î¸)â€‹â‰¥i=1âˆ‘Nâ€‹ziâ€‹âˆ‘â€‹Q(ziâ€‹)logQ(ziâ€‹)p(xiâ€‹,ziâ€‹âˆ£Î¸)â€‹â€‹ . æ¨å¯¼: . æŠŠå¼ä¸­çš„logå‡½æ•°ä½“çœ‹æˆæ˜¯ä¸€ä¸ªæ•´ä½“ï¼Œç”±äºlog(x)çš„äºŒé˜¶å¯¼æ•°ä¸ºâˆ’1x2- frac{1}{x^2}âˆ’x21â€‹, å°äº0ï¼Œä¸ºå‡¹å‡½æ•°ã€‚æ‰€ä»¥ä½¿ç”¨Jensenä¸ç­‰å¼æ—¶ï¼Œåº”ç”¨ç¬¬äºŒæ¡å‡†åˆ™ï¼šf(E[X])â‰¥E[f(x)]f(E[X]) geq E[f(x)]f(E[X])â‰¥E[f(x)]ã€‚ | f(Eziâˆ¼Q[p(xi,ziâˆ£Î¸)Q(zi)])â‰¥Eziâˆ¼Q[f(p(xi,ziâˆ£Î¸)Q(zi))]f left(E_{z_{i} sim Q} left[ frac{p left(x_{i}, z_{i} | theta right)}{Q left(z_{i} right)} right] right) geq E_{z_{i} sim Q} left[f left( frac{p left(x_{i}, z_{i} | theta right)}{Q left(z_{i} right)} right) right]f(Eziâ€‹âˆ¼Qâ€‹[Q(ziâ€‹)p(xiâ€‹,ziâ€‹âˆ£Î¸)â€‹])â‰¥Eziâ€‹âˆ¼Qâ€‹[f(Q(ziâ€‹)p(xiâ€‹,ziâ€‹âˆ£Î¸)â€‹)] . è¿™é‡Œï¼ŒQ(zi)Q left(z_{i} right)Q(ziâ€‹)æ˜¯ziz_{i}ziâ€‹çš„å‡½æ•°ï¼Œ ä¸”âˆ‘ziQ(zi)=1 sum_{z_{i}} Q left(z_{i} right)=1âˆ‘ziâ€‹â€‹Q(ziâ€‹)=1ã€‚ | ç”±æ•°å­¦æœŸæœ›Exâˆ¼p[g(X)]=âˆ‘xg(x)p(x)E_{x sim p}[g(X)]= sum_{x} g(x) p(x)Exâˆ¼pâ€‹[g(X)]=âˆ‘xâ€‹g(x)p(x)ï¼Œä¸Šå¼å¯ä»¥ç†è§£ä¸º: p(x)p(x)p(x)å¯¹åº”Q(zi)Q left(z_{i} right)Q(ziâ€‹), g(x)å¯¹åº”logâ¡p(xi,ziâˆ£Î¸)Q(zi) log frac{p left(x_{i}, z_{i} vert theta right)}{Q left(z_{i} right)}logQ(ziâ€‹)p(xiâ€‹,ziâ€‹âˆ£Î¸)â€‹è¡¨ç¤ºziz_{i}ziâ€‹çš„å‡½æ•°ã€‚ . | ä¼¼ç„¶å‡½æ•°: L(Î¸)â‰¥J(z,Q) mathcal{L}( theta) geq mathcal{J}(z,Q)L(Î¸)â‰¥J(z,Q)ï¼ˆzzzä¸ºéšå«å˜é‡ï¼‰ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸æ–­çš„æœ€å¤§åŒ–J mathcal{J}Jçš„ä¸‹ç•Œï¼Œæ¥ä½¿å¾—L(Î¸) mathcal{L}( theta)L(Î¸)ä¸æ–­æé«˜ï¼Œæœ€ç»ˆè¾¾åˆ°å®ƒçš„æœ€å¤§å€¼ã€‚ | æœ€å¤§åŒ–L(Î¸) mathcal{L}( theta)L(Î¸)å‡½æ•°çš„ä¸‹ç•Œï¼Œå³è®©g(x)g(x)g(x)ä¸ºå¸¸æ•°c: . p(xi,ziâˆ¥Î¸)Q(zi)=c frac{p left(x_{i}, z_{i} | theta right)}{Q left(z_{i} right)}=cQ(ziâ€‹)p(xiâ€‹,ziâ€‹âˆ¥Î¸)â€‹=c . Jensenä¸ç­‰å¼ä¸­è¯´åˆ°ï¼Œå½“è‡ªå˜é‡X=E(X)æ—¶ï¼Œå³ä¸ºå¸¸æ•°çš„æ—¶å€™ï¼Œç­‰å¼æˆç«‹! . å˜æ¢å…¬å¼, å¯¹æ‰€æœ‰zzzæ±‚å’Œå¾—: . p(xi,ziâˆ£Î¸)=câ‹…Q(zi)âˆ‘zip(xi,ziâˆ£Î¸)=câ‹…âˆ‘ziQ(zi)c=âˆ‘zip(xi,ziâˆ£Î¸) begin{aligned} p left(x_{i}, z_{i} | theta right) &amp;=c cdot Q left(z_{i} right) sum_{z_{i}} p left(x_{i}, z_{i} | theta right) &amp;=c cdot sum_{z_{i}} Q left(z_{i} right) c &amp;= sum_{z_{i}} p left(x_{i}, z_{i} | theta right) end{aligned}p(xiâ€‹,ziâ€‹âˆ£Î¸)ziâ€‹âˆ‘â€‹p(xiâ€‹,ziâ€‹âˆ£Î¸)câ€‹=câ‹…Q(ziâ€‹)=câ‹…ziâ€‹âˆ‘â€‹Q(ziâ€‹)=ziâ€‹âˆ‘â€‹p(xiâ€‹,ziâ€‹âˆ£Î¸)â€‹ . å…¶ä¸­ï¼Œâˆ‘ziQ(zi)=1 sum_{z_{i}} Q left(z_{i} right) = 1âˆ‘ziâ€‹â€‹Q(ziâ€‹)=1, ä¹Ÿå¾—ï¼š . Q(zi)=p(xi,ziâˆ£Î¸)âˆ‘zip(xi,ziâˆ£Î¸)=p(ziâˆ£xi,Î¸) begin{aligned} Q left(z_{i} right) &amp;= frac{p left(x_{i}, z_{i} | theta right)}{ sum_{z_{i}} p left(x_{i}, z_{i} | theta right)} &amp;=p left(z_{i} | x_{i}, theta right) end{aligned}Q(ziâ€‹)â€‹=âˆ‘ziâ€‹â€‹p(xiâ€‹,ziâ€‹âˆ£Î¸)p(xiâ€‹,ziâ€‹âˆ£Î¸)â€‹=p(ziâ€‹âˆ£xiâ€‹,Î¸)â€‹ . è‡³æ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†åœ¨å›ºå®šå‚æ•°Î¸åï¼Œä½¿ä¸‹ç•Œæ‹‰å‡çš„Q(z)Q(z)Q(z)çš„è®¡ç®—å…¬å¼å°±æ˜¯åéªŒæ¦‚ç‡ï¼ˆæ¡ä»¶æ¦‚ç‡ï¼‰ï¼Œä¸€å¹¶è§£å†³äº†Q(z)Q(z)Q(z)å¦‚ä½•é€‰æ‹©çš„é—®é¢˜ã€‚æ­¤æ­¥å°±æ˜¯EMç®—æ³•çš„E-stepã€‚ . æ‰§è¡ŒE-Stepåä¸ä¸‹ç•Œé‡åˆï¼Œæ­¤æ—¶ä¼¼ç„¶å˜ä¸ºï¼š . L(Î¸(t);X)=âˆ‘i=1Nâˆ‘ziQ(t)(zi)logâ¡p(xi,ziâˆ£Î¸(t))Q(t)(zi) mathcal{L} left( theta^{(t)} ; X right)= sum_{i=1}^{N} sum_{z_{i}} Q^{(t)} left(z_{i} right) log frac{p left(x_{i}, z_{i} | theta^{(t)} right)}{Q^{(t)} left(z_{i} right)}L(Î¸(t);X)=i=1âˆ‘Nâ€‹ziâ€‹âˆ‘â€‹Q(t)(ziâ€‹)logQ(t)(ziâ€‹)p(xiâ€‹,ziâ€‹âˆ£Î¸(t))â€‹ . è¿™æ—¶ï¼Œå¯¹å…¬å¼æ±‚å¯¼ . Î¸(t+1)=argâ¡maxâ¡Î¸L(Î¸;X) theta^{(t+1)}= underset{ theta}{ arg max } mathcal{L}( theta ; X)Î¸(t+1)=Î¸argmaxâ€‹L(Î¸;X) . å¾—åˆ°t+1æ­¥çš„ä¼¼ç„¶å‡½æ•°L(Î¸(t+1);X) mathcal{L} left( theta^{(t+1)} ; X right)L(Î¸(t+1);X)ã€‚ é€šè¿‡ä¸æ–­çš„è¿­ä»£ï¼Œå¯ä»¥å¾—åˆ°ä½¿ä¼¼ç„¶å‡½æ•°L(Î¸) mathcal{L}( theta)L(Î¸)æœ€å¤§åŒ–çš„å‚æ•°Î¸ï¼Œç›´è‡³å‡½æ•°æ”¶æ•›ã€‚ åªéœ€è¦è¯æ˜L(Î¸(t+1);X)â‰¥L(Î¸(t);X) mathcal{L} left( theta^{(t+1)} ; X right) geq mathcal{L} left( theta^{(t)} ; X right)L(Î¸(t+1);X)â‰¥L(Î¸(t);X), åˆ™å¯è¯æ˜EMçš„æ”¶æ•›æ€§: . L(Î¸(t+1);X)=âˆ‘i=1Nlogâ¡âˆ‘ziQ(t)(zi)p(xi,ziâˆ£Î¸(t+1))Q(t)(zi)â‰¥âˆ‘i=1Nâˆ‘ziQ(t)(zi)logâ¡p(xi,ziâˆ£Î¸(t+1))Q(t)(zi)â‰¥âˆ‘i=1Nâˆ‘ziQ(t)(zi)logâ¡p(xi,ziâˆ£Î¸(t))Q(t)(zi)=L(Î¸(t);X) begin{aligned} mathcal{L} left( theta^{(t+1)} ; X right) &amp;= sum_{i=1}^{N} log sum_{z_{i}} Q^{(t)} left(z_{i} right) frac{p left(x_{i}, z_{i} | theta^{(t+1)} right)}{Q^{(t)} left(z_{i} right)} &amp; geq sum_{i=1}^{N} sum_{z_{i}} Q^{(t)} left(z_{i} right) log frac{p left(x_{i}, z_{i} | theta^{(t+1)} right)}{Q^{(t)} left(z_{i} right)} &amp; geq sum_{i=1}^{N} sum_{z_{i}} Q^{(t)} left(z_{i} right) log frac{p left(x_{i}, z_{i} | theta^{(t)} right)}{Q^{(t)} left(z_{i} right)} &amp;= mathcal{L} left( theta^{(t)} ; X right) end{aligned}L(Î¸(t+1);X)â€‹=i=1âˆ‘Nâ€‹logziâ€‹âˆ‘â€‹Q(t)(ziâ€‹)Q(t)(ziâ€‹)p(xiâ€‹,ziâ€‹âˆ£Î¸(t+1))â€‹â‰¥i=1âˆ‘Nâ€‹ziâ€‹âˆ‘â€‹Q(t)(ziâ€‹)logQ(t)(ziâ€‹)p(xiâ€‹,ziâ€‹âˆ£Î¸(t+1))â€‹â‰¥i=1âˆ‘Nâ€‹ziâ€‹âˆ‘â€‹Q(t)(ziâ€‹)logQ(t)(ziâ€‹)p(xiâ€‹,ziâ€‹âˆ£Î¸(t))â€‹=L(Î¸(t);X)â€‹ . 5. æ±‚è§£GMM . (1) GMM E-Step: . å·²çŸ¥Î¸(t) theta^{(t)}Î¸(t), æ±‚Q(t+1)(zi)Q^{(t+1)} left(z_{i} right)Q(t+1)(ziâ€‹): . Q(t+1)(zi)=p(xi,ziâˆ£Î¸(t))p(xiâˆ£Î¸(t))=p(xi,ziâˆ£Î¸(t))âˆ‘lâˆˆzip(xi,lâˆ£Î¸(t))=p(xiâˆ£zi,Î¸(t))p(ziâˆ£Î¸(t))âˆ‘lâˆˆzip(xiâˆ£l,Î¸(t))p(lâˆ£Î¸(t))=N(Î¼zi,Î£zi)Ï€ziâˆ‘lâˆˆziN(Î¼l,Î£l)Ï€l begin{aligned} Q^{(t+1)} left(z_{i} right) &amp;= frac{p left(x_{i}, z_{i} | theta^{(t)} right)}{p left(x_{i} | theta^{(t)} right)} &amp;= frac{p left(x_{i}, z_{i} | theta^{(t)} right)}{ sum_{l in z_{i}} p left(x_{i}, l | theta^{(t)} right)} &amp;= frac{p left(x_{i} | z_{i}, theta^{(t)} right) p left(z_{i} | theta^{(t)} right)}{ sum_{l in z_{i}} p left(x_{i} | l, theta^{(t)} right) p left(l | theta^{(t)} right)} &amp;= frac{ mathcal{N} left( mu_{z_{i}}, Sigma_{z_{i}} right) pi_{z_{i}}}{ sum_{l in z_{i}} mathcal{N} left( mu_{l}, Sigma_{l} right) pi_{l}} end{aligned}Q(t+1)(ziâ€‹)â€‹=p(xiâ€‹âˆ£Î¸(t))p(xiâ€‹,ziâ€‹âˆ£Î¸(t))â€‹=âˆ‘lâˆˆziâ€‹â€‹p(xiâ€‹,lâˆ£Î¸(t))p(xiâ€‹,ziâ€‹âˆ£Î¸(t))â€‹=âˆ‘lâˆˆziâ€‹â€‹p(xiâ€‹âˆ£l,Î¸(t))p(lâˆ£Î¸(t))p(xiâ€‹âˆ£ziâ€‹,Î¸(t))p(ziâ€‹âˆ£Î¸(t))â€‹=âˆ‘lâˆˆziâ€‹â€‹N(Î¼lâ€‹,Î£lâ€‹)Ï€lâ€‹N(Î¼ziâ€‹â€‹,Î£ziâ€‹â€‹)Ï€ziâ€‹â€‹â€‹â€‹ . (2) GMM M-Step: . å·²çŸ¥Q(t+1)(zi)Q^{(t+1)} left(z_{i} right)Q(t+1)(ziâ€‹), æ±‚ Î¸(t+1) theta^{(t+1)}Î¸(t+1): . L(Î¸;X)=âˆ‘iNâˆ‘lKQi(l)logâ¡p(xi,lâˆ£Î¸)Qi(l)=âˆ‘iNâˆ‘lKQi(l)logâ¡p(xi,lâˆ£Î¸)âˆ’âˆ‘iNâˆ‘lKQi(l)logâ¡Qi(l)=âˆ‘iNâˆ‘lKQi(l)logâ¡p(xi,lâˆ£Î¸)âˆ’ConstantÂ =âˆ‘iNâˆ‘lKQi(l)logâ¡Ï€lN(Î¼l,Î£l)âˆ’ConstantÂ =âˆ‘iNâˆ‘lKQi(l)logâ¡Ï€l+âˆ‘iNâˆ‘lKQi(l)logâ¡N(Î¼l,Î£l)âˆ’Constant begin{aligned} mathcal{L}( theta ; X) &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log frac{p left(x_{i}, l | theta right)}{Q_{i}(l)} &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log p left(x_{i}, l | theta right)- sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log Q_{i}(l) &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log p left(x_{i}, l | theta right)- text {Constant } &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log pi_{l} mathcal{N} left( mu_{l}, Sigma_{l} right)- text {Constant } &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log pi_{l}+ sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log mathcal{N} left( mu_{l}, Sigma_{l} right)- text {Constant} end{aligned}L(Î¸;X)â€‹=iâˆ‘Nâ€‹lâˆ‘Kâ€‹Qiâ€‹(l)logQiâ€‹(l)p(xiâ€‹,lâˆ£Î¸)â€‹=iâˆ‘Nâ€‹lâˆ‘Kâ€‹Qiâ€‹(l)logp(xiâ€‹,lâˆ£Î¸)âˆ’iâˆ‘Nâ€‹lâˆ‘Kâ€‹Qiâ€‹(l)logQiâ€‹(l)=iâˆ‘Nâ€‹lâˆ‘Kâ€‹Qiâ€‹(l)logp(xiâ€‹,lâˆ£Î¸)âˆ’ConstantÂ =iâˆ‘Nâ€‹lâˆ‘Kâ€‹Qiâ€‹(l)logÏ€lâ€‹N(Î¼lâ€‹,Î£lâ€‹)âˆ’ConstantÂ =iâˆ‘Nâ€‹lâˆ‘Kâ€‹Qiâ€‹(l)logÏ€lâ€‹+iâˆ‘Nâ€‹lâˆ‘Kâ€‹Qiâ€‹(l)logN(Î¼lâ€‹,Î£lâ€‹)âˆ’Constantâ€‹ . (3) æ±‚ Ï€ piÏ€: . âˆ€lâˆˆ{1,â‹¯â€‰,K},âˆ‚L(Î¸;X)âˆ‚Ï€l=0Â s.t.Â âˆ‘lKÏ€l=1 begin{aligned} forall_{l in {1, cdots, K }}, &amp; frac{ partial mathcal{L}( theta ; X)}{ partial pi_{l}}=0 &amp; text { s.t. } sum_{l}^{K} pi_{l}=1 end{aligned}âˆ€lâˆˆ{1,â‹¯,K}â€‹,â€‹âˆ‚Ï€lâ€‹âˆ‚L(Î¸;X)â€‹=0Â s.t.Â lâˆ‘Kâ€‹Ï€lâ€‹=1â€‹ . æ‹‰æ ¼æœ—æ—¥ä¹˜æ³•çº¦æŸ . {LÏ€l=âˆ‚L(Î¸;X)âˆ‚Ï€l+Î»(âˆ‘lKÏ€lâˆ’1)=0LÎ»=âˆ‘lKÏ€lâˆ’1=0 left { begin{aligned} L_{ pi_{l}} &amp;= frac{ partial mathcal{L}( theta ; X)}{ partial pi_{l}}+ lambda left( sum_{l}^{K} pi_{l}-1 right)=0 L_{ lambda} &amp;= sum_{l}^{K} pi_{l}-1=0 end{aligned} right.â©âªâªâªâªâªâªâ¨âªâªâªâªâªâªâ§â€‹LÏ€lâ€‹â€‹LÎ»â€‹â€‹=âˆ‚Ï€lâ€‹âˆ‚L(Î¸;X)â€‹+Î»(lâˆ‘Kâ€‹Ï€lâ€‹âˆ’1)=0=lâˆ‘Kâ€‹Ï€lâ€‹âˆ’1=0â€‹ . æ±‚å¯¼ï¼š . {1Ï€1âˆ‘iNQi(1)âˆ’Î»=0â‹®1Ï€lâˆ‘iNQi(l)âˆ’Î»=0 left { begin{array}{c} frac{1}{ pi_{1}} sum_{i}^{N} Q_{i}(1)- lambda=0 vdots frac{1}{ pi_{l}} sum_{i}^{N} Q_{i}(l)- lambda=0 end{array} right.â©âªâªâªâ¨âªâªâªâ§â€‹Ï€1â€‹1â€‹âˆ‘iNâ€‹Qiâ€‹(1)âˆ’Î»=0â‹®Ï€lâ€‹1â€‹âˆ‘iNâ€‹Qiâ€‹(l)âˆ’Î»=0â€‹ . ç›¸åŠ å¾—ï¼š . âˆ‘lKâˆ‘iNQi(l)=Î»âˆ‘lKÏ€l=Î» sum_{l}^{K} sum_{i}^{N} Q_{i}(l)= lambda sum_{l}^{K} pi_{l}= lambdalâˆ‘Kâ€‹iâˆ‘Nâ€‹Qiâ€‹(l)=Î»lâˆ‘Kâ€‹Ï€lâ€‹=Î» . ç”± Qi(l)=p(lâˆ£xi,Î¸)Q_{i}(l)=p left(l vert x_{i}, theta right)Qiâ€‹(l)=p(lâˆ£xiâ€‹,Î¸), å¾— . âˆ‘lKâˆ‘iNQi(l)=âˆ‘iNâˆ‘lKQi(l)=âˆ‘iNâˆ‘lKp(lâˆ£xi,Î¸)=âˆ‘iN1=N begin{aligned} sum_{l}^{K} sum_{i}^{N} Q_{i}(l) &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) &amp;= sum_{i}^{N} sum_{l}^{K} p left(l | x_{i}, theta right) &amp;= sum_{i}^{N} 1 &amp;=N end{aligned}lâˆ‘Kâ€‹iâˆ‘Nâ€‹Qiâ€‹(l)â€‹=iâˆ‘Nâ€‹lâˆ‘Kâ€‹Qiâ€‹(l)=iâˆ‘Nâ€‹lâˆ‘Kâ€‹p(lâˆ£xiâ€‹,Î¸)=iâˆ‘Nâ€‹1=Nâ€‹ . åˆ™ . Ï€l=1Î»âˆ‘iNQi(l)=1Nâˆ‘iNQi(l)=1Nâˆ‘iNp(lâˆ£xi,Î¸) begin{aligned} pi_{l} &amp;= frac{1}{ lambda} sum_{i}^{N} Q_{i}(l) &amp;= frac{1}{N} sum_{i}^{N} Q_{i}(l) &amp;= frac{1}{N} sum_{i}^{N} p left(l | x_{i}, theta right) end{aligned}Ï€lâ€‹â€‹=Î»1â€‹iâˆ‘Nâ€‹Qiâ€‹(l)=N1â€‹iâˆ‘Nâ€‹Qiâ€‹(l)=N1â€‹iâˆ‘Nâ€‹p(lâˆ£xiâ€‹,Î¸)â€‹ . (4) è®¡ç®—Î¼ muÎ¼ . âˆ‘iNâˆ‘lKQi(l)logâ¡N(Î¼l,Î£l)=âˆ‘iNâˆ‘lKQi(l)logâ¡12Ï€Ïƒleâˆ’(xiâˆ’Î¼l)22Ïƒl2=âˆ‘iNâˆ‘lKQi(l){âˆ’12logâ¡2Ï€âˆ’12logâ¡Ïƒl2âˆ’(xiâˆ’Î¼l)22Ïƒl2} begin{aligned} &amp; sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log mathcal{N} left( mu_{l}, Sigma_{l} right) &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log frac{1}{ sqrt{2 pi} sigma_{l}} e^{- frac{ left(x_{i}- mu_{l} right)^{2}}{2 sigma_{l}^{2}}} &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) left {- frac{1}{2} log 2 pi- frac{1}{2} log sigma_{l}^{2}- frac{ left(x_{i}- mu_{l} right)^{2}}{2 sigma_{l}^{2}} right } end{aligned}â€‹iâˆ‘Nâ€‹lâˆ‘Kâ€‹Qiâ€‹(l)logN(Î¼lâ€‹,Î£lâ€‹)=iâˆ‘Nâ€‹lâˆ‘Kâ€‹Qiâ€‹(l)log2Ï€ . â€‹Ïƒlâ€‹1â€‹eâˆ’2Ïƒl2â€‹(xiâ€‹âˆ’Î¼lâ€‹)2â€‹=iâˆ‘Nâ€‹lâˆ‘Kâ€‹Qiâ€‹(l){âˆ’21â€‹log2Ï€âˆ’21â€‹logÏƒl2â€‹âˆ’2Ïƒl2â€‹(xiâ€‹âˆ’Î¼lâ€‹)2â€‹}â€‹ . æ±‚åå¯¼ï¼š . âˆ‚L(Î¸;X)âˆ‚Î¼l=âˆ‘iNQi(l)xiâˆ’Î¼lÏƒ2=0 begin{aligned} frac{ partial mathcal{L}( theta ; X)}{ partial mu_{l}} &amp;= sum_{i}^{N} Q_{i}(l) frac{x_{i}- mu_{l}}{ sigma^{2}} &amp;=0 end{aligned}âˆ‚Î¼lâ€‹âˆ‚L(Î¸;X)â€‹â€‹=iâˆ‘Nâ€‹Qiâ€‹(l)Ïƒ2xiâ€‹âˆ’Î¼lâ€‹â€‹=0â€‹ . å¾—Î¼ muÎ¼ï¼š . Î¼l=âˆ‘iNQi(l)xiâˆ‘iNQi(l) mu_{l}= frac{ sum_{i}^{N} Q_{i}(l) x_{i}}{ sum_{i}^{N} Q_{i}(l)}Î¼lâ€‹=âˆ‘iNâ€‹Qiâ€‹(l)âˆ‘iNâ€‹Qiâ€‹(l)xiâ€‹â€‹ . âˆ‚L(Î¸;X)âˆ‚Ïƒl2=âˆ‘iNQi(l){âˆ’12Ïƒl2+(xiâˆ’Î¼l)22Ïƒl4}=0 begin{aligned} &amp; frac{ partial mathcal{L}( theta ; X)}{ partial sigma_{l}^{2}}= sum_{i}^{N} Q_{i}(l) left {- frac{1}{2 sigma_{l}^{2}}+ frac{ left(x_{i}- mu_{l} right)^{2}}{2 sigma_{l}^{4}} right } &amp;=0 end{aligned}â€‹âˆ‚Ïƒl2â€‹âˆ‚L(Î¸;X)â€‹=iâˆ‘Nâ€‹Qiâ€‹(l){âˆ’2Ïƒl2â€‹1â€‹+2Ïƒl4â€‹(xiâ€‹âˆ’Î¼lâ€‹)2â€‹}=0â€‹ . (5) è®¡ç®—Ïƒ sigmaÏƒ . âˆ‚L(Î¸;X)âˆ‚Ïƒl2=âˆ‘iNQi(l){âˆ’12Ïƒl2+(xiâˆ’Î¼l)22Ïƒl4}=0 begin{aligned} frac{ partial mathcal{L}( theta ; X)}{ partial sigma_{l}^{2}} &amp;= sum_{i}^{N} Q_{i}(l) left {- frac{1}{2 sigma_{l}^{2}}+ frac{ left(x_{i}- mu_{l} right)^{2}}{2 sigma_{l}^{4}} right } &amp;=0 end{aligned}âˆ‚Ïƒl2â€‹âˆ‚L(Î¸;X)â€‹â€‹=iâˆ‘Nâ€‹Qiâ€‹(l){âˆ’2Ïƒl2â€‹1â€‹+2Ïƒl4â€‹(xiâ€‹âˆ’Î¼lâ€‹)2â€‹}=0â€‹ . å¾—åˆ° . Ïƒl=âˆ‘iNQi(l)(xiâˆ’Î¼l)2âˆ‘iNQi(l) sigma_{l}= frac{ sum_{i}^{N} Q_{i}(l) left(x_{i}- mu_{l} right)^{2}}{ sum_{i}^{N} Q_{i}(l)}Ïƒlâ€‹=âˆ‘iNâ€‹Qiâ€‹(l)âˆ‘iNâ€‹Qiâ€‹(l)(xiâ€‹âˆ’Î¼lâ€‹)2â€‹ . 6 ä»KLæ•£åº¦è§’åº¦è§£é‡ŠEM . KL(qâˆ¥p)=âˆ‘zq(z)logâ¡q(z)p(zâˆ£x,Î¸)=âˆ‘zq(z)logâ¡q(z)p(xâˆ£Î¸)p(z,xâˆ£Î¸)=âˆ’âˆ‘zq(z)logâ¡p(z,xâˆ£Î¸)q(z)+âˆ‘zq(z)logâ¡p(xâˆ£Î¸)=âˆ’âˆ‘zq(z)logâ¡p(z,xâˆ£Î¸)q(z)+logâ¡p(xâˆ£Î¸)âˆ‘zq(z)=âˆ’âˆ‘zq(z)logâ¡p(z,xâˆ£Î¸)q(z)+logâ¡p(xâˆ£Î¸)logâ¡p(xâˆ£Î¸)=KL(qâˆ¥p)+âˆ‘zq(z)logâ¡p(z,xâˆ£Î¸)q(z)=KL(qâˆ¥p)+L(q,Î¸) begin{aligned} K L(q | p) &amp;= sum_{z} q(z) log frac{q(z)}{p(z | x, theta)} &amp;= sum_{z} q(z) log frac{q(z) p(x | theta)}{p(z, x | theta)} &amp;=- sum_{z} q(z) log frac{p(z, x | theta)}{q(z)}+ sum_{z} q(z) log p(x | theta) &amp;=- sum_{z} q(z) log frac{p(z, x | theta)}{q(z)}+ log p(x | theta) sum_{z} q(z) &amp;=- sum_{z} q(z) log frac{p(z, x | theta)}{q(z)}+ log p(x | theta) log p(x | theta) &amp;=K L(q | p)+ sum_{z} q(z) log frac{p(z, x | theta)}{q(z)} &amp;=K L(q | p)+ mathcal{L}(q, theta) end{aligned}KL(qâˆ¥p)logp(xâˆ£Î¸)â€‹=zâˆ‘â€‹q(z)logp(zâˆ£x,Î¸)q(z)â€‹=zâˆ‘â€‹q(z)logp(z,xâˆ£Î¸)q(z)p(xâˆ£Î¸)â€‹=âˆ’zâˆ‘â€‹q(z)logq(z)p(z,xâˆ£Î¸)â€‹+zâˆ‘â€‹q(z)logp(xâˆ£Î¸)=âˆ’zâˆ‘â€‹q(z)logq(z)p(z,xâˆ£Î¸)â€‹+logp(xâˆ£Î¸)zâˆ‘â€‹q(z)=âˆ’zâˆ‘â€‹q(z)logq(z)p(z,xâˆ£Î¸)â€‹+logp(xâˆ£Î¸)=KL(qâˆ¥p)+zâˆ‘â€‹q(z)logq(z)p(z,xâˆ£Î¸)â€‹=KL(qâˆ¥p)+L(q,Î¸)â€‹ . å‚è€ƒï¼š . å¾äº¦è¾¾-æœºå™¨å­¦ä¹ -EM .",
            "url": "https://zqfang.github.io/machine%20learning/2020/01/26/ML-EM.html",
            "relUrl": "/machine%20learning/2020/01/26/ML-EM.html",
            "date": " â€¢ Jan 26, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "SuperFast RNA-seq",
            "content": "salmon-tximport-deseq2 . Step 0: install salmon and download transcriptome cdna from gencode . conda install salmon wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_32/gencode.v32.transcripts.fa.gz . Step 1. build salmon index . salmon index -p 8 --gencode -t gencode.v32.transcripts.fa.gz -i salmonIndex_hg38 . Step 2: quantification . salmon quant -i salmonIndex_hg38 -l A -1 ${fn}/${samp}_1.fastq.gz -2 ${fn}/${samp}_2.fastq.gz -p 8 --validateMappings -o quants/${samp}_quant . Step 3: merge quantification outputs . use tximport in R . # R code library(tximport) library(readr) suppressMessages(library(&#39;EnsDb.Hsapiens.v86&#39;)) txdb &lt;- EnsDb.Hsapiens.v86 k &lt;- keys(txdb, keytype = &quot;GENEID&quot;) df &lt;- select(txdb, keys = k, keytype = &quot;GENEID&quot;, columns = c(&quot;TXID&quot;,&quot;GENEID&quot;)) tx2gene &lt;- df[, 2:1] # tx ID, then gene ID #tx2gene &lt;- read.table(tx2gene, header= T, sep=&quot; t&quot;, stringsAsFactors = F) samples &lt;- unlist(strsplit(sample_ids,&quot;,&quot;)) salmon.files &lt;- file.path(&#39;salmon&#39;,samples, &quot;quant.sf&quot;) names(salmon.files) &lt;- samples all(file.exists(salmon.files)) # get transcript level results txi.transcripts &lt;- tximport(salmon.files, type = &quot;salmon&quot;, txOut = TRUE, tx2gene = tx2gene,) # ignoreTxVersion = TRUE) # get gene level results txi.salmon &lt;- summarizeToGene(txi.transcripts, tx2gene) #save raw counts salmon.counts&lt;- txi.salmon$counts salmon.counts&lt;- as.data.frame(salmon.counts) write.table(salmon.counts, out_counts, sep=&quot; t&quot;, quote=F) #save gene tpms salmon.TPM&lt;- txi.salmon$abundance salmon.TPM&lt;- as.data.frame(salmon.TPM) write.table(salmon.TPM, out_tpm, sep=&quot; t&quot;, quote=F) #save transcripts tpms salmon.trans.TPM&lt;- txi.transcripts$abundance salmon.trans.TPM&lt;- as.data.frame(salmon.trans.TPM) write.table(salmon.trans.TPM, outTrans_tpm, sep=&quot; t&quot;, quote=F) save(txi.salmon, file=&quot;txi.salmon.RData&quot;) . Step 4: Differentially expressed gene analysis . DESeq2 pipeline demo . load(&quot;txi.salmon.RData&quot;) dds &lt;- DESeqDataSetFromTximport(txi.salmon, sampleTable, ~condition) dds$condition &lt;- relevel(dds$condition, ref=ctrl) dds &lt;- DESeq(dds, parallel=TRUE) res &lt;- results(dds, contrast=c(&quot;condition&quot;, treat, ctrl)) resOrdered &lt;- res[order(res$padj),] resOrdered = as.data.frame(resOrdered) write.table(resOrdered, file=&quot;degs.txt&quot;, quote=F, sep=&quot; t&quot;) .",
            "url": "https://zqfang.github.io/make%20bioinfo%20uncool%20again/2020/01/20/RNAseqTurbo.html",
            "relUrl": "/make%20bioinfo%20uncool%20again/2020/01/20/RNAseqTurbo.html",
            "date": " â€¢ Jan 20, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Shortcut to bulk RNA-seq analysis",
            "content": "hisat2-htseq-deseq2 . 3.1 transcriptom mapping . step 0: install tools . conda install htseq hisat2 stringtie . step 1: build index and extract splice sites . build index . hisat2-build -p {threads} genome/hg38.fa hisat2_index/hg38 . extract known splice sites for alignmnet . hisat2_extract_splice_sites.py gencode.gtf &gt; hisat2_index/splicesites.txt hisat2_extract_exons.py gencode.gtf &gt; histat2_index/exon.txt . step2: mapping . hisat2 --dta --threads ${threads} -x hisat2_index/hg38 --known-splicesite-infile hisat2_index/splicesites.txt -1 R1.fq.gz -2 R2.fq.gz -S output.sam . step 3: sam to bam . samtools view -Sbh -q 25 -@ ${threads} -o ouput.bam input.sam . step 4: bam sort and index . samtools sort -@ ${threads} input.bam &gt; output.sorted.bam samtools index input.sorted.bam #generate input.sorted.bam.bai . step 5: bam to bigwig . bamCoverage -p ${threads} --normalizeUsing RPKM # note: other normalization options -b input.sorted.bam -o output.bw . 3.2 Differentially expressed genes analysis . step 1: count reads . htseq-count -r pos -s no --additional-attr gene_name --additional-attr gene_type -f bam input.sorted.bam gencode.gtf &gt; output.count . step2: differentially expressed genes analysis . (1) construct read count table . option 1: HTSeq count file input . library(&quot;DESeq2&quot;) directory &lt;- &quot;/path/to/your/readCountFiles/&quot; sampleFiles &lt;- grep(&quot;count&quot;, list.files(directory), value=TRUE) condition &lt;- factor(c(&quot;KO&quot;,&quot;KO&quot;, &quot;WT&quot;,&quot;WT&quot;), levels = c(&quot;WT&quot;, &quot;KO&quot;)) # phenotable sampleTable &lt;- data.frame(sampleName = sampleFiles, fileName = sampleFiles, condition = condition) # construct read count table ddsHTSeq &lt;- DESeqDataSetFromHTSeqCount(sampleTable = sampleTable, directory = directory, design= ~ condition) . option 2: combined read count file into a single table first, then run . library(DESeq2) # read count table database &lt;- read.table(file = &quot;raw.counts.csv&quot;, sep = &quot;,&quot;, header = TRUE, row.names = 1) database &lt;- round(as.matrix(database)) # set level condition &lt;- factor(c(&quot;KO&quot;,&quot;KO&quot;, &quot;WT&quot;,&quot;WT&quot;), levels = c(&quot;WT&quot;, &quot;KO&quot;)) # build DESeq object coldata &lt;- data.frame(row.names = colnames(database), condition) dds &lt;- DESeqDataSetFromMatrix(countData=database, colData=coldata, design=~condition + treatmement) . (2) run DESeq2 and get output . library(DESeq2) dds &lt;- dds[ rowSums(counts(dds)) &gt; 1, ] # run statistical test dds &lt;- DESeq(dds) # get results res &lt;- results(dds) # summary(res) count_r &lt;- counts(dds, normalized=T) #normalized count matrix # export results res &lt;- res[order(res$padj),] diff_gene &lt;- subset(res, padj &lt; 0.05 &amp; (log2FoldChange &gt; 1 | log2FoldChange &lt; -1)) diff_gene &lt;- row.names(diff_gene) resdata &lt;- merge(as.data.frame(res), as.data.frame(counts(dds, normalized=TRUE)), by=&quot;row.names&quot;, sort=FALSE) write.csv(resdata, file = &quot;DEGs.csv&quot;, row.names = FALSE) . 3.3 Gene set enrichrment analysis . GO . clusterprofiler | Enrichr (GSEApy) | GSEA | . 3.4 Alternative splicing analysis . rMATS .",
            "url": "https://zqfang.github.io/make%20bioinfo%20uncool%20again/2020/01/20/RNAseq.html",
            "relUrl": "/make%20bioinfo%20uncool%20again/2020/01/20/RNAseq.html",
            "date": " â€¢ Jan 20, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Shortcut to ChIP-seq analysis",
            "content": "bowtie2-macs2-deeptools . 2.1 Genome mapping . Step 0: install software . # install miniconda, then call conda conda install -c bioconda bowtie2 hisat2 samtools deeptools . step 1: build index . bowtie2-build hg38.fa bowtie2_index/hg38 . step 2: mapping . Unpaired data . bowtie2 -p ${threads} -x index/hg38 -U input.fastq.gz -S ouput.sam . Paired data . bowtie2 -p 4 -x index/hg38 -1 input_R1.fastq.gz -2 input_R2.fastq.gz -S ouput.sam . step 3: sam to bam . samtools view -Sbh -q 25 -@ ${threads} -o ouput.bam input.sam . step 4: bam sort and index . samtools sort -@ ${threads} input.bam &gt; output.sorted.bam samtools index input.sorted.bam #generate input.sorted.bam.bai . step 5: bam to bigwig . bamCoverage -p ${threads} --normalizeUsing RPKM # note: other normalization options --centerReads -e 200 -b input.sorted.bam -o output.bw . 2.2 Peaks analysis . note:: macs2 (&gt;v2.2.x) supports python 3. . step 0: install tools . conda install macs2 bedtools pygenometracks . step 1: callpeaks (1) narrow peaks, e.g. TFs, h3k4m3 . # bam file input macs2 callpeak -t ChIP.elute.sorted.bam -c ChIP.input.sorted.bam -f BAM -g hs # organism -B -q 0.05 -n ${outFileName} --outdir macs_out . (2) Broad peaks, e.g. h3k27me3 . # sam file also works fine macs2 callpeak -t ./bowtie_out/WTme2ChIP.sam -c ./bowtie_out/ESCInput.sam -f SAM -g mm -B --SPMR --nomodel --extsize 147 --broad -n WTme2ChIP --outdir macs_out . step 2: advanced analysis . tools: bedtools, deeptools, pyGenomeTracks, igv | genome algebra overlap with other peaks: bedtools | | . | visualization heatmap: deeptools | signal tracks: pyGenomeTracks, igv | . | .",
            "url": "https://zqfang.github.io/make%20bioinfo%20uncool%20again/2020/01/20/ChIP.html",
            "relUrl": "/make%20bioinfo%20uncool%20again/2020/01/20/ChIP.html",
            "date": " â€¢ Jan 20, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Cheatsheet for command line",
            "content": "usefull tools for linux command line . a. Make terminal cool, install OhMyZsh . # install zsh sudo apt-get install zsh # ubuntu # change default shell to zsh chsh -s /usr/bin/zsh # install ohmyzsh sh -c &quot;$(wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)&quot; source ~/.zshrc . b. Terminal keyboard short cuts . Jump to head: Ctrl + a | Jump to end: Ctrl + e | Delete strings ahead: Ctrl + u | Delete strings follow: Ctrl + k | . c. Program keeps running in the background . 1. Run cmd using nohup . nohup command [options] &amp; . 2. Run cmd using Tmux . Outside Tmux: Typically these are run outside, but you can also run them inside an existing session . a. Start New Session . tmux new -s myname . b. Attach To Existing Session . tmux attach -t myname #by name tmux attach 4 #by number (in this case 4) . c. List Sessions . tmux ls . d. Kill Session . tmux kill-session -t myname . Inside Tmux Session: Start each command with CTRL + b, release, then press one of the following: . Panes Â  . % | vertical split | . â€ | horizontal split | . d | detach from session (it keeps running in the background) | . x | kill pane | . Up/Down/Left/Right | move between panes | . PageUP/PageDown | CTRL+c to exit the PageUp/Down mode | . Fn+Up/Down | PageUp/Down: Mac keyboard | . : + resize-pane -D | Resizes the current pane down | . : + resize-pane -U | Resizes the current pane upward | . : + resize-pane -L | Resizes the current pane left | . : + resize-pane -R | Resizes the current pane right | . : + resize-pane -D 20 | Resizes the current pane down by 20 cells | . d. File compression and decompression . Decompression . File type Cmd e.g. . *.tar | tar -xvf | Â  | . *.tar.gz or *.tgz | tar -xvzf | Â  | . *bz2 | bzip2 -d or bunzip2 | Â  | . *.tar.bz2 | tar -xjf | Â  | . *.Z | uncompress | Â  | . *.tar.Z | tar -xZf | Â  | . *.rar | unrar e or rar x | unrar e file.rar | . *.zip | unzip | Â  | . *.gz | gunzip | Â  | . Compression . File type Cmd e.g. . *.tar | tar -cvf | Â  | . *.tar.gz or *.tgz | tar -cvzf | Â  | . *bz2 | bzip2 -z | Â  | . *.tar.bz2 | tar -cjf | Â  | . *.Z | compress | Â  | . *.tar.Z | tar -cZf | Â  | . *.rar | rar a | rar a -ep1 newname /home/user/cpp | . *.zip | zip | Â  | . *.gz | gzip | Â  | . For rar installation . sudo apt-get install rar . e. Handy tricks for handling filepath . very useful to strip file sufix, path et.al. . # e.g. var=./home/fastq/filename_R1.fq.gz # extract filename ${var#*/} # -&gt; home/fastq/filename_R1.fq.gz var1=${var##*/} # -&gt; filename_R1.fq.gz # remove file suffix ${var1%.*} # -&gt; filename_R1.fq ${var1%%.*} # -&gt; filename_R1 # get basebame var2=$(basename &quot;${var}&quot; .fq.gz) #-&gt; filename_R1 .",
            "url": "https://zqfang.github.io/make%20bioinfo%20uncool%20again/2020/01/20/Begin.html",
            "relUrl": "/make%20bioinfo%20uncool%20again/2020/01/20/Begin.html",
            "date": " â€¢ Jan 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Zhuoqing Fang. Iâ€™m working on computational genomics. I open this blog to keep track of my hardship with machine learning. .",
          "url": "https://zqfang.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
      ,"page4": {
          "title": "Publications",
          "content": "SOX21 Ensures Rostral Forebrain Identity by Suppression of WNT8B during Neural Regionalization of Human Embryonic Stem Cells . Published in Stem Cell Reports, 2019 . . My PhD thesis . . Download here . CDK11 safeguards the identity of human embryonic stem cells via fineâ€tuning signaling pathways . Published in Journal of Cellular Physiology, 2019 . . Ding J, Fang Z, Liu X, Zhu Z, Wen C, Wang H, et al. CDK11 safeguards the identity of human embryonic stem cells via fine-tuning signaling pathways. J Cell Physiol. 2019 . . Download here . Stk40 deletion elevates c-JUN protein level and impairs mesoderm differentiation . Published in Journal of Biological Chemistry, 2019 . . Hu J, Li S, Sun X, Fang Z, Wang L, Xiao F, et al. Stk40 deletion elevates c-JUN protein level and impairs mesoderm differentiation. J Biol Chem. 2019;294(25):9959-72. . . Download here . Transcription coactivator Cited1 acts as an inducer of trophoblast-like state from mouse embryonic stem cells through the activation of BMP signaling . Published in Cell Death &amp; Disease, 2018 . . Xu Y, Luo X, Fang Z, Zheng X, Zeng Y, Zhu C, et al. Transcription coactivator Cited1 acts as an inducer of trophoblast-like state from mouse embryonic stem cells through the activation of BMP signaling. Cell Death Dis. 2018;9(9):924. . . Download here . Bioconda: sustainable and comprehensive software distribution for the life sciences . Published in Nature Methods, 2018 . . Iâ€™m honored to be one of the contributors. It is definitely worthy. Thanks for the Bioconda team, so we could install bioinformatic tools so easy! The open source Bioconda project is a milestone in computational biology. It saves lots of time when trying to install bioinfo tools. . . Download here . Single-cell analysis reveals lineage segregation in early post-implantation mouse embryos . Published in Journal of Biological Chemistry, 2017 . . Single cell analysis of mouse E5.5, E6.5 embryos. In this work, I dissected the embryos and prepared the single cell cDNA libraries. . . Download here . Deletion of Stk40 impairs definitive erythropoiesis in the mouse fetal liver . Published in Cell Death &amp; Disease, 2017 . . Wang L, Yu H, Cheng H, He K, Fang Z, et al. Deletion of Stk40 impairs definitive erythropoiesis in the mouse fetal liver. Cell Death Dis. 2017;8:e2722 . . Download here . Itch, an E3 ligase of Oct4, is required for embryonic stem cell self-renewal and pluripotency induction . Published in Journal of Cellular Physiology, 2012 . . Liao B, Zhong XM, Xu HM, Xiao F, Fang ZQ, et al. Itch, an E3 ligase of Oct4, is required for embryonic stem cell self-renewal and pluripotency induction. Journal of Cellular Physiology 2013;228:1443-51 . . Download here .",
          "url": "https://zqfang.github.io/publications/",
          "relUrl": "/publications/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://zqfang.github.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}