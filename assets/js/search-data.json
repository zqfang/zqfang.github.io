{
  
    
        "post0": {
            "title": "Hidden Markov Model (HMM)",
            "content": "隐马可夫模型（HMM）描述隐藏的马可夫链随机生成观测序列的过程，属于生成模型。 HMM在语音识别、自然语言处理、生物信息、模式识别等领域由广泛应用。 . 1. HMM的定义 . 隐马可夫模型是关于时序的概率模型， 描述由一个隐藏的马可夫链随机生成不可观测的状态，再由各个状态生成一个观测，从而产生观测随机序列的过程。 . 简而言之，隐马可夫链随机成状态序列（state sequence），而每个状态生成观测，产生观测序列（observation sequence）。序列的一个位置可以看作一个时刻。 . 令QQQ 表示所有可能状态的集合：Q={q1,q2,⋯ ,qN}Q = { q_1, q_2, cdots, q_N }Q={q1​,q2​,⋯,qN​}; 令VVV 表示所有可能的观测集合：V={v1,v2,⋯ ,vM}V = {v_1, v_2, cdots, v_M }V={v1​,v2​,⋯,vM​}; 令III 表示长度为T的状态序列： I=(i1,i2,⋯ ,iT)I = (i_1, i_2, cdots, i_T)I=(i1​,i2​,⋯,iT​); 令OOO 表示对应的是观测序列： O=(o1,o2,⋯ ,oT)O = (o_1, o_2, cdots, o_T)O=(o1​,o2​,⋯,oT​). . 令AAA是转移概率矩阵： . A=[aij]N×NA = [a_{ij}]_{N times N}A=[aij​]N×N​ . 其中， . aij=P(it+1=qj∣it=qj),i=1,2,⋯ ,N;j=1,2,⋯ ,Na_{ij} = P(i_{t+1} = q_j | i_t = q_j), i=1,2, cdots, N; j = 1,2, cdots, Naij​=P(it+1​=qj​∣it​=qj​),i=1,2,⋯,N;j=1,2,⋯,N . 是在时刻ttt处于状态qiq_iqi​的条件下生成观测t+1t +1t+1转移到状态qjq_jqj​的概率。 . 令BBB是观测概率矩阵： . B=[bj(k)]N×MB = [b_j(k)]_{N times M}B=[bj​(k)]N×M​ . 其中， . bj(k)=P(ot=vk∣it=qj),k=1,2,⋯ ,M;j=1,2,⋯ ,Nb_j(k) = P(o_t = v_k | i_t = q_j), k=1,2, cdots, M; j=1,2, cdots, Nbj​(k)=P(ot​=vk​∣it​=qj​),k=1,2,⋯,M;j=1,2,⋯,N . 是在时刻ttt处于状态qjq_jqj​的条件下生成观测vkv_kvk​ 的概率。 . 令π piπ是初始状态概率向量： . π=(πi) pi = ( pi_i)π=(πi​) . 其中， . πi=P(i1=qi),i=1,2,⋯ ,N pi_{i} = P(i_1 = q_i),i=1,2, cdots, Nπi​=P(i1​=qi​),i=1,2,⋯,N . 是时刻t=1处于状态qiq_iqi​的概率. . 隐马可夫模型λ lambdaλ由π piπ， AAA，BBB决定。 . λ=(A,B,π) lambda = (A, B, pi)λ=(A,B,π) . 其中，π piπ和AAA决定状态序列，BBB决定观测序列。 . 隐马可夫模型的两个基本假设 . 齐次马可夫性 隐马可夫链在任意时刻t的状态前一时刻状态，与其他时刻的隐状态和观测无关， 也与时刻t无关： | . | P(it∣it−1,Ot−1,⋯ ,i1,o1)=P(it∣it−1),t=1,2,⋯ ,TP(i_t | i_{t-1}, O_{t-1}, cdots, i_1, o_1) = P(i_t | i_{t-1}), t = 1,2, cdots,TP(it​∣it−1​,Ot−1​,⋯,i1​,o1​)=P(it​∣it−1​),t=1,2,⋯,T . 观测独立性 任意时刻的观测只依赖改时刻的马可夫链状态，与其他观测和状态无关: | . P(ot∣iT,OT,iT−1,oT−1⋯ ,it+1,Ot+1,it−1,Ot−1,i1,o1)=P(ot∣it)P(o_t | i_{T}, O_{T}, i_{T-1}, o_{T-1} cdots, i_{t+1}, O_{t+1}, i_{t-1}, O_{t-1}, i_1, o_1) = P(o_t | i_{t})P(ot​∣iT​,OT​,iT−1​,oT−1​⋯,it+1​,Ot+1​,it−1​,Ot−1​,i1​,o1​)=P(ot​∣it​) | 2. HMM的3个基本问题 . 概率计算：给定模型λ=(A,B,π) lambda = (A, B, pi)λ=(A,B,π)和观测序列 O=(o1,o2,⋯ ,oT)O = (o_1, o_2, cdots, o_T)O=(o1​,o2​,⋯,oT​)， 求概率P(O∣λ)P(O | lambda)P(O∣λ) . 学习: 已知观测序列O=(o1,o2,⋯ ,oT)O = (o_1, o_2, cdots, o_T)O=(o1​,o2​,⋯,oT​)，估计模型参数λ=(A,B,π) lambda = (A, B, pi)λ=(A,B,π)， 使概率P(O∣λ)P(O vert lambda)P(O∣λ)最大（用极大似然估计）。 . 预测：给定模型λ=(A,B,π) lambda = (A, B, pi)λ=(A,B,π)和观测序列 O=(o1,o2,⋯ ,oT)O = (o_1, o_2, cdots, o_T)O=(o1​,o2​,⋯,oT​)，求条件概率P(I∣O)P(I | O)P(I∣O)最大的状态序列 I=(i1,i2,⋯ ,iT)I = (i_1, i_2, cdots, i_T)I=(i1​,i2​,⋯,iT​). . 2.1 概率计算前向（forward）和后向（backward）算法 . 2.1.1 前向算法 . 给定模型λ lambdaλ，当时刻ttt时，状态为qiq_iqi​，部分观测序列为o1,o2,⋯ ,oto_1, o_2, cdots, o_to1​,o2​,⋯,ot​，记： . αt(i)=P(o1,o2,⋯ ,ot,it=qi∣λ) alpha_{t}(i) = P(o_1, o_2, cdots, o_t, i_t = q_i | lambda)αt​(i)=P(o1​,o2​,⋯,ot​,it​=qi​∣λ) . 输入： 隐马可夫模型 λ lambdaλ， 观测序列OOO; 输出： 观测序列概率P(O∣λ)P(O | lambda)P(O∣λ) . （1）初值 . α1(i)=πibi(o1),i=1,2,⋯ ,N alpha_{1}(i)= pi_{i} b_{i} left(o_{1} right), quad i=1,2, cdots, Nα1​(i)=πi​bi​(o1​),i=1,2,⋯,N . （2）递推 对 t=1,2,⋯ ,T−1,t = 1,2, cdots, T-1,t=1,2,⋯,T−1, . αt+1(i)=[∑j=1Nαt(j)aji]bi(ot+1),i=1,2,⋯ ,N alpha_{t+1}(i)= left[ sum_{j=1}^{N} alpha_{t}(j) a_{j i} right] b_{i} left(o_{t+1} right), quad i=1,2, cdots, Nαt+1​(i)=[j=1∑N​αt​(j)aji​]bi​(ot+1​),i=1,2,⋯,N . （3）终止 . P(O∣λ)=∑i=1NαT(i)P(O | lambda)= sum_{i=1}^{N} alpha_{T}(i)P(O∣λ)=i=1∑N​αT​(i) . 2.1.2 后向算法 . 给定模型λ lambdaλ，当时刻ttt时，状态为qiq_iqi​，部分观测序列为o1,o2,⋯ ,oto_1, o_2, cdots, o_to1​,o2​,⋯,ot​，记： . βt(i)=P(ot+1,ot+2,⋯ ,oT∣it=qi,λ) beta_{t}(i) = P(o_{t+1}, o_{t+2}, cdots, o_T | i_t = q_i, lambda)βt​(i)=P(ot+1​,ot+2​,⋯,oT​∣it​=qi​,λ) . 输入： 隐马可夫模型 λ lambdaλ， 观测序列OOO; 输出： 观测序列概率P(O∣λ)P(O | lambda)P(O∣λ) . （1）初始 令最终时刻所有状态qiq_iqi​ . βT(i)=1,i=1,2,⋯ ,N beta_T(i) = 1, i=1,2, cdots, NβT​(i)=1,i=1,2,⋯,N . （2）递推 对t=T−1,T−2,⋯ ,1t=T-1, T-2, cdots, 1t=T−1,T−2,⋯,1 . βt(i)=∑j=1Naijbj(ot+1)βt+1(j),i=1,2,⋯ ,N beta_{t}(i)= sum_{j=1}^{N} a_{i j} b_{j} left(o_{t+1} right) beta_{t+1}(j), quad i=1,2, cdots, Nβt​(i)=j=1∑N​aij​bj​(ot+1​)βt+1​(j),i=1,2,⋯,N . （3）终止 . P(O∣λ)=∑i=1Nπibi(o1)β1(i)P(O | lambda)= sum_{i=1}^{N} pi_{i} b_{i} left(o_{1} right) beta_{1}(i)P(O∣λ)=i=1∑N​πi​bi​(o1​)β1​(i) . 利用前后向概率定义，可以将观测序列概率P(O∣λ)P(O vert lambda)P(O∣λ)统一写成 . P(O∣λ)=∑i=1N∑j=1Nαt(i)aijbj(ot+1)βt+1(j),t=1,2,⋯ ,T−1P(O | lambda)= sum_{i=1}^{N} sum_{j=1}^{N} alpha_{t}(i) a_{i j} b_{j} left(o_{t+1} right) beta_{t+1}(j), quad t=1,2, cdots, T-1P(O∣λ)=i=1∑N​j=1∑N​αt​(i)aij​bj​(ot+1​)βt+1​(j),t=1,2,⋯,T−1 . 2.2 学习问题的EM算法 . 实质上求一个隐变量的概率模型的参数估计： . P(O∣λ)=∑IP(O∣I,λ)P(I∣λ)P(O | lambda)= sum_{I} P(O | I, lambda) P(I | lambda)P(O∣λ)=I∑​P(O∣I,λ)P(I∣λ) . 参数估计由EM算法实现: (待续) . 输入：观测数据O=(o1,o2,⋯ ,oTO = (o_1, o_2, cdots, o_TO=(o1​,o2​,⋯,oT​; 输出：隐马可夫模型参数 . （1）初始化 对 n=0， 选取aij(0)a_{ij}^{(0)}aij(0)​,bj(k)(0)b_{j}(k)^{(0)}bj​(k)(0),πi(0) pi_{i}^{(0)}πi(0)​,得到模型λ=(A(0),B(0),π(0)) lambda = (A^{(0)}, B^{(0)}, pi^{(0)})λ=(A(0),B(0),π(0)) （2）递推 对n=1,2,⋯ ,n=1,2, cdots,n=1,2,⋯, . aij(n+1)=∑t=1T−1ξt(i,j)∑t=1T−1γt(i)a_{i j}^{(n+1)}= frac{ sum_{t=1}^{T-1} xi_{t}(i, j)}{ sum_{t=1}^{T-1} gamma_{t}(i)}aij(n+1)​=∑t=1T−1​γt​(i)∑t=1T−1​ξt​(i,j)​ . bj(k)(n+1)=∑t=1,ot=vkTγt(j)∑t=1Tγt(j)b_{j}(k)^{(n+1)}= frac{ sum_{t=1, o_{t}=v_{k}}^{T} gamma_{t}(j)}{ sum_{t=1}^{T} gamma_{t}(j)}bj​(k)(n+1)=∑t=1T​γt​(j)∑t=1,ot​=vk​T​γt​(j)​ . πi(n+1)=γ1(i) pi_{i}^{(n+1)}= gamma_{1}(i)πi(n+1)​=γ1​(i) . 其中，时刻ttt处于qiq_iqi​，且时刻t+1t+1t+1处于状态qjq_jqj​的概率, 记 . ξt(i,j)=P(it=qi,it+1=qj∣O,λ) xi_{t}(i, j)=P left(i_{t}=q_{i}, i_{t+1}=q_{j} | O, lambda right)ξt​(i,j)=P(it​=qi​,it+1​=qj​∣O,λ) . 那么 . ξt(i,j)=P(it=qi,it+1=qj,O∣λ)P(O∣λ)=P(it=qi,it+1=qj,O∣λ)∑i=1N∑j=1NP(it=qi,it+1=qj,O∣λ) xi_{t}(i, j)= frac{P left(i_{t}=q_{i}, i_{t+1}=q_{j}, O | lambda right)}{P(O | lambda)}= frac{P left(i_{t}=q_{i}, i_{t+1}=q_{j}, O | lambda right)}{ sum_{i=1}^{N} sum_{j=1}^{N} P left(i_{t}=q_{i}, i_{t+1}=q_{j}, O | lambda right)}ξt​(i,j)=P(O∣λ)P(it​=qi​,it+1​=qj​,O∣λ)​=∑i=1N​∑j=1N​P(it​=qi​,it+1​=qj​,O∣λ)P(it​=qi​,it+1​=qj​,O∣λ)​ . 和时刻ttt处于qiq_iqi​的概率， 有 . γt(i)=P(it=qi∣O,λ)=P(it=qi,O∣λ)P(O∣λ) gamma_{t}(i)=P left(i_{t}=q_{i} | O, lambda right)= frac{P left(i_{t}=q_{i}, O | lambda right)}{P(O | lambda)}γt​(i)=P(it​=qi​∣O,λ)=P(O∣λ)P(it​=qi​,O∣λ)​ . （3）终止 . 得到模型参数λ(n+1)=(A(n+1),B(n+1),π(n+1)) lambda^{(n+1)} = (A^{(n+1)}, B^{(n+1)}, pi^{(n+1)})λ(n+1)=(A(n+1),B(n+1),π(n+1)) . 2.3 预测算法 . 包括近似算法和维特比算法（Viterbi algorithm） . 2.3.1 近似算法 . 在每个时刻ttt， 选择在该时刻最可能出现的状态it∗i^*_tit∗​，从而得到一个状态序列I∗=(ii∗,ii∗,⋯ ,iT∗)I^* = (i^*_i, i^*_i, cdots, i^*_T)I∗=(ii∗​,ii∗​,⋯,iT∗​)，将它最为预测结果。 . 给定模型λ lambdaλ和观测序列OOO， 在时刻ttt处于状态qiq_iqi​的概率γt(i) gamma_t(i)γt​(i)是 . γt(i)=αt(i)βt(i)P(O∣λ)=αt(i)βt(i)∑j=1Nαt(j)βt(j) gamma_{t}(i)= frac{ alpha_{t}(i) beta_{t}(i)}{P(O | lambda)}= frac{ alpha_{t}(i) beta_{t}(i)}{ sum_{j=1}^{N} alpha_{t}(j) beta_{t}(j)}γt​(i)=P(O∣λ)αt​(i)βt​(i)​=∑j=1N​αt​(j)βt​(j)αt​(i)βt​(i)​ . 而每一时刻ttt最有可能的状态it∗i_{t}^{*}it∗​是 . it∗=arg⁡max⁡1⩽i⩽N[γt(i)],t=1,2,⋯ ,Ti_{t}^{*}= arg max _{1 leqslant i leqslant N} left[ gamma_{t}(i) right], quad t=1,2, cdots, Tit∗​=arg1⩽i⩽Nmax​[γt​(i)],t=1,2,⋯,T . 从而得到状态序列I∗=(ii∗,ii∗,⋯ ,iT∗)I^* = (i^*_i, i^*_i, cdots, i^*_T)I∗=(ii∗​,ii∗​,⋯,iT∗​) . 缺点： 不能保证预测状态序列整体是最有可能的状态序列，因为预测的状态序列实际可能由不发生的部分。 . 2.3.2 维特比算法 . 实质是运用动态规划求概率最大路径，从而解决HMM的预测问题 . 只需从时刻t=1t=1t=1开始，递推地计算在时刻ttt状态为qiq_iqi​的各条部分路径的最大概率，直至得到时刻t=Tt = Tt=T状态为iii的各条路径的最大概率。时刻 t=Tt = Tt=T 的最大概率即为最优路径的概率P∗P^*P∗, 最优路径的终结点iT∗i^*_TiT∗​也同时得到。之后，为了找出最优路径的各个结点，从终结点iT∗i^*_TiT∗​开始，由后向前逐步求得结点 iT−1∗,⋯ ,i1∗i^*_{T-1}, cdots, i^*_1iT−1∗​,⋯,i1∗​，得到最优路径 I∗=(ii∗,ii∗,⋯ ,iT∗)I^* = (i^*_i, i^*_i, cdots, i^*_T)I∗=(ii∗​,ii∗​,⋯,iT∗​)。这就是维特比算法。 . 定义在时刻t状态i的所有单个路径中概率最大值为 . δt(i)=max⁡i1,i2,⋯ ,it−1P(it=i,it−1,⋯ ,i1,ot,⋯ ,o1∣λ),i=1,2,⋯ ,N delta_{t}(i)= max _{i_{1}, i_{2}, cdots, i_{t-1}} P left(i_{t}=i, i_{t-1}, cdots, i_{1}, o_{t}, cdots, o_{1} | lambda right), quad i=1,2, cdots, Nδt​(i)=i1​,i2​,⋯,it−1​max​P(it​=i,it−1​,⋯,i1​,ot​,⋯,o1​∣λ),i=1,2,⋯,N . 因此 . δt+1(i)=max⁡i1,i2,⋯ ,itP(it+1=i,it,⋯ ,i1,ot+1,⋯ ,o1∣λ)=max⁡1⩽j⩽N[δt(j)aji]bi(ot+1),i=1,2,⋯ ,N;t=1,2,⋯ ,T−1 begin{aligned} delta_{t+1}(i) &amp;= max _{i_{1}, i_{2}, cdots, i_{t}} P left(i_{t+1}=i, i_{t}, cdots, i_{1}, o_{t+1}, cdots, o_{1} | lambda right) &amp;= max _{1 leqslant j leqslant N} left[ delta_{t}(j) a_{j i} right] b_{i} left(o_{t+1} right), quad i=1,2, cdots, N ; quad t=1,2, cdots, T-1 end{aligned}δt+1​(i)​=i1​,i2​,⋯,it​max​P(it+1​=i,it​,⋯,i1​,ot+1​,⋯,o1​∣λ)=1⩽j⩽Nmax​[δt​(j)aji​]bi​(ot+1​),i=1,2,⋯,N;t=1,2,⋯,T−1​ . 定义在时刻t状态i的所有单个路径中概率最大路径的第t-1 个节点为 . Ψt(i)=arg⁡max⁡1⩽j⩽N[δt−1(j)aji],i=1,2,⋯ ,N Psi_{t}(i)= arg max _{1 leqslant j leqslant N} left[ delta_{t-1}(j) a_{j i} right], quad i=1,2, cdots, NΨt​(i)=arg1⩽j⩽Nmax​[δt−1​(j)aji​],i=1,2,⋯,N . 输入： 隐马可夫模型 λ lambdaλ， 观测序列OOO; 输出： 最优路径I∗=(ii∗,ii∗,⋯ ,iT∗)I^* = (i^*_i, i^*_i, cdots, i^*_T)I∗=(ii∗​,ii∗​,⋯,iT∗​) . （1）初始化 . δ1(i)=πibi(o1),i=1,2,⋯ ,NΨ1(i)=0,i=1,2,⋯ ,N begin{array}{c} delta_{1}(i)= pi_{i} b_{i} left(o_{1} right), quad i=1,2, cdots, N Psi_{1}(i)=0, quad i=1,2, cdots, N end{array}δ1​(i)=πi​bi​(o1​),i=1,2,⋯,NΨ1​(i)=0,i=1,2,⋯,N​ . （2）递推 . δt(i)=max⁡1⩽j⩽N[δt−1(j)aji]bi(ot),i=1,2,⋯ ,NΨt(i)=arg⁡max⁡1⩽j⩽N[δt−1(j)aji],i=1,2,⋯ ,N begin{array}{c} delta_{t}(i)= max _{1 leqslant j leqslant N} left[ delta_{t-1}(j) a_{j i} right] b_{i} left(o_{t} right), quad i=1,2, cdots, N Psi_{t}(i)= arg max _{1 leqslant j leqslant N} left[ delta_{t-1}(j) a_{j i} right], quad i=1,2, cdots, N end{array}δt​(i)=max1⩽j⩽N​[δt−1​(j)aji​]bi​(ot​),i=1,2,⋯,NΨt​(i)=argmax1⩽j⩽N​[δt−1​(j)aji​],i=1,2,⋯,N​ . （3）终止 . P∗=max⁡1⩽i⩽NδT(i)iT∗=arg⁡max⁡1⩽i⩽N[δT(i)] begin{array}{c} P^* = max _{1 leqslant i leqslant N} delta_T(i) i^*_T = arg max _{1 leqslant i leqslant N} [ delta_T(i)] end{array}P∗=max1⩽i⩽N​δT​(i)iT∗​=argmax1⩽i⩽N​[δT​(i)]​ . （4）最优路径回溯 对t=T−1,T−2,⋯ ,1t=T-1, T-2, cdots, 1t=T−1,T−2,⋯,1 . it∗=Ψt+1(it+1∗)i^*_t = Psi_{t+1}(i^*_{t+1})it∗​=Ψt+1​(it+1∗​) . 参考： 《统计学习方法》 .",
            "url": "https://zqfang.github.io/machine%20learning/2020/05/03/ML-HMM.html",
            "relUrl": "/machine%20learning/2020/05/03/ML-HMM.html",
            "date": " • May 3, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Latent semantic analysis (LSA)",
            "content": "潜在语义分析（LSA）是一种非监督学习方法，用于文本话题分析。其特点是通过矩阵分解发现文本于单词之间的基于话题的语义关系。 . 潜在语义分析是一种非概率话题分析模型。步骤为： . 先将文本集合表示为单词-文本矩阵 | 对单词-文本矩阵进行奇异值分解（SVD）或非负矩阵分解（NMF），从而得到话题向量空间，以及文本在话题向量空间的表示 | 1. 单词向量空间 . 给定由n个文本的集合 D={d1,d2,⋯ ,dn}D = {d_1, d_2, cdots, d_n }D={d1​,d2​,⋯,dn​} ,以及所有文本中单词集 W={w1,w2,⋯ ,wm}W = {w_1, w_2, cdots, w_m }W={w1​,w2​,⋯,wm​}, 则单词在文本中出现的数据用单词-文本矩阵(word-document matrix)表示, 记为： . X=[x11x12⋯x1nx21x22⋯x2n⋮⋮⋮xm1xm2⋯xmn]X= left[ begin{array}{cccc} x_{11} &amp; x_{12} &amp; cdots &amp; x_{1 n} x_{21} &amp; x_{22} &amp; cdots &amp; x_{2 n} vdots &amp; vdots &amp; &amp; vdots x_{m 1} &amp; x_{m 2} &amp; cdots &amp; x_{m n} end{array} right]X=⎣⎢⎢⎢⎢⎡​x11​x21​⋮xm1​​x12​x22​⋮xm2​​⋯⋯⋯​x1n​x2n​⋮xmn​​⎦⎥⎥⎥⎥⎤​ . 其中， xijx_{ij}xij​表示单词wiw_iwi​在文本djd_jdj​中出现的频数或权值。由于单词种类多，而每个文本中单词种类通常少，所以为单词-文本矩阵是稀疏矩阵。 . 权值常用单词频率-逆文本频率(term frequeny-inverse document frequency, TF-IDF)表示: . TFIDFij=tfijtf⋅jlog⁡dfdfi,i=1,2,⋯ ,m;j=1,2,⋯ ,n mathrm{TFIDF}_{i j}= frac{ mathrm{tf}_{i j}}{ mathrm{tf}_{ cdot j}} log frac{ mathrm{df}}{ mathrm{df}_{i}}, quad i=1,2, cdots, m ; quad j=1,2, cdots, nTFIDFij​=tf⋅j​tfij​​logdfi​df​,i=1,2,⋯,m;j=1,2,⋯,n . 其中， tfij mathrm{tf}_{ij}tfij​ 是单词wiw_iwi​在文本djd_jdj​中的频数; tf⋅j mathrm{tf}_{ cdot j}tf⋅j​ 是djd_jdj​中出现的所有单词频数之和; dfi mathrm{df}_{i}dfi​ 是含有单词wiw_iwi​的文本数; df mathrm{df}df 是文本集合DDD的全部文本数。 . 单词-文本矩阵的第j列向量xjx_jxj​表示文本djd_jdj​： . xj=[x1jx2j⋮xmj],j=1,2,⋯ ,nx_{j}= left[ begin{array}{c} x_{1 j} x_{2 j} vdots x_{m j} end{array} right], quad j=1,2, cdots, nxj​=⎣⎢⎢⎢⎢⎡​x1j​x2j​⋮xmj​​⎦⎥⎥⎥⎥⎤​,j=1,2,⋯,n . 两个单词向量内积或者标准化内积表示对应文本之间的语义相似度， 因此文本did_idi​与djd_jdj​的之间的相似度： . xi⋅xj,xi⋅xj∥xi∥∥xj∥x_{i} cdot x_{j}, quad frac{x_{i} cdot x_{j}}{ left |x_{i} right | left |x_{j} right |}xi​⋅xj​,∥xi​∥∥xj​∥xi​⋅xj​​ . 单词向量空间模型的优缺点 . 优点： 简单，计算高效 | 缺点： 一词多义(polysemy)和多词一义(synonymy)不能很好处理， 存在相似度计算不准确的问题 | . 2. 话题（topic）向量空间 . 话题没有严格的定义，是指文本讨论的内容或主题。而基于话题的模型是为了解决单词向量不能很好处理一词多义(polysemy)和多词一义(synonymy)的问题。 . 给定由n个文本的集合 D={d1,d2,⋯ ,dn}D = {d_1, d_2, cdots, d_n }D={d1​,d2​,⋯,dn​}, 以及所有文本中单词集 W={w1,w2,⋯ ,wm}W = {w_1, w_2, cdots, w_m }W={w1​,w2​,⋯,wm​}, 则单词在文本中出现的数据用单词-文本矩阵(word-document matrix)表示, 记为： . X=[x11x12⋯x1nx21x22⋯x2n⋮⋮⋮xm1xm2⋯xmn]X= left[ begin{array}{cccc} x_{11} &amp; x_{12} &amp; cdots &amp; x_{1 n} x_{21} &amp; x_{22} &amp; cdots &amp; x_{2 n} vdots &amp; vdots &amp; &amp; vdots x_{m 1} &amp; x_{m 2} &amp; cdots &amp; x_{m n} end{array} right]X=⎣⎢⎢⎢⎢⎡​x11​x21​⋮xm1​​x12​x22​⋮xm2​​⋯⋯⋯​x1n​x2n​⋮xmn​​⎦⎥⎥⎥⎥⎤​ . 假设所有文本含有k个话题， 每个话题l由一个定义在单词集合W傻姑娘的m维向量： . tl=[t1lt2l⋮tml],l=1,2,⋯ ,kt_{l}= left[ begin{array}{c} t_{1 l} t_{2 l} vdots t_{m l} end{array} right], quad l=1,2, cdots, ktl​=⎣⎢⎢⎢⎢⎡​t1l​t2l​⋮tml​​⎦⎥⎥⎥⎥⎤​,l=1,2,⋯,k . 则有话题向量矩阵T： . T=[t11t12⋯t1kt21t22⋯t2k⋮⋮⋮tm1tm2⋯tmk]T= left[ begin{array}{cccc} t_{11} &amp; t_{12} &amp; cdots &amp; t_{1 k} t_{21} &amp; t_{22} &amp; cdots &amp; t_{2 k} vdots &amp; vdots &amp; &amp; vdots t_{m 1} &amp; t_{m 2} &amp; cdots &amp; t_{m k} end{array} right]T=⎣⎢⎢⎢⎢⎡​t11​t21​⋮tm1​​t12​t22​⋮tm2​​⋯⋯⋯​t1k​t2k​⋮tmk​​⎦⎥⎥⎥⎥⎤​ . 3. 文本在话题向量空间的表示 . 文本djd_jdj​在单词向量空间用xjx_jxj​表示，将xjx_jxj​投影到话题向量空间TTT，得到话题向量空间yjy_jyj​，yjy_jyj​是个k维向量: . yj=[y1jy2j⋮ykj],j=1,2,⋯ ,ny_{j}= left[ begin{array}{c} y_{1 j} y_{2 j} vdots y_{k j} end{array} right], quad j=1,2, cdots, nyj​=⎣⎢⎢⎢⎢⎡​y1j​y2j​⋮ykj​​⎦⎥⎥⎥⎥⎤​,j=1,2,⋯,n 　 . 因此，话题-文本矩阵YYY： . Y=[y11y12⋯y1ny21y22⋯y2n⋮⋮⋮yk1yk2⋯ykn]Y= left[ begin{array}{cccc} y_{11} &amp; y_{12} &amp; cdots &amp; y_{1 n} y_{21} &amp; y_{22} &amp; cdots &amp; y_{2 n} vdots &amp; vdots &amp; &amp; vdots y_{k 1} &amp; y_{k 2} &amp; cdots &amp; y_{k n} end{array} right]Y=⎣⎢⎢⎢⎢⎡​y11​y21​⋮yk1​​y12​y22​⋮yk2​​⋯⋯⋯​y1n​y2n​⋮ykn​​⎦⎥⎥⎥⎥⎤​ . 4. 单词向量空间到话题向量空间转换 . 因此，单词向量空间的文本向量xjx_jxj​，可以用话题向量空间yjy_jyj​近似表示，即以k个话题向量以yjy_jyj​为系数的线性组合： . xj≈y1jt1+y2jt2+⋯+ykjtk,j=1,2,⋯ ,nx_{j} approx y_{1 j} t_{1}+y_{2 j} t_{2}+ cdots+y_{k j} t_{k}, quad j=1,2, cdots, nxj​≈y1j​t1​+y2j​t2​+⋯+ykj​tk​,j=1,2,⋯,n . 矩阵形式： . X≈TYX approx TYX≈TY . 这就是潜在语义分析。 . 参考：《统计学习方法》 .",
            "url": "https://zqfang.github.io/machine%20learning/2020/04/30/ML-LSA.html",
            "relUrl": "/machine%20learning/2020/04/30/ML-LSA.html",
            "date": " • Apr 30, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Conditional random field (CRF)",
            "content": "CRF条件随机场，可应用于标注问题 . 概率无向图模型Probabilistic undirected graphical model(Markov random field) 是一个可以由无向图表示的联合概率分布 . 1. 模型定义 . 概率图模型：由图（Graph）表示的概率分布。 . 令无向图 G = (V, E) 表示联合概率分布P(Y)，即G中， . 节点v∈Vv in Vv∈V 表示随机变量Yv,Y=(Yv)v∈VY_{v}, Y= left(Y_{v} right)_{v in V}Yv​,Y=(Yv​)v∈V​； | 边e∈Ee in Ee∈E表示随机变量之间的概率依赖关系 | . 无向图表示的随机变量存在: . 成对马可夫性 pariwise Markov property 指任意两个没有边连接的节点，在给定随机变量组（其他所有节点）条件下，该两节点是条件独立 | . | 局部马可夫性 local Markov property | 全局马可夫性 global Markov property | . 概率无向图模型：无向图 G=(V,E)G = (V, E)G=(V,E) 表示联合概率分布 P(Y)P(Y)P(Y)，如果联合概率分布 P(Y)P(Y)P(Y) 满足成对、局部或全局马可夫性，就称此联合概率分布 P(Y)P(Y)P(Y) 为概率无向图模型，或马可夫随机场 . 团（clique）：图G中任何两个节点均有边连接的节点子集 最大团（maximal clique）：团C中不能再加任何一个节点使它成为更大的团，则称最大团 . 2. 条件随机场 . 条件随机场指给定随机变量X条件下， 随机变量Y的马可夫随机场。 . 2.1 条件随机场： . 若随机变量YYY构成一个由无向图G=(V,E)G = (V, E)G=(V,E)表示的马可夫随机场，即 . P(Yv∣X,Yw,w≠v)=P(Yv∣X,Yw,w∼v)P left(Y_{v} | X, Y_{w}, w neq v right)=P left(Y_{v} | X, Y_{w}, w sim v right)P(Yv​∣X,Yw​,w​=v)=P(Yv​∣X,Yw​,w∼v) . 对于任意节点vvv成立， 则称条件概率分布P(Y∣X)P(Y vert X)P(Y∣X)为条件随机场。其中w∼vw sim vw∼v表示在图G=(V,E)G = (V, E)G=(V,E)中与节点vvv有边连接的所有节点www， w≠vw neq vw​=v表示节点v以外的所有节点。 . 2.2 线性链条件随机场（ linear chain conditional random field） . 线性链条件随机场也是对数线性模型(log linear model)，定义为： . P(Yi∣X,Y1,⋯ ,Yi−1,Yi+1,⋯ ,Yn)=P(Yi∣X,Yi−1,Yi+1)i=1,2,⋯ ,nP left(Y_{i} | X, Y_{1}, cdots, Y_{i-1}, Y_{i+1}, cdots, Y_{n} right)=P left(Y_{i} | X, Y_{i-1}, Y_{i+1} right) i = 1,2, cdots, nP(Yi​∣X,Y1​,⋯,Yi−1​,Yi+1​,⋯,Yn​)=P(Yi​∣X,Yi−1​,Yi+1​)i=1,2,⋯,n . 在条件概率模型P(Y∣X)P(Y vert X)P(Y∣X)中， YYY是输出变量，表示标记序列（状态序列，参见HMM）；XXX使输入变量，表示需要标注的观测序列。利用训练集，通过极大似然估计或正则化的极大似然估计得到条件概率模型P^(Y∣X) hat{P}(Y vert X)P^(Y∣X);预测时，对于给定输入序列xxx，求条件概率P^(Y∣X) hat{P}(Y vert X)P^(Y∣X)最大的输出序列y^ hat{y}y^​。 . 2.3 条件随机场的参数化形式 . 设P(Y∣X)P(Y vert X)P(Y∣X)为线性链条件随机场，X取值为x， Y取值为y的条件概率具有如下形式： . P(y∣x)=1Z(x)exp⁡(∑i,kλktk(yi−1,yi,x,i)+∑i,lμlsl(yi,x,i))P(y | x)= frac{1}{Z(x)} exp left( sum_{i, k} lambda_{k} t_{k} left(y_{i-1}, y_{i}, x, i right)+ sum_{i, l} mu_{l} s_{l} left(y_{i}, x, i right) right)P(y∣x)=Z(x)1​exp⎝⎜⎛​i,k∑​λk​tk​(yi−1​,yi​,x,i)+i,l∑​μl​sl​(yi​,x,i)⎠⎟⎞​ . 其中， . Z(x)=∑yexp⁡(∑i,kλktk(yi−1,yi,x,i)+∑i,lμlsl(yi,x,i))Z(x)= sum_{y} exp left( sum_{i, k} lambda_{k} t_{k} left(y_{i-1}, y_{i}, x, i right)+ sum_{i, l} mu_{l} s_{l} left(y_{i}, x, i right) right)Z(x)=y∑​exp⎝⎜⎛​i,k∑​λk​tk​(yi−1​,yi​,x,i)+i,l∑​μl​sl​(yi​,x,i)⎠⎟⎞​ . 式中，tkt_{k}tk​和sls_{l}sl​是特征函数, λk lambda_{k}λk​和μl mu_{l}μl​是对应的权值。 Z(x)Z(x)Z(x)是规范化因子。在所有可能输出的序列上进行求和操作。 . 关于特征函数： . 令tkt_{k}tk​是定义在边上的特征函数，称为转移特征，依赖当前和前一个位置 | 令sls_{l}sl​是定义在节点上的特征函数，称为状态特征，依赖当前位置 | 特征函数tkt_{k}tk​和sls_{l}sl​取值0或1；满足条件取1，反之0 | 条件随机长完全由特征函数tkt_{k}tk​和sls_{l}sl​， 和对应的权值λk lambda_{k}λk​和μl mu_{l}μl​确定。 | . 2.4 条件随机场的矩阵形式 . 对于观测序列x的每个位置，y在m个标记中取值，可以定义一个m阶的矩阵随机变量： . Mi(x)=[Mi(yi−1,yi∣x)]M_{i}(x) = [ M_{i}(y_{i-1}, y_{i}|x)]Mi​(x)=[Mi​(yi−1​,yi​∣x)] . 矩阵随机变量元素为 . Mi(yi−1,yi∣x)=exp⁡(Wi(yi−1,yi∣x))Wi(yi−1,yi∣x)=∑k=1Kwkfk(yi−1,yi,x,i) begin{aligned} &amp;M_{i} left(y_{i-1}, y_{i} | x right)= exp left(W_{i} left(y_{i-1}, y_{i} | x right) right) &amp;W_{i} left(y_{i-1}, y_{i} | x right)= sum_{k=1}^{K} w_{k} f_{k} left(y_{i-1}, y_{i}, x, i right) end{aligned}​Mi​(yi−1​,yi​∣x)=exp(Wi​(yi−1​,yi​∣x))Wi​(yi−1​,yi​∣x)=k=1∑K​wk​fk​(yi−1​,yi​,x,i)​ . 这里wkw_kwk​为 . wk={λk,k=1,2,⋯ ,K1μl,k=K1+l;l=1,2,⋯ ,K2w_{k}= left { begin{array}{ll} lambda_{k}, &amp; k=1,2, cdots, K_{1} mu_{l}, &amp; k=K_{1}+l ; l=1,2, cdots, K_{2} end{array} right.wk​={λk​,μl​,​k=1,2,⋯,K1​k=K1​+l;l=1,2,⋯,K2​​ . 和fkf_kfk​为 . fk(yi−1,yi,x,i)={tk(yi−1,yi,x,i),k=1,2,⋯ ,K1sl(yi,x,i),k=K1+l;l=1,2,⋯ ,K2f_{k} left(y_{i-1}, y_{i}, x, i right)= left { begin{array}{ll} t_{k} left(y_{i-1}, y_{i}, x, i right), &amp; k=1,2, cdots, K_{1} s_{l} left(y_{i}, x, i right), &amp; k=K_{1}+l ; l=1,2, cdots, K_{2} end{array} right.fk​(yi−1​,yi​,x,i)={tk​(yi−1​,yi​,x,i),sl​(yi​,x,i),​k=1,2,⋯,K1​k=K1​+l;l=1,2,⋯,K2​​ . 于是，条件概率Pw(y∣x)P_{w}(y vert x)Pw​(y∣x): . Pw(y∣x)=1Zw(x)∏i=1n+1Mi(yi−1,yi∣x)P_{w}(y | x)= frac{1}{Z_{w}(x)} prod_{i=1}^{n+1} M_{i} left(y_{i-1}, y_{i} | x right)Pw​(y∣x)=Zw​(x)1​i=1∏n+1​Mi​(yi−1​,yi​∣x) . 其中， . Zw(x)=[M1(x)M2(x)⋯Mn+1(x)]start,stopZ_{w}(x)= left[M_{1}(x) M_{2}(x) cdots M_{n+1}(x) right]_{ mathrm{start}, mathrm{stop}}Zw​(x)=[M1​(x)M2​(x)⋯Mn+1​(x)]start,stop​ . 注， y0=starty_{0} = mathrm{start}y0​=start，表示开始状态； yn+1=stopy_{n+1} = mathrm{stop}yn+1​=stop， 表示终止状态 . 参考：《统计学习方法》 .",
            "url": "https://zqfang.github.io/machine%20learning/2020/04/29/ML-CRF.html",
            "relUrl": "/machine%20learning/2020/04/29/ML-CRF.html",
            "date": " • Apr 29, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Convert Seurat Robj to Scanpy h5ad",
            "content": "It costed me a lot of time to convert seurat objects to scanpy. It’s not a pleasant experience. Finally, I solved it. . 1. Install Seurat v3.0.2, or python kernel will always died!!! . Don’t know why latest seurat not work. . 2. Set the R version for rpy2 . # user defined R installation import os # path to your libR.so, only Seurat v3.0.2 works! # create a conda R env for seurat 3.0.2 first os.environ[&#39;R_HOME&#39;] = &#39;/home/fangzq/miniconda/envs/seurat/lib/R&#39; # path depends on where you installed Python. os.environ[&#39;R_USER&#39;] = &#39;/home/fangzq/miniconda/lib/python3.7/site-packages/rpy2&#39; . 3. Now, you’er good to go . import scanpy as sc import glob . Install anndata2ri first . import anndata2ri from rpy2.robjects import r from rpy2.robjects.conversion import localconverter # activate rpy2 env anndata2ri.activate() . robjs = glob.glob(&quot;data/*Robj&quot;) . Convert to h5ad . r(&#39;library(Seurat)&#39;) for robj in robjs: r(f&#39;x&lt;-load(&quot;{robj}&quot;)&#39;) r(&#39;y=get(x)&#39;) r(&#39;rm(x)&#39;) adata = r(&#39;as.SingleCellExperiment(UpdateSeuratObject(y))&#39;) adata.write_h5ad(filename=robj.replace(&quot;Robj&quot;,&quot;h5ad&quot;)) . 4. other way . seurat -&gt; loom -&gt; scanpy . It’s much easier, but I did not test. . save to loom format fist. pbmc.loom &lt;- as.loom(pbmc.seurat, filename = &quot;../output/pbmc3k.loom&quot;, verbose = FALSE) pbmc.loom . | read into scanpy pbmc3k = sc.read_loom(&quot;../output/pbmc3k.loom&quot;) . |",
            "url": "https://zqfang.github.io/make%20bioinfo%20uncool%20again/2020/04/28/Seurat2Scanpy.html",
            "relUrl": "/make%20bioinfo%20uncool%20again/2020/04/28/Seurat2Scanpy.html",
            "date": " • Apr 28, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "How to do deep learning using custom Jupyter kernels on Sherlock",
            "content": "A recipe for interactive computing using custom Jupyter kernels on Stanford’s Sherlock. . Setting up custom conda environment on Sherlock’s login node . 1. Download and install Miniconda . wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh # install bash Miniconda3-latest-Linux-x86_64.sh conda config --set always_yes yes . 2. Install jupyter notebook/lab and secure your notebooks with a password . # install the default py3 kernel for jupyter notebook conda install ipython jupyter notebook jupyterlab # add password jupyter notebook password . 3. (Optional) Add custom conda environment. i.e. fastai . conda create -n fastai ipython ipykernel # add the custom to Jupyter notebook conda activate fastai python -m ipykernel install --user --name fastai --display-name FastAI . you could also add R, Julia etc kernel. . 4. Install pytorch/tensorflow . You should select the existed cuda version which installed in Sherlock . conda install -c pytorch pytorch torchvision cudatoolkit=10.1 . tensorflow . conda install tensorflow-gpu cudatoolkit=10.1 . 5. Load gpu modules. Select the corresponding cuda version you’ve just installed . # this is my version module load cuda/10.1.168 module load cudnn/7.6.4 module load nccl . 6. now, open ipython, run . import torch print(torch.cuda.is_avilable()) . if print out is True, then you’er OK to use GPUs. . Follow these steps on your local machine . see details here. . 7. Download the forward repo . git clone https://github.com/vsoch/forward cd forward . 8. Generate your parameters . bash setup.sh . Select Sherlock partition: gpu . 9. SSH Credentials . bash hosts/sherlock_ssh.sh &gt;&gt; ~/.ssh/config . 10. create a sbatch script in forward/sbatches/sherlock and save as jupyter-gpu.sbatch . #!/bin/bash PORT=$1 NOTEBOOK_DIR=$2 if [ -z &quot;$NOTEBOOK_DIR&quot; ]; then cd $SCRATCH else cd $NOTEBOOK_DIR fi ## to compile libtorch C++ code, load these modules # module load gcc/7.3.0 # module load gdb # module load cmake # export CC=$(which gcc) # export CXX=$(which g++) # select cuda version you need module load cuda/10.1.168 module load cudnn/7.6.4 module load nccl # activate fastai env source activate fastai jupyter lab --no-browser --port=$PORT . 11. Start a session . The default working directory is $SCRATCH . bash start.sh jupyter-gpu . change the working directory . bash start.sh jupyter /path/to/dir . 12. open your browser in local machine and type . if your port is 51888, then . http://localhost:51888/ . here is my jupyter lab computing environment. Have fun! . fastai kernel . . Test GPUs . . 13. Resume a session . bash resume.sh jupyter-gpu # or bash resume.sh jupyter-gpu /path/to/dir . 14. Stop a session . bash end.sh jupyter-gpu # or bash end.sh jupyter-gpu /path/to/dir .",
            "url": "https://zqfang.github.io/machine%20learning/2020/02/10/ML-Sherlock.html",
            "relUrl": "/machine%20learning/2020/02/10/ML-Sherlock.html",
            "date": " • Feb 10, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Graph",
            "content": "Graphs . Some data structures to keep in my mind. . BinaryHeap: Complete binary tree MaxHeap: Parent &gt; Both Children | IndexMaxHeap | MinHeap: Parent &lt; Both Children | IndexMinHeap | Priority queue (MaxHeap) | . | BinarySearchTree Not always complete binary tree | Value: leftChild &lt; Parent &lt; rightChild | . | DenseGraph | SparseGraph | . Code snippets take from Play with Algorithm . 1. Dense Graph . #include &lt;iostream&gt; #include &lt;vector&gt; #include &lt;cassert&gt; using namespace std; // 稠密图 - 邻接矩阵 class DenseGraph{ private: int n, m; bool directed; vector&lt;vector&lt;bool&gt;&gt; g; public: DenseGraph( int n , bool directed ){ this-&gt;n = n; this-&gt;m = 0; this-&gt;directed = directed; for( int i = 0 ; i &lt; n ; i ++ ) g.push_back( vector&lt;bool&gt;(n, false) ); } ~DenseGraph(){ } int V(){ return n;} int E(){ return m;} void addEdge( int v , int w ){ assert( v &gt;= 0 &amp;&amp; v &lt; n ); assert( w &gt;= 0 &amp;&amp; w &lt; n ); if( hasEdge( v , w ) ) return; g[v][w] = true; if( !directed ) g[w][v] = true; m ++; } bool hasEdge( int v , int w ){ assert( v &gt;= 0 &amp;&amp; v &lt; n ); assert( w &gt;= 0 &amp;&amp; w &lt; n ); return g[v][w]; } void show(){ for( int i = 0 ; i &lt; n ; i ++ ){ for( int j = 0 ; j &lt; n ; j ++ ) cout&lt;&lt;g[i][j]&lt;&lt;&quot; t&quot;; cout&lt;&lt;endl; } } class adjIterator{ private: DenseGraph &amp;G; int v; int index; public: adjIterator(DenseGraph &amp;graph, int v): G(graph){ this-&gt;v = v; this-&gt;index = -1; } int begin(){ index = -1; return next(); } int next(){ for( index += 1 ; index &lt; G.V() ; index ++ ) if( G.g[v][index] ) return index; return -1; } bool end(){ return index &gt;= G.V(); } }; }; . 2. Sparse Graph . #include &lt;iostream&gt; #include &lt;vector&gt; #include &lt;cassert&gt; using namespace std; // 稀疏图 - 邻接表 class SparseGraph{ private: int n, m; bool directed; vector&lt;vector&lt;int&gt;&gt; g; public: SparseGraph( int n , bool directed ){ this-&gt;n = n; this-&gt;m = 0; this-&gt;directed = directed; for( int i = 0 ; i &lt; n ; i ++ ) g.push_back( vector&lt;int&gt;() ); } ~SparseGraph(){ } int V(){ return n;} int E(){ return m;} void addEdge( int v, int w ){ assert( v &gt;= 0 &amp;&amp; v &lt; n ); assert( w &gt;= 0 &amp;&amp; w &lt; n ); g[v].push_back(w); if( v != w &amp;&amp; !directed ) g[w].push_back(v); m ++; } bool hasEdge( int v , int w ){ assert( v &gt;= 0 &amp;&amp; v &lt; n ); assert( w &gt;= 0 &amp;&amp; w &lt; n ); for( int i = 0 ; i &lt; g[v].size() ; i ++ ) if( g[v][i] == w ) return true; return false; } void show(){ for( int i = 0 ; i &lt; n ; i ++ ){ cout&lt;&lt;&quot;vertex &quot;&lt;&lt;i&lt;&lt;&quot;: t&quot;; for( int j = 0 ; j &lt; g[i].size() ; j ++ ) cout&lt;&lt;g[i][j]&lt;&lt;&quot; t&quot;; cout&lt;&lt;endl; } } class adjIterator{ private: SparseGraph &amp;G; int v; int index; public: adjIterator(SparseGraph &amp;graph, int v): G(graph){ this-&gt;v = v; this-&gt;index = 0; } int begin(){ index = 0; if( G.g[v].size() ) return G.g[v][index]; return -1; } int next(){ index ++; if( index &lt; G.g[v].size() ) return G.g[v][index]; return -1; } bool end(){ return index &gt;= G.g[v].size(); } }; }; .",
            "url": "https://zqfang.github.io/algorithm%20and%20data%20structure/2020/02/09/CS-graph.html",
            "relUrl": "/algorithm%20and%20data%20structure/2020/02/09/CS-graph.html",
            "date": " • Feb 9, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Sort",
            "content": "Sort algorithms Code snippets are taken from Play with Algorithm . Some algorithm to keep in my mind. . selectionSort | mergeSort | quickSort two way | three way | . | heapSort | . 1. selectionSort . #include &lt;iostream&gt; #include &lt;algorithm&gt; using namespace std; template&lt;typename T&gt; void insertionSort(T arr[], int n){ for( int i = 1 ; i &lt; n ; i ++ ) { T e = arr[i]; int j; for (j = i; j &gt; 0 &amp;&amp; arr[j-1] &gt; e; j--) arr[j] = arr[j-1]; arr[j] = e; } return; } // 对arr[l...r]范围的数组进行插入排序 template&lt;typename T&gt; void insertionSort(T arr[], int l, int r){ for( int i = l+1 ; i &lt;= r ; i ++ ) { T e = arr[i]; int j; for (j = i; j &gt; l &amp;&amp; arr[j-1] &gt; e; j--) arr[j] = arr[j-1]; arr[j] = e; } return; } . 2. mergeSort . #include &lt;iostream&gt; #include &lt;algorithm&gt; #include &quot;InsertionSort.h&quot; using namespace std; template&lt;typename T&gt; void __merge(T arr[], int l, int mid, int r){ T aux[r-l+1]; for( int i = l ; i &lt;= r; i ++ ) aux[i-l] = arr[i]; int i = l, j = mid+1; for( int k = l ; k &lt;= r; k ++ ){ if( i &gt; mid ) { arr[k] = aux[j-l]; j ++;} else if( j &gt; r ){ arr[k] = aux[i-l]; i ++;} else if( aux[i-l] &lt; aux[j-l] ){ arr[k] = aux[i-l]; i ++;} else { arr[k] = aux[j-l]; j ++;} } } template&lt;typename T&gt; void __mergeSort(T arr[], int l, int r){ if( r - l &lt;= 15 ){ insertionSort(arr, l, r); return; } int mid = (l+r)/2; __mergeSort(arr, l, mid); __mergeSort(arr, mid+1, r); if( arr[mid] &gt; arr[mid+1] ) __merge(arr, l, mid, r); } template&lt;typename T&gt; void mergeSort(T arr[], int n){ __mergeSort( arr , 0 , n-1 ); } template &lt;typename T&gt; void mergeSortBU(T arr[], int n){ for( int i = 0 ; i &lt; n ; i += 16 ) insertionSort(arr,i,min(i+15,n-1)); for( int sz = 16; sz &lt;= n ; sz += sz ) for( int i = 0 ; i &lt; n - sz ; i += sz+sz ) if( arr[i+sz-1] &gt; arr[i+sz] ) __merge(arr, i, i+sz-1, min(i+sz+sz-1,n-1) ); } . 3. qucikSort . #include &lt;iostream&gt; #include &lt;ctime&gt; #include &lt;algorithm&gt; #include &quot;InsertionSort.h&quot; using namespace std; template &lt;typename T&gt; int _partition(T arr[], int l, int r){ // select a rand index in arr, and swap swap( arr[l] , arr[rand()%(r-l+1)+l] ); T v = arr[l]; int i = l+1, j = r; while( true ){ while( i &lt;= r &amp;&amp; arr[i] &lt; v ) i ++; while( j &gt;= l+1 &amp;&amp; arr[j] &gt; v ) j --; if( i &gt; j ) break; swap( arr[i] , arr[j] ); i ++; j --; } swap( arr[l] , arr[j]); return j; } template &lt;typename T&gt; void _quickSort(T arr[], int l, int r){ if( r - l &lt;= 15 ){ insertionSort(arr,l,r); return; } int p = _partition(arr, l, r); _quickSort(arr, l, p-1 ); _quickSort(arr, p+1, r); } template &lt;typename T&gt; void quickSort(T arr[], int n){ srand(time(NULL)); _quickSort(arr, 0, n-1); } template &lt;typename T&gt; void __quickSort3Ways(T arr[], int l, int r){ if( r - l &lt;= 15 ){ insertionSort(arr,l,r); return; } swap( arr[l], arr[rand()%(r-l+1)+l ] ); T v = arr[l]; int lt = l; // arr[l+1...lt] &lt; v int gt = r + 1; // arr[gt...r] &gt; v int i = l+1; // arr[lt+1...i) == v while( i &lt; gt ){ if( arr[i] &lt; v ){ swap( arr[i], arr[lt+1]); i ++; lt ++; } else if( arr[i] &gt; v ){ swap( arr[i], arr[gt-1]); gt --; } else{ // arr[i] == v i ++; } } swap( arr[l] , arr[lt] ); __quickSort3Ways(arr, l, lt-1); __quickSort3Ways(arr, gt, r); } template &lt;typename T&gt; void quickSort3Ways(T arr[], int n){ srand(time(NULL)); __quickSort3Ways( arr, 0, n-1); } . 4. heapSort . #include &quot;Heap.h&quot; using namespace std; template&lt;typename T&gt; void heapSort2(T arr[], int n){ MaxHeap&lt;T&gt; maxheap = MaxHeap&lt;T&gt;(arr,n); for( int i = n-1 ; i &gt;= 0 ; i-- ) arr[i] = maxheap.extractMax(); } template&lt;typename T&gt; void heapSort1(T arr[], int n){ MaxHeap&lt;T&gt; maxheap = MaxHeap&lt;T&gt;(n); for( int i = 0 ; i &lt; n ; i ++ ) maxheap.insert(arr[i]); for( int i = n-1 ; i &gt;= 0 ; i-- ) arr[i] = maxheap.extractMax(); } .",
            "url": "https://zqfang.github.io/algorithm%20and%20data%20structure/2020/02/08/CS-sort.html",
            "relUrl": "/algorithm%20and%20data%20structure/2020/02/08/CS-sort.html",
            "date": " • Feb 8, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Binary tree",
            "content": "Binary trees . Difference . Some data structures to keep in my mind. . BinaryHeap: Complete binary tree MaxHeap: Parent &gt; Both Children | IndexMaxHeap | MinHeap: Parent &lt; Both Children | IndexMinHeap | Priority queue (MaxHeap) | . | BinarySearchTree Not always complete binary tree | Value: leftChild &lt; Parent &lt; rightChild | . | DenseGraph | SparseGraph | . Code snippets take from Play with Algorithm . 1. MaxHeap . #include &lt;algorithm&gt; #include &lt;cassert&gt; using namespace std; template&lt;typename Item&gt; class MaxHeap{ private: Item *data; int count; int capacity; void shiftUp(int k){ while( k &gt; 1 &amp;&amp; data[k/2] &lt; data[k] ){ swap( data[k/2], data[k] ); k /= 2; } } void shiftDown(int k){ while( 2*k &lt;= count ){ int j = 2*k; // which child is larger, then swap if( j+1 &lt;= count &amp;&amp; data[j+1] &gt; data[j] ) j ++; if( data[k] &gt;= data[j] ) break; swap( data[k] , data[j] ); k = j; } } public: MaxHeap(int capacity){ data = new Item[capacity+1]; count = 0; this-&gt;capacity = capacity; } MaxHeap(Item arr[], int n){ data = new Item[n+1]; capacity = n; for( int i = 0 ; i &lt; n ; i ++ ) data[i+1] = arr[i]; count = n; for( int i = count/2 ; i &gt;= 1 ; i -- ) shiftDown(i); } ~MaxHeap(){ delete[] data; } int size(){ return count; } bool isEmpty(){ return count == 0; } void insert(Item item){ assert( count + 1 &lt;= capacity ); data[count+1] = item; shiftUp(count+1); count ++; } Item extractMax(){ assert( count &gt; 0 ); Item ret = data[1]; swap( data[1] , data[count] ); count --; shiftDown(1); return ret; } Item getMax(){ assert( count &gt; 0 ); return data[1]; } }; . 2. IndexMaxHeap . Need 3 vector: data, indexes, reverse . Code . #include &lt;algorithm&gt; #include &lt;cassert&gt; using namespace std; template&lt;typename Item&gt; class IndexMaxHeap{ private: Item *data; int *indexes; int *reverse; int count; int capacity; void shiftUp( int k ){ while( k &gt; 1 &amp;&amp; data[indexes[k/2]] &lt; data[indexes[k]] ){ swap( indexes[k/2] , indexes[k] ); reverse[indexes[k/2]] = k/2; reverse[indexes[k]] = k; k /= 2; } } void shiftDown( int k ){ while( 2*k &lt;= count ){ int j = 2*k; if( j + 1 &lt;= count &amp;&amp; data[indexes[j+1]] &gt; data[indexes[j]] ) j += 1; if( data[indexes[k]] &gt;= data[indexes[j]] ) break; swap( indexes[k] , indexes[j] ); reverse[indexes[k]] = k; reverse[indexes[j]] = j; k = j; } } public: IndexMaxHeap(int capacity){ data = new Item[capacity+1]; indexes = new int[capacity+1]; reverse = new int[capacity+1]; for( int i = 0 ; i &lt;= capacity ; i ++ ) reverse[i] = 0; count = 0; this-&gt;capacity = capacity; } ~IndexMaxHeap(){ delete[] data; delete[] indexes; delete[] reverse; } int size(){ return count; } bool isEmpty(){ return count == 0; } // 传入的i对用户而言,是从0索引的 void insert(int i, Item item){ assert( count + 1 &lt;= capacity ); assert( i + 1 &gt;= 1 &amp;&amp; i + 1 &lt;= capacity ); i += 1; data[i] = item; indexes[count+1] = i; reverse[i] = count+1; count++; shiftUp(count); } Item extractMax(){ assert( count &gt; 0 ); Item ret = data[indexes[1]]; swap( indexes[1] , indexes[count] ); reverse[indexes[count]] = 0; reverse[indexes[1]] = 1; count--; shiftDown(1); return ret; } int extractMaxIndex(){ assert( count &gt; 0 ); int ret = indexes[1] - 1; swap( indexes[1] , indexes[count] ); reverse[indexes[count]] = 0; reverse[indexes[1]] = 1; count--; shiftDown(1); return ret; } Item getMax(){ assert( count &gt; 0 ); return data[indexes[1]]; } int getMaxIndex(){ assert( count &gt; 0 ); return indexes[1]-1; } bool contain( int i ){ assert( i + 1 &gt;= 1 &amp;&amp; i + 1 &lt;= capacity ); return reverse[i+1] != 0; } Item getItem( int i ){ assert( contain(i) ); return data[i+1]; } void change( int i , Item newItem ){ assert( contain(i) ); i += 1; data[i] = newItem; // 找到indexes[j] = i, j表示data[i]在堆中的位置 // 之后shiftUp(j), 再shiftDown(j) // for( int j = 1 ; j &lt;= count ; j ++ ) // if( indexes[j] == i ){ // shiftUp(j); // shiftDown(j); // return; // } int j = reverse[i]; shiftUp( j ); shiftDown( j ); } // test reverse index bool testReverseIndex(){ int *copyIndexes = new int[count+1]; int *copyReverseIndexes = new int[count+1]; for( int i = 0 ; i &lt;= count ; i ++ ){ copyIndexes[i] = indexes[i]; copyReverseIndexes[i] = reverse[i]; } copyIndexes[0] = copyReverseIndexes[0] = 0; std::sort(copyIndexes, copyIndexes + count + 1); std::sort(copyReverseIndexes, copyReverseIndexes + count + 1); bool res = true; for( int i = 1 ; i &lt;= count ; i ++ ) if( copyIndexes[i-1] + 1 != copyIndexes[i] || copyReverseIndexes[i-1] + 1 != copyReverseIndexes[i] ) res = res || false; delete[] copyIndexes; delete[] copyReverseIndexes; if( !res ){ cout&lt;&lt;&quot;Error 1&quot;&lt;&lt;endl; return res; } for( int i = 1 ; i &lt;= count ; i ++ ) if( reverse[ indexes[i] ] != i ){ cout&lt;&lt;&quot;Error 2&quot;&lt;&lt;endl; return false; } return true; } }; . 3. BinarySearchTree . #include &lt;iostream&gt; #include &lt;queue&gt; #include &lt;cassert&gt; using namespace std; template &lt;typename Key, typename Value&gt; class BST{ private: struct Node{ Key key; Value value; Node *left; Node *right; Node(Key key, Value value){ this-&gt;key = key; this-&gt;value = value; this-&gt;left = this-&gt;right = NULL; } Node(Node *node){ this-&gt;key = node-&gt;key; this-&gt;value = node-&gt;value; this-&gt;left = node-&gt;left; this-&gt;right = node-&gt;right; } }; Node *root; int count; public: BST(){ root = NULL; count = 0; } ~BST(){ destroy( root ); } int size(){ return count; } bool isEmpty(){ return count == 0; } void insert(Key key, Value value){ root = insert(root, key, value); } bool contain(Key key){ return contain(root, key); } Value* search(Key key){ return search( root , key ); } // 前序遍历 void preOrder(){ preOrder(root); } // 中序遍历 void inOrder(){ inOrder(root); } // 后序遍历 void postOrder(){ postOrder(root); } // 层序遍历 void levelOrder(){ queue&lt;Node*&gt; q; q.push(root); while( !q.empty() ){ Node *node = q.front(); q.pop(); cout&lt;&lt;node-&gt;key&lt;&lt;endl; if( node-&gt;left ) q.push( node-&gt;left ); if( node-&gt;right ) q.push( node-&gt;right ); } } // 寻找最小的键值 Key minimum(){ assert( count != 0 ); Node* minNode = minimum( root ); return minNode-&gt;key; } // 寻找最大的键值 Key maximum(){ assert( count != 0 ); Node* maxNode = maximum(root); return maxNode-&gt;key; } // 从二叉树中删除最小值所在节点 void removeMin(){ if( root ) root = removeMin( root ); } // 从二叉树中删除最大值所在节点 void removeMax(){ if( root ) root = removeMax( root ); } // 从二叉树中删除键值为key的节点 void remove(Key key){ root = remove(root, key); } private: // 向以node为根的二叉搜索树中,插入节点(key, value) // 返回插入新节点后的二叉搜索树的根 Node* insert(Node *node, Key key, Value value){ if( node == NULL ){ count ++; return new Node(key, value); } if( key == node-&gt;key ) node-&gt;value = value; else if( key &lt; node-&gt;key ) node-&gt;left = insert( node-&gt;left , key, value); else // key &gt; node-&gt;key node-&gt;right = insert( node-&gt;right, key, value); return node; } // 查看以node为根的二叉搜索树中是否包含键值为key的节点 bool contain(Node* node, Key key){ if( node == NULL ) return false; if( key == node-&gt;key ) return true; else if( key &lt; node-&gt;key ) return contain( node-&gt;left , key ); else // key &gt; node-&gt;key return contain( node-&gt;right , key ); } // 在以node为根的二叉搜索树中查找key所对应的value Value* search(Node* node, Key key){ if( node == NULL ) return NULL; if( key == node-&gt;key ) return &amp;(node-&gt;value); else if( key &lt; node-&gt;key ) return search( node-&gt;left , key ); else // key &gt; node-&gt;key return search( node-&gt;right, key ); } // 对以node为根的二叉搜索树进行前序遍历 void preOrder(Node* node){ if( node != NULL ){ cout&lt;&lt;node-&gt;key&lt;&lt;endl; preOrder(node-&gt;left); preOrder(node-&gt;right); } } // 对以node为根的二叉搜索树进行中序遍历 void inOrder(Node* node){ if( node != NULL ){ inOrder(node-&gt;left); cout&lt;&lt;node-&gt;key&lt;&lt;endl; inOrder(node-&gt;right); } } // 对以node为根的二叉搜索树进行后序遍历 void postOrder(Node* node){ if( node != NULL ){ postOrder(node-&gt;left); postOrder(node-&gt;right); cout&lt;&lt;node-&gt;key&lt;&lt;endl; } } void destroy(Node* node){ if( node != NULL ){ destroy( node-&gt;left ); destroy( node-&gt;right ); delete node; count --; } } // 在以node为根的二叉搜索树中,返回最小键值的节点 Node* minimum(Node* node){ if( node-&gt;left == NULL ) return node; return minimum(node-&gt;left); } // 在以node为根的二叉搜索树中,返回最大键值的节点 Node* maximum(Node* node){ if( node-&gt;right == NULL ) return node; return maximum(node-&gt;right); } // 删除掉以node为根的二分搜索树中的最小节点 // 返回删除节点后新的二分搜索树的根 Node* removeMin(Node* node){ if( node-&gt;left == NULL ){ Node* rightNode = node-&gt;right; delete node; count --; return rightNode; } node-&gt;left = removeMin(node-&gt;left); return node; } // 删除掉以node为根的二分搜索树中的最大节点 // 返回删除节点后新的二分搜索树的根 Node* removeMax(Node* node){ if( node-&gt;right == NULL ){ Node* leftNode = node-&gt;left; delete node; count --; return leftNode; } node-&gt;right = removeMax(node-&gt;right); return node; } // 删除掉以node为根的二分搜索树中键值为key的节点 // 返回删除节点后新的二分搜索树的根 Node* remove(Node* node, Key key){ if( node == NULL ) return NULL; if( key &lt; node-&gt;key ){ node-&gt;left = remove( node-&gt;left , key ); return node; } else if( key &gt; node-&gt;key ){ node-&gt;right = remove( node-&gt;right, key ); return node; } else{ // key == node-&gt;key if( node-&gt;left == NULL ){ Node *rightNode = node-&gt;right; delete node; count --; return rightNode; } if( node-&gt;right == NULL ){ Node *leftNode = node-&gt;left; delete node; count--; return leftNode; } // node-&gt;left != NULL &amp;&amp; node-&gt;right != NULL Node *successor = new Node(minimum(node-&gt;right)); count ++; successor-&gt;right = removeMin(node-&gt;right); successor-&gt;left = node-&gt;left; delete node; count --; return successor; } } }; .",
            "url": "https://zqfang.github.io/algorithm%20and%20data%20structure/2020/02/08/CS-binarytree.html",
            "relUrl": "/algorithm%20and%20data%20structure/2020/02/08/CS-binarytree.html",
            "date": " • Feb 8, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "What is Big O",
            "content": "What on earth is Big O? . Time complexity and space complexity . Time complexity . O(f(n)): number of commands need to execute. proportional to f(n). 表示运行算法所需要执行的指令数，和f(n)成正。 . 严格来讲，O(f(n))表示算法执行的上界。业界默认为算法执行的最低上界(最坏情况）。 n represents the data scale 数据规模 . when n is a large number, the constant is usually ignored. . algorithm n of cmd . Binary reserach $O(logn)$ | $a*logn$ | . Max/min in an array $O(n)$ | b*n | . merge sort $O(nlogn)$ | $c*nlogn$ | . select sort $O(n^2)$ | $d*n^2$ | . quick sort $O(nlogn)$ | e*nlogn | . adjacent graph | $O(V+E)$ | . Lazy Prim | $O(ElogE)$ | . Prim | $O(ElogV)$ | . Kruskal | $O(ElogE)$ | . Dijkstra | $O(ElogV)$ | . Bellman-Ford | $O(EV)$ | . minimum span tree Shortest path tree (Single source shortest path) . Space complexity . cmd complexity . new an array | $O(n)$ | . new 2d array | $O(n^2)$ | . new an constant space | $O(1)$ | . recursive function: the depth (n) of a recursive function, the extra space need $O(n)$. . Make sense of n . If you want to solve the problem in 1 second, then an algorithm of . complexity cmds n . $O(n^2)$ | could exec cmd n = $10^4$ | . $O(n)$ | could exec cmd n = $10^8$ | . $O(nlogn)$ | could exec cmd n = $10^7$ | . Example . binarySearch | find from n element find from $n/2$ element find from $n/4$ element ... find from 1 . That’s, need how many steps of search when n = 1? $log_{2}n = O(logn)$. . int2string. Set num &gt; 0 | string int2string(int num) { string s=&quot;&quot;; while(num) { s += &#39;0&#39; + num%10; num /= 10; } reverse(s); // O(n) return s; } . That is, how many “/10” steps when num = 0? $log_{10}n = O(logn)$. . Case: two nested for loop, not always $O(n^2)$ | void hello(int n){ for (int sz =1; sz &lt; n; sz ++ sz) // logn here for( int i=1; i &lt; n;; i++) //n cout&lt;&lt;&quot;hello, complex&quot; &lt;&lt;endl; } . So, should be $O(nlogn)$ . isPrime: $O( sqrt{n})$ // set n &gt; 1 bool isPrime(int n){ for( int x =2; x*x &lt;= n; x++){ if( n%x == 0) return false; return true; } } . | recursive function | single recursive function call int binarySearch(int arr[], int l, int r, int target) { if (l&gt;r) return -1; int mid = l +(r-l)/2; if (arr[mid] == target) return mid; else if (arr[mid] &gt; target) return binarySearch(arr, ;, mid-1, target); else return binarySearch(arr, mid+1, r, target); } . each step need O(1), so overall complexity depend on recursive exec depth. That is, if each function call needs time T, then time complexity: O(T*depth) -&gt; O(n). . | . Another example: recursion depth logn, them time complexity O(logn). . double pow( double x, int n){ assert(n &gt;=0); if (n==0) return 1.0; double t = pow(x, n/2); if( n%2) return x*t*t; return t*t; } . multi recursive exec how many exec step? int f(int n) { assert(n &gt;=0); if(n == 0) return 1; return f(n-1) + f(n); } . that’s, count how many nodes on a full binary tree. $2^{n+1} -1 = O(2^n)$ . | . how to think about this? . void mergeSort(int arr[]. int l. int r){ if (l &gt;=r) return; int mid = (l+r) /2; mergeSort(arr, l, mid); mergeSort(arr, mid+1, r); merge(arr, l, mid, r); } . For binary tree, complexity for each level O(n), while tree depth O(logn). Overall, O(nlogn) . Amortized time i.e. dynamic vector/stack/deque | template&lt;T&gt; class MyVector{ private: T* data; int capacity; int size; //O(n) void resize(int newCapacity){ assert(newCapacity &gt;= size); T* newData = new T[newCapacity]; for(int i = 0; i &lt; size; i++ ){ newData[i] = data[i]; } delete[] data; data = newData; capacity = newCapacity; } public: MyVector() { data = new T[10]; capacity = 10; size = 0; } ~MyVector() { delete[] data; } // Average: O(1) void push_back(T e){ //assert(size &lt; capacity) if (size == capacity) resize (2 *capacity); data[size++] = e; } // Average O(1) T pop_back(){ assert (size &gt;0); T ret = data[size-1]; size --; // note the denominator here. To advoid ossilation of space complexity if(size == capacity / 4) resize(capacity /2); return ret; } }; .",
            "url": "https://zqfang.github.io/algorithm%20and%20data%20structure/2020/02/01/CS-algo.html",
            "relUrl": "/algorithm%20and%20data%20structure/2020/02/01/CS-algo.html",
            "date": " • Feb 1, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Statistical Modeling and Inference",
            "content": "A breif review over the foundations of statistical inference . Statistical Models and Inference . statistical inference: a formal approach to characterizing a random phenomenon using observations, either by providing a description of a past phenomenon or by giving some predictions about future phenomenon of similar nature. . 1. Statistical Models . The first step in statistical inference is to specify a statistical model, under some simplifying assumptions (i.e. independence assumptions). . Hierarchical models: the probability distribution of one parameter is dependent on the values of other hierachical paramters (i.e. conditional independent). | . Steps: . Set assumptions (i.e. independent), parameter and model. Make explicit assumptions on the probability distributions. focus on parametric modeling, because of limited data | nonparametric not consider here, used for hypothesis testing or when sample size is very large. | . | Once the model is specified, then choose a method of inference, as well as an algorithm to obtain estimates. Most commonly use: Maximum likelihood inference | Bayesian inference | . | 2. Maximum likelihood inference . Quantifying confidence: the Fisher Information Matrix Newton’s algorithm Approximate Techniques . Monte Carlo Sampling for intractable likelihoods | Composite likelihood | . 3. Bayesian Inference . A statistical model describes the uncertainty about how the data was produced. The ultimate aim for statistical inference is to obtain information about the unknown parameter θ thetaθ given the data D mathcal{D}D. . Frequentist: θ thetaθ is fixed but unknown quantity. | Bayesian: use a fully probabilistic model and treat θ thetaθ as a random quantity. To do so, chose an appropriate prior distribution P(θ) mathbb{P}( theta)P(θ), which reflects the knowledge (i.e. uncertainty) about θ thetaθ prior to the experiment | the goal is to update the knowledge given the information contained in the data D mathcal{D}D. | the updated knowledge (i.e. reduced uncertainty) is encapsulated in the posterior distribution P(θ∣D) mathbb{P}( theta vert mathcal{D})P(θ∣D), which is calculated via Bayes’theorem. | . | . P(θ∣D)=P(D∣θ)P(θ)P(D) mathbb{P}( boldsymbol{ theta} | mathcal{D})= frac{ mathbb{P}( mathcal{D} | boldsymbol{ theta}) mathbb{P}( boldsymbol{ theta})}{ mathbb{P}( mathcal{D})}P(θ∣D)=P(D)P(D∣θ)P(θ)​ . The bayesian paradigm boils down to the slogan: posterior ∝ propto∝ likelihood × times× prior . 3.1 Choice of prior distributions . Conjugate priors the prior and the posterior lie in the same class of distributions. | often chosen, because it leads to a well-known form of the posterior, which simplifies the calculations | . | choose a prior that contains as little information about the parameter as possible at first choice would, of course, be a locally uniform prior. Under a uniform prior we have P(θ∣D)∝L(θ) mathbb{P}( boldsymbol{ theta} vert boldsymbol{D}) propto mathcal{L}( boldsymbol{ theta})P(θ∣D)∝L(θ). | Jeffrey’s prior, but often hard to come by | . | 3.2 Bayesian point estimates and confidence intervals . Bayesian point estimates: the posterior mean, mode and median . θ^=E[θ∣D]=∫θP(θ∣D)dθ hat{ theta}= mathbb{E}[ theta | D]= int theta mathbb{P}( theta | mathcal{D}) mathrm{d} thetaθ^=E[θ∣D]=∫θP(θ∣D)dθ . confidence: highest posterior density (HPD) region . for a threshold value π piπ, the region Cα={θ:P(θ∣D)&gt;π} mathcal{C}_{ alpha}= { theta: mathbb{P}( theta vert mathcal{D})&gt; pi }Cα​={θ:P(θ∣D)&gt;π}, we get . ∫CαP(θ∣D)dθ=1−α int_{C_{ alpha}} mathbb{P}( theta | mathcal{D}) mathrm{d} theta=1- alpha∫Cα​​P(θ∣D)dθ=1−α . This region Cα mathcal{C}_{ alpha}Cα​ is the HPD region. . 3.3 Markov Chain Monte Carlo . A common challenge in Bayesian inference is that the integral . P(D)=∫P(D∣θ)P(θ)dθ mathbb{P}(D)= int mathbb{P}(D | theta) mathbb{P}( theta) d_{ theta}P(D)=∫P(D∣θ)P(θ)dθ​ . can’t be solved analytically. . to be continued… . 3.4 Empirical Bayes for Latent Variable Problems . The first step is to infer point estimates for the parameters at higher levels by integrating out those at lower levels, and the infer posterior distributions for lower level parameters while setting those at a higher level to their point estimate. . 3.5 Approximate Bayesian Computation . Approximate Bayesian computation (ABC) is a class of simulation-based techniques to conduct Bayesian inference under models with intractable likelihoods . 3.6 Model selection . how to compare the several candidate models explaining the data D mathcal{D}D? the most commonly used methods: . likelihood ratio statistic | model posterior probabilities | Bayes factors | others: cross-validation, Akaike’s information criterion (AIC), Bayesian information criterion (BIC) | . 4. Naive Bayes and Bayesian estimation . Naive Bayes and Bayesian estimation are two different concepts! . Naive Bayes is a statistical learning method. For a give training set, learn the join probability distribution of P(X,Y)P(X,Y)P(X,Y). Based on this model, for a given input xxx, output a yyy with maximal posterior probability (Bayes theorem). . Set prior prob distribution: . P(Y=ck),k=1,2,⋯ ,KP left(Y=c_{k} right), quad k=1,2, cdots, KP(Y=ck​),k=1,2,⋯,K . Conditional prob distribution: . P(X=x∣Y=ck)=P(X(1)=x(1),⋯ ,X(n)=x(n)∣Y=ck),k=1,2,⋯ ,KP left(X=x | Y=c_{k} right)=P left(X^{(1)}=x^{(1)}, cdots, X^{(n)}=x^{(n)} | Y=c_{k} right), quad k=1,2, cdots, KP(X=x∣Y=ck​)=P(X(1)=x(1),⋯,X(n)=x(n)∣Y=ck​),k=1,2,⋯,K . Naive Bayes make a strong assumption that conditional prob distribution are all conditional independent, which is: . P(X=x∣Y=ck)=P(X(1)=x(1),⋯ ,X(n)=x(n)∣Y=ck)=∏j=1nP(X(j)=x(j)∣Y=ck) begin{aligned} P left(X=x | Y=c_{k} right) &amp;=P left(X^{(1)}=x^{(1)}, cdots, X^{(n)}=x^{(n)} | Y=c_{k} right) &amp;= prod_{j=1}^{n} P left(X^{(j)}=x^{(j)} | Y=c_{k} right) end{aligned}P(X=x∣Y=ck​)​=P(X(1)=x(1),⋯,X(n)=x(n)∣Y=ck​)=j=1∏n​P(X(j)=x(j)∣Y=ck​)​ . then, posterior prob is: . P(Y=ck∣X=x)=P(X=x∣Y=ck)P(Y=ck)∑kP(X=x∣Y=ck)P(Y=ck)P left(Y=c_{k} | X=x right)= frac{P left(X=x | Y=c_{k} right) P left(Y=c_{k} right)}{ sum_{k} P left(X=x | Y=c_{k} right) P left(Y=c_{k} right)}P(Y=ck​∣X=x)=∑k​P(X=x∣Y=ck​)P(Y=ck​)P(X=x∣Y=ck​)P(Y=ck​)​ . If conditional probability of each input variable is not independent, then model become Baysian Network! . Naive Bayes Classifier is: . y=f(x)=arg⁡max⁡ckP(Y=ck)∏jP(X(j)=x(j)∣Y=ck)∑kP(Y=ck)∏jP(X(j)=x(j)∣Y=ck)y=f(x)= arg max _{c_{k}} frac{P left(Y=c_{k} right) prod_{j} P left(X^{(j)}=x^{(j)} | Y=c_{k} right) }{ sum_{k} P left(Y=c_{k} right) prod_{j} P left(X^{(j)}=x^{(j)} | Y=c_{k} right)}y=f(x)=argck​max​∑k​P(Y=ck​)∏j​P(X(j)=x(j)∣Y=ck​)P(Y=ck​)∏j​P(X(j)=x(j)∣Y=ck​)​ . and it’s short form: . y=f(x)=arg⁡max⁡ckP(Y=ck)∏jP(X(j)=x(j)∣Y=ck)y=f(x)= arg max _{c_{k}} P left(Y=c_{k} right) prod_{j} P left(X^{(j)}=x^{(j)} | Y=c_{k} right)y=f(x)=argck​max​P(Y=ck​)j∏​P(X(j)=x(j)∣Y=ck​) .",
            "url": "https://zqfang.github.io/machine%20learning/2020/01/30/ML-Bayes.html",
            "relUrl": "/machine%20learning/2020/01/30/ML-Bayes.html",
            "date": " • Jan 30, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Loss function for multi-label classification",
            "content": "Multi-label classification, tasks commonly be seen on health record data (multi symptoms). . Loss function design: . Multi binary cross-entropy each class has a binary output . | Label smoothing, another regularization technique . | It’s designed to make the model a little bit less certain of it’s decision by changing a little bit its target: instead of wanting to predict 1 for the correct class and 0 for all the others, we ask it to predict 1-ε for the correct class and ε for all the others, with ε a (small) positive number and N the number of classes. This can be written as: . loss=(1−ε)ce(i)+ε∑ce(j)/N text {loss}=(1- varepsilon) c e(i)+ varepsilon sum c e(j) / Nloss=(1−ε)ce(i)+ε∑ce(j)/N . where ce(x) is cross-entropy of x (i.e. −log(px)), and i is the correct class. . finally, for multi-label loss function: . (1−ϵ)∑i(−log⁡pin)+ϵN∑(−log⁡pi)(1- epsilon) sum_{i} left(- frac{ log p_{i}}{n} right)+ frac{ epsilon}{N} sum left(- log p_{i} right)(1−ϵ)i∑​(−nlogpi​​)+Nϵ​∑(−logpi​) . See the fastai implementation here: LabelSmoothingCrossEntropy . about line 285: . class LabelSmoothingCrossEntropy(Module): y_int = True def __init__(self, eps:float=0.1, reduction=&#39;mean&#39;): self.eps,self.reduction = eps,reduction def forward(self, output, target): c = output.size()[-1] log_preds = F.log_softmax(output, dim=-1) if self.reduction==&#39;sum&#39;: loss = -log_preds.sum() else: loss = -log_preds.sum(dim=-1) if self.reduction==&#39;mean&#39;: loss = loss.mean() return loss*self.eps/c + (1-self.eps) * F.nll_loss(log_preds, target.long(), reduction=self.reduction) def activation(self, out): return F.softmax(out, dim=-1) def decodes(self, out): return out.argmax(dim=-1) .",
            "url": "https://zqfang.github.io/machine%20learning/2020/01/29/ML-Lossfunc.html",
            "relUrl": "/machine%20learning/2020/01/29/ML-Lossfunc.html",
            "date": " • Jan 29, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Mento Carlos",
            "content": "蒙特卡罗方法，又称统计模拟方法(statistical simulation method), 通过概率模型的随机抽样进行进行近似数值计算的方法。 . 1. 蒙特卡罗方法的核心 . 蒙特卡罗方法的核心是随机抽样(random sampling) . 2. Monte Carlo intergration: . 计算h(x)积分 . ∫Xh(x)dx int_{ mathcal{X}} h(x) mathrm{d} x∫X​h(x)dx . 将h(x)分解成f(x)和概率密度函数p(x)的乘积，即函数h(x)的积分可以表示为函数f(x)关于概率密度函数p(x)的数学期望： . ∫Xh(x)dx=∫Xf(x)p(x)dx=Ep(x)[f(x)] int_{ mathcal{X}} h(x) mathrm{d} x= int_{ mathcal{X}} f(x) p(x) mathrm{d} x=E_{p(x)}[f(x)]∫X​h(x)dx=∫X​f(x)p(x)dx=Ep(x)​[f(x)] . 因此，可利用样本均值计算近似积分： . ∫Xh(x)dx=Ep(x)[f(x)]≈1n∑i=1nf(xi) int_{ mathcal{X}} h(x) mathrm{d} x=E_{p(x)}[f(x)] approx frac{1}{n} sum_{i=1}^{n} f left(x_{i} right)∫X​h(x)dx=Ep(x)​[f(x)]≈n1​i=1∑n​f(xi​) . 变形 . Ep(z)[f(z)]=∫f(z)p(z)dz=∫f(z)p(z)q(z)⏟newf~(z)q(z)dz≈1N∑n=1Nf(zi)p(zi)q(zi) begin{aligned} mathrm{E}_{p(z)}[f(z)] &amp;= int f(z) p(z) dz &amp;= int underbrace{f(z) frac{p(z)}{q(z)}}_{new tilde{f}(z)} q(z) dz &amp; approx frac{1}{N} sum_{n=1}^{N} f(z^{i}) frac{p(z^{i})}{q(z^{i})} end{aligned}Ep(z)​[f(z)]​=∫f(z)p(z)dz=∫newf~​(z) . f(z)q(z)p(z)​​​q(z)dz≈N1​n=1∑N​f(zi)q(zi)p(zi)​​ . 3. Markov Chain Monte Carlo . Markov chain or markov process: . P(Xt∣X0,X1,⋯ ,Xt−1)=P(Xt∣Xt−1),t=1,2,⋯P left(X_{t} | X_{0}, X_{1}, cdots, X_{t-1} right)=P left(X_{t} | X_{t-1} right), quad t=1,2, cdotsP(Xt​∣X0​,X1​,⋯,Xt−1​)=P(Xt​∣Xt−1​),t=1,2,⋯ .",
            "url": "https://zqfang.github.io/machine%20learning/2020/01/26/ML-MonteCarlo.html",
            "relUrl": "/machine%20learning/2020/01/26/ML-MonteCarlo.html",
            "date": " • Jan 26, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Expectation Maximization",
            "content": "Maximum Likelihood Estimation Gaussian Mixture Model Expectation Maximization . 1. Probability and likelihood . likehood &amp; maximum likehood . 在非正式场合似然（likelihood）和概率（Probability）几乎是一对同义词，但是在统计学中似然和概率却是两个不同的概念。 . 概率: 在特定环境下某件事情发生的可能性，也就是结果没有产生之前依据环境所对应的参数来预测某件事情发生的可能性。 . 比如抛硬币，抛之前我们不知道最后是哪一面朝上，但是根据硬币的性质我们可以推测任何一面朝上的可能性均为50%，这个概率只有在抛硬币之前才是有意义的，抛完硬币后的结果便是确定的； . 似然: 刚好相反，是在确定的结果下去推测产生这个结果的可能环境（参数）。 . 假设随机抛掷一枚硬币1,000次，结果500次人头朝上，500次数字朝上，那么两面朝上的概率均为50%。运用出现的结果来判断这个事情本身的性质（参数），也就是似然。 . 当结果和参数相互对应，似然和概率在数值上相等。 用 θ 表示环境对应的参数，x 表示结果，那么概率可以表示为： . P(x∣θ)P(x | theta )P(x∣θ) . p(x vertθ) 是条件概率的表示方法。θ 是前置条件，理解为在 θ 的前提下，事件 x 发生的概率，相对应的似然可以表示为: . L(θ∣x) mathcal{L}( theta | x)L(θ∣x) . 可以理解为已知结果为 x ，参数为 θ (似然函数里 θ 是变量，这里说的参数和变量是相对与概率而言的)对应的概率，即： . L(θ∣x)=P(x∣θ) mathcal{L}( theta | x)=P(x | theta)L(θ∣x)=P(x∣θ) . 两者在数值上相等，但是意义并不相同，L mathcal{L}L 是关于 θ 的函数，而 P 则是关于 x 的函数。 . 2. Maximum Likelihood Estimation . 单高斯模型 x∼N(μ,Σ)x sim mathcal{N}( mu, Sigma)x∼N(μ,Σ), xi∈Dx_{i} in mathcal{D}xi​∈D, 那么对参数 μ muμ和 Σ SigmaΣ 进行估计，只需要最大化log-likelihood函数： . log⁡p(X)=∑i=1Nlog⁡N(xi∣μ,Σ)=∑i=1Nlog⁡12πσe−(xi−μ)22σ2=∑i=1Nlog⁡12πσ+∑i=1N−(xi−μ)22σ2=−N2log⁡2π−N2log⁡σ2−12σ2∑i=1N(xi−μ)2 begin{aligned} log p(X) &amp;= sum_{i=1}^{N} log mathcal{N} left(x_{i} | mu, Sigma right) &amp;= sum_{i=1}^{N} log frac{1}{ sqrt{2 pi} sigma} e^{- frac{ left(x_{i}- mu right)^{2}}{2 sigma^{2}}} &amp;= sum_{i=1}^{N} log frac{1}{ sqrt{2 pi} sigma}+ sum_{i=1}^{N}- frac{ left(x_{i}- mu right)^{2}}{2 sigma^{2}} &amp;=- frac{N}{2} log 2 pi- frac{N}{2} log sigma^{2}- frac{1}{2 sigma^{2}} sum_{i=1}^{N} left(x_{i}- mu right)^{2} end{aligned}logp(X)​=i=1∑N​logN(xi​∣μ,Σ)=i=1∑N​log2π . ​σ1​e−2σ2(xi​−μ)2​=i=1∑N​log2π . ​σ1​+i=1∑N​−2σ2(xi​−μ)2​=−2N​log2π−2N​logσ2−2σ21​i=1∑N​(xi​−μ)2​ . 求偏导数，得到参数估计： . ∂log⁡p(X)∂μ=1σ2∑i=1N(xi−μ)=0⇒μ=1N∑i=1Nxi∂log⁡p(X)∂σ2=−N2σ2+12σ4∑i=1N(xi−μ)2=0⇒σ2=1N∑i=1N(xi−μ)2 begin{aligned} frac{ partial log p(X)}{ partial mu} &amp;= frac{1}{ sigma^{2}} sum_{i=1}^{N} left(x_{i}- mu right)=0 &amp; Rightarrow mu= frac{1}{N} sum_{i=1}^{N} x_{i} frac{ partial log p(X)}{ partial sigma^{2}} &amp;=- frac{N}{2 sigma^{2}}+ frac{1}{2 sigma^{4}} sum_{i=1}^{N} left(x_{i}- mu right)^{2}=0 &amp; Rightarrow sigma^{2}= frac{1}{N} sum_{i=1}^{N} left(x_{i}- mu right)^{2} end{aligned}∂μ∂logp(X)​∂σ2∂logp(X)​​=σ21​i=1∑N​(xi​−μ)=0⇒μ=N1​i=1∑N​xi​=−2σ2N​+2σ41​i=1∑N​(xi​−μ)2=0⇒σ2=N1​i=1∑N​(xi​−μ)2​ . 3. Gaussian Mixture Model . 如果有K个高斯线性叠加: . p(x)=∑k=1KπkN(x∣μk,Σk) s.t. ∑k=1Kπk=10≤πk≤1 begin{aligned} p(x)=&amp; sum_{k=1}^{K} pi_{k} mathcal{N} left(x | mu_{k}, Sigma_{k} right) &amp; text { s.t. } sum_{k=1}^{K} pi_{k}=1 &amp; 0 leq pi_{k} leq 1 end{aligned}p(x)=​k=1∑K​πk​N(x∣μk​,Σk​) s.t. k=1∑K​πk​=10≤πk​≤1​ . 那么对数似然函数为 . log⁡p(X)=∑i=1Nlog⁡{∑k=1KπkN(xi∣μk,Σk)} log p(X)= sum_{i=1}^{N} log left { sum_{k=1}^{K} pi_{k} mathcal{N} left(x_{i} | mu_{k}, Sigma_{k} right) right }logp(X)=i=1∑N​log{k=1∑K​πk​N(xi​∣μk​,Σk​)} . 因为对数里有求和，因此无法无法直接通过最大似然估计方法进行参数估计。 . 其中，如果πk pi_{k}πk​是每个高斯出现的概率p(k)p(k)p(k)，则高斯混合模型分解为以p(k)p(k)p(k)获得一个高斯分布，然后在分布中获得xxx，因此xxx边缘概率分布为： . p(x)=∑k=1Kp(k)p(x∣k)p(x)= sum_{k=1}^{K} p(k) p(x | k)p(x)=k=1∑K​p(k)p(x∣k) . 后验概率p(k∣x)p(k vert x)p(k∣x)表示xxx属于每个高斯的概率（离散值）: . p(k∣x)=p(x∣k)p(k)∑lp(x∣l)p(l)=πkN(x∣μk,Σk)∑lπlN(x∣μl,Σl) begin{aligned} p(k | x) &amp;= frac{p(x | k) p(k)}{ sum_{l} p(x | l) p(l)} &amp;= frac{ pi_{k} mathcal{N} left(x | mu_{k}, Sigma_{k} right)}{ sum_{l} pi_{l} mathcal{N} left(x | mu_{l}, Sigma_{l} right)} end{aligned}p(k∣x)​=∑l​p(x∣l)p(l)p(x∣k)p(k)​=∑l​πl​N(x∣μl​,Σl​)πk​N(x∣μk​,Σk​)​​ . 4. Expectation Maximization . 思想： 通过引入隐变量，运用迭代方法，求解混合高斯模型 . θ(t+1)=arg⁡max⁡θL(θ;X) theta^{(t+1)}= underset{ theta}{ arg max } mathcal{L}( theta ; X)θ(t+1)=θargmax​L(θ;X) . 引入隐变量Zi(状态i）， z服从多项分布，选择zi的概率为p(zi),则高斯混合模型为： . zi∼Multinoimal⁡(π1,⋯ ,πk)xi∣zi∼N(μzi,Σzi) begin{aligned} z_{i} &amp; sim operatorname{Multinoimal} left( pi_{1}, cdots, pi_{k} right) x_{i} | z_{i} &amp; sim mathcal{N} left( mu_{z_{i}}, Sigma_{z_{i}} right) end{aligned}zi​xi​∣zi​​∼Multinoimal(π1​,⋯,πk​)∼N(μzi​​,Σzi​​)​ . 步骤： . E-Step: 在现有θ(t) theta^{(t)}θ(t)下最大化似然下界, 计算隐变量z的期望Q(zi)=p(zi∣xi,θ)Q left(z_{i} right)=p left(z_{i} vert x_{i}, theta right)Q(zi​)=p(zi​∣xi​,θ) 作为其下界 | M-Step: 在上面Q(zi)Q(z_{i})Q(zi​)下计算参数列表θ thetaθ来最大化似然 | . (0) 理解EM的前提 . 凹凸函数: ∀x∈R,f′′(x)≥0 forall_{x in mathbb{R}}, f^{ prime prime}(x) geq 0∀x∈R​,f′′(x)≥0,则fff为凸函数。 当x为向量，如果其hessian矩阵H是半正定的(H≥0H geq 0H≥0),则fff为凸函数 如果f′′(x)&gt;0f^{ prime prime}(x)&gt;0f′′(x)&gt;0或者H&gt;0H&gt;0H&gt;0, fff是严格凸函数。 如果f′′(x)&lt;0f^{ prime prime}(x)&lt;0f′′(x)&lt;0或者H&gt;0H&gt;0H&gt;0, fff是凹函数。 . Jensen 不等式: . 如果fff为凸函数, 则E[f(X)]≥f(E[X])E[f(X)] geq f(E[X])E[f(X)]≥f(E[X])。当且仅当x是常数时，E[f(x)]=f(E[x])E[f(x)]=f(E[x])E[f(x)]=f(E[x])。 | 如果fff是凹函数, 则E[f(X)]≤f(E[X])E[f(X)] leq f(E[X])E[f(X)]≤f(E[X])。 | 引入隐变量后，变换对数似然函数: . L(θ;X)=∑i=1Nlog⁡p(xi∣θ)=∑i=1Nlog⁡∑zip(xi,zi∣θ)=∑i=1Nlog⁡∑ziQ(zi)p(xi,zi∣θ)Q(zi)≥∑i=1N∑ziQ(zi)log⁡p(xi,zi∣θ)Q(zi) begin{aligned} mathcal{L}( theta ; X) &amp;= sum_{i=1}^{N} log p left(x_{i} | theta right) &amp;= sum_{i=1}^{N} log sum_{z_{i}} p left(x_{i}, z_{i} | theta right) &amp;= sum_{i=1}^{N} log sum_{z_{i}} Q left(z_{i} right) frac{p left(x_{i}, z_{i} | theta right)}{Q left(z_{i} right)} &amp; geq sum_{i=1}^{N} sum_{z_{i}} Q left(z_{i} right) log frac{p left(x_{i}, z_{i} | theta right)}{Q left(z_{i} right)} end{aligned}L(θ;X)​=i=1∑N​logp(xi​∣θ)=i=1∑N​logzi​∑​p(xi​,zi​∣θ)=i=1∑N​logzi​∑​Q(zi​)Q(zi​)p(xi​,zi​∣θ)​≥i=1∑N​zi​∑​Q(zi​)logQ(zi​)p(xi​,zi​∣θ)​​ . 推导: . 把式中的log函数体看成是一个整体，由于log(x)的二阶导数为−1x2- frac{1}{x^2}−x21​, 小于0，为凹函数。所以使用Jensen不等式时，应用第二条准则：f(E[X])≥E[f(x)]f(E[X]) geq E[f(x)]f(E[X])≥E[f(x)]。 | f(Ezi∼Q[p(xi,zi∣θ)Q(zi)])≥Ezi∼Q[f(p(xi,zi∣θ)Q(zi))]f left(E_{z_{i} sim Q} left[ frac{p left(x_{i}, z_{i} | theta right)}{Q left(z_{i} right)} right] right) geq E_{z_{i} sim Q} left[f left( frac{p left(x_{i}, z_{i} | theta right)}{Q left(z_{i} right)} right) right]f(Ezi​∼Q​[Q(zi​)p(xi​,zi​∣θ)​])≥Ezi​∼Q​[f(Q(zi​)p(xi​,zi​∣θ)​)] . 这里，Q(zi)Q left(z_{i} right)Q(zi​)是ziz_{i}zi​的函数， 且∑ziQ(zi)=1 sum_{z_{i}} Q left(z_{i} right)=1∑zi​​Q(zi​)=1。 | 由数学期望Ex∼p[g(X)]=∑xg(x)p(x)E_{x sim p}[g(X)]= sum_{x} g(x) p(x)Ex∼p​[g(X)]=∑x​g(x)p(x)，上式可以理解为: p(x)p(x)p(x)对应Q(zi)Q left(z_{i} right)Q(zi​), g(x)对应log⁡p(xi,zi∣θ)Q(zi) log frac{p left(x_{i}, z_{i} vert theta right)}{Q left(z_{i} right)}logQ(zi​)p(xi​,zi​∣θ)​表示ziz_{i}zi​的函数。 . | 似然函数: L(θ)≥J(z,Q) mathcal{L}( theta) geq mathcal{J}(z,Q)L(θ)≥J(z,Q)（zzz为隐含变量），那么我们可以通过不断的最大化J mathcal{J}J的下界，来使得L(θ) mathcal{L}( theta)L(θ)不断提高，最终达到它的最大值。 | 最大化L(θ) mathcal{L}( theta)L(θ)函数的下界，即让g(x)g(x)g(x)为常数c: . p(xi,zi∥θ)Q(zi)=c frac{p left(x_{i}, z_{i} | theta right)}{Q left(z_{i} right)}=cQ(zi​)p(xi​,zi​∥θ)​=c . Jensen不等式中说到，当自变量X=E(X)时，即为常数的时候，等式成立! . 变换公式, 对所有zzz求和得: . p(xi,zi∣θ)=c⋅Q(zi)∑zip(xi,zi∣θ)=c⋅∑ziQ(zi)c=∑zip(xi,zi∣θ) begin{aligned} p left(x_{i}, z_{i} | theta right) &amp;=c cdot Q left(z_{i} right) sum_{z_{i}} p left(x_{i}, z_{i} | theta right) &amp;=c cdot sum_{z_{i}} Q left(z_{i} right) c &amp;= sum_{z_{i}} p left(x_{i}, z_{i} | theta right) end{aligned}p(xi​,zi​∣θ)zi​∑​p(xi​,zi​∣θ)c​=c⋅Q(zi​)=c⋅zi​∑​Q(zi​)=zi​∑​p(xi​,zi​∣θ)​ . 其中，∑ziQ(zi)=1 sum_{z_{i}} Q left(z_{i} right) = 1∑zi​​Q(zi​)=1, 也得： . Q(zi)=p(xi,zi∣θ)∑zip(xi,zi∣θ)=p(zi∣xi,θ) begin{aligned} Q left(z_{i} right) &amp;= frac{p left(x_{i}, z_{i} | theta right)}{ sum_{z_{i}} p left(x_{i}, z_{i} | theta right)} &amp;=p left(z_{i} | x_{i}, theta right) end{aligned}Q(zi​)​=∑zi​​p(xi​,zi​∣θ)p(xi​,zi​∣θ)​=p(zi​∣xi​,θ)​ . 至此，我们推出了在固定参数θ后，使下界拉升的Q(z)Q(z)Q(z)的计算公式就是后验概率（条件概率），一并解决了Q(z)Q(z)Q(z)如何选择的问题。此步就是EM算法的E-step。 . 执行E-Step后与下界重合，此时似然变为： . L(θ(t);X)=∑i=1N∑ziQ(t)(zi)log⁡p(xi,zi∣θ(t))Q(t)(zi) mathcal{L} left( theta^{(t)} ; X right)= sum_{i=1}^{N} sum_{z_{i}} Q^{(t)} left(z_{i} right) log frac{p left(x_{i}, z_{i} | theta^{(t)} right)}{Q^{(t)} left(z_{i} right)}L(θ(t);X)=i=1∑N​zi​∑​Q(t)(zi​)logQ(t)(zi​)p(xi​,zi​∣θ(t))​ . 这时，对公式求导 . θ(t+1)=arg⁡max⁡θL(θ;X) theta^{(t+1)}= underset{ theta}{ arg max } mathcal{L}( theta ; X)θ(t+1)=θargmax​L(θ;X) . 得到t+1步的似然函数L(θ(t+1);X) mathcal{L} left( theta^{(t+1)} ; X right)L(θ(t+1);X)。 通过不断的迭代，可以得到使似然函数L(θ) mathcal{L}( theta)L(θ)最大化的参数θ，直至函数收敛。 只需要证明L(θ(t+1);X)≥L(θ(t);X) mathcal{L} left( theta^{(t+1)} ; X right) geq mathcal{L} left( theta^{(t)} ; X right)L(θ(t+1);X)≥L(θ(t);X), 则可证明EM的收敛性: . L(θ(t+1);X)=∑i=1Nlog⁡∑ziQ(t)(zi)p(xi,zi∣θ(t+1))Q(t)(zi)≥∑i=1N∑ziQ(t)(zi)log⁡p(xi,zi∣θ(t+1))Q(t)(zi)≥∑i=1N∑ziQ(t)(zi)log⁡p(xi,zi∣θ(t))Q(t)(zi)=L(θ(t);X) begin{aligned} mathcal{L} left( theta^{(t+1)} ; X right) &amp;= sum_{i=1}^{N} log sum_{z_{i}} Q^{(t)} left(z_{i} right) frac{p left(x_{i}, z_{i} | theta^{(t+1)} right)}{Q^{(t)} left(z_{i} right)} &amp; geq sum_{i=1}^{N} sum_{z_{i}} Q^{(t)} left(z_{i} right) log frac{p left(x_{i}, z_{i} | theta^{(t+1)} right)}{Q^{(t)} left(z_{i} right)} &amp; geq sum_{i=1}^{N} sum_{z_{i}} Q^{(t)} left(z_{i} right) log frac{p left(x_{i}, z_{i} | theta^{(t)} right)}{Q^{(t)} left(z_{i} right)} &amp;= mathcal{L} left( theta^{(t)} ; X right) end{aligned}L(θ(t+1);X)​=i=1∑N​logzi​∑​Q(t)(zi​)Q(t)(zi​)p(xi​,zi​∣θ(t+1))​≥i=1∑N​zi​∑​Q(t)(zi​)logQ(t)(zi​)p(xi​,zi​∣θ(t+1))​≥i=1∑N​zi​∑​Q(t)(zi​)logQ(t)(zi​)p(xi​,zi​∣θ(t))​=L(θ(t);X)​ . 5. 求解GMM . (1) GMM E-Step: . 已知θ(t) theta^{(t)}θ(t), 求Q(t+1)(zi)Q^{(t+1)} left(z_{i} right)Q(t+1)(zi​): . Q(t+1)(zi)=p(xi,zi∣θ(t))p(xi∣θ(t))=p(xi,zi∣θ(t))∑l∈zip(xi,l∣θ(t))=p(xi∣zi,θ(t))p(zi∣θ(t))∑l∈zip(xi∣l,θ(t))p(l∣θ(t))=N(μzi,Σzi)πzi∑l∈ziN(μl,Σl)πl begin{aligned} Q^{(t+1)} left(z_{i} right) &amp;= frac{p left(x_{i}, z_{i} | theta^{(t)} right)}{p left(x_{i} | theta^{(t)} right)} &amp;= frac{p left(x_{i}, z_{i} | theta^{(t)} right)}{ sum_{l in z_{i}} p left(x_{i}, l | theta^{(t)} right)} &amp;= frac{p left(x_{i} | z_{i}, theta^{(t)} right) p left(z_{i} | theta^{(t)} right)}{ sum_{l in z_{i}} p left(x_{i} | l, theta^{(t)} right) p left(l | theta^{(t)} right)} &amp;= frac{ mathcal{N} left( mu_{z_{i}}, Sigma_{z_{i}} right) pi_{z_{i}}}{ sum_{l in z_{i}} mathcal{N} left( mu_{l}, Sigma_{l} right) pi_{l}} end{aligned}Q(t+1)(zi​)​=p(xi​∣θ(t))p(xi​,zi​∣θ(t))​=∑l∈zi​​p(xi​,l∣θ(t))p(xi​,zi​∣θ(t))​=∑l∈zi​​p(xi​∣l,θ(t))p(l∣θ(t))p(xi​∣zi​,θ(t))p(zi​∣θ(t))​=∑l∈zi​​N(μl​,Σl​)πl​N(μzi​​,Σzi​​)πzi​​​​ . (2) GMM M-Step: . 已知Q(t+1)(zi)Q^{(t+1)} left(z_{i} right)Q(t+1)(zi​), 求 θ(t+1) theta^{(t+1)}θ(t+1): . L(θ;X)=∑iN∑lKQi(l)log⁡p(xi,l∣θ)Qi(l)=∑iN∑lKQi(l)log⁡p(xi,l∣θ)−∑iN∑lKQi(l)log⁡Qi(l)=∑iN∑lKQi(l)log⁡p(xi,l∣θ)−Constant =∑iN∑lKQi(l)log⁡πlN(μl,Σl)−Constant =∑iN∑lKQi(l)log⁡πl+∑iN∑lKQi(l)log⁡N(μl,Σl)−Constant begin{aligned} mathcal{L}( theta ; X) &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log frac{p left(x_{i}, l | theta right)}{Q_{i}(l)} &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log p left(x_{i}, l | theta right)- sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log Q_{i}(l) &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log p left(x_{i}, l | theta right)- text {Constant } &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log pi_{l} mathcal{N} left( mu_{l}, Sigma_{l} right)- text {Constant } &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log pi_{l}+ sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log mathcal{N} left( mu_{l}, Sigma_{l} right)- text {Constant} end{aligned}L(θ;X)​=i∑N​l∑K​Qi​(l)logQi​(l)p(xi​,l∣θ)​=i∑N​l∑K​Qi​(l)logp(xi​,l∣θ)−i∑N​l∑K​Qi​(l)logQi​(l)=i∑N​l∑K​Qi​(l)logp(xi​,l∣θ)−Constant =i∑N​l∑K​Qi​(l)logπl​N(μl​,Σl​)−Constant =i∑N​l∑K​Qi​(l)logπl​+i∑N​l∑K​Qi​(l)logN(μl​,Σl​)−Constant​ . (3) 求 π piπ: . ∀l∈{1,⋯ ,K},∂L(θ;X)∂πl=0 s.t. ∑lKπl=1 begin{aligned} forall_{l in {1, cdots, K }}, &amp; frac{ partial mathcal{L}( theta ; X)}{ partial pi_{l}}=0 &amp; text { s.t. } sum_{l}^{K} pi_{l}=1 end{aligned}∀l∈{1,⋯,K}​,​∂πl​∂L(θ;X)​=0 s.t. l∑K​πl​=1​ . 拉格朗日乘法约束 . {Lπl=∂L(θ;X)∂πl+λ(∑lKπl−1)=0Lλ=∑lKπl−1=0 left { begin{aligned} L_{ pi_{l}} &amp;= frac{ partial mathcal{L}( theta ; X)}{ partial pi_{l}}+ lambda left( sum_{l}^{K} pi_{l}-1 right)=0 L_{ lambda} &amp;= sum_{l}^{K} pi_{l}-1=0 end{aligned} right.⎩⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎧​Lπl​​Lλ​​=∂πl​∂L(θ;X)​+λ(l∑K​πl​−1)=0=l∑K​πl​−1=0​ . 求导： . {1π1∑iNQi(1)−λ=0⋮1πl∑iNQi(l)−λ=0 left { begin{array}{c} frac{1}{ pi_{1}} sum_{i}^{N} Q_{i}(1)- lambda=0 vdots frac{1}{ pi_{l}} sum_{i}^{N} Q_{i}(l)- lambda=0 end{array} right.⎩⎪⎪⎪⎨⎪⎪⎪⎧​π1​1​∑iN​Qi​(1)−λ=0⋮πl​1​∑iN​Qi​(l)−λ=0​ . 相加得： . ∑lK∑iNQi(l)=λ∑lKπl=λ sum_{l}^{K} sum_{i}^{N} Q_{i}(l)= lambda sum_{l}^{K} pi_{l}= lambdal∑K​i∑N​Qi​(l)=λl∑K​πl​=λ . 由 Qi(l)=p(l∣xi,θ)Q_{i}(l)=p left(l vert x_{i}, theta right)Qi​(l)=p(l∣xi​,θ), 得 . ∑lK∑iNQi(l)=∑iN∑lKQi(l)=∑iN∑lKp(l∣xi,θ)=∑iN1=N begin{aligned} sum_{l}^{K} sum_{i}^{N} Q_{i}(l) &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) &amp;= sum_{i}^{N} sum_{l}^{K} p left(l | x_{i}, theta right) &amp;= sum_{i}^{N} 1 &amp;=N end{aligned}l∑K​i∑N​Qi​(l)​=i∑N​l∑K​Qi​(l)=i∑N​l∑K​p(l∣xi​,θ)=i∑N​1=N​ . 则 . πl=1λ∑iNQi(l)=1N∑iNQi(l)=1N∑iNp(l∣xi,θ) begin{aligned} pi_{l} &amp;= frac{1}{ lambda} sum_{i}^{N} Q_{i}(l) &amp;= frac{1}{N} sum_{i}^{N} Q_{i}(l) &amp;= frac{1}{N} sum_{i}^{N} p left(l | x_{i}, theta right) end{aligned}πl​​=λ1​i∑N​Qi​(l)=N1​i∑N​Qi​(l)=N1​i∑N​p(l∣xi​,θ)​ . (4) 计算μ muμ . ∑iN∑lKQi(l)log⁡N(μl,Σl)=∑iN∑lKQi(l)log⁡12πσle−(xi−μl)22σl2=∑iN∑lKQi(l){−12log⁡2π−12log⁡σl2−(xi−μl)22σl2} begin{aligned} &amp; sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log mathcal{N} left( mu_{l}, Sigma_{l} right) &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log frac{1}{ sqrt{2 pi} sigma_{l}} e^{- frac{ left(x_{i}- mu_{l} right)^{2}}{2 sigma_{l}^{2}}} &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) left {- frac{1}{2} log 2 pi- frac{1}{2} log sigma_{l}^{2}- frac{ left(x_{i}- mu_{l} right)^{2}}{2 sigma_{l}^{2}} right } end{aligned}​i∑N​l∑K​Qi​(l)logN(μl​,Σl​)=i∑N​l∑K​Qi​(l)log2π . ​σl​1​e−2σl2​(xi​−μl​)2​=i∑N​l∑K​Qi​(l){−21​log2π−21​logσl2​−2σl2​(xi​−μl​)2​}​ . 求偏导： . ∂L(θ;X)∂μl=∑iNQi(l)xi−μlσ2=0 begin{aligned} frac{ partial mathcal{L}( theta ; X)}{ partial mu_{l}} &amp;= sum_{i}^{N} Q_{i}(l) frac{x_{i}- mu_{l}}{ sigma^{2}} &amp;=0 end{aligned}∂μl​∂L(θ;X)​​=i∑N​Qi​(l)σ2xi​−μl​​=0​ . 得μ muμ： . μl=∑iNQi(l)xi∑iNQi(l) mu_{l}= frac{ sum_{i}^{N} Q_{i}(l) x_{i}}{ sum_{i}^{N} Q_{i}(l)}μl​=∑iN​Qi​(l)∑iN​Qi​(l)xi​​ . ∂L(θ;X)∂σl2=∑iNQi(l){−12σl2+(xi−μl)22σl4}=0 begin{aligned} &amp; frac{ partial mathcal{L}( theta ; X)}{ partial sigma_{l}^{2}}= sum_{i}^{N} Q_{i}(l) left {- frac{1}{2 sigma_{l}^{2}}+ frac{ left(x_{i}- mu_{l} right)^{2}}{2 sigma_{l}^{4}} right } &amp;=0 end{aligned}​∂σl2​∂L(θ;X)​=i∑N​Qi​(l){−2σl2​1​+2σl4​(xi​−μl​)2​}=0​ . (5) 计算σ sigmaσ . ∂L(θ;X)∂σl2=∑iNQi(l){−12σl2+(xi−μl)22σl4}=0 begin{aligned} frac{ partial mathcal{L}( theta ; X)}{ partial sigma_{l}^{2}} &amp;= sum_{i}^{N} Q_{i}(l) left {- frac{1}{2 sigma_{l}^{2}}+ frac{ left(x_{i}- mu_{l} right)^{2}}{2 sigma_{l}^{4}} right } &amp;=0 end{aligned}∂σl2​∂L(θ;X)​​=i∑N​Qi​(l){−2σl2​1​+2σl4​(xi​−μl​)2​}=0​ . 得到 . σl=∑iNQi(l)(xi−μl)2∑iNQi(l) sigma_{l}= frac{ sum_{i}^{N} Q_{i}(l) left(x_{i}- mu_{l} right)^{2}}{ sum_{i}^{N} Q_{i}(l)}σl​=∑iN​Qi​(l)∑iN​Qi​(l)(xi​−μl​)2​ . 6 从KL散度角度解释EM . KL(q∥p)=∑zq(z)log⁡q(z)p(z∣x,θ)=∑zq(z)log⁡q(z)p(x∣θ)p(z,x∣θ)=−∑zq(z)log⁡p(z,x∣θ)q(z)+∑zq(z)log⁡p(x∣θ)=−∑zq(z)log⁡p(z,x∣θ)q(z)+log⁡p(x∣θ)∑zq(z)=−∑zq(z)log⁡p(z,x∣θ)q(z)+log⁡p(x∣θ)log⁡p(x∣θ)=KL(q∥p)+∑zq(z)log⁡p(z,x∣θ)q(z)=KL(q∥p)+L(q,θ) begin{aligned} K L(q | p) &amp;= sum_{z} q(z) log frac{q(z)}{p(z | x, theta)} &amp;= sum_{z} q(z) log frac{q(z) p(x | theta)}{p(z, x | theta)} &amp;=- sum_{z} q(z) log frac{p(z, x | theta)}{q(z)}+ sum_{z} q(z) log p(x | theta) &amp;=- sum_{z} q(z) log frac{p(z, x | theta)}{q(z)}+ log p(x | theta) sum_{z} q(z) &amp;=- sum_{z} q(z) log frac{p(z, x | theta)}{q(z)}+ log p(x | theta) log p(x | theta) &amp;=K L(q | p)+ sum_{z} q(z) log frac{p(z, x | theta)}{q(z)} &amp;=K L(q | p)+ mathcal{L}(q, theta) end{aligned}KL(q∥p)logp(x∣θ)​=z∑​q(z)logp(z∣x,θ)q(z)​=z∑​q(z)logp(z,x∣θ)q(z)p(x∣θ)​=−z∑​q(z)logq(z)p(z,x∣θ)​+z∑​q(z)logp(x∣θ)=−z∑​q(z)logq(z)p(z,x∣θ)​+logp(x∣θ)z∑​q(z)=−z∑​q(z)logq(z)p(z,x∣θ)​+logp(x∣θ)=KL(q∥p)+z∑​q(z)logq(z)p(z,x∣θ)​=KL(q∥p)+L(q,θ)​ . 参考： . 徐亦达-机器学习-EM .",
            "url": "https://zqfang.github.io/machine%20learning/2020/01/26/ML-EM.html",
            "relUrl": "/machine%20learning/2020/01/26/ML-EM.html",
            "date": " • Jan 26, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "SuperFast RNA-seq",
            "content": "salmon-tximport-deseq2 . Step 0: install salmon and download transcriptome cdna from gencode . conda install salmon wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_32/gencode.v32.transcripts.fa.gz . Step 1. build salmon index . salmon index -p 8 --gencode -t gencode.v32.transcripts.fa.gz -i salmonIndex_hg38 . Step 2: quantification . salmon quant -i salmonIndex_hg38 -l A -1 ${fn}/${samp}_1.fastq.gz -2 ${fn}/${samp}_2.fastq.gz -p 8 --validateMappings -o quants/${samp}_quant . Step 3: merge quantification outputs . use tximport in R . # R code library(tximport) library(readr) suppressMessages(library(&#39;EnsDb.Hsapiens.v86&#39;)) txdb &lt;- EnsDb.Hsapiens.v86 k &lt;- keys(txdb, keytype = &quot;GENEID&quot;) df &lt;- select(txdb, keys = k, keytype = &quot;GENEID&quot;, columns = c(&quot;TXID&quot;,&quot;GENEID&quot;)) tx2gene &lt;- df[, 2:1] # tx ID, then gene ID #tx2gene &lt;- read.table(tx2gene, header= T, sep=&quot; t&quot;, stringsAsFactors = F) samples &lt;- unlist(strsplit(sample_ids,&quot;,&quot;)) salmon.files &lt;- file.path(&#39;salmon&#39;,samples, &quot;quant.sf&quot;) names(salmon.files) &lt;- samples all(file.exists(salmon.files)) # get transcript level results txi.transcripts &lt;- tximport(salmon.files, type = &quot;salmon&quot;, txOut = TRUE, tx2gene = tx2gene,) # ignoreTxVersion = TRUE) # get gene level results txi.salmon &lt;- summarizeToGene(txi.transcripts, tx2gene) #save raw counts salmon.counts&lt;- txi.salmon$counts salmon.counts&lt;- as.data.frame(salmon.counts) write.table(salmon.counts, out_counts, sep=&quot; t&quot;, quote=F) #save gene tpms salmon.TPM&lt;- txi.salmon$abundance salmon.TPM&lt;- as.data.frame(salmon.TPM) write.table(salmon.TPM, out_tpm, sep=&quot; t&quot;, quote=F) #save transcripts tpms salmon.trans.TPM&lt;- txi.transcripts$abundance salmon.trans.TPM&lt;- as.data.frame(salmon.trans.TPM) write.table(salmon.trans.TPM, outTrans_tpm, sep=&quot; t&quot;, quote=F) save(txi.salmon, file=&quot;txi.salmon.RData&quot;) . Step 4: Differentially expressed gene analysis . DESeq2 pipeline demo . load(&quot;txi.salmon.RData&quot;) dds &lt;- DESeqDataSetFromTximport(txi.salmon, sampleTable, ~condition) dds$condition &lt;- relevel(dds$condition, ref=ctrl) dds &lt;- DESeq(dds, parallel=TRUE) res &lt;- results(dds, contrast=c(&quot;condition&quot;, treat, ctrl)) resOrdered &lt;- res[order(res$padj),] resOrdered = as.data.frame(resOrdered) write.table(resOrdered, file=&quot;degs.txt&quot;, quote=F, sep=&quot; t&quot;) .",
            "url": "https://zqfang.github.io/make%20bioinfo%20uncool%20again/2020/01/20/RNAseqTurbo.html",
            "relUrl": "/make%20bioinfo%20uncool%20again/2020/01/20/RNAseqTurbo.html",
            "date": " • Jan 20, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Shortcut to bulk RNA-seq analysis",
            "content": "hisat2-htseq-deseq2 . 3.1 transcriptom mapping . step 0: install tools . conda install htseq hisat2 stringtie . step 1: build index and extract splice sites . build index . hisat2-build -p {threads} genome/hg38.fa hisat2_index/hg38 . extract known splice sites for alignmnet . hisat2_extract_splice_sites.py gencode.gtf &gt; hisat2_index/splicesites.txt hisat2_extract_exons.py gencode.gtf &gt; histat2_index/exon.txt . step2: mapping . hisat2 --dta --threads ${threads} -x hisat2_index/hg38 --known-splicesite-infile hisat2_index/splicesites.txt -1 R1.fq.gz -2 R2.fq.gz -S output.sam . step 3: sam to bam . samtools view -Sbh -q 25 -@ ${threads} -o ouput.bam input.sam . step 4: bam sort and index . samtools sort -@ ${threads} input.bam &gt; output.sorted.bam samtools index input.sorted.bam #generate input.sorted.bam.bai . step 5: bam to bigwig . bamCoverage -p ${threads} --normalizeUsing RPKM # note: other normalization options -b input.sorted.bam -o output.bw . 3.2 Differentially expressed genes analysis . step 1: count reads . htseq-count -r pos -s no --additional-attr gene_name --additional-attr gene_type -f bam input.sorted.bam gencode.gtf &gt; output.count . step2: differentially expressed genes analysis . (1) construct read count table . option 1: HTSeq count file input . library(&quot;DESeq2&quot;) directory &lt;- &quot;/path/to/your/readCountFiles/&quot; sampleFiles &lt;- grep(&quot;count&quot;, list.files(directory), value=TRUE) condition &lt;- factor(c(&quot;KO&quot;,&quot;KO&quot;, &quot;WT&quot;,&quot;WT&quot;), levels = c(&quot;WT&quot;, &quot;KO&quot;)) # phenotable sampleTable &lt;- data.frame(sampleName = sampleFiles, fileName = sampleFiles, condition = condition) # construct read count table ddsHTSeq &lt;- DESeqDataSetFromHTSeqCount(sampleTable = sampleTable, directory = directory, design= ~ condition) . option 2: combined read count file into a single table first, then run . library(DESeq2) # read count table database &lt;- read.table(file = &quot;raw.counts.csv&quot;, sep = &quot;,&quot;, header = TRUE, row.names = 1) database &lt;- round(as.matrix(database)) # set level condition &lt;- factor(c(&quot;KO&quot;,&quot;KO&quot;, &quot;WT&quot;,&quot;WT&quot;), levels = c(&quot;WT&quot;, &quot;KO&quot;)) # build DESeq object coldata &lt;- data.frame(row.names = colnames(database), condition) dds &lt;- DESeqDataSetFromMatrix(countData=database, colData=coldata, design=~condition + treatmement) . (2) run DESeq2 and get output . library(DESeq2) dds &lt;- dds[ rowSums(counts(dds)) &gt; 1, ] # run statistical test dds &lt;- DESeq(dds) # get results res &lt;- results(dds) # summary(res) count_r &lt;- counts(dds, normalized=T) #normalized count matrix # export results res &lt;- res[order(res$padj),] diff_gene &lt;- subset(res, padj &lt; 0.05 &amp; (log2FoldChange &gt; 1 | log2FoldChange &lt; -1)) diff_gene &lt;- row.names(diff_gene) resdata &lt;- merge(as.data.frame(res), as.data.frame(counts(dds, normalized=TRUE)), by=&quot;row.names&quot;, sort=FALSE) write.csv(resdata, file = &quot;DEGs.csv&quot;, row.names = FALSE) . 3.3 Gene set enrichrment analysis . GO . clusterprofiler | Enrichr (GSEApy) | GSEA | . 3.4 Alternative splicing analysis . rMATS .",
            "url": "https://zqfang.github.io/make%20bioinfo%20uncool%20again/2020/01/20/RNAseq.html",
            "relUrl": "/make%20bioinfo%20uncool%20again/2020/01/20/RNAseq.html",
            "date": " • Jan 20, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Shortcut to ChIP-seq analysis",
            "content": "bowtie2-macs2-deeptools . 2.1 Genome mapping . Step 0: install software . # install miniconda, then call conda conda install -c bioconda bowtie2 hisat2 samtools deeptools . step 1: build index . bowtie2-build hg38.fa bowtie2_index/hg38 . step 2: mapping . Unpaired data . bowtie2 -p ${threads} -x index/hg38 -U input.fastq.gz -S ouput.sam . Paired data . bowtie2 -p 4 -x index/hg38 -1 input_R1.fastq.gz -2 input_R2.fastq.gz -S ouput.sam . step 3: sam to bam . samtools view -Sbh -q 25 -@ ${threads} -o ouput.bam input.sam . step 4: bam sort and index . samtools sort -@ ${threads} input.bam &gt; output.sorted.bam samtools index input.sorted.bam #generate input.sorted.bam.bai . step 5: bam to bigwig . bamCoverage -p ${threads} --normalizeUsing RPKM # note: other normalization options --centerReads -e 200 -b input.sorted.bam -o output.bw . 2.2 Peaks analysis . note:: macs2 (&gt;v2.2.x) supports python 3. . step 0: install tools . conda install macs2 bedtools pygenometracks . step 1: callpeaks (1) narrow peaks, e.g. TFs, h3k4m3 . # bam file input macs2 callpeak -t ChIP.elute.sorted.bam -c ChIP.input.sorted.bam -f BAM -g hs # organism -B -q 0.05 -n ${outFileName} --outdir macs_out . (2) Broad peaks, e.g. h3k27me3 . # sam file also works fine macs2 callpeak -t ./bowtie_out/WTme2ChIP.sam -c ./bowtie_out/ESCInput.sam -f SAM -g mm -B --SPMR --nomodel --extsize 147 --broad -n WTme2ChIP --outdir macs_out . step 2: advanced analysis . tools: bedtools, deeptools, pyGenomeTracks, igv | genome algebra overlap with other peaks: bedtools | | . | visualization heatmap: deeptools | signal tracks: pyGenomeTracks, igv | . | .",
            "url": "https://zqfang.github.io/make%20bioinfo%20uncool%20again/2020/01/20/ChIP.html",
            "relUrl": "/make%20bioinfo%20uncool%20again/2020/01/20/ChIP.html",
            "date": " • Jan 20, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Cheatsheet for command line",
            "content": "usefull tools for linux command line . a. Make terminal cool, install OhMyZsh . # install zsh sudo apt-get install zsh # ubuntu # change default shell to zsh chsh -s /usr/bin/zsh # install ohmyzsh sh -c &quot;$(wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)&quot; source ~/.zshrc . b. Terminal keyboard short cuts . Jump to head: Ctrl + a | Jump to end: Ctrl + e | Delete strings ahead: Ctrl + u | Delete strings follow: Ctrl + k | . c. Program keeps running in the background . 1. Run cmd using nohup . nohup command [options] &amp; . 2. Run cmd using Tmux . Outside Tmux: Typically these are run outside, but you can also run them inside an existing session . a. Start New Session . tmux new -s myname . b. Attach To Existing Session . tmux attach -t myname #by name tmux attach 4 #by number (in this case 4) . c. List Sessions . tmux ls . d. Kill Session . tmux kill-session -t myname . Inside Tmux Session: Start each command with CTRL + b, release, then press one of the following: . Panes   . % | vertical split | . ” | horizontal split | . d | detach from session (it keeps running in the background) | . x | kill pane | . Up/Down/Left/Right | move between panes | . PageUP/PageDown | CTRL+c to exit the PageUp/Down mode | . Fn+Up/Down | PageUp/Down: Mac keyboard | . : + resize-pane -D | Resizes the current pane down | . : + resize-pane -U | Resizes the current pane upward | . : + resize-pane -L | Resizes the current pane left | . : + resize-pane -R | Resizes the current pane right | . : + resize-pane -D 20 | Resizes the current pane down by 20 cells | . d. File compression and decompression . Decompression . File type Cmd e.g. . *.tar | tar -xvf |   | . *.tar.gz or *.tgz | tar -xvzf |   | . *bz2 | bzip2 -d or bunzip2 |   | . *.tar.bz2 | tar -xjf |   | . *.Z | uncompress |   | . *.tar.Z | tar -xZf |   | . *.rar | unrar e or rar x | unrar e file.rar | . *.zip | unzip |   | . *.gz | gunzip |   | . Compression . File type Cmd e.g. . *.tar | tar -cvf |   | . *.tar.gz or *.tgz | tar -cvzf |   | . *bz2 | bzip2 -z |   | . *.tar.bz2 | tar -cjf |   | . *.Z | compress |   | . *.tar.Z | tar -cZf |   | . *.rar | rar a | rar a -ep1 newname /home/user/cpp | . *.zip | zip |   | . *.gz | gzip |   | . For rar installation . sudo apt-get install rar . e. Handy tricks for handling filepath . very useful to strip file sufix, path et.al. . # e.g. var=./home/fastq/filename_R1.fq.gz # extract filename ${var#*/} # -&gt; home/fastq/filename_R1.fq.gz var1=${var##*/} # -&gt; filename_R1.fq.gz # remove file suffix ${var1%.*} # -&gt; filename_R1.fq ${var1%%.*} # -&gt; filename_R1 # get basebame var2=$(basename &quot;${var}&quot; .fq.gz) #-&gt; filename_R1 .",
            "url": "https://zqfang.github.io/make%20bioinfo%20uncool%20again/2020/01/20/Begin.html",
            "relUrl": "/make%20bioinfo%20uncool%20again/2020/01/20/Begin.html",
            "date": " • Jan 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Zhuoqing Fang. I’m now a postdoc working on computational genomics. I open this blog to keep track of my hardship with machine learning. .",
          "url": "https://zqfang.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
      ,"page4": {
          "title": "Publications",
          "content": "SOX21 Ensures Rostral Forebrain Identity by Suppression of WNT8B during Neural Regionalization of Human Embryonic Stem Cells . Published in Stem Cell Reports, 2019 . . My PhD thesis . . Download here . CDK11 safeguards the identity of human embryonic stem cells via fine‐tuning signaling pathways . Published in Journal of Cellular Physiology, 2019 . . Ding J, Fang Z, Liu X, Zhu Z, Wen C, Wang H, et al. CDK11 safeguards the identity of human embryonic stem cells via fine-tuning signaling pathways. J Cell Physiol. 2019 . . Download here . Stk40 deletion elevates c-JUN protein level and impairs mesoderm differentiation . Published in Journal of Biological Chemistry, 2019 . . Hu J, Li S, Sun X, Fang Z, Wang L, Xiao F, et al. Stk40 deletion elevates c-JUN protein level and impairs mesoderm differentiation. J Biol Chem. 2019;294(25):9959-72. . . Download here . Transcription coactivator Cited1 acts as an inducer of trophoblast-like state from mouse embryonic stem cells through the activation of BMP signaling . Published in Cell Death &amp; Disease, 2018 . . Xu Y, Luo X, Fang Z, Zheng X, Zeng Y, Zhu C, et al. Transcription coactivator Cited1 acts as an inducer of trophoblast-like state from mouse embryonic stem cells through the activation of BMP signaling. Cell Death Dis. 2018;9(9):924. . . Download here . Bioconda: sustainable and comprehensive software distribution for the life sciences . Published in Nature Methods, 2018 . . I’m honored to be one of the contributors. It is definitely worthy. Thanks for the Bioconda team, so we could install bioinformatic tools so easy! The open source Bioconda project is a milestone in computational biology. It saves lots of time when trying to install bioinfo tools. . . Download here . Single-cell analysis reveals lineage segregation in early post-implantation mouse embryos . Published in Journal of Biological Chemistry, 2017 . . Single cell analysis of mouse E5.5, E6.5 embryos. In this work, I dissected the embryos and prepared the single cell cDNA libraries. . . Download here . Deletion of Stk40 impairs definitive erythropoiesis in the mouse fetal liver . Published in Cell Death &amp; Disease, 2017 . . Wang L, Yu H, Cheng H, He K, Fang Z, et al. Deletion of Stk40 impairs definitive erythropoiesis in the mouse fetal liver. Cell Death Dis. 2017;8:e2722 . . Download here . Itch, an E3 ligase of Oct4, is required for embryonic stem cell self-renewal and pluripotency induction . Published in Journal of Cellular Physiology, 2012 . . Liao B, Zhong XM, Xu HM, Xiao F, Fang ZQ, et al. Itch, an E3 ligase of Oct4, is required for embryonic stem cell self-renewal and pluripotency induction. Journal of Cellular Physiology 2013;228:1443-51 . . Download here .",
          "url": "https://zqfang.github.io/publications/",
          "relUrl": "/publications/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://zqfang.github.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}