{
  
    
        "post0": {
            "title": "C++ Notes",
            "content": "C++ Notes . Just some C/C++ code snippets to keep in mind. C/C++ is tremendous complicated, but it’s extremely powerful. . Table of Contents . char to Int Pointer Return Reference Array as Argument Object Instantization Const Friend Singleton . Char to Int . Char is ASCII C store Char as ASCII by default. So, Char is equal to ASCII code. char a = &#39;A&#39;; // 65 int c = a; //c = 65 . | Char to Int, and vice versa cpp char c=’5’ int res = c -‘0’ ; // 5 | int i=5; char res = I + ‘0’; // ‘5’ . 3. `Char` as subscript of array: legal! cpp int test[200] = {0}; Test[&#39;A&#39;] = 1; // legal Test[&#39;b&#39;] = 2; // legal . This is useful when create hashmap. e.g. counting chars . Pointer . Pointer syntax Rule: read from right to left . int a; // an int int *a; // a pointer point to int int **a; // secondary int pointer, point to another int pointer int a[10]; // int array int *a[10]; // a poiter array, point to int int (*a)[10]; // a int pointer point to an int array int (*a)(int); // a pointer point to a function, will return an int int (*a[10])(int); // a poiter array, point to a function，will return an int . Declare two pointers . int* a, b; // equal to int* a; int b; int *a, *b; // correct way . Return Reference . When Return Reference . return ref if given object’s ref/pointer. | return value if temp object created // ref RMB&amp; RMB::operator++() { yuan++; return *this; // already existed object, created outside } // value RMB RMB::operator++(int) { RMB temp(yuan);//create object yuan++; return temp;//return a copy of temp } . Back to top . | . Array . Array as formal arguments . An Array could not copy to anther Array directly, so call-by-value is not allowed. So, use array pointer: . //these are same void print(const int*); void print(const int[]); void print(const int[5]); . multi-dimension array . void print(const int(*p)[3], int rowsize); void print(const int p[][5], int rowsize); . When use pointer to an Array, the dimension is unknown. So, need an extra argument to specify it explicitly. . Example: . void print1(int (*p)[3]) { cout&lt;&lt;p[1][1]&lt;&lt;endl; } void print2(int p[][3]) { cout&lt;&lt;p[0][0]&lt;&lt;endl; } int a[2][3]={ {1,2},{3,4} }; print1(a); // 4 print2(a); // 1 int b[2][4]={ {1,2,5,6},{3,4,7,8} }; print1(b); // error . Back to top . Const . 1. const before or behind type/class, the syntax semantic are same . // they are same const int x; // (int x) is const/inmutable int const x; // (const x) has type int . 2. Pointer with const: const int* p, int const* p and int *const p . Dirty trick: use * as a separator, const restrict the type according to the side where it belong to . point to const: These two expression are same . // -&gt; (const int) | p; p : a mutable pointer points to a const/immutable int const int * p; // -&gt; (int const) | p; p2: a mutable pointer points a const which has type int int const * p2; . const pointer: But these two not the same . // -&gt; int | (const p); p3: a const pointer, point to an mutable int int * const p3; // -&gt; (const int) | const p; p4: a const pointer, pointing to an immutable/const int const int * const p4; . 3. class member func with const: () const . a. const object . could not change class variable | could not call non-const function cpp class Number { public: void set(int num) { number = num; } int get() { return number; } int get2() const {return number;} int number = 0; }; | . // Example const Number n; n.number = 1; // Error, n is const n.set(1); // Error, n is const, non-const set() n.get(); // Error, non-const get() n.get2(); // OK . b. `() const` - could not change class variable, except static - could get variable cpp class Number { private: int a; static int b; const int c = 20; public: void set() { a = 10; // error when `this` argument has type &#39;const&#39; void set2() const { b = 20; // OK } int get() const { // OK return a; // did not change a } }; const Number n; n.set(); // Error n.set2(); // OK n.get(); // OK . Easy to understand, when pointer this is const . void Number::set(const Number *const this, int num) { number = num; } // illegal -&gt; const this . c. () const overloading . Back to top . Object Instantization . without new stack ClassName object(param) // A a(1);  ClassName object2 = ClassName(param); // A b = A(1); . | with new ClassName *object = new ClassName(param);//A *a = new A(); delete object; . | copy constructor // . | Smart Pointer std::unique_ptr&lt;ClassName&gt; object (new ClassName(param)); // recommend this way of instantization std::unique_ptr&lt;ClassName&gt; object = std::make_unique&lt;ClassName&gt;(param); . Back to top . | Friend . The friend declaration appears in a class body and grants a function or another class access to private and protected members of the class where the friend declaration appears. . friend function: . Declare anywhere inside a class, but define outside . // function friend &lt;type&gt; &lt;Name&gt;(&lt;arguments&gt;); . Example: . class A { public: A(int _a):a(_a){}; // non-member function friend int getA_a(A &amp;_classA); private: int a; }; // without the friend keyword int getA_a(A &amp;_classA) { //access member by formal arguments return _classA.a; } A _classA(3); std::cout&lt;&lt;getA_a(_classA); // 3 . friend class: . delare inside class, define outside . // class friend class &lt;Name&gt;; . Note: friend class X {}; is an error . Example: . class B { public: B(int _b):b(_b){}; friend class C; // friend class private: int b; }; class C { public: int getB_b(B _classB){ //access member by formal arguments return _classB.b; }; }; B _classB(3); C _classC; // an instance of a friend class _classC.getB_b(_classB); . Others . class Y { int data; // the non-member function operator&lt;&lt; will have access to Y&#39;s private members friend std::ostream&amp; operator&lt;&lt;(std::ostream&amp; out, const Y&amp; o); friend char* X::foo(int); // members of other classes can be friends too friend X::X(char), X::~X(); // constructors and destructors can be friends }; // this operator&lt;&lt; still needs to be defined, as a non-member std::ostream&amp; operator&lt;&lt;(std::ostream&amp; out, const Y&amp; y) { // can access private member Y::data return out &lt;&lt; y.data; } . Back to top . Singleton . #include &lt;iostream&gt; class Singleton { private: /* Here will be the instance stored. */ static Singleton* instance; /* Private constructor to prevent instancing. */ Singleton() {}; public: /* Static access method. */ static Singleton* getInstance() { if (instance == 0) instance = new Singleton(); return instance; } }; /* NULL, because instance will be initialized on demand. */ Singleton* Singleton::instance = 0; . Usage . int main() { //new Singleton(); // Won&#39;t work Singleton* s = Singleton::getInstance(); // Ok Singleton* r = Singleton::getInstance(); /* The addresses will be the same. */ std::cout &lt;&lt; s &lt;&lt; std::endl; std::cout &lt;&lt; r &lt;&lt; std::endl; } . Back to top .",
            "url": "https://zqfang.github.io/coding/2020/05/31/Code-CPP-notes.html",
            "relUrl": "/coding/2020/05/31/Code-CPP-notes.html",
            "date": " • May 31, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Swift Cheat Sheet",
            "content": "Swift Cheat Sheet . Stolen from iwasrobbed. I simplify it and add some more. . It’s a high level and a quick reference to Swift. The purpose of this cheat sheet is to teach myself and get answers within 10s. . Table of Contents . Code Document | Data Types | Operators | Operator Overloading | Declaring Classes | Declarations | Lazy Property | Property Observer | Literals | Functions | Constants and Variables | Naming Conventions | Closures | Generics | Control Statements | Extension | Protocol | Protocol Extension | Error Handling | Passing Information | User Defaults | Common Patterns | Unicode Support Code Documentation . Two ways of commenting: . // | /* … */ Two ways of documenting with markdown (Reconigzed by xcode): | /// | /** … */ | . | . Markdown . a few keywords that xcode can recognized automatically, with the format like **- **. . The most common: Prameters, Throws, Returns . /** - Prameters: - argument1: This is arg1 - argument2: This is arg2 - Returns: The results string. - Throws: `Error` if nil */ . Other keywords . /** - Precondition: - Postcondition: - Requires: All the information in the object should be sorted - Invariant: The object will maintained sorted - Complexity: O(n^2) - Important: - Warning: Very computation consuming - Attention: Same as Warning - Note: something to keep in mind - Remark: Same as note */ . Metadata . /** - Author: - Authors: - Copyright: - Date: - Since: - Version: */ . Back to top . MARK . Using MARK to organize your code: . // MARK: - Use mark to logically organize your code // Declare some functions or variables here // MARK: - They also show up nicely in the properties/functions list in Xcode // Declare some more functions or variables here . FIXME . Using FIXME to remember to fix your code: . // Some broken code might be here // FIXME: Use fixme to create a reminder to fix broken code later . FIXME works a lot like MARK because it makes organizing code easier, but it’s used exclusively when you need to remember to fix something. . TODO . Using TODO to remember to add, delete, or generally refactor your code: . // Some incomplete code might be here // TODO: Use todo to create a reminder to finish things up later . TODO is very similar to FIXME and MARK, but it’s used exclusively when you need to remember to add, delete, or change your code later. . Auto-generating method documentation: In a method’s preceding line, press ⌥ Option + ⌘ Command + / to automatically generate a documentation stub for your method. . Back to top . Data Types . Size . Permissible sizes of data types are determined by how many bytes of memory are allocated for that specific type and whether it’s a 32-bit or 64-bit environment. In a 32-bit environment, long is given 4 bytes, which equates to a total range of 2^(4*8) (with 8 bits in a byte) or 4294967295. In a 64-bit environment, long is given 8 bytes, which equates to 2^(8*8) or 1.84467440737096e19. . For a complete guide to 64-bit changes, please see the transition document. . C Primitives . Unless you have a good reason to use C primitives, you should just use the Swift types to ensure compability going foward. . In fact, Swift just aliases C types to a Swift equivalent: . // C char is aliased as an Int8 and unsigned as UInt8 let aChar = CChar() let anUnsignedChar = CUnsignedChar() print(&quot;C char size: (MemoryLayout.size(ofValue: aChar)) with min: (Int8.min) and max: (Int8.max)&quot;) // C char size: 1 with min: -128 and max: 127 print(&quot;C unsigned char size: (MemoryLayout.size(ofValue: anUnsignedChar)) with min: (UInt8.min) and max: (UInt8.max)&quot;) // C unsigned char size: 1 with min: 0 and max: 255 // C short is aliased as an Int16 and unsigned as UInt16 let aShort = CShort() let unsignedShort = CUnsignedShort() print(&quot;C short size: (MemoryLayout.size(ofValue: aShort)) with min: (Int16.min) and max: (Int16.max)&quot;) // C short size: 2 with min: -32768 and max: 32767 print(&quot;C unsigned short size: (MemoryLayout.size(ofValue: unsignedShort)) with min: (UInt16.min) and max: (UInt16.max)&quot;) // C unsigned short size: 2 with min: 0 and max: 65535 // C int is aliased as an Int32 and unsigned as UInt32 let anInt = CInt() let unsignedInt = CUnsignedInt() print(&quot;C int size: (MemoryLayout.size(ofValue: anInt)) with min: (Int32.min) and max: (Int32.max)&quot;) // C int size: 4 with min: -2147483648 and max: 2147483647 print(&quot;C unsigned int size: (MemoryLayout.size(ofValue: unsignedInt)) with min: (UInt32.min) and max: (UInt32.max)&quot;) // C unsigned int size: 4 with min: 0 and max: 4294967295 // C long is aliased as an Int and unsigned as UInt let aLong = CLong() let unsignedLong = CUnsignedLong() print(&quot;C long size: (MemoryLayout.size(ofValue: aLong)) with min: (Int.min) and max: (Int.max)&quot;) // C long size: 8 with min: -9223372036854775808 and max: 9223372036854775807 print(&quot;C unsigned long size: (MemoryLayout.size(ofValue: unsignedLong)) with min: (UInt.min) and max: (UInt.max)&quot;) // C unsigned long size: 8 with min: 0 and max: 18446744073709551615 // C long long is aliased as an Int64 and unsigned as UInt64 let aLongLong = CLongLong() let unsignedLongLong = CUnsignedLongLong() print(&quot;C long long size: (MemoryLayout.size(ofValue: aLongLong)) with min: (Int64.min) and max: (Int64.max)&quot;) // C long long size: 8 with min: -9223372036854775808 and max: 9223372036854775807 print(&quot;C unsigned long long size: (MemoryLayout.size(ofValue: unsignedLongLong)) with min: (UInt64.min) and max: (UInt64.max)&quot;) // C unsigned long long size: 8 with min: 0 and max: 18446744073709551615 . From the docs: . C Type Swift Type . bool | CBool | . char, signed char | CChar | . unsigned char | CUnsignedChar | . short | CShort | . unsigned short | CUnsignedShort | . int | CInt | . unsigned int | CUnsignedInt | . long | CLong | . unsigned long | CUnsignedLong | . long long | CLongLong | . unsigned long long | CUnsignedLongLong | . wchar_t | CWideChar | . char16_t | CChar16 | . char32_t | CChar32 | . float | CFloat | . double | CDouble | . Integers . Integers can be signed or unsigned. When signed, they can be either positive or negative and when unsigned, they can only be positive. . Apple states: Unless you need to work with a specific size of integer, always use Int for integer values in your code. This aids code consistency and interoperability. Even on 32-bit platforms, Int […] is large enough for many integer ranges. . Fixed width integer types with their accompanying byte sizes as the variable names: . // Exact integer types let aOneByteInt: Int8 = 127 let aOneByteUnsignedInt: UInt8 = 255 let aTwoByteInt: Int16 = 32767 let aTwoByteUnsignedInt: UInt16 = 65535 let aFourByteInt: Int32 = 2147483647 let aFourByteUnsignedInt: UInt32 = 4294967295 let anEightByteInt: Int64 = 9223372036854775807 let anEightByteUnsignedInt: UInt64 = 18446744073709551615 // Minimum integer types let aTinyInt: Int8 = 127 let aTinyUnsignedInt: UInt8 = 255 let aMediumInt: Int16 = 32767 let aMediumUnsignedInt: UInt16 = 65535 let aNormalInt: Int32 = 2147483647 let aNormalUnsignedInt: UInt32 = 4294967295 let aBigInt: Int64 = 9223372036854775807 let aBigUnsignedInt: UInt64 = 18446744073709551615 // The largest supported integer type let theBiggestInt: IntMax = 9223372036854775807 let theBiggestUnsignedInt: UIntMax = 18446744073709551615 . Floating Point . Floats cannot be signed or unsigned. . // Single precision (32-bit) floating-point. Use it when floating-point values do not require 64-bit precision. let aFloat = Float() print(&quot;Float size: (MemoryLayout.size(ofValue: aFloat))&quot;) // Float size: 4 // Double precision (64-bit) floating-point. Use it when floating-point values must be very large or particularly precise. let aDouble = Double() print(&quot;Double size: (MemoryLayout.size(ofValue: aDouble))&quot;) // Double size: 8 . Boolean . // Boolean let isBool: Bool = true // Or false . In Objective-C comparative statements, 0 and nil were considered false and any non-zero/non-nil values were considered true. However, this is not the case in Swift. Instead, you’ll need to directly check their value such as if x == 0 or if object != nil . Primitives . nil : Used to specify a null object pointer. When classes are first initialized, all properties of the class point to nil. . Enum &amp; Bitmask Types . Enumeration types can be defined as follows: . // Specifying a typed enum with a name (recommended way) enum UITableViewCellStyle: Int { case default, valueOne, valueTwo, subtitle } // Accessing it: let cellStyle: UITableViewCellStyle = .default . As of Swift 3, all enum options should be named in lowerCamelCased. . Working with Bitmasks . Newer Swift versions have a nice substitute for the old NS_OPTIONS macro for creating bitmasks to compare to. . An example for posterity: . struct Options: OptionSet { let rawValue: Int init(rawValue: Int) { self.rawValue = rawValue } init(number: Int) { self.init(rawValue: 1 &lt;&lt; number) } static let OptionOne = Options(number: 0) static let OptionTwo = Options(number: 1) static let OptionThree = Options(number: 2) } let options: Options = [.OptionOne, .OptionTwo] options.contains(.OptionOne) // true options.contains(.OptionThree) // false . Type Casting . Sometimes it is necessary to cast an object into a specific class or data type. Examples of this would be casting from a Float to an Int or from a UITableViewCell to a subclass such as RPTableViewCell. . Checking Types . Swift uses is and as both for checking object types as well as conformance to a given protocol. . Operator: is . Checking object type using is: . if item is Movie { movieCount += 1 print(&quot;It is a movie.&quot;) } else if item is Song { songCount += 1 print(&quot;It is a song.&quot;) } . The is operator returns true if an instance is of that object type, or conforms to the specified protocol, and returns false if it does not. . Operators: as? and as! . If you want to be able to easily access the data during one of these checks, you can use as? to optionally (or as! to force) unwrap the object when necessary: . for item in library { if let movie = item as? Movie { print(&quot;Director: (movie.director)&quot;) } else if let song = item as? Song { print(&quot;Artist: (song.artist)&quot;) } } . The as? version of the downcast operator returns an optional value of the object or protocol’s type, and this value is nil if the downcast fails or this instance does not conform to the specified protocol. . The as! version of the downcast operator forces the downcast to the specified object or protocol type and triggers a runtime error if the downcast does not succeed. . Casting from Generic Types . If you’re working with AnyObject objects given from the Cocoa API, you can use: . for movie in someObjects as! [Movie] { // do stuff } . If given an array with Any objects, you can use a switch statement with the type defined for each case: . var things = [Any]() for thing in things { switch thing { case 0 as Int: print(&quot;Zero as an Int&quot;) case let someString as! String: print(&quot;S string value of &quot; (someString) &quot;&quot;) case let (x, y) as! (Double, Double): print(&quot;An (x, y) point at (x), (y)&quot;) case let movie as! Movie: print(&quot;A movie called &#39; (movie.name)&#39; by director (movie.director)&quot;) default: print(&quot;Didn&#39;t match any of the cases specified&quot;) } } . Basic Casting . Swift also offers some simple methods of casting between it’s given data types. . // Example 1: let aDifferentDataType: Float = 3.14 let anInt: Int = Int(aDifferentDataType) // Example 2: let aString: String = String(anInt) . Back to top . Operators . Swift supports most standard C operators and improves several capabilities to eliminate common coding errors. The assignment operator = does not return a value, to prevent it from being mistakenly used when the equal to operator == is intended. . Arithmetic operators (+, -, *, /, %) detect and disallow value overflow, to avoid unexpected results when working with numbers that become larger or smaller than the allowed value range of the type that stores them. . Arithmetic Operators . Operator Purpose   .   | + | Addition | .   | - | Subtraction | .   | * | Multiplication | .   | / | Division | .   | % | Remainder | . Comparative Operators . Operator Purpose . == | Equal to | . === | Identical to | . != | Not equal to | . !== | Not identical to | . ~= | Pattern match | . &gt; | Greater than | . &lt; | Less than | . &gt;= | Greater than or equal to | . &lt;= | Less than or equal to | . Assignment Operators . Operator Purpose . = | Assign | . += | Addition | . -= | Subtraction | . *= | Multiplication | . /= | Division | . %= | Remainder | . &amp;= | Bitwise AND | . |= | Bitwise Inclusive OR | . ^= | Exclusive OR | . «= | Shift Left | . »= | Shift Right | . Logical Operators . Operator Purpose . ! | NOT | . &amp;&amp; | Logical AND | . || | Logical OR | . Range Operators . Operator Purpose . ..&lt; | Half-open range | . … | Closed range | . Bitwise Operators . Operator Purpose . &amp; | Bitwise AND | . | | Bitwise Inclusive OR | . ^ | Exclusive OR | . ~ | Unary complement (bit inversion) | . « | Shift Left | . » | Shift Right | . Overflow and Underflow Operators . Typically, assigning or incrementing an integer, float, or double past it’s range would result in a runtime error. However, if you’d instead prefer to safely truncate the number of available bits, you can opt-in to have the variable overflow or underflow using the following operators: . Operator Purpose . &amp;+ | Addition | . &amp;- | Subtraction | . &amp;* | Multiplication | . Example for unsigned integers (works similarly for signed): . var willOverflow = UInt8.max // willOverflow equals 255, which is the largest value a UInt8 can hold willOverflow = willOverflow &amp;+ 1 // willOverflow is now equal to 0 var willUnderflow = UInt8.min // willUnderflow equals 0, which is the smallest value a UInt8 can hold willUnderflow = willUnderflow &amp;- 1 // willUnderflow is now equal to 255 . Other Operators . Operator Purpose . ?? | Nil coalescing | . ?: | Ternary conditional | . ! | Force unwrap object value | . ? | Safely unwrap object value | . Back to top . Operator Overloading . Swift allows you to overwrite existing operators or define new operators for existing or custom types. For example, this is why in Swift you can join strings using the + operator, even though it is typically used for math. . Operator overloading is limited to the following symbols, / = - + * % &lt; &gt; ! &amp; | ^ . ~, however you cannot overload the = operator by itself (it must be combined with another symbol). . Operators can be specified as: . prefix: goes before an object such as -negativeNumber | infix: goes between two objects, such as a + b | postfix: goes after an object, such as unwrapMe! | . Custom operators . associativity: defines how operators of the same precedence are grouped together (left, right) | precedence: gives some operators higher priority than others; these operators are applied first. | . Refer Operator Declarations to see full details about operator associativity and precedence. . Example: . DefaultPrecedence group . // declare first and set rules with a precedence group infix operator ** // use DefaultPrecedence group . Custom Precedence group . // define a custom precedence group precedencegroup ExponentiationPrecedence { higherThan: MultiplicationPrecedence associativity: right // none, left, right //assignment: false } // now, replace original declaration of ** with infix operator **: ExponentiationPrecedence . That’s it. . // impelment infix func ** (x: Double, p: Double) -&gt; Double { return pow(x, p) } 2**3 // 8 2**3**2 // 512 1+2**3**2 // 513 5*2**3**2 // 2560 . see also docs . Back to top . Declaring Classes . Classes are typically declared using separate .swift files, but multiple classes can also be created within the same file if you’d like to organize it that way. . Unlike Objective-C, there’s no need for an interface file (.h) in Swift. . The implementation file should contain (in this order): . Any needed import statements | A class declaration which contains any constants or variables necessary for the class | All public and private functions | . Example: . MyClass.swift . import UIKit class MyClass { // Declare any constants or variables at the top let kRPErrorDomain = &quot;com.myIncredibleApp.errors&quot; var x: Int, y: Int // MARK: - Class Methods, e.g. MyClass.functionName() class func alert() { print(&quot;This is a class function.&quot;) } // MARK: - Instance Methods, e.g. myClass.functionName() init(x: Int, y: Int) { self.x = x self.y = y } // MARK: - Private Methods private func pointLocation() -&gt; String { return &quot;x: (x), y: (y)&quot; } } . Instantiation . When you want to create a new instance of a class, you use the syntax: . let myClass = MyClass(x: 1, y: 2) . where x and y are variables that are passed in at the time of instantiation. . Back to top . Declarations . More info here in the docs. . Preprocessor . Swift doesn’t come with a preprocessor so it only supports a limited number of statements for build time. Things like #define have been replaced with global constants defined outside of a class. . Directive Purpose . #if | An if conditional statement | . #elif | An else if conditional statement | . #else | An else conditional statement | . #endif | An end if conditional statement | . Imports . Directive Purpose . import | Imports a framework | . Constants &amp; Variables . Directive Purpose . let | Declares local or global constant | . var | Declares a local or global variable | . class | Declares a class-level constant or variable | . static | Declares a static type | . Classes, Structure, Functions and Protocols . Directive Purpose . typealias | Introduces a named alias of an existing type | . enum | Introduces a named enumeration | . struct | Introduces a named structure | . class | Begins the declaration of a class | . init | Introduces an initializer for a class, struct or enum | . init? | Produces an optional instance or an implicitly unwrapped optional instance; can return nil | . deinit | Declares a function called automatically when there are no longer any references to a class object, just before the class object is deallocated | . func | Begins the declaration of a function | . protocol | Begins the declaration of a formal protocol | . static | Defines as type-level within struct or enum | . convenience | Delegate the init process to another initializer or to one of the class’s designated initializers | . extension | Extend the behavior of class, struct, or enum | . subscript | Adds subscripting support for objects of a particular type, normally for providing a convenient syntax for accessing elements in a collective, list or sequence | . override | Marks overriden initializers | . Operators . Directive Purpose . operator | Introduces a new infix, prefix, or postfix operator | . Declaration Modifiers . Directive Purpose . dynamic | Marks a member declaration so that access is always dynamically dispatched using the Objective-C runtime and never inlined or devirtualized by the compiler | . final | Specifies that a class can’t be subclassed, or that a property, function, or subscript of a class can’t be overridden in any subclass | . lazy | Indicates that the property’s initial value is calculated and stored at most once, when the property is first accessed | . optional | Specifies that a protocol’s property, function, or subscript isn’t required to be implemented by conforming members | . required | Marks the initializer so that every subclass must implement it | . weak | Indicates that the variable or property has a weak reference to the object stored as its value | . Access Control . Directive Purpose . open | Can be subclassed outside of its own module and its methods overridden as well; truly open to modification by others and useful for framework builders | . public | Can only be subclassed by its own module or have its methods overridden by others within the same module | . internal | (Default) Indicates the entities are only available to the entire module that includes the definition, e.g. an app or framework target | . fileprivate | Indicates the entities are available only from within the source file where they are defined | . private | Indicates the entities are available only from within the declaring scope within the file where they are defined (e.g. within the { } brackets only) | . public class AccessLevelsShowCase { // Property accessible for other modules public var somePublicProperty = 0 // Property accessible from the module var someInternelProperty = 1 // Property accessible from its own defining source file fileprivate func someFilePrivateMethod() {} // Property accessible fro its enclosing declaration private func somePrivateMethod() {} } . Back to top . Literals . Literals are compiler directives which provide a shorthand notation for creating common objects. . Syntax What it does . &quot;string&quot; | Returns a String object | . 28 | Returns an Int | . 3.14, 0xFp2, 1.25e2 | Returns a Double object | . true, false | Returns a Bool object | . [] | Returns an Array object | . [keyName:value] | Returns a Dictionary object | . 0b | Returns a binary digit | . 0o | Returns an octal digit | . 0x | Returns a hexadecimal digit | . Strings . Special characters can be included: . Null Character: 0 | Backslash: (can be used to escape a double quote) | Horizontal Tab: t | Line Feed: n | Carriage Return: r | Double Quote: &quot; | Single Quote: &#39; | Unicode scalar: u{n} where n is between one and eight hexadecimal digits | . Multiline string literal . let json = &quot;&quot;&quot; { &quot;username&quot;: &quot;David&quot;, &quot;loginCount&quot;: 2} &quot;&quot;&quot; . Array Access Syntax . let example = [ &quot;hi&quot;, &quot;there&quot;, 23, true ] print(&quot;item at index 0: (example[0])&quot;) . Dictionary Access Syntax . let example = [ &quot;hi&quot; : &quot;there&quot;, &quot;iOS&quot; : &quot;people&quot; ] if let value = example[&quot;hi&quot;] { print(&quot;hi (value)&quot;) } . Mutability . For mutable literals, declare it with var; immutable with let. . Back to top . Functions . Declaration Syntax . Functions without a return type use this format: . // Does not return anything or take any arguments func doWork() { // Code } . class precedes declarations of class functions: . // Call on a class, e.g. MyClass.someClassFunction() class func someClassFunction() { // Code } . static is similar to class functions where you don’t need an instance of the class or struct in order to call a method on it: . // Call on a class/struct, e.g. MyStruct.someStaticFunction() static func someStaticFunction() { // Code } . Declare instance functions: . // Called on an instance of a class, e.g. myClass.someInstanceFunction() func doMoreWork() { // Code } . Function arguments are declared within the parentheses: . // Draws a point func draw(point: CGPoint) . Return types are declared as follows: . // Returns a String object for the given String argument func sayHelloToMyLilFriend(lilFriendsName: String) -&gt; String { return &quot;Oh hello, (lilFriendsName). Cup of tea?&quot; } . You can have multiple return values, referred to as a tuple: . // Returns multiple objects func sayHelloToMyLilFriend(lilFriendsName: String) -&gt; (msg: String, nameLength: Int) { return (&quot;Oh hello, (lilFriendsName). Cup of tea?&quot;, countElements(lilFriendsName)) } var hello = sayHelloToMyLilFriend(&quot;Rob&quot;) print(hello.msg) // &quot;Oh hello, Rob. Cup of tea?&quot; print(hello.nameLength) // 3 . And those multiple return values can be optional: . func sayHelloToMyLilFriend(lilFriendsName: String) -&gt; (msg: String, nameLength: Int)? . By default, external parameter names are given when you call the function, but you can specify that one or more are not shown in the method signature by putting a _ symbol in front of the parameter name: . func sayHelloToMyLilFriend(_ lilFriendsName: String) { // Code } sayHelloToMyLilFriend(&quot;Rob&quot;) . or you can rename the variable once within the method scope: . func sayHelloToMyLilFriend(friendsName lilFriendsName: String) { // Code } sayHelloToMyLilFriend(friendsName: &quot;Rob&quot;) // and local variable is `lilFriendsName` . You can also specify default values for the parameters: . func sayHelloToMyLilFriend(_ lilFriendsName: String = &quot;Rob&quot;) { // Code } sayHelloToMyLilFriend() // &quot;Oh hello, Rob. Cup of tea?&quot; sayHelloToMyLilFriend(&quot;Jimbob&quot;) // &quot;Oh hello, Jimbob. Cup of tea?&quot; . Swift also supports variadic parameters so you can have an open-ended number of parameters passed in: . func sayHelloToMyLilFriends(_ lilFriendsName: String...) { // Code } sayHelloToMyLilFriends(&quot;Rob&quot;, &quot;Jimbob&quot;, &quot;Cletus&quot;) // &quot;Oh hello, Rob, Jimbob and Cletus. Cup of tea?&quot; . And lastly, you can also use a prefix to declare input parameters as inout. . An in-out parameter has a value that is passed in to the function, is modified by the function, and is passed back out of the function to replace the original value. . You may remember inout parameters from Objective-C where you had to sometimes pass in an &amp;error parameter to certain methods, where the &amp; symbol specifies that you’re actually passing in a pointer to the object instead of the object itself. The same applies to Swift’s inout parameters now as well. . Calling Functions . Functions are called using dot syntax: myClass.doWork() or self.sayHelloToMyLilFriend(&quot;Rob Phillips&quot;) . self is a reference to the function’s containing class. . At times, it is necessary to call a function in the superclass using super.someMethod(). . Back to top . Constants and Variables . Declaring a constant or variable allows you to maintain a reference to an object within a class or to pass objects between classes. . Constants are defined with let and variables with var. By nature, constants are obviously immutable (i.e. cannot be changed once they are instantiated) and variables are mutable. . class MyClass { let text = &quot;Hello&quot; // Constant var isComplete: Bool // Variable } . There are many ways to declare properties in Swift, so here are a few examples: . var myInt = 1 // inferred type var myExplicitInt: Int = 1 // explicit type var x = 1, y = 2, z = 3 // declare multiple variables let (a,b) = (1,2) // declare multiple constants . Getters and Setters . In Objective-C, variables were backed by getters, setters, and private instance variables created at build time. However, in Swift getters and setters are only used for computed properties and constants actually don’t have a getter or setter at all. . The getter is used to read the value, and the setter is used to write the value. The setter clause is optional, and when only a getter is needed, you can omit both clauses and simply return the requested value directly. However, if you provide a setter clause, you must also provide a getter clause. . You can overrride the getter and setter of a property to create the illusion of the Objective-C property behavior, but you’d need to store them as a private property with a different name (not recommended for most scenarios): . private var _x: Int = 0 var x: Int { get { print(&quot;Accessing x...&quot;) return _x } set { print(&quot;Setting x...&quot;) _x = newValue } } . Property Observer . Swift also has callbacks for when a property will be or was set using willSet and didSet shown below: . willset: before assignment | didSet: after assignment | . class LightBulb { static let maxCurrent = 30 var current = 0 { willSet(newCurrent) { // do something before value assignment // newValue -&gt; newCurrent print(&quot;Current value changed, the change is (abs(current- newCurrent))&quot;) } didSet { // do somthing afther value assignment if current == LightBulb.maxCurrent { print(&quot;current get to maximum point&quot;) } // oldValue } } } let bulb = LightBulb() bulb.current = 20 bulb.current = 30 bulb.current = 40 . Back to top . Lazy Property . lazy: only compute once and remember the value, won’t re-compute if called again. | . class ClosedRange { let start: Int let end: Int var width: Int { return end - start +1 } // note the = lazy var sum: Int = { var res = 0 print(&quot;run&quot;) for i in self.start...self.end{ res += 1 } return }() // don&#39;t forget () init?(start: Int, end: Int){ if start &gt; end { return nil } self.start = start self.end = end } } // example if let range = ClosedRange(start: 0, end: 10_000) { range.width //1001 range.sum // will print out &quot;run&quot; range.sum // range.sum // } . Back to top . Accessing . Local Variables . Local variables and constants only exist within the scope of a function. . func doWork() { let localStringVariable = &quot;Some local string variable.&quot; self.doSomething(string: localStringVariable) } . Back to top . Naming Conventions . The general rule of thumb: Clarity and brevity are both important, but clarity should never be sacrificed for brevity. . Functions and Properties . These both use camelCase where the first letter of the first word is lowercase and the first letter of each additional word is capitalized. . Class names and Protocols . These both use CapitalCase where the first letter of every word is capitalized. . Enums . The options in an enum should be lowerCamelCased . Functions . These should use verbs if they perform some action (e.g. performInBackground). You should be able to infer what is happening, what arguments a function takes, or what is being returned just by reading a function signature. . Example: . // Correct func move(from start: Point, to end: Point) {} // Incorrect (likely too expressive, but arguable) func moveBetweenPoints(from start: Point, to end: Point) {} // Incorrect (not expressive enough and lacking argument clarity) func move(x: Point, y: Point) {} . Back to top . Closures . Closures in Swift are similar to blocks in Objective-C and are essentially chunks of code, typically organized within a {} clause, that are passed between functions or to execute code as a callback within a function. Swift’s func functions are actually just a special case of a closure in use. . Syntax . { (params) -&gt; returnType in statements } . Examples . // Map just iterates over the array and performs whatever is in the closure on each item let people = [&quot;Rob&quot;, &quot;Jimbob&quot;, &quot;Cletus&quot;] people.map({ (person: String) -&gt; String in &quot;Oh hai, (person)...&quot; }) // Oh hai, Rob // Oh hai, Jimbob // Oh hai, Cletus // Closure for alphabetically reversing an array of names, where sorted is a Swift library function let names = [&quot;Francesca&quot;, &quot;Joe&quot;, &quot;Bill&quot;, &quot;Sally&quot;, ] var reversed = names.sorted { (s1: String, s2: String) -&gt; Bool in return s1 &gt; s2 } // Or on a single line: reversed = names.sorted{ (s1: String, s2: String) -&gt; Bool in return s1 &gt; s2 } // Or because Swift can infer the Bool type: reversed = names.sorted { s1, s2 in return s1 &gt; s2 } // Or because the return statement is implied: reversed = names.sorted { s1, s2 in s1 &gt; s2 } // Or even shorter using shorthand argument names, such as $0, $1, $2, etc.: reversed = names.sorted { $0 &gt; $1 } // Or just ridiculously short because Swift&#39;s String greater-than operator implementation exactly matches this function definition: reversed = names.sorted(by: &gt;) . If the closure is the last parameter to the function, you can also use the trailing closure pattern. This is especially useful when the closure code is especially long and you’d like some extra space to organize it: . func someFunctionThatTakesAClosure(closure: () -&gt; ()) { // function body goes here } // Instead of calling like this: someFunctionThatTakesAClosure({ // closure&#39;s body goes here }) // You can use trailing closure like this: someFunctionThatTakesAClosure() { // trailing closure&#39;s body goes here } . Capturing Values . A closure can capture constants and variables from the surrounding context in which it is defined. The closure can then refer to and modify the values of those constants and variables from within its body, even if the original scope that defined the constants and variables no longer exists. . In Swift, the simplest form of a closure that can capture values is a nested function, written within the body of another function. A nested function can capture any of its outer function’s arguments and can also capture any constants and variables defined within the outer function. . func makeIncrementor(forIncrement amount: Int) -&gt; () -&gt; Int { var runningTotal = 0 func incrementor() -&gt; Int { runningTotal += amount return runningTotal } return incrementor } . Swift determines what should be captured by reference and what should be copied by value. You don’t need to annotate a variable to say that they can be used within the nested function. Swift also handles all memory management involved in disposing of variables when they are no longer needed by the function. . Capturing Self . If you create a closure that references self.* it will capture self and retain a strong reference to it. This is sometimes the intended behavior, but often could lead to retain cycles where both objects won’t get deallocated at the end of their lifecycles. . The two best options are to use unowned or weak. This might look a bit messy, but saves a lot of headache. . Use unowned when you know the closure will only be called if self still exists, but you don’t want to create a strong (retain) reference. . Use weak if there is a chance that self will not exist, or if the closure is not dependent upon self and will run without it. If you do use weak also remember that self will be an optional variable and should be checked for existence. . typealias SomeClosureType = (_ value: String) -&gt; () class SomeClass { fileprivate var currentValue = &quot;&quot; init() { someMethod { (value) in // Retained self self.currentValue = value } someMethod { [unowned self] (value) in // Not retained, but expected to exist self.currentValue = value } someMethod { [weak self] value in // Not retained, not expected to exist // Or, alternatively you could do guard let sSelf = self else { return } // Or, alternatively use `self?` without the guard sSelf.currentValue = value } } func someMethod(closure: SomeClosureType) { closure(&quot;Hai&quot;) } } . Reference: Apple: Automatic Reference Counting . Back to top . Generics . Coming soon… . Back to top . Control Statements . Swift uses all of the same control statements that other languages have: . If-Else If-Else . if someTestCondition { // Code to execute if the condition is true } else if someOtherTestCondition { // Code to execute if the other test condition is true } else { // Code to execute if the prior conditions are false } . As you can see, parentheses are optional. . Ternary Operators . The shorthand notation for an if-else statement is a ternary operator of the form: someTestCondition ? doIfTrue : doIfFalse . Example: . func stringForTrueOrFalse(trueOrFalse: Bool) -&gt; String { return trueOrFalse ? &quot;True&quot; : &quot;False&quot; } . Nil Coalescing Operators . In Swift, we need to consider the use of optional values. One very basic way to handle nil cases is with an if-else statement: . func stringForOptionalExistence(optionalValue: String?) -&gt; String { if optionalValue != nil { return optionalValue } else { return &quot;Empty&quot; } } . In this particular case, we are returning optionalValue if it is not nil, and &quot;Empty&quot; if optionalValue is nil. The shorthand notation for this type of if(!=nil)-else statement is a nil coalescing operator of the form: optionalValue ?? nonOptionalValue . Example: . func stringForOptionalExistence(optionalValue: String?) -&gt; String { return optionalValue ?? &quot;Empty&quot; } . For Loops . Swift enables you to use ranges inside of for loops now: . for index in 1...5 { print(&quot; (index) times 5 is (index * 5)&quot;) } // Or if you don&#39;t need the value of the index let base = 3, power = 10 var answer = 1 for _ in 1...power { answer *= base } print(&quot; (base) to the power of (power) is (answer)&quot;) // prints &quot;3 to the power of 10 is 59049&quot; . Enumerating arrays &amp; dictionaries . // We explicitly cast to the Movie class from AnyObject class for movie in someObjects as [Movie] { // Code to execute each time } // Enumerating simple array let names = [&quot;Anna&quot;, &quot;Alex&quot;, &quot;Brian&quot;, &quot;Jack&quot;] for name in names { print(&quot;Hello, (name)!&quot;) } // Enumerating simple dictionary let numberOfLegs = [&quot;spider&quot;: 8, &quot;ant&quot;: 6, &quot;cat&quot;: 4] for (animalName, legCount) in numberOfLegs { print(&quot; (animalName)s have (legCount) legs&quot;) } . If you need to cast to a certain object type, see the earlier discussion about the as! and as? keywords. . While Loop . while someTestCondition { // Code to execute while the condition is true } . Repeat While Loop . repeat { // Code to execute while the condition is true } while someTestCondition . Switch . Switch statements are often used in place of if statements if there is a need to test if a certain variable matches the value of another constant or variable. For example, you may want to test if an error code integer you received matches an existing constant value or if it’s a new error code. . switch errorStatusCode { case .network: // Code to execute if it matches case .wifi: // Code to execute if it matches default: // Code to execute if nothing else matched } . Switch statements in Swift do not fall through the bottom of each case and into the next one by default. Instead, the entire switch statement finishes its execution as soon as the first matching switch case is completed, without requiring an explicit break statement. This makes the switch statement safer and easier to use than in C, and avoids executing more than one switch case by mistake. . Exiting Loops . Although break is not required in Swift, you can still use a break statement to match and ignore a particular case, or to break out of a matched case before that case has completed its execution. . return : Stops execution and returns to the calling function. It can also be used to return a value from a function. | break : Used to stop the execution of a loop. | . Back to top . Extension . Extensions add new functionality to an existing class, structure, enumeration or protocol type . extension String { // Extending String type to calculate if a String instance is truthy of falsy var boolValue:Bool { if self == &quot;1&quot; return true } return false } let isTrue = &quot;0&quot;.boolValue . Back to top . Protocol . Define . protocol Codable { // definitions var description: String var mustBeSettable: Int { get set } var doesNotNeedToBeSettable: Int { get } func dance () -&gt; Double static func someTypeMethod() mutating func toggle() // modify (or mutate) the instance it belongs to init(someParameter: Int) // require specific initializers } . Usage . import Foundation struct UserInfo: Codable { let username: String let loginCount: Int } extension UserInfo: CustomStringConvertible { var description: String { return &quot; (username) has tried to login (loginCount) time(s)&quot; } } . Protocol Extension . Protocols can be extended to provide method, initializer, subscript, and computed property implementations to conforming types. . Very Important and Useful: Implementation to any method or computed property requirement of that protocol can only be in extension . extension RandomNumberGenerator { func randomBool() -&gt; Bool { return random() &gt; 0.5 } } . By creating an extension on the protocol, all conforming types automatically gain this method implementation without any additional modification. . You can use protocol extensions to provide a default implementation to any method or computed property requirement of that protocol. . extension PrettyTextRepresentable { var prettyTextualDescription: String { return textualDescription } } . Back to top . Error Handling . Representing an Error . enum BeverageMachineError: Error { case invalidSelection case insufficientFunds case outOfStock } func selectBeverage (_ selection: Int) throws -&gt; String { // do something return &quot;Waiting for beverage...&quot; } // us do...catch to handle error throwed by func let message:String do { message = try selectBeverage(20) } catch BeverageMachineError.invalidSelection { print(&quot;Invalid selection&quot;) } catch BeverageMachineError.insufficientFunds { print(&quot;Insufficient Funds&quot;) } catch BeverageMachineError.outOfStock { print(&quot;Out of Stock&quot;) } catch { print (&quot;Generic error&quot;) } // if throw error, return nil let message = try? selectBeverage(10) // if throw error, get a runtime error let message = try! selectBeverage(10) . Back to top . Passing Information . Coming soon… . Back to top . User Defaults . User defaults are basically a way of storing simple preference values which can be saved and restored across app launches. It is not meant to be used as a data storage layer, like Core Data or sqlite. . Storing Values . let userDefaults = UserDefaults.standard userDefaults.setValue(&quot;Some Value&quot;, forKey: &quot;RPSomeUserPreference&quot;) . Retrieving Values . let userDefaults = UserDefaults.standard let someValue = userDefaults.value(forKey: &quot;RPSomeUserPreference&quot;) as AnyObject? . There are also other convenience functions on UserDefaults instances such as bool(forKey:...), string(forKey:...), etc. . Back to top . Common Patterns . For a comprehensive list of design patterns, as established by the Gang of Four, look here: Design Patterns in Swift . Singletons . Singleton’s are a special kind of class where only one instance of the class exists for the current process. They are a convenient way to share data between different parts of an app without creating global variables or having to pass the data around manually, but they should be used sparingly since they often create tighter coupling between classes. . To turn a class into a singleton, you use the following implementation where the function name is prefixed with shared plus another word which best describes your class. For example, if the class is a network or location manager, you would name the function sharedManager instead of sharedInstance. . class MyClass { // MARK: - Instantiation // Naming convention: // sharedInstance, sharedManager, sharedController, etc. // depending on the class type static let sharedInstance = MyClass() // This prevents others from using the default &#39;()&#39; initializer for this class. private init() {} var isReady = true // More class code here } . Explanation: The static constant sharedInstance is run as dispatch_once the first time that variable is accessed to make sure the initialization is atomic. This ensures it is thread safe, fast, lazy, and also bridged to ObjC for free. More from here. . Usage: You would get a reference to that singleton class in another class with the following code: . // Now you could do let myClass = MyClass.sharedInstance let answer = myClass.isReady ? &quot;Yep!&quot; : &quot;Nope!&quot; print(&quot;Are you ready to rock and roll? (answer)&quot;) . Back to top . Unicode Support . Although I don’t recommend this, Swift will compile even if you use emoji’s in your code since it offers Unicode support. . More info from Apple here . Back to top .",
            "url": "https://zqfang.github.io/coding/2020/05/30/Code-Swift-cheat-sheet.html",
            "relUrl": "/coding/2020/05/30/Code-Swift-cheat-sheet.html",
            "date": " • May 30, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Boosting",
            "content": "提升（Boosting）方法： 通过改变训练样本的权重（概率分布），学习过个分类器，并将这些分类器线性组合，提高分类性能。 . 1. AdaBoost . AdaBoost通过提高被前一轮弱分类器错误分类样本的权值，从而降低被正确分类样本的权值，并采取甲醛多数表决的方法达到分类目的。 . 输入：训练数据集T={(x1,y1),(x2,y2),⋯ ,(xN,yN)}T= {(x_1, y_1), (x_2, y_2), cdots, (x_N, y_N) }T={(x1​,y1​),(x2​,y2​),⋯,(xN​,yN​)}, Y={−1,+1} mathcal{Y} = {-1,+1 }Y={−1,+1}; 输出：分类器G(x)G(x)G(x) . 1). 初始化训练数据权值分布 . D1=(w11,⋯ ,w1i,⋯ ,w1N),w1i=1N,i=1,2,⋯ ,ND_1 = (w_{11}, cdots, w_{1i}, cdots, w_{1N}), w_{1i} = frac{1}{N}, i = 1,2, cdots,ND1​=(w11​,⋯,w1i​,⋯,w1N​),w1i​=N1​,i=1,2,⋯,N . 2) 对 m=1，2，⋯ ,Mm = 1，2， cdots, Mm=1，2，⋯,M a.对权值分布DmD_mDm​的训练数据集学习，得到基本分类器 . Gm(x):X→{−1,+1}G_{m}(x): mathcal{X} rightarrow {-1,+1 }Gm​(x):X→{−1,+1} . b.计算G(x)G(x)G(x)在训练数据集上的分类误差率 . em=∑i=1NP(Gm(xi)≠yi)=∑i=1NwmiI(Gm(xi)≠yi)e_{m}= sum_{i=1}^{N} P left(G_{m} left(x_{i} right) neq y_{i} right)= sum_{i=1}^{N} w_{m i} I left(G_{m} left(x_{i} right) neq y_{i} right)em​=i=1∑N​P(Gm​(xi​)​=yi​)=i=1∑N​wmi​I(Gm​(xi​)​=yi​) . c. 计算G(x)G(x)G(x)的系数 . αm=12log⁡1−emem alpha_{m}= frac{1}{2} log frac{1-e_{m}}{e_{m}}αm​=21​logem​1−em​​ . d. 更新训练数据的权值分布 . Dm+1=(wm+1,1,⋯ ,wm+1,i,⋯ ,wm+1,N)wm+1,i=wmiZmexp⁡(−αmyiGm(xi)),i=1,2,⋯ ,N begin{array}{c} D_{m+1}= left(w_{m+1,1}, cdots, w_{m+1, i}, cdots, w_{m+1, N} right) w_{m+1, i}= frac{w_{m i}}{Z_{m}} exp left(- alpha_{m} y_{i} G_{m} left(x_{i} right) right), quad i=1,2, cdots, N end{array}Dm+1​=(wm+1,1​,⋯,wm+1,i​,⋯,wm+1,N​)wm+1,i​=Zm​wmi​​exp(−αm​yi​Gm​(xi​)),i=1,2,⋯,N​ . 其中， . Zm=∑i=1Nwmiexp⁡(−αmyiGm(xi))Z_{m}= sum_{i=1}^{N} w_{m i} exp left(- alpha_{m} y_{i} G_{m} left(x_{i} right) right)Zm​=i=1∑N​wmi​exp(−αm​yi​Gm​(xi​)) . 3）构建基本线性分类器组合 . f(x)=∑m=1MαmGm(x)f(x)= sum_{m=1}^{M} alpha_{m} G_{m}(x)f(x)=m=1∑M​αm​Gm​(x) . 得到最终分类器 . G(x)=sign⁡(f(x))=sign⁡(∑m=1MαmGm(x)) begin{aligned} G(x) &amp;= operatorname{sign}(f(x)) &amp;= operatorname{sign} left( sum_{m=1}^{M} alpha_{m} G_{m}(x) right) end{aligned}G(x)​=sign(f(x))=sign(m=1∑M​αm​Gm​(x))​ . 1.1 AdaBoost算法误差 . AdaBoost算法最终分类器训练误差界为 . 1N∑i=1NI(G(xi)≠yi)⩽1N∑iexp⁡(−yif(xi))=∏mZm frac{1}{N} sum_{i=1}^{N} I left(G left(x_{i} right) neq y_{i} right) leqslant frac{1}{N} sum_{i} exp left(-y_{i} f left(x_{i} right) right)= prod_{m} Z_{m}N1​i=1∑N​I(G(xi​)​=yi​)⩽N1​i∑​exp(−yi​f(xi​))=m∏​Zm​ . 这一定理说明，每一轮选取适当的GmG_mGm​使ZmZ_mZm​最小，从而使训练误差下降最快。 . 对于二分类问题： . ∏m=1MZm=∏m=1M[2em(1−em)]=∏m=1M(1−4γm2)⩽exp⁡(−2∑m=1Mγm2) begin{aligned} prod_{m=1}^{M} Z_{m} &amp;= prod_{m=1}^{M}[2 sqrt{e_{m} left(1-e_{m} right)}] &amp;= prod_{m=1}^{M} sqrt{ left(1-4 gamma_{m}^{2} right)} &amp; leqslant exp left(-2 sum_{m=1}^{M} gamma_{m}^{2} right) end{aligned}m=1∏M​Zm​​=m=1∏M​[2em​(1−em​) . ​]=m=1∏M​(1−4γm2​) . ​⩽exp(−2m=1∑M​γm2​)​ . 其中， γm=12−em gamma_{m}= frac{1}{2}-e_{m}γm​=21​−em​ . 1.2 AdaBoost算法解释 . AdaBooost可以认为：模型为加法模型，损失函数为指数函数，学习算法为前向分布算法的二分类学习方法 . 1.2.1 前向分步算法 . 考虑加法模型（additive model） . f(x)=∑m=1Mβmb(x;γm)f(x)= sum_{m=1}^{M} beta_{m} b left(x ; gamma_{m} right)f(x)=m=1∑M​βm​b(x;γm​) . 其中，b(x;γm)b(x; gamma_m)b(x;γm​)为基函数，gammamgamma_mgammam​为参数， βm beta_mβm​为系数。 . 在给定训练集和损失函数L(y,f(x))L(y,f(x))L(y,f(x))的条件下，学习加法模型f(x)f(x)f(x)成为经验风险极小化（损失函数极小化）问题： . min⁡βm,γm∑i=1NL(yi,∑m=1Mβmb(xi;γm)) min _{ beta_{m}, gamma_{m}} sum_{i=1}^{N} L left(y_{i}, sum_{m=1}^{M} beta_{m} b left(x_{i} ; gamma_{m} right) right)βm​,γm​min​i=1∑N​L(yi​,m=1∑M​βm​b(xi​;γm​)) . 前向分布算法思想是： 从前向后，每一步只学一个基函数及其系数，逐步逼近优化目标函数，达到优化步骤简化的目的。 . 因此，每一步只需优化如下损失函数： . min⁡β,γ∑i=1NL(yi,βb(xi;γ)) min _{ beta, gamma} sum_{i=1}^{N} L left(y_{i}, beta b left(x_{i} ; gamma right) right)β,γmin​i=1∑N​L(yi​,βb(xi​;γ)) . 算法步骤 . 输入：训练数据集T={(x1,y1),(x2,y2),⋯ ,(xN,yN)}T= {(x_1, y_1), (x_2, y_2), cdots, (x_N, y_N) }T={(x1​,y1​),(x2​,y2​),⋯,(xN​,yN​)}, 损失函数L(y,f(x))L(y,f(x))L(y,f(x));基函数集{b(x;γ} {b(x; gamma }{b(x;γ}; 输出：加法模型f(x)f(x)f(x) . 1）初始化f0(x)=0f_0(x) = 0f0​(x)=0 2) 对m=1,2,⋯ ,Mm = 1,2, cdots, Mm=1,2,⋯,M a.极小化损失函数 . (βm,γm)=arg⁡min⁡β,γ∑i=1NL(yi,fm−1(xi)+βb(xi;γ)) left( beta_{m}, gamma_{m} right)= arg min _{ beta, gamma} sum_{i=1}^{N} L left(y_{i}, f_{m-1} left(x_{i} right)+ beta b left(x_{i} ; gamma right) right)(βm​,γm​)=argβ,γmin​i=1∑N​L(yi​,fm−1​(xi​)+βb(xi​;γ)) . 得到参数βm beta_mβm​, γm gamma_mγm​。 . b.更新 . fm(x)=fm−1(x)+βmb(x;γm)f_{m}(x)=f_{m-1}(x)+ beta_{m} b left(x ; gamma_{m} right)fm​(x)=fm−1​(x)+βm​b(x;γm​) . 3）得到加法模型 . f(x)=fM(x)=∑m=1Mβmb(x;γm)f(x)=f_{M}(x)= sum_{m=1}^{M} beta_{m} b left(x ; gamma_{m} right)f(x)=fM​(x)=m=1∑M​βm​b(x;γm​) . 2. Boosting Tree . 提升树🌲是以决策树为基本分类器的提升方法 . 2.1 提升树模型 . 采用加法模型（基函数的线性组合）与前向分布算法： . fM(x)=∑m=1MT(x;Θm)f_{M}(x)= sum_{m=1}^{M} T left(x ; Theta_{m} right)fM​(x)=m=1∑M​T(x;Θm​) . 其中 T(x;Θm)T left(x ; Theta_{m} right)T(x;Θm​)表示决策树，Θm Theta_{m}Θm​决策树参数， MMM为树的个数 . 2.2 提升树算法 . 采用加法模型和前向分布算法实现学习优化的过程。 . 首先确定提升树f0(x)=0f_{0}(x)=0f0​(x)=0， 第mmm步的模型是 . fm(x)=fm−1(x)+T(x;Θm)f_{m}(x)=f_{m-1}(x)+T left(x ; Theta_{m} right)fm​(x)=fm−1​(x)+T(x;Θm​) . 其中， fm−1(x)f_{m-1}(x)fm−1​(x)为当前模型，通过经验风险极小化确定下一刻决策树的参数Θm Theta_{m}Θm​： . Θ^m=arg⁡min⁡Θm∑i=1NL(yi,fm−1(xi)+T(xi;Θm)) hat{ Theta}_{m}= arg min _{ Theta_{m}} sum_{i=1}^{N} L left(y_{i}, f_{m-1} left(x_{i} right)+T left(x_{i} ; Theta_{m} right) right)Θ^m​=argΘm​min​i=1∑N​L(yi​,fm−1​(xi​)+T(xi​;Θm​)) . 2.2.1 回归问题提升树 . 训练数据集T={(x1,y1),(x2,y2),⋯ ,(xN,yN)}T= {(x_1, y_1), (x_2, y_2), cdots, (x_N, y_N) }T={(x1​,y1​),(x2​,y2​),⋯,(xN​,yN​)}, xi∈X⊆Rnx_{i} in mathcal{X} subseteq mathbf{R}^{n}xi​∈X⊆Rn,X mathcal{X}X为输入空间， Y⊆R mathcal{Y} subseteq mathbf{R}Y⊆R; . 将输入空间划分为JJJ个互不相交的区域R1，R2,⋯ ,RJR1，R2, cdots, R_JR1，R2,⋯,RJ​， 并且每个区域上确定输出的常量cjc_jcj​，那么树可以表示为： . T(x;Θ)=∑j=1JcjI(x∈Rj)T(x ; Theta)= sum_{j=1}^{J} c_{j} I left(x in R_{j} right)T(x;Θ)=j=1∑J​cj​I(x∈Rj​) . 其中， . Θ={(R1,c1),(R2,c2),⋯ ,(RJ,cJ)} Theta= left { left(R_{1}, c_{1} right), left(R_{2}, c_{2} right), cdots, left(R_{J}, c_{J} right) right }Θ={(R1​,c1​),(R2​,c2​),⋯,(RJ​,cJ​)} 表示树的却与划分和各个取悦是那个的常数。 . 采用一下前向分布算法 . f0(x)=0fm(x)=fm−1(x)+T(x;Θm),m=1,2,⋯ ,MfM(x)=∑m=1MT(x;Θm) begin{aligned} &amp;f_{0}(x)=0 &amp; begin{array}{l} f_{m}(x)=f_{m-1}(x)+T left(x ; Theta_{m} right), quad m=1,2, cdots, M f_{M}(x)= sum_{m=1}^{M} T left(x ; Theta_{m} right) end{array} end{aligned}​f0​(x)=0fm​(x)=fm−1​(x)+T(x;Θm​),m=1,2,⋯,MfM​(x)=∑m=1M​T(x;Θm​)​​ . 求解Θ^m hat{ Theta}_{m}Θ^m​， 若用平方误差损失函数： . L(y,f(x))=(y−f(x))2L(y, f(x))=(y-f(x))^{2}L(y,f(x))=(y−f(x))2 . 则损失函数为： . L(y,fm−1(x)+T(x;Θm))=[y−fm−1(x)−T(x;Θm)]2=[r−T(x;Θm)]2 begin{aligned} L left(y, f_{m-1}(x)+T left(x ; Theta_{m} right) right) &amp;= left[y-f_{m-1}(x)-T left(x ; Theta_{m} right) right]^{2} &amp;= left[r-T left(x ; Theta_{m} right) right]^{2} end{aligned}L(y,fm−1​(x)+T(x;Θm​))​=[y−fm−1​(x)−T(x;Θm​)]2=[r−T(x;Θm​)]2​ . 这里， . r=y−fm−1(x)r=y-f_{m-1}(x)r=y−fm−1​(x) . 是当前模型拟合数据的残差（residual）。因此对于回归问题提升树，只需拟合当前模型残差。得到T(x;Θm)T left(x ; Theta_{m} right)T(x;Θm​)，更新模型，得到fm(x)f_m(x)fm​(x)。 . 3. 梯度提升 . 当损失函数不是简单的平方损失、指数损失时，提升树的优化就很难。梯度提升算法利用最速下降法的近似方法，计算损失函数的负梯度在当前模型的值 . −[∂L(y,f(xi))∂f(xi)]f(x)=fm−1(x)- left[ frac{ partial L left(y, f left(x_{i} right) right)}{ partial f left(x_{i} right)} right]_{f(x)=f_{m-1}(x)}−[∂f(xi​)∂L(y,f(xi​))​]f(x)=fm−1​(x)​ . 并将其作为回归问题提升树算法中的残差近似值，拟合一个回归树。 . 输入： 训练数据集T={(x1,y1),(x2,y2),⋯ ,(xN,yN)}T= {(x_1, y_1), (x_2, y_2), cdots, (x_N, y_N) }T={(x1​,y1​),(x2​,y2​),⋯,(xN​,yN​)}, xi∈X⊆Rnx_{i} in mathcal{X} subseteq mathbf{R}^{n}xi​∈X⊆Rn,X mathcal{X}X为输入空间， Y⊆R mathcal{Y} subseteq mathbf{R}Y⊆R; 损失函数L(y,f(x))L(y,f(x))L(y,f(x)) 输出： 回归树f^(x) hat f(x)f^​(x) . 1）初始化 . f0(x)=arg⁡min⁡c∑i=1NL(yi,c)f_{0}(x)= arg min _{c} sum_{i=1}^{N} L left(y_{i}, c right)f0​(x)=argcmin​i=1∑N​L(yi​,c) . 2) 对 m=1，2，⋯ ,Mm=1，2， cdots, Mm=1，2，⋯,M a. 对i=1，2，⋯ ,Ni=1，2， cdots, Ni=1，2，⋯,N计算 . rmi=−[∂L(yi,f(xi))∂f(xi)]f(x)=fm−1(x)r_{m i}=- left[ frac{ partial L left(y_{i}, f left(x_{i} right) right)}{ partial f left(x_{i} right)} right]_{f(x)=f_{m-1}(x)}rmi​=−[∂f(xi​)∂L(yi​,f(xi​))​]f(x)=fm−1​(x)​ . b) 对rmir_{mi}rmi​拟合一个回归树，得到第mmm颗树的节点区域RmjR_{mj}Rmj​ . c) 对j=1,2,⋯ ,Jj=1,2, cdots, Jj=1,2,⋯,J, 计算 . cmj=arg⁡min⁡c∑xi∈RmjL(yi,fm−1(xi)+c)c_{m j}= arg min _{c} sum_{x_{i} in R_{m j}} L left(y_{i}, f_{m-1} left(x_{i} right)+c right)cmj​=argcmin​xi​∈Rmj​∑​L(yi​,fm−1​(xi​)+c) . d)更新 . fm(x)=fm−1(x)+∑j=1JcmjI(x∈Rmj)f_{m}(x)=f_{m-1}(x)+ sum_{j=1}^{J} c_{m j} I left(x in R_{m j} right)fm​(x)=fm−1​(x)+j=1∑J​cmj​I(x∈Rmj​) . 3)得到回归树 . f^(x)=fM(x)=∑m=1M∑j=1JcmjI(x∈Rmj) hat{f}(x)=f_{M}(x)= sum_{m=1}^{M} sum_{j=1}^{J} c_{m j} I left(x in R_{m j} right)f^​(x)=fM​(x)=m=1∑M​j=1∑J​cmj​I(x∈Rmj​) . 参考： 李航《统计学习方法》 .",
            "url": "https://zqfang.github.io/machine%20learning/2020/05/05/ML-Boosting.html",
            "relUrl": "/machine%20learning/2020/05/05/ML-Boosting.html",
            "date": " • May 5, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Hidden Markov Model (HMM)",
            "content": "隐马可夫模型（HMM）描述隐藏的马可夫链随机生成观测序列的过程，属于生成模型。 HMM在语音识别、自然语言处理、生物信息、模式识别等领域由广泛应用。 . 1. HMM的定义 . 隐马可夫模型是关于时序的概率模型， 描述由一个隐藏的马可夫链随机生成不可观测的状态，再由各个状态生成一个观测，从而产生观测随机序列的过程。 . 简而言之，隐马可夫链随机成状态序列（state sequence），而每个状态生成观测，产生观测序列（observation sequence）。序列的一个位置可以看作一个时刻。 . 令QQQ 表示所有可能状态的集合：Q={q1,q2,⋯ ,qN}Q = { q_1, q_2, cdots, q_N }Q={q1​,q2​,⋯,qN​}; 令VVV 表示所有可能的观测集合：V={v1,v2,⋯ ,vM}V = {v_1, v_2, cdots, v_M }V={v1​,v2​,⋯,vM​}; 令III 表示长度为T的状态序列： I=(i1,i2,⋯ ,iT)I = (i_1, i_2, cdots, i_T)I=(i1​,i2​,⋯,iT​); 令OOO 表示对应的是观测序列： O=(o1,o2,⋯ ,oT)O = (o_1, o_2, cdots, o_T)O=(o1​,o2​,⋯,oT​). . 令AAA是转移概率矩阵： . A=[aij]N×NA = [a_{ij}]_{N times N}A=[aij​]N×N​ . 其中， . aij=P(it+1=qj∣it=qj),i=1,2,⋯ ,N;j=1,2,⋯ ,Na_{ij} = P(i_{t+1} = q_j | i_t = q_j), i=1,2, cdots, N; j = 1,2, cdots, Naij​=P(it+1​=qj​∣it​=qj​),i=1,2,⋯,N;j=1,2,⋯,N . 是在时刻ttt处于状态qiq_iqi​的条件下生成观测t+1t +1t+1转移到状态qjq_jqj​的概率。 . 令BBB是观测概率矩阵： . B=[bj(k)]N×MB = [b_j(k)]_{N times M}B=[bj​(k)]N×M​ . 其中， . bj(k)=P(ot=vk∣it=qj),k=1,2,⋯ ,M;j=1,2,⋯ ,Nb_j(k) = P(o_t = v_k | i_t = q_j), k=1,2, cdots, M; j=1,2, cdots, Nbj​(k)=P(ot​=vk​∣it​=qj​),k=1,2,⋯,M;j=1,2,⋯,N . 是在时刻ttt处于状态qjq_jqj​的条件下生成观测vkv_kvk​ 的概率。 . 令π piπ是初始状态概率向量： . π=(πi) pi = ( pi_i)π=(πi​) . 其中， . πi=P(i1=qi),i=1,2,⋯ ,N pi_{i} = P(i_1 = q_i),i=1,2, cdots, Nπi​=P(i1​=qi​),i=1,2,⋯,N . 是时刻t=1处于状态qiq_iqi​的概率. . 隐马可夫模型λ lambdaλ由π piπ， AAA，BBB决定。 . λ=(A,B,π) lambda = (A, B, pi)λ=(A,B,π) . 其中，π piπ和AAA决定状态序列，BBB决定观测序列。 . 隐马可夫模型的两个基本假设 . 齐次马可夫性 隐马可夫链在任意时刻t的状态前一时刻状态，与其他时刻的隐状态和观测无关， 也与时刻t无关： | . | P(it∣it−1,Ot−1,⋯ ,i1,o1)=P(it∣it−1),t=1,2,⋯ ,TP(i_t | i_{t-1}, O_{t-1}, cdots, i_1, o_1) = P(i_t | i_{t-1}), t = 1,2, cdots,TP(it​∣it−1​,Ot−1​,⋯,i1​,o1​)=P(it​∣it−1​),t=1,2,⋯,T . 观测独立性 任意时刻的观测只依赖改时刻的马可夫链状态，与其他观测和状态无关: | . P(ot∣iT,OT,iT−1,oT−1⋯ ,it+1,Ot+1,it−1,Ot−1,i1,o1)=P(ot∣it)P(o_t | i_{T}, O_{T}, i_{T-1}, o_{T-1} cdots, i_{t+1}, O_{t+1}, i_{t-1}, O_{t-1}, i_1, o_1) = P(o_t | i_{t})P(ot​∣iT​,OT​,iT−1​,oT−1​⋯,it+1​,Ot+1​,it−1​,Ot−1​,i1​,o1​)=P(ot​∣it​) | 2. HMM的3个基本问题 . 概率计算：给定模型λ=(A,B,π) lambda = (A, B, pi)λ=(A,B,π)和观测序列 O=(o1,o2,⋯ ,oT)O = (o_1, o_2, cdots, o_T)O=(o1​,o2​,⋯,oT​)， 求概率P(O∣λ)P(O | lambda)P(O∣λ) . 学习: 已知观测序列O=(o1,o2,⋯ ,oT)O = (o_1, o_2, cdots, o_T)O=(o1​,o2​,⋯,oT​)，估计模型参数λ=(A,B,π) lambda = (A, B, pi)λ=(A,B,π)， 使概率P(O∣λ)P(O vert lambda)P(O∣λ)最大（用极大似然估计）。 . 预测：给定模型λ=(A,B,π) lambda = (A, B, pi)λ=(A,B,π)和观测序列 O=(o1,o2,⋯ ,oT)O = (o_1, o_2, cdots, o_T)O=(o1​,o2​,⋯,oT​)，求条件概率P(I∣O)P(I | O)P(I∣O)最大的状态序列 I=(i1,i2,⋯ ,iT)I = (i_1, i_2, cdots, i_T)I=(i1​,i2​,⋯,iT​). . 2.1 概率计算前向（forward）和后向（backward）算法 . 2.1.1 前向算法 . 给定模型λ lambdaλ，当时刻ttt时，状态为qiq_iqi​，部分观测序列为o1,o2,⋯ ,oto_1, o_2, cdots, o_to1​,o2​,⋯,ot​，记： . αt(i)=P(o1,o2,⋯ ,ot,it=qi∣λ) alpha_{t}(i) = P(o_1, o_2, cdots, o_t, i_t = q_i | lambda)αt​(i)=P(o1​,o2​,⋯,ot​,it​=qi​∣λ) . 输入： 隐马可夫模型 λ lambdaλ， 观测序列OOO; 输出： 观测序列概率P(O∣λ)P(O | lambda)P(O∣λ) . （1）初值 . α1(i)=πibi(o1),i=1,2,⋯ ,N alpha_{1}(i)= pi_{i} b_{i} left(o_{1} right), quad i=1,2, cdots, Nα1​(i)=πi​bi​(o1​),i=1,2,⋯,N . （2）递推 对 t=1,2,⋯ ,T−1,t = 1,2, cdots, T-1,t=1,2,⋯,T−1, . αt+1(i)=[∑j=1Nαt(j)aji]bi(ot+1),i=1,2,⋯ ,N alpha_{t+1}(i)= left[ sum_{j=1}^{N} alpha_{t}(j) a_{j i} right] b_{i} left(o_{t+1} right), quad i=1,2, cdots, Nαt+1​(i)=[j=1∑N​αt​(j)aji​]bi​(ot+1​),i=1,2,⋯,N . （3）终止 . P(O∣λ)=∑i=1NαT(i)P(O | lambda)= sum_{i=1}^{N} alpha_{T}(i)P(O∣λ)=i=1∑N​αT​(i) . 2.1.2 后向算法 . 给定模型λ lambdaλ，当时刻ttt时，状态为qiq_iqi​，部分观测序列为o1,o2,⋯ ,oto_1, o_2, cdots, o_to1​,o2​,⋯,ot​，记： . βt(i)=P(ot+1,ot+2,⋯ ,oT∣it=qi,λ) beta_{t}(i) = P(o_{t+1}, o_{t+2}, cdots, o_T | i_t = q_i, lambda)βt​(i)=P(ot+1​,ot+2​,⋯,oT​∣it​=qi​,λ) . 输入： 隐马可夫模型 λ lambdaλ， 观测序列OOO; 输出： 观测序列概率P(O∣λ)P(O | lambda)P(O∣λ) . （1）初始 令最终时刻所有状态qiq_iqi​ . βT(i)=1,i=1,2,⋯ ,N beta_T(i) = 1, i=1,2, cdots, NβT​(i)=1,i=1,2,⋯,N . （2）递推 对t=T−1,T−2,⋯ ,1t=T-1, T-2, cdots, 1t=T−1,T−2,⋯,1 . βt(i)=∑j=1Naijbj(ot+1)βt+1(j),i=1,2,⋯ ,N beta_{t}(i)= sum_{j=1}^{N} a_{i j} b_{j} left(o_{t+1} right) beta_{t+1}(j), quad i=1,2, cdots, Nβt​(i)=j=1∑N​aij​bj​(ot+1​)βt+1​(j),i=1,2,⋯,N . （3）终止 . P(O∣λ)=∑i=1Nπibi(o1)β1(i)P(O | lambda)= sum_{i=1}^{N} pi_{i} b_{i} left(o_{1} right) beta_{1}(i)P(O∣λ)=i=1∑N​πi​bi​(o1​)β1​(i) . 利用前后向概率定义，可以将观测序列概率P(O∣λ)P(O vert lambda)P(O∣λ)统一写成 . P(O∣λ)=∑i=1N∑j=1Nαt(i)aijbj(ot+1)βt+1(j),t=1,2,⋯ ,T−1P(O | lambda)= sum_{i=1}^{N} sum_{j=1}^{N} alpha_{t}(i) a_{i j} b_{j} left(o_{t+1} right) beta_{t+1}(j), quad t=1,2, cdots, T-1P(O∣λ)=i=1∑N​j=1∑N​αt​(i)aij​bj​(ot+1​)βt+1​(j),t=1,2,⋯,T−1 . 2.2 学习问题的EM算法 . 实质上求一个隐变量的概率模型的参数估计： . P(O∣λ)=∑IP(O∣I,λ)P(I∣λ)P(O | lambda)= sum_{I} P(O | I, lambda) P(I | lambda)P(O∣λ)=I∑​P(O∣I,λ)P(I∣λ) . 参数估计由EM算法实现: (待续) . 输入：观测数据O=(o1,o2,⋯ ,oTO = (o_1, o_2, cdots, o_TO=(o1​,o2​,⋯,oT​; 输出：隐马可夫模型参数 . （1）初始化 对 n=0， 选取aij(0)a_{ij}^{(0)}aij(0)​,bj(k)(0)b_{j}(k)^{(0)}bj​(k)(0),πi(0) pi_{i}^{(0)}πi(0)​,得到模型λ=(A(0),B(0),π(0)) lambda = (A^{(0)}, B^{(0)}, pi^{(0)})λ=(A(0),B(0),π(0)) （2）递推 对n=1,2,⋯ ,n=1,2, cdots,n=1,2,⋯, . aij(n+1)=∑t=1T−1ξt(i,j)∑t=1T−1γt(i)a_{i j}^{(n+1)}= frac{ sum_{t=1}^{T-1} xi_{t}(i, j)}{ sum_{t=1}^{T-1} gamma_{t}(i)}aij(n+1)​=∑t=1T−1​γt​(i)∑t=1T−1​ξt​(i,j)​ . bj(k)(n+1)=∑t=1,ot=vkTγt(j)∑t=1Tγt(j)b_{j}(k)^{(n+1)}= frac{ sum_{t=1, o_{t}=v_{k}}^{T} gamma_{t}(j)}{ sum_{t=1}^{T} gamma_{t}(j)}bj​(k)(n+1)=∑t=1T​γt​(j)∑t=1,ot​=vk​T​γt​(j)​ . πi(n+1)=γ1(i) pi_{i}^{(n+1)}= gamma_{1}(i)πi(n+1)​=γ1​(i) . 其中，时刻ttt处于qiq_iqi​，且时刻t+1t+1t+1处于状态qjq_jqj​的概率, 记 . ξt(i,j)=P(it=qi,it+1=qj∣O,λ) xi_{t}(i, j)=P left(i_{t}=q_{i}, i_{t+1}=q_{j} | O, lambda right)ξt​(i,j)=P(it​=qi​,it+1​=qj​∣O,λ) . 那么 . ξt(i,j)=P(it=qi,it+1=qj,O∣λ)P(O∣λ)=P(it=qi,it+1=qj,O∣λ)∑i=1N∑j=1NP(it=qi,it+1=qj,O∣λ) xi_{t}(i, j)= frac{P left(i_{t}=q_{i}, i_{t+1}=q_{j}, O | lambda right)}{P(O | lambda)}= frac{P left(i_{t}=q_{i}, i_{t+1}=q_{j}, O | lambda right)}{ sum_{i=1}^{N} sum_{j=1}^{N} P left(i_{t}=q_{i}, i_{t+1}=q_{j}, O | lambda right)}ξt​(i,j)=P(O∣λ)P(it​=qi​,it+1​=qj​,O∣λ)​=∑i=1N​∑j=1N​P(it​=qi​,it+1​=qj​,O∣λ)P(it​=qi​,it+1​=qj​,O∣λ)​ . 和时刻ttt处于qiq_iqi​的概率， 有 . γt(i)=P(it=qi∣O,λ)=P(it=qi,O∣λ)P(O∣λ) gamma_{t}(i)=P left(i_{t}=q_{i} | O, lambda right)= frac{P left(i_{t}=q_{i}, O | lambda right)}{P(O | lambda)}γt​(i)=P(it​=qi​∣O,λ)=P(O∣λ)P(it​=qi​,O∣λ)​ . （3）终止 . 得到模型参数λ(n+1)=(A(n+1),B(n+1),π(n+1)) lambda^{(n+1)} = (A^{(n+1)}, B^{(n+1)}, pi^{(n+1)})λ(n+1)=(A(n+1),B(n+1),π(n+1)) . 2.3 预测算法 . 包括近似算法和维特比算法（Viterbi algorithm） . 2.3.1 近似算法 . 在每个时刻ttt， 选择在该时刻最可能出现的状态it∗i^*_tit∗​，从而得到一个状态序列I∗=(ii∗,ii∗,⋯ ,iT∗)I^* = (i^*_i, i^*_i, cdots, i^*_T)I∗=(ii∗​,ii∗​,⋯,iT∗​)，将它最为预测结果。 . 给定模型λ lambdaλ和观测序列OOO， 在时刻ttt处于状态qiq_iqi​的概率γt(i) gamma_t(i)γt​(i)是 . γt(i)=αt(i)βt(i)P(O∣λ)=αt(i)βt(i)∑j=1Nαt(j)βt(j) gamma_{t}(i)= frac{ alpha_{t}(i) beta_{t}(i)}{P(O | lambda)}= frac{ alpha_{t}(i) beta_{t}(i)}{ sum_{j=1}^{N} alpha_{t}(j) beta_{t}(j)}γt​(i)=P(O∣λ)αt​(i)βt​(i)​=∑j=1N​αt​(j)βt​(j)αt​(i)βt​(i)​ . 而每一时刻ttt最有可能的状态it∗i_{t}^{*}it∗​是 . it∗=arg⁡max⁡1⩽i⩽N[γt(i)],t=1,2,⋯ ,Ti_{t}^{*}= arg max _{1 leqslant i leqslant N} left[ gamma_{t}(i) right], quad t=1,2, cdots, Tit∗​=arg1⩽i⩽Nmax​[γt​(i)],t=1,2,⋯,T . 从而得到状态序列I∗=(ii∗,ii∗,⋯ ,iT∗)I^* = (i^*_i, i^*_i, cdots, i^*_T)I∗=(ii∗​,ii∗​,⋯,iT∗​) . 缺点： 不能保证预测状态序列整体是最有可能的状态序列，因为预测的状态序列实际可能由不发生的部分。 . 2.3.2 维特比算法 . 实质是运用动态规划求概率最大路径，从而解决HMM的预测问题 . 只需从时刻t=1t=1t=1开始，递推地计算在时刻ttt状态为qiq_iqi​的各条部分路径的最大概率，直至得到时刻t=Tt = Tt=T状态为iii的各条路径的最大概率。时刻 t=Tt = Tt=T 的最大概率即为最优路径的概率P∗P^*P∗, 最优路径的终结点iT∗i^*_TiT∗​也同时得到。之后，为了找出最优路径的各个结点，从终结点iT∗i^*_TiT∗​开始，由后向前逐步求得结点 iT−1∗,⋯ ,i1∗i^*_{T-1}, cdots, i^*_1iT−1∗​,⋯,i1∗​，得到最优路径 I∗=(ii∗,ii∗,⋯ ,iT∗)I^* = (i^*_i, i^*_i, cdots, i^*_T)I∗=(ii∗​,ii∗​,⋯,iT∗​)。这就是维特比算法。 . 定义在时刻t状态i的所有单个路径中概率最大值为 . δt(i)=max⁡i1,i2,⋯ ,it−1P(it=i,it−1,⋯ ,i1,ot,⋯ ,o1∣λ),i=1,2,⋯ ,N delta_{t}(i)= max _{i_{1}, i_{2}, cdots, i_{t-1}} P left(i_{t}=i, i_{t-1}, cdots, i_{1}, o_{t}, cdots, o_{1} | lambda right), quad i=1,2, cdots, Nδt​(i)=i1​,i2​,⋯,it−1​max​P(it​=i,it−1​,⋯,i1​,ot​,⋯,o1​∣λ),i=1,2,⋯,N . 因此 . δt+1(i)=max⁡i1,i2,⋯ ,itP(it+1=i,it,⋯ ,i1,ot+1,⋯ ,o1∣λ)=max⁡1⩽j⩽N[δt(j)aji]bi(ot+1),i=1,2,⋯ ,N;t=1,2,⋯ ,T−1 begin{aligned} delta_{t+1}(i) &amp;= max _{i_{1}, i_{2}, cdots, i_{t}} P left(i_{t+1}=i, i_{t}, cdots, i_{1}, o_{t+1}, cdots, o_{1} | lambda right) &amp;= max _{1 leqslant j leqslant N} left[ delta_{t}(j) a_{j i} right] b_{i} left(o_{t+1} right), quad i=1,2, cdots, N ; quad t=1,2, cdots, T-1 end{aligned}δt+1​(i)​=i1​,i2​,⋯,it​max​P(it+1​=i,it​,⋯,i1​,ot+1​,⋯,o1​∣λ)=1⩽j⩽Nmax​[δt​(j)aji​]bi​(ot+1​),i=1,2,⋯,N;t=1,2,⋯,T−1​ . 定义在时刻t状态i的所有单个路径中概率最大路径的第t-1 个节点为 . Ψt(i)=arg⁡max⁡1⩽j⩽N[δt−1(j)aji],i=1,2,⋯ ,N Psi_{t}(i)= arg max _{1 leqslant j leqslant N} left[ delta_{t-1}(j) a_{j i} right], quad i=1,2, cdots, NΨt​(i)=arg1⩽j⩽Nmax​[δt−1​(j)aji​],i=1,2,⋯,N . 输入： 隐马可夫模型 λ lambdaλ， 观测序列OOO; 输出： 最优路径I∗=(ii∗,ii∗,⋯ ,iT∗)I^* = (i^*_i, i^*_i, cdots, i^*_T)I∗=(ii∗​,ii∗​,⋯,iT∗​) . （1）初始化 . δ1(i)=πibi(o1),i=1,2,⋯ ,NΨ1(i)=0,i=1,2,⋯ ,N begin{array}{c} delta_{1}(i)= pi_{i} b_{i} left(o_{1} right), quad i=1,2, cdots, N Psi_{1}(i)=0, quad i=1,2, cdots, N end{array}δ1​(i)=πi​bi​(o1​),i=1,2,⋯,NΨ1​(i)=0,i=1,2,⋯,N​ . （2）递推 . δt(i)=max⁡1⩽j⩽N[δt−1(j)aji]bi(ot),i=1,2,⋯ ,NΨt(i)=arg⁡max⁡1⩽j⩽N[δt−1(j)aji],i=1,2,⋯ ,N begin{array}{c} delta_{t}(i)= max _{1 leqslant j leqslant N} left[ delta_{t-1}(j) a_{j i} right] b_{i} left(o_{t} right), quad i=1,2, cdots, N Psi_{t}(i)= arg max _{1 leqslant j leqslant N} left[ delta_{t-1}(j) a_{j i} right], quad i=1,2, cdots, N end{array}δt​(i)=max1⩽j⩽N​[δt−1​(j)aji​]bi​(ot​),i=1,2,⋯,NΨt​(i)=argmax1⩽j⩽N​[δt−1​(j)aji​],i=1,2,⋯,N​ . （3）终止 . P∗=max⁡1⩽i⩽NδT(i)iT∗=arg⁡max⁡1⩽i⩽N[δT(i)] begin{array}{c} P^* = max _{1 leqslant i leqslant N} delta_T(i) i^*_T = arg max _{1 leqslant i leqslant N} [ delta_T(i)] end{array}P∗=max1⩽i⩽N​δT​(i)iT∗​=argmax1⩽i⩽N​[δT​(i)]​ . （4）最优路径回溯 对t=T−1,T−2,⋯ ,1t=T-1, T-2, cdots, 1t=T−1,T−2,⋯,1 . it∗=Ψt+1(it+1∗)i^*_t = Psi_{t+1}(i^*_{t+1})it∗​=Ψt+1​(it+1∗​) . 参考： 李航《统计学习方法》 .",
            "url": "https://zqfang.github.io/machine%20learning/2020/05/03/ML-HMM.html",
            "relUrl": "/machine%20learning/2020/05/03/ML-HMM.html",
            "date": " • May 3, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Latent semantic analysis (LSA)",
            "content": "潜在语义分析（LSA）是一种非监督学习方法，用于文本话题分析。其特点是通过矩阵分解发现文本于单词之间的基于话题的语义关系。 . 潜在语义分析是一种非概率话题分析模型。步骤为： . 先将文本集合表示为单词-文本矩阵 | 对单词-文本矩阵进行奇异值分解（SVD）或非负矩阵分解（NMF），从而得到话题向量空间，以及文本在话题向量空间的表示 | 1. 单词向量空间 . 给定由n个文本的集合 D={d1,d2,⋯ ,dn}D = {d_1, d_2, cdots, d_n }D={d1​,d2​,⋯,dn​} ,以及所有文本中单词集 W={w1,w2,⋯ ,wm}W = {w_1, w_2, cdots, w_m }W={w1​,w2​,⋯,wm​}, 则单词在文本中出现的数据用单词-文本矩阵(word-document matrix)表示, 记为： . X=[x11x12⋯x1nx21x22⋯x2n⋮⋮⋮xm1xm2⋯xmn]X= left[ begin{array}{cccc} x_{11} &amp; x_{12} &amp; cdots &amp; x_{1 n} x_{21} &amp; x_{22} &amp; cdots &amp; x_{2 n} vdots &amp; vdots &amp; &amp; vdots x_{m 1} &amp; x_{m 2} &amp; cdots &amp; x_{m n} end{array} right]X=⎣⎢⎢⎢⎢⎡​x11​x21​⋮xm1​​x12​x22​⋮xm2​​⋯⋯⋯​x1n​x2n​⋮xmn​​⎦⎥⎥⎥⎥⎤​ . 其中， xijx_{ij}xij​表示单词wiw_iwi​在文本djd_jdj​中出现的频数或权值。由于单词种类多，而每个文本中单词种类通常少，所以为单词-文本矩阵是稀疏矩阵。 . 权值常用单词频率-逆文本频率(term frequeny-inverse document frequency, TF-IDF)表示: . TFIDFij=tfijtf⋅jlog⁡dfdfi,i=1,2,⋯ ,m;j=1,2,⋯ ,n mathrm{TFIDF}_{i j}= frac{ mathrm{tf}_{i j}}{ mathrm{tf}_{ cdot j}} log frac{ mathrm{df}}{ mathrm{df}_{i}}, quad i=1,2, cdots, m ; quad j=1,2, cdots, nTFIDFij​=tf⋅j​tfij​​logdfi​df​,i=1,2,⋯,m;j=1,2,⋯,n . 其中， tfij mathrm{tf}_{ij}tfij​ 是单词wiw_iwi​在文本djd_jdj​中的频数; tf⋅j mathrm{tf}_{ cdot j}tf⋅j​ 是djd_jdj​中出现的所有单词频数之和; dfi mathrm{df}_{i}dfi​ 是含有单词wiw_iwi​的文本数; df mathrm{df}df 是文本集合DDD的全部文本数。 . 单词-文本矩阵的第j列向量xjx_jxj​表示文本djd_jdj​： . xj=[x1jx2j⋮xmj],j=1,2,⋯ ,nx_{j}= left[ begin{array}{c} x_{1 j} x_{2 j} vdots x_{m j} end{array} right], quad j=1,2, cdots, nxj​=⎣⎢⎢⎢⎢⎡​x1j​x2j​⋮xmj​​⎦⎥⎥⎥⎥⎤​,j=1,2,⋯,n . 两个单词向量内积或者标准化内积表示对应文本之间的语义相似度， 因此文本did_idi​与djd_jdj​的之间的相似度： . xi⋅xj,xi⋅xj∥xi∥∥xj∥x_{i} cdot x_{j}, quad frac{x_{i} cdot x_{j}}{ left |x_{i} right | left |x_{j} right |}xi​⋅xj​,∥xi​∥∥xj​∥xi​⋅xj​​ . 单词向量空间模型的优缺点 . 优点： 简单，计算高效 | 缺点： 一词多义(polysemy)和多词一义(synonymy)不能很好处理， 存在相似度计算不准确的问题 | . 2. 话题（topic）向量空间 . 话题没有严格的定义，是指文本讨论的内容或主题。而基于话题的模型是为了解决单词向量不能很好处理一词多义(polysemy)和多词一义(synonymy)的问题。 . 给定由n个文本的集合 D={d1,d2,⋯ ,dn}D = {d_1, d_2, cdots, d_n }D={d1​,d2​,⋯,dn​}, 以及所有文本中单词集 W={w1,w2,⋯ ,wm}W = {w_1, w_2, cdots, w_m }W={w1​,w2​,⋯,wm​}, 则单词在文本中出现的数据用单词-文本矩阵(word-document matrix)表示, 记为： . X=[x11x12⋯x1nx21x22⋯x2n⋮⋮⋮xm1xm2⋯xmn]X= left[ begin{array}{cccc} x_{11} &amp; x_{12} &amp; cdots &amp; x_{1 n} x_{21} &amp; x_{22} &amp; cdots &amp; x_{2 n} vdots &amp; vdots &amp; &amp; vdots x_{m 1} &amp; x_{m 2} &amp; cdots &amp; x_{m n} end{array} right]X=⎣⎢⎢⎢⎢⎡​x11​x21​⋮xm1​​x12​x22​⋮xm2​​⋯⋯⋯​x1n​x2n​⋮xmn​​⎦⎥⎥⎥⎥⎤​ . 假设所有文本含有k个话题， 每个话题l由一个定义在单词集合W傻姑娘的m维向量： . tl=[t1lt2l⋮tml],l=1,2,⋯ ,kt_{l}= left[ begin{array}{c} t_{1 l} t_{2 l} vdots t_{m l} end{array} right], quad l=1,2, cdots, ktl​=⎣⎢⎢⎢⎢⎡​t1l​t2l​⋮tml​​⎦⎥⎥⎥⎥⎤​,l=1,2,⋯,k . 则有话题向量矩阵T： . T=[t11t12⋯t1kt21t22⋯t2k⋮⋮⋮tm1tm2⋯tmk]T= left[ begin{array}{cccc} t_{11} &amp; t_{12} &amp; cdots &amp; t_{1 k} t_{21} &amp; t_{22} &amp; cdots &amp; t_{2 k} vdots &amp; vdots &amp; &amp; vdots t_{m 1} &amp; t_{m 2} &amp; cdots &amp; t_{m k} end{array} right]T=⎣⎢⎢⎢⎢⎡​t11​t21​⋮tm1​​t12​t22​⋮tm2​​⋯⋯⋯​t1k​t2k​⋮tmk​​⎦⎥⎥⎥⎥⎤​ . 3. 文本在话题向量空间的表示 . 文本djd_jdj​在单词向量空间用xjx_jxj​表示，将xjx_jxj​投影到话题向量空间TTT，得到话题向量空间yjy_jyj​，yjy_jyj​是个k维向量: . yj=[y1jy2j⋮ykj],j=1,2,⋯ ,ny_{j}= left[ begin{array}{c} y_{1 j} y_{2 j} vdots y_{k j} end{array} right], quad j=1,2, cdots, nyj​=⎣⎢⎢⎢⎢⎡​y1j​y2j​⋮ykj​​⎦⎥⎥⎥⎥⎤​,j=1,2,⋯,n 　 . 因此，话题-文本矩阵YYY： . Y=[y11y12⋯y1ny21y22⋯y2n⋮⋮⋮yk1yk2⋯ykn]Y= left[ begin{array}{cccc} y_{11} &amp; y_{12} &amp; cdots &amp; y_{1 n} y_{21} &amp; y_{22} &amp; cdots &amp; y_{2 n} vdots &amp; vdots &amp; &amp; vdots y_{k 1} &amp; y_{k 2} &amp; cdots &amp; y_{k n} end{array} right]Y=⎣⎢⎢⎢⎢⎡​y11​y21​⋮yk1​​y12​y22​⋮yk2​​⋯⋯⋯​y1n​y2n​⋮ykn​​⎦⎥⎥⎥⎥⎤​ . 4. 单词向量空间到话题向量空间转换 . 因此，单词向量空间的文本向量xjx_jxj​，可以用话题向量空间yjy_jyj​近似表示，即以k个话题向量以yjy_jyj​为系数的线性组合： . xj≈y1jt1+y2jt2+⋯+ykjtk,j=1,2,⋯ ,nx_{j} approx y_{1 j} t_{1}+y_{2 j} t_{2}+ cdots+y_{k j} t_{k}, quad j=1,2, cdots, nxj​≈y1j​t1​+y2j​t2​+⋯+ykj​tk​,j=1,2,⋯,n . 矩阵形式： . X≈TYX approx TYX≈TY . 这就是潜在语义分析。 . 参考： 李航《统计学习方法》 .",
            "url": "https://zqfang.github.io/machine%20learning/2020/04/30/ML-LSA.html",
            "relUrl": "/machine%20learning/2020/04/30/ML-LSA.html",
            "date": " • Apr 30, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Probability and P-value",
            "content": "Probability and P-value . 1. PDF (probability density function) . PDF：概率密度函数（probability density function）, 连续型随机变量的概率密度函数是一个描述某个确定的取值点附近的可能性的函数。 数学表示：用PDF在某一区间上的积分来刻画随机变量落在这个区间中的概率 | Pr⁡(a≤X≤b)=∫abfX(x)dx operatorname{Pr}(a leq X leq b)= int_{a}^{b} f_{X}(x) d xPr(a≤X≤b)=∫ab​fX​(x)dx . 2.PMF (probability mass function) . PMF : 概率质量函数（probability mass function), 在概率论中，概率质量函数是离散随机变量在各特定取值上的概率。 数学表示： PMF其实就是高中所学的离散型随机变量的分布律。 . fX(x)=Pr⁡(X=x)f_{X}(x)= operatorname{Pr}(X=x)fX​(x)=Pr(X=x) . 3. CDF (cumulative distribution function) . CDF : 累积分布函数 (cumulative distribution function)，是概率密度函数的积分，能完整描述一个实随机变量X的概率分布。 . CDF是PDF的（从负无穷-oo到当前值的）积分，PDF是CDF的导数．（为了便于概率的计算，引入CDF的概念） CDF相当于其左侧的面积，也相当于小于该值的概率，负无穷的CDF值为０，正无穷的CDF值总为１． 对于连续变量，有 . FX(x)=Pr⁡(X≤x)=∫−∞xfX(t)dtF_{X}(x)= operatorname{Pr}(X leq x)= int_{- infty}^{x} f_{X}(t) d tFX​(x)=Pr(X≤x)=∫−∞x​fX​(t)dt . 对于离散型变量，有如 . FX(x)=Pr⁡(X≤x)={0 if x&lt;012 if 0≤x&lt;11 if x≥1F_{X}(x)= operatorname{Pr}(X leq x)= left { begin{array}{l} 0 text { if } x&lt;0 frac{1}{2} text { if } 0 leq x&lt;1 1 text { if } x geq 1 end{array} right.FX​(x)=Pr(X≤x)=⎩⎪⎨⎪⎧​0 if x&lt;021​ if 0≤x&lt;11 if x≥1​ . 4. Central Limit Theorem . 中心极限定理（Central Limit Theorem） . 给定一个任意分布的总体， 每次从这些总体中随机抽取 n 个抽样，一共抽 m 次， 然后把这 m 组抽样分别求出平均值， 当m足够大时，这m次的平均值的分布（称为抽样分布）接近正态分布。 . 5. Confidence interval . 置信区间（confidence interval） 置信区间是指由样本统计量所构造的总体参数的估计区间。 置信区间展现的是这个参数的真实值落在测量值（推测值）的周围的可信程度。 . 6. Probability and P-value . A p-value is the probability that random chance generated the data, or something else that is equal or rarer. . A p-value is composed of three parts: . The probability random chance would result in the observation. | The probability of observing something else that is equally rare. | The probability of observing something rarer or more extreme. | youtube: https://youtu.be/5Z9OIYA8He8 youtube: https://youtu.be/JQc3yx0-Q9E . But probability . Probalibility=NumberofoutcomesofinterestThetotalnumberofoutcomesProbalibility = frac{Number of outcomes of interest} {The total number of outcomes}Probalibility=ThetotalnumberofoutcomesNumberofoutcomesofinterest​ . In hypothesis testing, p-values are numbers, between 0 and 1, that, how small does a p-value have to be before we are confident that interested A is dffirrent from B. .",
            "url": "https://zqfang.github.io/statistic/2020/04/29/STATS-Proba-Pvalue.html",
            "relUrl": "/statistic/2020/04/29/STATS-Proba-Pvalue.html",
            "date": " • Apr 29, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Conditional random field (CRF)",
            "content": "CRF条件随机场，可应用于标注问题 . 概率无向图模型Probabilistic undirected graphical model(Markov random field) 是一个可以由无向图表示的联合概率分布 . 0. 概率图模型分类 . 概率图模型（probabilistic graphical model, PGM），是一种学习任务的框架描述，它将学习任务归结为计算变量的概率分布。 . 按照概率图中变量关系的不同，概率图模型可以大致分为两类： . 贝叶斯网络：有向图模型，使用有向无环图表达关系（通常，变量间存在显式的因果关系） | 马尔科夫网络：无向图模型，使用无图表达关系（通常，变量间存有关系，但是难以显式表达） | 同时存有有向边和无向边的模型，如条件随机场（conditional random field）和链图（chain graph），单独看做一类局部有向模型。 | 贝叶斯网络 . 可以分为静态贝叶斯网络和动态贝叶斯网络。相比于静态贝叶斯网络，动态（dynamic）贝叶斯网络主要用于时序数据建模（如语音识别、自然语言处理、轨迹数据挖掘等）。其中，一种结构最简单的动态贝叶斯网络就是隐马尔可夫模型（hidden markov model, HMM）。一般来说，贝叶斯网络中每一个结点都对应于一个先验概率分布或者条件概率分布，因此整体的联合分布可以直接分解为所有单个结点所对应的分布的乘积。 | . 马尔可夫网 . 由于变量之间没有明确的因果关系，它的联合概率分布通常会表达为一系列势函数（potential function）的乘积。通常情况下，这些乘积的积分并不等于1，因此，还要对其进行归一化才能形成一个有效的概率分布——这一点往往在实际应用中给参数估计造成非常大的困难。 | . 按照表示的抽象级别不同，概率图模型可以分为： . 基于随机变量的概率图模型，如贝叶斯网、马尔可夫网、条件随机场和链图等 | 基于模板的概率图模型．这类模型根据应用场景不同又可分为两种： 暂态模型，包括动态贝叶斯网（Dynamic Bayesian Network, DBN）和状态观测模型，其中状态观测模型又包括线性动态系统（Linear Dynamic System, LDS）如卡尔曼滤波器，还有隐马尔可夫模型（Hidden Markov Model, HMM）； | 对象关系领域的概率图模型，包括盘模型（Plate Model，PM）、概率关系模型（Probabilistic Relational Model, PRM）和关系马尔可夫网（Relational Markov Network, RMN）。 | . | . 参考：概率图模型总览 . 1. 模型定义 . 概率图模型：由图（Graph）表示的概率分布。 . 令无向图 G = (V, E) 表示联合概率分布P(Y)，即G中， . 节点v∈Vv in Vv∈V 表示随机变量Yv,Y=(Yv)v∈VY_{v}, Y= left(Y_{v} right)_{v in V}Yv​,Y=(Yv​)v∈V​； | 边e∈Ee in Ee∈E表示随机变量之间的概率依赖关系 | . 无向图表示的随机变量存在: . 成对马可夫性 pariwise Markov property 指任意两个没有边连接的节点，在给定随机变量组（其他所有节点）条件下，该两节点是条件独立 | . | 局部马可夫性 local Markov property | 全局马可夫性 global Markov property | . 概率无向图模型：无向图 G=(V,E)G = (V, E)G=(V,E) 表示联合概率分布 P(Y)P(Y)P(Y)，如果联合概率分布 P(Y)P(Y)P(Y) 满足成对、局部或全局马可夫性，就称此联合概率分布 P(Y)P(Y)P(Y) 为概率无向图模型，或马可夫随机场 . 团（clique）：图G中任何两个节点均有边连接的节点子集 最大团（maximal clique）：团C中不能再加任何一个节点使它成为更大的团，则称最大团 . 2. 条件随机场 . 条件随机场指给定随机变量X条件下， 随机变量Y的马可夫随机场。 . 2.1 条件随机场： . 若随机变量YYY构成一个由无向图G=(V,E)G = (V, E)G=(V,E)表示的马可夫随机场，即 . P(Yv∣X,Yw,w≠v)=P(Yv∣X,Yw,w∼v)P left(Y_{v} | X, Y_{w}, w neq v right)=P left(Y_{v} | X, Y_{w}, w sim v right)P(Yv​∣X,Yw​,w​=v)=P(Yv​∣X,Yw​,w∼v) . 对于任意节点vvv成立， 则称条件概率分布P(Y∣X)P(Y vert X)P(Y∣X)为条件随机场。其中w∼vw sim vw∼v表示在图G=(V,E)G = (V, E)G=(V,E)中与节点vvv有边连接的所有节点www， w≠vw neq vw​=v表示节点v以外的所有节点。 . 2.2 线性链条件随机场（ linear chain conditional random field） . 线性链条件随机场也是对数线性模型(log linear model)，定义为： . P(Yi∣X,Y1,⋯ ,Yi−1,Yi+1,⋯ ,Yn)=P(Yi∣X,Yi−1,Yi+1)i=1,2,⋯ ,nP left(Y_{i} | X, Y_{1}, cdots, Y_{i-1}, Y_{i+1}, cdots, Y_{n} right)=P left(Y_{i} | X, Y_{i-1}, Y_{i+1} right) i = 1,2, cdots, nP(Yi​∣X,Y1​,⋯,Yi−1​,Yi+1​,⋯,Yn​)=P(Yi​∣X,Yi−1​,Yi+1​)i=1,2,⋯,n . 在条件概率模型P(Y∣X)P(Y vert X)P(Y∣X)中， YYY是输出变量，表示标记序列（状态序列，参见HMM）；XXX使输入变量，表示需要标注的观测序列。利用训练集，通过极大似然估计或正则化的极大似然估计得到条件概率模型P^(Y∣X) hat{P}(Y vert X)P^(Y∣X);预测时，对于给定输入序列xxx，求条件概率P^(Y∣X) hat{P}(Y vert X)P^(Y∣X)最大的输出序列y^ hat{y}y^​。 . 2.3 条件随机场的参数化形式 . 设P(Y∣X)P(Y vert X)P(Y∣X)为线性链条件随机场，X取值为x， Y取值为y的条件概率具有如下形式： . P(y∣x)=1Z(x)exp⁡(∑i,kλktk(yi−1,yi,x,i)+∑i,lμlsl(yi,x,i))P(y | x)= frac{1}{Z(x)} exp left( sum_{i, k} lambda_{k} t_{k} left(y_{i-1}, y_{i}, x, i right)+ sum_{i, l} mu_{l} s_{l} left(y_{i}, x, i right) right)P(y∣x)=Z(x)1​exp⎝⎜⎛​i,k∑​λk​tk​(yi−1​,yi​,x,i)+i,l∑​μl​sl​(yi​,x,i)⎠⎟⎞​ . 其中， . Z(x)=∑yexp⁡(∑i,kλktk(yi−1,yi,x,i)+∑i,lμlsl(yi,x,i))Z(x)= sum_{y} exp left( sum_{i, k} lambda_{k} t_{k} left(y_{i-1}, y_{i}, x, i right)+ sum_{i, l} mu_{l} s_{l} left(y_{i}, x, i right) right)Z(x)=y∑​exp⎝⎜⎛​i,k∑​λk​tk​(yi−1​,yi​,x,i)+i,l∑​μl​sl​(yi​,x,i)⎠⎟⎞​ . 式中，tkt_{k}tk​和sls_{l}sl​是特征函数, λk lambda_{k}λk​和μl mu_{l}μl​是对应的权值。 Z(x)Z(x)Z(x)是规范化因子。在所有可能输出的序列上进行求和操作。 . 关于特征函数： . 令tkt_{k}tk​是定义在边上的特征函数，称为转移特征，依赖当前和前一个位置 | 令sls_{l}sl​是定义在节点上的特征函数，称为状态特征，依赖当前位置 | 特征函数tkt_{k}tk​和sls_{l}sl​取值0或1；满足条件取1，反之0 | 条件随机长完全由特征函数tkt_{k}tk​和sls_{l}sl​， 和对应的权值λk lambda_{k}λk​和μl mu_{l}μl​确定。 | . 2.4 条件随机场的矩阵形式 . 对于观测序列x的每个位置，y在m个标记中取值，可以定义一个m阶的矩阵随机变量： . Mi(x)=[Mi(yi−1,yi∣x)]M_{i}(x) = [ M_{i}(y_{i-1}, y_{i}|x)]Mi​(x)=[Mi​(yi−1​,yi​∣x)] . 矩阵随机变量元素为 . Mi(yi−1,yi∣x)=exp⁡(Wi(yi−1,yi∣x))Wi(yi−1,yi∣x)=∑k=1Kwkfk(yi−1,yi,x,i) begin{aligned} &amp;M_{i} left(y_{i-1}, y_{i} | x right)= exp left(W_{i} left(y_{i-1}, y_{i} | x right) right) &amp;W_{i} left(y_{i-1}, y_{i} | x right)= sum_{k=1}^{K} w_{k} f_{k} left(y_{i-1}, y_{i}, x, i right) end{aligned}​Mi​(yi−1​,yi​∣x)=exp(Wi​(yi−1​,yi​∣x))Wi​(yi−1​,yi​∣x)=k=1∑K​wk​fk​(yi−1​,yi​,x,i)​ . 这里wkw_kwk​为 . wk={λk,k=1,2,⋯ ,K1μl,k=K1+l;l=1,2,⋯ ,K2w_{k}= left { begin{array}{ll} lambda_{k}, &amp; k=1,2, cdots, K_{1} mu_{l}, &amp; k=K_{1}+l ; l=1,2, cdots, K_{2} end{array} right.wk​={λk​,μl​,​k=1,2,⋯,K1​k=K1​+l;l=1,2,⋯,K2​​ . 和fkf_kfk​为 . fk(yi−1,yi,x,i)={tk(yi−1,yi,x,i),k=1,2,⋯ ,K1sl(yi,x,i),k=K1+l;l=1,2,⋯ ,K2f_{k} left(y_{i-1}, y_{i}, x, i right)= left { begin{array}{ll} t_{k} left(y_{i-1}, y_{i}, x, i right), &amp; k=1,2, cdots, K_{1} s_{l} left(y_{i}, x, i right), &amp; k=K_{1}+l ; l=1,2, cdots, K_{2} end{array} right.fk​(yi−1​,yi​,x,i)={tk​(yi−1​,yi​,x,i),sl​(yi​,x,i),​k=1,2,⋯,K1​k=K1​+l;l=1,2,⋯,K2​​ . 于是，条件概率Pw(y∣x)P_{w}(y vert x)Pw​(y∣x): . Pw(y∣x)=1Zw(x)∏i=1n+1Mi(yi−1,yi∣x)P_{w}(y | x)= frac{1}{Z_{w}(x)} prod_{i=1}^{n+1} M_{i} left(y_{i-1}, y_{i} | x right)Pw​(y∣x)=Zw​(x)1​i=1∏n+1​Mi​(yi−1​,yi​∣x) . 其中， . Zw(x)=[M1(x)M2(x)⋯Mn+1(x)]start,stopZ_{w}(x)= left[M_{1}(x) M_{2}(x) cdots M_{n+1}(x) right]_{ mathrm{start}, mathrm{stop}}Zw​(x)=[M1​(x)M2​(x)⋯Mn+1​(x)]start,stop​ . 注， y0=starty_{0} = mathrm{start}y0​=start，表示开始状态； yn+1=stopy_{n+1} = mathrm{stop}yn+1​=stop， 表示终止状态 . 参考： 李航《统计学习方法》 .",
            "url": "https://zqfang.github.io/machine%20learning/2020/04/29/ML-CRF.html",
            "relUrl": "/machine%20learning/2020/04/29/ML-CRF.html",
            "date": " • Apr 29, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Convert Seurat Robj to Scanpy h5ad",
            "content": "It costed me a lot of time to convert seurat objects to scanpy. It’s not a pleasant experience. Finally, I solved it. . 1. Install Seurat v3.0.2, or python kernel will always died!!! . Don’t know why latest seurat not work. . 2. Set the R version for rpy2 . # user defined R installation import os # path to your libR.so, only Seurat v3.0.2 works! # create a conda R env for seurat 3.0.2 first os.environ[&#39;R_HOME&#39;] = &#39;/home/fangzq/miniconda/envs/seurat/lib/R&#39; # path depends on where you installed Python. os.environ[&#39;R_USER&#39;] = &#39;/home/fangzq/miniconda/lib/python3.7/site-packages/rpy2&#39; . 3. Now, you’er good to go . import scanpy as sc import glob . Install anndata2ri first . import anndata2ri from rpy2.robjects import r from rpy2.robjects.conversion import localconverter # activate rpy2 env anndata2ri.activate() . robjs = glob.glob(&quot;data/*Robj&quot;) . Convert to h5ad . r(&#39;library(Seurat)&#39;) for robj in robjs: r(f&#39;x&lt;-load(&quot;{robj}&quot;)&#39;) r(&#39;y=get(x)&#39;) r(&#39;rm(x)&#39;) r(&#39;DefaultAssay(y) &lt;- &quot;RNA&quot;&#39;) # get raw count matrix to save # seurat2 object # adata = r(&#39;as.SingleCellExperiment(UpdateSeuratObject(y))&#39;) adata = r(&#39;as.SingleCellExperiment(y)&#39;) adata.write_h5ad(filename=robj.replace(&quot;Robj&quot;,&quot;h5ad&quot;)) . 4. other way . seurat -&gt; loom -&gt; scanpy . It’s much easier, but I did not test. . save to loom format fist. pbmc.loom &lt;- as.loom(pbmc.seurat, filename = &quot;../output/pbmc3k.loom&quot;, verbose = FALSE) pbmc.loom . read into scanpy . pbmc3k = sc.read_loom(&quot;../output/pbmc3k.loom&quot;) . | use sceasy to save h5ad. |",
            "url": "https://zqfang.github.io/make%20bioinfo%20uncool%20again/2020/04/28/Seurat2Scanpy.html",
            "relUrl": "/make%20bioinfo%20uncool%20again/2020/04/28/Seurat2Scanpy.html",
            "date": " • Apr 28, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "How to do deep learning using custom Jupyter kernels on Sherlock",
            "content": "A recipe for interactive computing using custom Jupyter kernels on Stanford’s Sherlock. . Setting up custom conda environment on Sherlock’s login node . 1. Download and install Miniconda . wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh # install bash Miniconda3-latest-Linux-x86_64.sh conda config --set always_yes yes . 2. Install jupyter notebook/lab and secure your notebooks with a password . # install the default py3 kernel for jupyter notebook conda install ipython jupyter notebook jupyterlab # add password jupyter notebook password . 3. (Optional) Add custom conda environment. i.e. fastai . conda create -n fastai ipython ipykernel # add the custom to Jupyter notebook conda activate fastai python -m ipykernel install --user --name fastai --display-name FastAI . you could also add R, Julia etc kernel. . 4. Install pytorch/tensorflow . You should select the existed cuda version which installed in Sherlock . conda install -c pytorch pytorch torchvision cudatoolkit=10.1 . tensorflow . conda install tensorflow-gpu cudatoolkit=10.1 . 5. Load gpu modules. Select the corresponding cuda version you’ve just installed . # this is my version module load cuda/10.1.168 module load cudnn/7.6.4 module load nccl . 6. now, open ipython, run . import torch print(torch.cuda.is_avilable()) . if print out is True, then you’er OK to use GPUs. . Follow these steps on your local machine . see details here. . 7. Download the forward repo . git clone https://github.com/vsoch/forward cd forward . 8. Generate your parameters . bash setup.sh . Select Sherlock partition: gpu . 9. SSH Credentials . bash hosts/sherlock_ssh.sh &gt;&gt; ~/.ssh/config . 10. create a sbatch script in forward/sbatches/sherlock and save as jupyter-gpu.sbatch . #!/bin/bash PORT=$1 NOTEBOOK_DIR=$2 if [ -z &quot;$NOTEBOOK_DIR&quot; ]; then cd $SCRATCH else cd $NOTEBOOK_DIR fi ## to compile libtorch C++ code, load these modules # module load gcc/7.3.0 # module load gdb # module load cmake # export CC=$(which gcc) # export CXX=$(which g++) # select cuda version you need module load cuda/10.1.168 module load cudnn/7.6.4 module load nccl # activate fastai env source activate fastai jupyter lab --no-browser --port=$PORT . 11. Start a session . The default working directory is $SCRATCH . bash start.sh jupyter-gpu . change the working directory . bash start.sh jupyter /path/to/dir . 12. open your browser in local machine and type . if your port is 51888, then . http://localhost:51888/ . here is my jupyter lab computing environment. Have fun! . fastai kernel . . Test GPUs . . 13. Resume a session . bash resume.sh jupyter-gpu # or bash resume.sh jupyter-gpu /path/to/dir . 14. Stop a session . bash end.sh jupyter-gpu # or bash end.sh jupyter-gpu /path/to/dir .",
            "url": "https://zqfang.github.io/machine%20learning/2020/02/10/ML-Sherlock.html",
            "relUrl": "/machine%20learning/2020/02/10/ML-Sherlock.html",
            "date": " • Feb 10, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Graph",
            "content": "Graphs . Some data structures to keep in my mind. . BinaryHeap: Complete binary tree MaxHeap: Parent &gt; Both Children | IndexMaxHeap | MinHeap: Parent &lt; Both Children | IndexMinHeap | Priority queue (MaxHeap) | . | BinarySearchTree Not always complete binary tree | Value: leftChild &lt; Parent &lt; rightChild | . | DenseGraph | SparseGraph | . Code snippets take from Play with Algorithm . 1. Dense Graph . #include &lt;iostream&gt; #include &lt;vector&gt; #include &lt;cassert&gt; using namespace std; // 稠密图 - 邻接矩阵 class DenseGraph{ private: int n, m; bool directed; vector&lt;vector&lt;bool&gt;&gt; g; public: DenseGraph( int n , bool directed ){ this-&gt;n = n; this-&gt;m = 0; this-&gt;directed = directed; for( int i = 0 ; i &lt; n ; i ++ ) g.push_back( vector&lt;bool&gt;(n, false) ); } ~DenseGraph(){ } int V(){ return n;} int E(){ return m;} void addEdge( int v , int w ){ assert( v &gt;= 0 &amp;&amp; v &lt; n ); assert( w &gt;= 0 &amp;&amp; w &lt; n ); if( hasEdge( v , w ) ) return; g[v][w] = true; if( !directed ) g[w][v] = true; m ++; } bool hasEdge( int v , int w ){ assert( v &gt;= 0 &amp;&amp; v &lt; n ); assert( w &gt;= 0 &amp;&amp; w &lt; n ); return g[v][w]; } void show(){ for( int i = 0 ; i &lt; n ; i ++ ){ for( int j = 0 ; j &lt; n ; j ++ ) cout&lt;&lt;g[i][j]&lt;&lt;&quot; t&quot;; cout&lt;&lt;endl; } } class adjIterator{ private: DenseGraph &amp;G; int v; int index; public: adjIterator(DenseGraph &amp;graph, int v): G(graph){ this-&gt;v = v; this-&gt;index = -1; } int begin(){ index = -1; return next(); } int next(){ for( index += 1 ; index &lt; G.V() ; index ++ ) if( G.g[v][index] ) return index; return -1; } bool end(){ return index &gt;= G.V(); } }; }; . 2. Sparse Graph . #include &lt;iostream&gt; #include &lt;vector&gt; #include &lt;cassert&gt; using namespace std; // 稀疏图 - 邻接表 class SparseGraph{ private: int n, m; bool directed; vector&lt;vector&lt;int&gt;&gt; g; public: SparseGraph( int n , bool directed ){ this-&gt;n = n; this-&gt;m = 0; this-&gt;directed = directed; for( int i = 0 ; i &lt; n ; i ++ ) g.push_back( vector&lt;int&gt;() ); } ~SparseGraph(){ } int V(){ return n;} int E(){ return m;} void addEdge( int v, int w ){ assert( v &gt;= 0 &amp;&amp; v &lt; n ); assert( w &gt;= 0 &amp;&amp; w &lt; n ); g[v].push_back(w); if( v != w &amp;&amp; !directed ) g[w].push_back(v); m ++; } bool hasEdge( int v , int w ){ assert( v &gt;= 0 &amp;&amp; v &lt; n ); assert( w &gt;= 0 &amp;&amp; w &lt; n ); for( int i = 0 ; i &lt; g[v].size() ; i ++ ) if( g[v][i] == w ) return true; return false; } void show(){ for( int i = 0 ; i &lt; n ; i ++ ){ cout&lt;&lt;&quot;vertex &quot;&lt;&lt;i&lt;&lt;&quot;: t&quot;; for( int j = 0 ; j &lt; g[i].size() ; j ++ ) cout&lt;&lt;g[i][j]&lt;&lt;&quot; t&quot;; cout&lt;&lt;endl; } } class adjIterator{ private: SparseGraph &amp;G; int v; int index; public: adjIterator(SparseGraph &amp;graph, int v): G(graph){ this-&gt;v = v; this-&gt;index = 0; } int begin(){ index = 0; if( G.g[v].size() ) return G.g[v][index]; return -1; } int next(){ index ++; if( index &lt; G.g[v].size() ) return G.g[v][index]; return -1; } bool end(){ return index &gt;= G.g[v].size(); } }; }; .",
            "url": "https://zqfang.github.io/algorithm%20and%20data%20structure/2020/02/09/CS-graph.html",
            "relUrl": "/algorithm%20and%20data%20structure/2020/02/09/CS-graph.html",
            "date": " • Feb 9, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Sort",
            "content": "Sort algorithms Code snippets are taken from Play with Algorithm . Some algorithm to keep in my mind. . selectionSort | insertionSort | mergeSort | quickSort two way | three way | . | heapSort | . 1. insertionSort . #include &lt;iostream&gt; #include &lt;algorithm&gt; using namespace std; template&lt;typename T&gt; void insertionSort(T arr[], int n){ for( int i = 1 ; i &lt; n ; i ++ ) { T e = arr[i]; int j; for (j = i; j &gt; 0 &amp;&amp; arr[j-1] &gt; e; j--) arr[j] = arr[j-1]; arr[j] = e; } return; } // 对arr[l...r]范围的数组进行插入排序 template&lt;typename T&gt; void insertionSort(T arr[], int l, int r){ for( int i = l+1 ; i &lt;= r ; i ++ ) { T e = arr[i]; int j; for (j = i; j &gt; l &amp;&amp; arr[j-1] &gt; e; j--) arr[j] = arr[j-1]; arr[j] = e; } return; } . 2. mergeSort . #include &lt;iostream&gt; #include &lt;algorithm&gt; #include &quot;InsertionSort.h&quot; using namespace std; template&lt;typename T&gt; void __merge(T arr[], int l, int mid, int r){ T aux[r-l+1]; for( int i = l ; i &lt;= r; i ++ ) aux[i-l] = arr[i]; int i = l, j = mid+1; for( int k = l ; k &lt;= r; k ++ ){ if( i &gt; mid ) { arr[k] = aux[j-l]; j ++;} else if( j &gt; r ){ arr[k] = aux[i-l]; i ++;} else if( aux[i-l] &lt; aux[j-l] ){ arr[k] = aux[i-l]; i ++;} else { arr[k] = aux[j-l]; j ++;} } } template&lt;typename T&gt; void __mergeSort(T arr[], int l, int r){ if( r - l &lt;= 15 ){ insertionSort(arr, l, r); return; } int mid = (l+r)/2; __mergeSort(arr, l, mid); __mergeSort(arr, mid+1, r); if( arr[mid] &gt; arr[mid+1] ) __merge(arr, l, mid, r); } template&lt;typename T&gt; void mergeSort(T arr[], int n){ __mergeSort( arr , 0 , n-1 ); } template &lt;typename T&gt; void mergeSortBU(T arr[], int n){ for( int i = 0 ; i &lt; n ; i += 16 ) insertionSort(arr,i,min(i+15,n-1)); for( int sz = 16; sz &lt;= n ; sz += sz ) for( int i = 0 ; i &lt; n - sz ; i += sz+sz ) if( arr[i+sz-1] &gt; arr[i+sz] ) __merge(arr, i, i+sz-1, min(i+sz+sz-1,n-1) ); } . 3. qucikSort . #include &lt;iostream&gt; #include &lt;ctime&gt; #include &lt;algorithm&gt; #include &quot;InsertionSort.h&quot; using namespace std; template &lt;typename T&gt; int _partition(T arr[], int l, int r){ // select a rand index in arr, and swap swap( arr[l] , arr[rand()%(r-l+1)+l] ); T v = arr[l]; int i = l+1, j = r; while( true ){ while( i &lt;= r &amp;&amp; arr[i] &lt; v ) i ++; while( j &gt;= l+1 &amp;&amp; arr[j] &gt; v ) j --; if( i &gt; j ) break; swap( arr[i] , arr[j] ); i ++; j --; } swap( arr[l] , arr[j]); return j; } template &lt;typename T&gt; void _quickSort(T arr[], int l, int r){ if( r - l &lt;= 15 ){ insertionSort(arr,l,r); return; } int p = _partition(arr, l, r); _quickSort(arr, l, p-1 ); _quickSort(arr, p+1, r); } template &lt;typename T&gt; void quickSort(T arr[], int n){ srand(time(NULL)); _quickSort(arr, 0, n-1); } template &lt;typename T&gt; void __quickSort3Ways(T arr[], int l, int r){ if( r - l &lt;= 15 ){ insertionSort(arr,l,r); return; } swap( arr[l], arr[rand()%(r-l+1)+l ] ); T v = arr[l]; int lt = l; // arr[l+1...lt] &lt; v int gt = r + 1; // arr[gt...r] &gt; v int i = l+1; // arr[lt+1...i) == v while( i &lt; gt ){ if( arr[i] &lt; v ){ swap( arr[i], arr[lt+1]); i ++; lt ++; } else if( arr[i] &gt; v ){ swap( arr[i], arr[gt-1]); gt --; } else{ // arr[i] == v i ++; } } swap( arr[l] , arr[lt] ); __quickSort3Ways(arr, l, lt-1); __quickSort3Ways(arr, gt, r); } template &lt;typename T&gt; void quickSort3Ways(T arr[], int n){ srand(time(NULL)); __quickSort3Ways( arr, 0, n-1); } . 4. heapSort . #include &quot;Heap.h&quot; using namespace std; template&lt;typename T&gt; void heapSort2(T arr[], int n){ MaxHeap&lt;T&gt; maxheap = MaxHeap&lt;T&gt;(arr,n); for( int i = n-1 ; i &gt;= 0 ; i-- ) arr[i] = maxheap.extractMax(); } template&lt;typename T&gt; void heapSort1(T arr[], int n){ MaxHeap&lt;T&gt; maxheap = MaxHeap&lt;T&gt;(n); for( int i = 0 ; i &lt; n ; i ++ ) maxheap.insert(arr[i]); for( int i = n-1 ; i &gt;= 0 ; i-- ) arr[i] = maxheap.extractMax(); } .",
            "url": "https://zqfang.github.io/algorithm%20and%20data%20structure/2020/02/08/CS-sort.html",
            "relUrl": "/algorithm%20and%20data%20structure/2020/02/08/CS-sort.html",
            "date": " • Feb 8, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Binary tree",
            "content": "Binary trees . Difference . Some data structures to keep in my mind. . BinaryHeap: Complete binary tree MaxHeap: Parent &gt; Both Children | IndexMaxHeap | MinHeap: Parent &lt; Both Children | IndexMinHeap | Priority queue (MaxHeap) | . | BinarySearchTree Not always complete binary tree | Value: leftChild &lt; Parent &lt; rightChild | . | DenseGraph | SparseGraph | . Code snippets take from Play with Algorithm . 1. MaxHeap . #include &lt;algorithm&gt; #include &lt;cassert&gt; using namespace std; template&lt;typename Item&gt; class MaxHeap{ private: Item *data; int count; int capacity; void shiftUp(int k){ while( k &gt; 1 &amp;&amp; data[k/2] &lt; data[k] ){ swap( data[k/2], data[k] ); k /= 2; } } void shiftDown(int k){ while( 2*k &lt;= count ){ int j = 2*k; // which child is larger, then swap if( j+1 &lt;= count &amp;&amp; data[j+1] &gt; data[j] ) j ++; if( data[k] &gt;= data[j] ) break; swap( data[k] , data[j] ); k = j; } } public: MaxHeap(int capacity){ data = new Item[capacity+1]; count = 0; this-&gt;capacity = capacity; } MaxHeap(Item arr[], int n){ data = new Item[n+1]; capacity = n; for( int i = 0 ; i &lt; n ; i ++ ) data[i+1] = arr[i]; count = n; for( int i = count/2 ; i &gt;= 1 ; i -- ) shiftDown(i); } ~MaxHeap(){ delete[] data; } int size(){ return count; } bool isEmpty(){ return count == 0; } void insert(Item item){ assert( count + 1 &lt;= capacity ); data[count+1] = item; shiftUp(count+1); count ++; } Item extractMax(){ assert( count &gt; 0 ); Item ret = data[1]; swap( data[1] , data[count] ); count --; shiftDown(1); return ret; } Item getMax(){ assert( count &gt; 0 ); return data[1]; } }; . 2. IndexMaxHeap . Need 3 vector: data, indexes, reverse . Code . #include &lt;algorithm&gt; #include &lt;cassert&gt; using namespace std; template&lt;typename Item&gt; class IndexMaxHeap{ private: Item *data; int *indexes; int *reverse; int count; int capacity; void shiftUp( int k ){ while( k &gt; 1 &amp;&amp; data[indexes[k/2]] &lt; data[indexes[k]] ){ swap( indexes[k/2] , indexes[k] ); reverse[indexes[k/2]] = k/2; reverse[indexes[k]] = k; k /= 2; } } void shiftDown( int k ){ while( 2*k &lt;= count ){ int j = 2*k; if( j + 1 &lt;= count &amp;&amp; data[indexes[j+1]] &gt; data[indexes[j]] ) j += 1; if( data[indexes[k]] &gt;= data[indexes[j]] ) break; swap( indexes[k] , indexes[j] ); reverse[indexes[k]] = k; reverse[indexes[j]] = j; k = j; } } public: IndexMaxHeap(int capacity){ data = new Item[capacity+1]; indexes = new int[capacity+1]; reverse = new int[capacity+1]; for( int i = 0 ; i &lt;= capacity ; i ++ ) reverse[i] = 0; count = 0; this-&gt;capacity = capacity; } ~IndexMaxHeap(){ delete[] data; delete[] indexes; delete[] reverse; } int size(){ return count; } bool isEmpty(){ return count == 0; } // 传入的i对用户而言,是从0索引的 void insert(int i, Item item){ assert( count + 1 &lt;= capacity ); assert( i + 1 &gt;= 1 &amp;&amp; i + 1 &lt;= capacity ); i += 1; data[i] = item; indexes[count+1] = i; reverse[i] = count+1; count++; shiftUp(count); } Item extractMax(){ assert( count &gt; 0 ); Item ret = data[indexes[1]]; swap( indexes[1] , indexes[count] ); reverse[indexes[count]] = 0; reverse[indexes[1]] = 1; count--; shiftDown(1); return ret; } int extractMaxIndex(){ assert( count &gt; 0 ); int ret = indexes[1] - 1; swap( indexes[1] , indexes[count] ); reverse[indexes[count]] = 0; reverse[indexes[1]] = 1; count--; shiftDown(1); return ret; } Item getMax(){ assert( count &gt; 0 ); return data[indexes[1]]; } int getMaxIndex(){ assert( count &gt; 0 ); return indexes[1]-1; } bool contain( int i ){ assert( i + 1 &gt;= 1 &amp;&amp; i + 1 &lt;= capacity ); return reverse[i+1] != 0; } Item getItem( int i ){ assert( contain(i) ); return data[i+1]; } void change( int i , Item newItem ){ assert( contain(i) ); i += 1; data[i] = newItem; // 找到indexes[j] = i, j表示data[i]在堆中的位置 // 之后shiftUp(j), 再shiftDown(j) // for( int j = 1 ; j &lt;= count ; j ++ ) // if( indexes[j] == i ){ // shiftUp(j); // shiftDown(j); // return; // } int j = reverse[i]; shiftUp( j ); shiftDown( j ); } // test reverse index bool testReverseIndex(){ int *copyIndexes = new int[count+1]; int *copyReverseIndexes = new int[count+1]; for( int i = 0 ; i &lt;= count ; i ++ ){ copyIndexes[i] = indexes[i]; copyReverseIndexes[i] = reverse[i]; } copyIndexes[0] = copyReverseIndexes[0] = 0; std::sort(copyIndexes, copyIndexes + count + 1); std::sort(copyReverseIndexes, copyReverseIndexes + count + 1); bool res = true; for( int i = 1 ; i &lt;= count ; i ++ ) if( copyIndexes[i-1] + 1 != copyIndexes[i] || copyReverseIndexes[i-1] + 1 != copyReverseIndexes[i] ) res = res || false; delete[] copyIndexes; delete[] copyReverseIndexes; if( !res ){ cout&lt;&lt;&quot;Error 1&quot;&lt;&lt;endl; return res; } for( int i = 1 ; i &lt;= count ; i ++ ) if( reverse[ indexes[i] ] != i ){ cout&lt;&lt;&quot;Error 2&quot;&lt;&lt;endl; return false; } return true; } }; . 3. BinarySearchTree . #include &lt;iostream&gt; #include &lt;queue&gt; #include &lt;cassert&gt; using namespace std; template &lt;typename Key, typename Value&gt; class BST{ private: struct Node{ Key key; Value value; Node *left; Node *right; Node(Key key, Value value){ this-&gt;key = key; this-&gt;value = value; this-&gt;left = this-&gt;right = NULL; } Node(Node *node){ this-&gt;key = node-&gt;key; this-&gt;value = node-&gt;value; this-&gt;left = node-&gt;left; this-&gt;right = node-&gt;right; } }; Node *root; int count; public: BST(){ root = NULL; count = 0; } ~BST(){ destroy( root ); } int size(){ return count; } bool isEmpty(){ return count == 0; } void insert(Key key, Value value){ root = insert(root, key, value); } bool contain(Key key){ return contain(root, key); } Value* search(Key key){ return search( root , key ); } // 前序遍历 void preOrder(){ preOrder(root); } // 中序遍历 void inOrder(){ inOrder(root); } // 后序遍历 void postOrder(){ postOrder(root); } // 层序遍历 void levelOrder(){ queue&lt;Node*&gt; q; q.push(root); while( !q.empty() ){ Node *node = q.front(); q.pop(); cout&lt;&lt;node-&gt;key&lt;&lt;endl; if( node-&gt;left ) q.push( node-&gt;left ); if( node-&gt;right ) q.push( node-&gt;right ); } } // 寻找最小的键值 Key minimum(){ assert( count != 0 ); Node* minNode = minimum( root ); return minNode-&gt;key; } // 寻找最大的键值 Key maximum(){ assert( count != 0 ); Node* maxNode = maximum(root); return maxNode-&gt;key; } // 从二叉树中删除最小值所在节点 void removeMin(){ if( root ) root = removeMin( root ); } // 从二叉树中删除最大值所在节点 void removeMax(){ if( root ) root = removeMax( root ); } // 从二叉树中删除键值为key的节点 void remove(Key key){ root = remove(root, key); } private: // 向以node为根的二叉搜索树中,插入节点(key, value) // 返回插入新节点后的二叉搜索树的根 Node* insert(Node *node, Key key, Value value){ if( node == NULL ){ count ++; return new Node(key, value); } if( key == node-&gt;key ) node-&gt;value = value; else if( key &lt; node-&gt;key ) node-&gt;left = insert( node-&gt;left , key, value); else // key &gt; node-&gt;key node-&gt;right = insert( node-&gt;right, key, value); return node; } // 查看以node为根的二叉搜索树中是否包含键值为key的节点 bool contain(Node* node, Key key){ if( node == NULL ) return false; if( key == node-&gt;key ) return true; else if( key &lt; node-&gt;key ) return contain( node-&gt;left , key ); else // key &gt; node-&gt;key return contain( node-&gt;right , key ); } // 在以node为根的二叉搜索树中查找key所对应的value Value* search(Node* node, Key key){ if( node == NULL ) return NULL; if( key == node-&gt;key ) return &amp;(node-&gt;value); else if( key &lt; node-&gt;key ) return search( node-&gt;left , key ); else // key &gt; node-&gt;key return search( node-&gt;right, key ); } // 对以node为根的二叉搜索树进行前序遍历 void preOrder(Node* node){ if( node != NULL ){ cout&lt;&lt;node-&gt;key&lt;&lt;endl; preOrder(node-&gt;left); preOrder(node-&gt;right); } } // 对以node为根的二叉搜索树进行中序遍历 void inOrder(Node* node){ if( node != NULL ){ inOrder(node-&gt;left); cout&lt;&lt;node-&gt;key&lt;&lt;endl; inOrder(node-&gt;right); } } // 对以node为根的二叉搜索树进行后序遍历 void postOrder(Node* node){ if( node != NULL ){ postOrder(node-&gt;left); postOrder(node-&gt;right); cout&lt;&lt;node-&gt;key&lt;&lt;endl; } } void destroy(Node* node){ if( node != NULL ){ destroy( node-&gt;left ); destroy( node-&gt;right ); delete node; count --; } } // 在以node为根的二叉搜索树中,返回最小键值的节点 Node* minimum(Node* node){ if( node-&gt;left == NULL ) return node; return minimum(node-&gt;left); } // 在以node为根的二叉搜索树中,返回最大键值的节点 Node* maximum(Node* node){ if( node-&gt;right == NULL ) return node; return maximum(node-&gt;right); } // 删除掉以node为根的二分搜索树中的最小节点 // 返回删除节点后新的二分搜索树的根 Node* removeMin(Node* node){ if( node-&gt;left == NULL ){ Node* rightNode = node-&gt;right; delete node; count --; return rightNode; } node-&gt;left = removeMin(node-&gt;left); return node; } // 删除掉以node为根的二分搜索树中的最大节点 // 返回删除节点后新的二分搜索树的根 Node* removeMax(Node* node){ if( node-&gt;right == NULL ){ Node* leftNode = node-&gt;left; delete node; count --; return leftNode; } node-&gt;right = removeMax(node-&gt;right); return node; } // 删除掉以node为根的二分搜索树中键值为key的节点 // 返回删除节点后新的二分搜索树的根 Node* remove(Node* node, Key key){ if( node == NULL ) return NULL; if( key &lt; node-&gt;key ){ node-&gt;left = remove( node-&gt;left , key ); return node; } else if( key &gt; node-&gt;key ){ node-&gt;right = remove( node-&gt;right, key ); return node; } else{ // key == node-&gt;key if( node-&gt;left == NULL ){ Node *rightNode = node-&gt;right; delete node; count --; return rightNode; } if( node-&gt;right == NULL ){ Node *leftNode = node-&gt;left; delete node; count--; return leftNode; } // node-&gt;left != NULL &amp;&amp; node-&gt;right != NULL Node *successor = new Node(minimum(node-&gt;right)); count ++; successor-&gt;right = removeMin(node-&gt;right); successor-&gt;left = node-&gt;left; delete node; count --; return successor; } } }; .",
            "url": "https://zqfang.github.io/algorithm%20and%20data%20structure/2020/02/08/CS-binarytree.html",
            "relUrl": "/algorithm%20and%20data%20structure/2020/02/08/CS-binarytree.html",
            "date": " • Feb 8, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "What is Big O",
            "content": "What on earth is Big O? . Time complexity and space complexity . Time complexity . O(f(n)): number of commands need to execute. proportional to f(n). 表示运行算法所需要执行的指令数，和f(n)成正。 . 严格来讲，O(f(n))表示算法执行的上界。业界默认为算法执行的最低上界(最坏情况）。 n represents the data scale 数据规模 . when n is a large number, the constant is usually ignored. . algorithm n of cmd . Binary reserach $O(logn)$ | $a*logn$ | . Max/min in an array $O(n)$ | b*n | . merge sort $O(nlogn)$ | $c*nlogn$ | . select sort $O(n^2)$ | $d*n^2$ | . quick sort $O(nlogn)$ | e*nlogn | . adjacent graph | $O(V+E)$ | . Lazy Prim | $O(ElogE)$ | . Prim | $O(ElogV)$ | . Kruskal | $O(ElogE)$ | . Dijkstra | $O(ElogV)$ | . Bellman-Ford | $O(EV)$ | . minimum span tree Shortest path tree (Single source shortest path) . Space complexity . cmd complexity . new an array | $O(n)$ | . new 2d array | $O(n^2)$ | . new an constant space | $O(1)$ | . recursive function: the depth (n) of a recursive function, the extra space need $O(n)$. . Make sense of n . If you want to solve the problem in 1 second, then an algorithm of . complexity cmds n . $O(n^2)$ | could exec cmd n = $10^4$ | . $O(n)$ | could exec cmd n = $10^8$ | . $O(nlogn)$ | could exec cmd n = $10^7$ | . Example . binarySearch | find from n element find from $n/2$ element find from $n/4$ element ... find from 1 . That’s, need how many steps of search when n = 1? $log_{2}n = O(logn)$. . int2string. Set num &gt; 0 | string int2string(int num) { string s=&quot;&quot;; while(num) { s += &#39;0&#39; + num%10; num /= 10; } reverse(s); // O(n) return s; } . That is, how many “/10” steps when num = 0? $log_{10}n = O(logn)$. . Case: two nested for loop, not always $O(n^2)$ | void hello(int n){ for (int sz =1; sz &lt; n; sz ++ sz) // logn here for( int i=1; i &lt; n;; i++) //n cout&lt;&lt;&quot;hello, complex&quot; &lt;&lt;endl; } . So, should be $O(nlogn)$ . isPrime: $O( sqrt{n})$ // set n &gt; 1 bool isPrime(int n){ for( int x =2; x*x &lt;= n; x++){ if( n%x == 0) return false; return true; } } . | recursive function | single recursive function call int binarySearch(int arr[], int l, int r, int target) { if (l&gt;r) return -1; int mid = l +(r-l)/2; if (arr[mid] == target) return mid; else if (arr[mid] &gt; target) return binarySearch(arr, ;, mid-1, target); else return binarySearch(arr, mid+1, r, target); } . each step need O(1), so overall complexity depend on recursive exec depth. That is, if each function call needs time T, then time complexity: O(T*depth) -&gt; O(n). . | . Another example: recursion depth logn, them time complexity O(logn). . double pow( double x, int n){ assert(n &gt;=0); if (n==0) return 1.0; double t = pow(x, n/2); if( n%2) return x*t*t; return t*t; } . multi recursive exec how many exec step? int f(int n) { assert(n &gt;=0); if(n == 0) return 1; return f(n-1) + f(n); } . that’s, count how many nodes on a full binary tree. $2^{n+1} -1 = O(2^n)$ . | . how to think about this? . void mergeSort(int arr[]. int l. int r){ if (l &gt;=r) return; int mid = (l+r) /2; mergeSort(arr, l, mid); mergeSort(arr, mid+1, r); merge(arr, l, mid, r); } . For binary tree, complexity for each level O(n), while tree depth O(logn). Overall, O(nlogn) . Amortized time i.e. dynamic vector/stack/deque | template&lt;T&gt; class MyVector{ private: T* data; int capacity; int size; //O(n) void resize(int newCapacity){ assert(newCapacity &gt;= size); T* newData = new T[newCapacity]; for(int i = 0; i &lt; size; i++ ){ newData[i] = data[i]; } delete[] data; data = newData; capacity = newCapacity; } public: MyVector() { data = new T[10]; capacity = 10; size = 0; } ~MyVector() { delete[] data; } // Average: O(1) void push_back(T e){ //assert(size &lt; capacity) if (size == capacity) resize (2 *capacity); data[size++] = e; } // Average O(1) T pop_back(){ assert (size &gt;0); T ret = data[size-1]; size --; // note the denominator here. To advoid ossilation of space complexity if(size == capacity / 4) resize(capacity /2); return ret; } }; .",
            "url": "https://zqfang.github.io/algorithm%20and%20data%20structure/2020/02/01/CS-algo.html",
            "relUrl": "/algorithm%20and%20data%20structure/2020/02/01/CS-algo.html",
            "date": " • Feb 1, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Statistical Modeling and Inference",
            "content": "A breif review over the foundations of statistical inference . Statistical Models and Inference . statistical inference: a formal approach to characterizing a random phenomenon using observations, either by providing a description of a past phenomenon or by giving some predictions about future phenomenon of similar nature. . 1. Statistical Models . The first step in statistical inference is to specify a statistical model, under some simplifying assumptions (i.e. independence assumptions). . Hierarchical models: the probability distribution of one parameter is dependent on the values of other hierachical paramters (i.e. conditional independent). | . Steps: . Set assumptions (i.e. independent), parameter and model. Make explicit assumptions on the probability distributions. focus on parametric modeling, because of limited data | nonparametric not consider here, used for hypothesis testing or when sample size is very large. | . | Once the model is specified, then choose a method of inference, as well as an algorithm to obtain estimates. Most commonly use: Maximum likelihood inference | Bayesian inference | . | 2. Maximum likelihood inference . Quantifying confidence: the Fisher Information Matrix Newton’s algorithm Approximate Techniques . Monte Carlo Sampling for intractable likelihoods | Composite likelihood | . 3. Bayesian Inference . A statistical model describes the uncertainty about how the data was produced. The ultimate aim for statistical inference is to obtain information about the unknown parameter θ thetaθ given the data D mathcal{D}D. . Frequentist: θ thetaθ is fixed but unknown quantity. | Bayesian: use a fully probabilistic model and treat θ thetaθ as a random quantity. To do so, chose an appropriate prior distribution P(θ) mathbb{P}( theta)P(θ), which reflects the knowledge (i.e. uncertainty) about θ thetaθ prior to the experiment | the goal is to update the knowledge given the information contained in the data D mathcal{D}D. | the updated knowledge (i.e. reduced uncertainty) is encapsulated in the posterior distribution P(θ∣D) mathbb{P}( theta vert mathcal{D})P(θ∣D), which is calculated via Bayes’theorem. | . | . P(θ∣D)=P(D∣θ)P(θ)P(D) mathbb{P}( boldsymbol{ theta} | mathcal{D})= frac{ mathbb{P}( mathcal{D} | boldsymbol{ theta}) mathbb{P}( boldsymbol{ theta})}{ mathbb{P}( mathcal{D})}P(θ∣D)=P(D)P(D∣θ)P(θ)​ . The bayesian paradigm boils down to the slogan: posterior ∝ propto∝ likelihood × times× prior . 3.1 Choice of prior distributions . Conjugate priors the prior and the posterior lie in the same class of distributions. | often chosen, because it leads to a well-known form of the posterior, which simplifies the calculations | . | choose a prior that contains as little information about the parameter as possible at first choice would, of course, be a locally uniform prior. Under a uniform prior we have P(θ∣D)∝L(θ) mathbb{P}( boldsymbol{ theta} vert boldsymbol{D}) propto mathcal{L}( boldsymbol{ theta})P(θ∣D)∝L(θ). | Jeffrey’s prior, but often hard to come by | . | 3.2 Bayesian point estimates and confidence intervals . Bayesian point estimates: the posterior mean, mode and median . θ^=E[θ∣D]=∫θP(θ∣D)dθ hat{ theta}= mathbb{E}[ theta | D]= int theta mathbb{P}( theta | mathcal{D}) mathrm{d} thetaθ^=E[θ∣D]=∫θP(θ∣D)dθ . confidence: highest posterior density (HPD) region . for a threshold value π piπ, the region Cα={θ:P(θ∣D)&gt;π} mathcal{C}_{ alpha}= { theta: mathbb{P}( theta vert mathcal{D})&gt; pi }Cα​={θ:P(θ∣D)&gt;π}, we get . ∫CαP(θ∣D)dθ=1−α int_{C_{ alpha}} mathbb{P}( theta | mathcal{D}) mathrm{d} theta=1- alpha∫Cα​​P(θ∣D)dθ=1−α . This region Cα mathcal{C}_{ alpha}Cα​ is the HPD region. . 3.3 Markov Chain Monte Carlo . A common challenge in Bayesian inference is that the integral . P(D)=∫P(D∣θ)P(θ)dθ mathbb{P}(D)= int mathbb{P}(D | theta) mathbb{P}( theta) d_{ theta}P(D)=∫P(D∣θ)P(θ)dθ​ . can’t be solved analytically. . to be continued… . 3.4 Empirical Bayes for Latent Variable Problems . The first step is to infer point estimates for the parameters at higher levels by integrating out those at lower levels, and the infer posterior distributions for lower level parameters while setting those at a higher level to their point estimate. . 3.5 Approximate Bayesian Computation . Approximate Bayesian computation (ABC) is a class of simulation-based techniques to conduct Bayesian inference under models with intractable likelihoods . 3.6 Model selection . how to compare the several candidate models explaining the data D mathcal{D}D? the most commonly used methods: . likelihood ratio statistic | model posterior probabilities | Bayes factors | others: cross-validation, Akaike’s information criterion (AIC), Bayesian information criterion (BIC) | . 4. Naive Bayes and Bayesian estimation . Naive Bayes and Bayesian estimation are two different concepts! . Naive Bayes is a statistical learning method. For a give training set, learn the join probability distribution of P(X,Y)P(X,Y)P(X,Y). Based on this model, for a given input xxx, output a yyy with maximal posterior probability (Bayes theorem). . Set prior prob distribution: . P(Y=ck),k=1,2,⋯ ,KP left(Y=c_{k} right), quad k=1,2, cdots, KP(Y=ck​),k=1,2,⋯,K . Conditional prob distribution: . P(X=x∣Y=ck)=P(X(1)=x(1),⋯ ,X(n)=x(n)∣Y=ck),k=1,2,⋯ ,KP left(X=x | Y=c_{k} right)=P left(X^{(1)}=x^{(1)}, cdots, X^{(n)}=x^{(n)} | Y=c_{k} right), quad k=1,2, cdots, KP(X=x∣Y=ck​)=P(X(1)=x(1),⋯,X(n)=x(n)∣Y=ck​),k=1,2,⋯,K . Naive Bayes make a strong assumption that conditional prob distribution are all conditional independent, which is: . P(X=x∣Y=ck)=P(X(1)=x(1),⋯ ,X(n)=x(n)∣Y=ck)=∏j=1nP(X(j)=x(j)∣Y=ck) begin{aligned} P left(X=x | Y=c_{k} right) &amp;=P left(X^{(1)}=x^{(1)}, cdots, X^{(n)}=x^{(n)} | Y=c_{k} right) &amp;= prod_{j=1}^{n} P left(X^{(j)}=x^{(j)} | Y=c_{k} right) end{aligned}P(X=x∣Y=ck​)​=P(X(1)=x(1),⋯,X(n)=x(n)∣Y=ck​)=j=1∏n​P(X(j)=x(j)∣Y=ck​)​ . then, posterior prob is: . P(Y=ck∣X=x)=P(X=x∣Y=ck)P(Y=ck)∑kP(X=x∣Y=ck)P(Y=ck)P left(Y=c_{k} | X=x right)= frac{P left(X=x | Y=c_{k} right) P left(Y=c_{k} right)}{ sum_{k} P left(X=x | Y=c_{k} right) P left(Y=c_{k} right)}P(Y=ck​∣X=x)=∑k​P(X=x∣Y=ck​)P(Y=ck​)P(X=x∣Y=ck​)P(Y=ck​)​ . If conditional probability of each input variable is not independent, then model become Baysian Network! . Naive Bayes Classifier is: . y=f(x)=arg⁡max⁡ckP(Y=ck)∏jP(X(j)=x(j)∣Y=ck)∑kP(Y=ck)∏jP(X(j)=x(j)∣Y=ck)y=f(x)= arg max _{c_{k}} frac{P left(Y=c_{k} right) prod_{j} P left(X^{(j)}=x^{(j)} | Y=c_{k} right) }{ sum_{k} P left(Y=c_{k} right) prod_{j} P left(X^{(j)}=x^{(j)} | Y=c_{k} right)}y=f(x)=argck​max​∑k​P(Y=ck​)∏j​P(X(j)=x(j)∣Y=ck​)P(Y=ck​)∏j​P(X(j)=x(j)∣Y=ck​)​ . and it’s short form: . y=f(x)=arg⁡max⁡ckP(Y=ck)∏jP(X(j)=x(j)∣Y=ck)y=f(x)= arg max _{c_{k}} P left(Y=c_{k} right) prod_{j} P left(X^{(j)}=x^{(j)} | Y=c_{k} right)y=f(x)=argck​max​P(Y=ck​)j∏​P(X(j)=x(j)∣Y=ck​) .",
            "url": "https://zqfang.github.io/machine%20learning/2020/01/30/ML-Bayes.html",
            "relUrl": "/machine%20learning/2020/01/30/ML-Bayes.html",
            "date": " • Jan 30, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Loss function for multi-label classification",
            "content": "Multi-label classification, tasks commonly be seen on health record data (multi symptoms). . Loss function design: . Multi binary cross-entropy each class has a binary output . | Label smoothing, another regularization technique . | It’s designed to make the model a little bit less certain of it’s decision by changing a little bit its target: instead of wanting to predict 1 for the correct class and 0 for all the others, we ask it to predict 1-ε for the correct class and ε for all the others, with ε a (small) positive number and N the number of classes. This can be written as: . loss=(1−ε)ce(i)+ε∑ce(j)/N text {loss}=(1- varepsilon) c e(i)+ varepsilon sum c e(j) / Nloss=(1−ε)ce(i)+ε∑ce(j)/N . where ce(x) is cross-entropy of x (i.e. −log(px)), and i is the correct class. . finally, for multi-label loss function: . (1−ϵ)∑i(−log⁡pin)+ϵN∑(−log⁡pi)(1- epsilon) sum_{i} left(- frac{ log p_{i}}{n} right)+ frac{ epsilon}{N} sum left(- log p_{i} right)(1−ϵ)i∑​(−nlogpi​​)+Nϵ​∑(−logpi​) . See the fastai implementation here: LabelSmoothingCrossEntropy . about line 285: . class LabelSmoothingCrossEntropy(Module): y_int = True def __init__(self, eps:float=0.1, reduction=&#39;mean&#39;): self.eps,self.reduction = eps,reduction def forward(self, output, target): c = output.size()[-1] log_preds = F.log_softmax(output, dim=-1) if self.reduction==&#39;sum&#39;: loss = -log_preds.sum() else: loss = -log_preds.sum(dim=-1) if self.reduction==&#39;mean&#39;: loss = loss.mean() return loss*self.eps/c + (1-self.eps) * F.nll_loss(log_preds, target.long(), reduction=self.reduction) def activation(self, out): return F.softmax(out, dim=-1) def decodes(self, out): return out.argmax(dim=-1) .",
            "url": "https://zqfang.github.io/machine%20learning/2020/01/29/ML-Lossfunc.html",
            "relUrl": "/machine%20learning/2020/01/29/ML-Lossfunc.html",
            "date": " • Jan 29, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Mento Carlos",
            "content": "蒙特卡罗方法，又称统计模拟方法(statistical simulation method), 通过概率模型的随机抽样进行进行近似数值计算的方法。 . 1. 蒙特卡罗方法的核心 . 蒙特卡罗方法的核心是随机抽样(random sampling) . 2. Monte Carlo intergration: . 计算h(x)积分 . ∫Xh(x)dx int_{ mathcal{X}} h(x) mathrm{d} x∫X​h(x)dx . 将h(x)分解成f(x)和概率密度函数p(x)的乘积，即函数h(x)的积分可以表示为函数f(x)关于概率密度函数p(x)的数学期望： . ∫Xh(x)dx=∫Xf(x)p(x)dx=Ep(x)[f(x)] int_{ mathcal{X}} h(x) mathrm{d} x= int_{ mathcal{X}} f(x) p(x) mathrm{d} x=E_{p(x)}[f(x)]∫X​h(x)dx=∫X​f(x)p(x)dx=Ep(x)​[f(x)] . 因此，可利用样本均值计算近似积分： . ∫Xh(x)dx=Ep(x)[f(x)]≈1n∑i=1nf(xi) int_{ mathcal{X}} h(x) mathrm{d} x=E_{p(x)}[f(x)] approx frac{1}{n} sum_{i=1}^{n} f left(x_{i} right)∫X​h(x)dx=Ep(x)​[f(x)]≈n1​i=1∑n​f(xi​) . 变形 . Ep(z)[f(z)]=∫f(z)p(z)dz=∫f(z)p(z)q(z)⏟newf~(z)q(z)dz≈1N∑n=1Nf(zi)p(zi)q(zi) begin{aligned} mathrm{E}_{p(z)}[f(z)] &amp;= int f(z) p(z) dz &amp;= int underbrace{f(z) frac{p(z)}{q(z)}}_{new tilde{f}(z)} q(z) dz &amp; approx frac{1}{N} sum_{n=1}^{N} f(z^{i}) frac{p(z^{i})}{q(z^{i})} end{aligned}Ep(z)​[f(z)]​=∫f(z)p(z)dz=∫newf~​(z) . f(z)q(z)p(z)​​​q(z)dz≈N1​n=1∑N​f(zi)q(zi)p(zi)​​ . 3. Markov Chain Monte Carlo . Markov chain or markov process: . P(Xt∣X0,X1,⋯ ,Xt−1)=P(Xt∣Xt−1),t=1,2,⋯P left(X_{t} | X_{0}, X_{1}, cdots, X_{t-1} right)=P left(X_{t} | X_{t-1} right), quad t=1,2, cdotsP(Xt​∣X0​,X1​,⋯,Xt−1​)=P(Xt​∣Xt−1​),t=1,2,⋯ . 参考： 李航《统计学习方法》 .",
            "url": "https://zqfang.github.io/machine%20learning/2020/01/26/ML-MonteCarlo.html",
            "relUrl": "/machine%20learning/2020/01/26/ML-MonteCarlo.html",
            "date": " • Jan 26, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Expectation Maximization",
            "content": "Maximum Likelihood Estimation Gaussian Mixture Model Expectation Maximization . 1. Probability and likelihood . likehood &amp; maximum likehood . 在非正式场合似然（likelihood）和概率（Probability）几乎是一对同义词，但是在统计学中似然和概率却是两个不同的概念。 . 概率: 在特定环境下某件事情发生的可能性，也就是结果没有产生之前依据环境所对应的参数来预测某件事情发生的可能性。 . 比如抛硬币，抛之前我们不知道最后是哪一面朝上，但是根据硬币的性质我们可以推测任何一面朝上的可能性均为50%，这个概率只有在抛硬币之前才是有意义的，抛完硬币后的结果便是确定的； . 似然: 刚好相反，是在确定的结果下去推测产生这个结果的可能环境（参数）。 . 假设随机抛掷一枚硬币1,000次，结果500次人头朝上，500次数字朝上，那么两面朝上的概率均为50%。运用出现的结果来判断这个事情本身的性质（参数），也就是似然。 . 当结果和参数相互对应，似然和概率在数值上相等。 用 θ 表示环境对应的参数，x 表示结果，那么概率可以表示为： . P(x∣θ)P(x | theta )P(x∣θ) . p(x vertθ) 是条件概率的表示方法。θ 是前置条件，理解为在 θ 的前提下，事件 x 发生的概率，相对应的似然可以表示为: . L(θ∣x) mathcal{L}( theta | x)L(θ∣x) . 可以理解为已知结果为 x ，参数为 θ (似然函数里 θ 是变量，这里说的参数和变量是相对与概率而言的)对应的概率，即： . L(θ∣x)=P(x∣θ) mathcal{L}( theta | x)=P(x | theta)L(θ∣x)=P(x∣θ) . 两者在数值上相等，但是意义并不相同，L mathcal{L}L 是关于 θ 的函数，而 P 则是关于 x 的函数。 . 2. Maximum Likelihood Estimation . 单高斯模型 x∼N(μ,Σ)x sim mathcal{N}( mu, Sigma)x∼N(μ,Σ), xi∈Dx_{i} in mathcal{D}xi​∈D, 那么对参数 μ muμ和 Σ SigmaΣ 进行估计，只需要最大化log-likelihood函数： . log⁡p(X)=∑i=1Nlog⁡N(xi∣μ,Σ)=∑i=1Nlog⁡12πσe−(xi−μ)22σ2=∑i=1Nlog⁡12πσ+∑i=1N−(xi−μ)22σ2=−N2log⁡2π−N2log⁡σ2−12σ2∑i=1N(xi−μ)2 begin{aligned} log p(X) &amp;= sum_{i=1}^{N} log mathcal{N} left(x_{i} | mu, Sigma right) &amp;= sum_{i=1}^{N} log frac{1}{ sqrt{2 pi} sigma} e^{- frac{ left(x_{i}- mu right)^{2}}{2 sigma^{2}}} &amp;= sum_{i=1}^{N} log frac{1}{ sqrt{2 pi} sigma}+ sum_{i=1}^{N}- frac{ left(x_{i}- mu right)^{2}}{2 sigma^{2}} &amp;=- frac{N}{2} log 2 pi- frac{N}{2} log sigma^{2}- frac{1}{2 sigma^{2}} sum_{i=1}^{N} left(x_{i}- mu right)^{2} end{aligned}logp(X)​=i=1∑N​logN(xi​∣μ,Σ)=i=1∑N​log2π . ​σ1​e−2σ2(xi​−μ)2​=i=1∑N​log2π . ​σ1​+i=1∑N​−2σ2(xi​−μ)2​=−2N​log2π−2N​logσ2−2σ21​i=1∑N​(xi​−μ)2​ . 求偏导数，得到参数估计： . ∂log⁡p(X)∂μ=1σ2∑i=1N(xi−μ)=0⇒μ=1N∑i=1Nxi∂log⁡p(X)∂σ2=−N2σ2+12σ4∑i=1N(xi−μ)2=0⇒σ2=1N∑i=1N(xi−μ)2 begin{aligned} frac{ partial log p(X)}{ partial mu} &amp;= frac{1}{ sigma^{2}} sum_{i=1}^{N} left(x_{i}- mu right)=0 &amp; Rightarrow mu= frac{1}{N} sum_{i=1}^{N} x_{i} frac{ partial log p(X)}{ partial sigma^{2}} &amp;=- frac{N}{2 sigma^{2}}+ frac{1}{2 sigma^{4}} sum_{i=1}^{N} left(x_{i}- mu right)^{2}=0 &amp; Rightarrow sigma^{2}= frac{1}{N} sum_{i=1}^{N} left(x_{i}- mu right)^{2} end{aligned}∂μ∂logp(X)​∂σ2∂logp(X)​​=σ21​i=1∑N​(xi​−μ)=0⇒μ=N1​i=1∑N​xi​=−2σ2N​+2σ41​i=1∑N​(xi​−μ)2=0⇒σ2=N1​i=1∑N​(xi​−μ)2​ . 3. Gaussian Mixture Model . 如果有K个高斯线性叠加: . p(x)=∑k=1KπkN(x∣μk,Σk) s.t. ∑k=1Kπk=10≤πk≤1 begin{aligned} p(x)=&amp; sum_{k=1}^{K} pi_{k} mathcal{N} left(x | mu_{k}, Sigma_{k} right) &amp; text { s.t. } sum_{k=1}^{K} pi_{k}=1 &amp; 0 leq pi_{k} leq 1 end{aligned}p(x)=​k=1∑K​πk​N(x∣μk​,Σk​) s.t. k=1∑K​πk​=10≤πk​≤1​ . 那么对数似然函数为 . log⁡p(X)=∑i=1Nlog⁡{∑k=1KπkN(xi∣μk,Σk)} log p(X)= sum_{i=1}^{N} log left { sum_{k=1}^{K} pi_{k} mathcal{N} left(x_{i} | mu_{k}, Sigma_{k} right) right }logp(X)=i=1∑N​log{k=1∑K​πk​N(xi​∣μk​,Σk​)} . 因为对数里有求和，因此无法无法直接通过最大似然估计方法进行参数估计。 . 其中，如果πk pi_{k}πk​是每个高斯出现的概率p(k)p(k)p(k)，则高斯混合模型分解为以p(k)p(k)p(k)获得一个高斯分布，然后在分布中获得xxx，因此xxx边缘概率分布为： . p(x)=∑k=1Kp(k)p(x∣k)p(x)= sum_{k=1}^{K} p(k) p(x | k)p(x)=k=1∑K​p(k)p(x∣k) . 后验概率p(k∣x)p(k vert x)p(k∣x)表示xxx属于每个高斯的概率（离散值）: . p(k∣x)=p(x∣k)p(k)∑lp(x∣l)p(l)=πkN(x∣μk,Σk)∑lπlN(x∣μl,Σl) begin{aligned} p(k | x) &amp;= frac{p(x | k) p(k)}{ sum_{l} p(x | l) p(l)} &amp;= frac{ pi_{k} mathcal{N} left(x | mu_{k}, Sigma_{k} right)}{ sum_{l} pi_{l} mathcal{N} left(x | mu_{l}, Sigma_{l} right)} end{aligned}p(k∣x)​=∑l​p(x∣l)p(l)p(x∣k)p(k)​=∑l​πl​N(x∣μl​,Σl​)πk​N(x∣μk​,Σk​)​​ . 4. Expectation Maximization . 思想： 通过引入隐变量，运用迭代方法，求解混合高斯模型 . θ(t+1)=arg⁡max⁡θL(θ;X) theta^{(t+1)}= underset{ theta}{ arg max } mathcal{L}( theta ; X)θ(t+1)=θargmax​L(θ;X) . 引入隐变量Zi(状态i）， z服从多项分布，选择zi的概率为p(zi),则高斯混合模型为： . zi∼Multinoimal⁡(π1,⋯ ,πk)xi∣zi∼N(μzi,Σzi) begin{aligned} z_{i} &amp; sim operatorname{Multinoimal} left( pi_{1}, cdots, pi_{k} right) x_{i} | z_{i} &amp; sim mathcal{N} left( mu_{z_{i}}, Sigma_{z_{i}} right) end{aligned}zi​xi​∣zi​​∼Multinoimal(π1​,⋯,πk​)∼N(μzi​​,Σzi​​)​ . 步骤： . E-Step: 在现有θ(t) theta^{(t)}θ(t)下最大化似然下界, 计算隐变量z的期望Q(zi)=p(zi∣xi,θ)Q left(z_{i} right)=p left(z_{i} vert x_{i}, theta right)Q(zi​)=p(zi​∣xi​,θ) 作为其下界 | M-Step: 在上面Q(zi)Q(z_{i})Q(zi​)下计算参数列表θ thetaθ来最大化似然 | . (0) 理解EM的前提 . 凹凸函数: ∀x∈R,f′′(x)≥0 forall_{x in mathbb{R}}, f^{ prime prime}(x) geq 0∀x∈R​,f′′(x)≥0,则fff为凸函数。 当x为向量，如果其hessian矩阵H是半正定的(H≥0H geq 0H≥0),则fff为凸函数 如果f′′(x)&gt;0f^{ prime prime}(x)&gt;0f′′(x)&gt;0或者H&gt;0H&gt;0H&gt;0, fff是严格凸函数。 如果f′′(x)&lt;0f^{ prime prime}(x)&lt;0f′′(x)&lt;0或者H&gt;0H&gt;0H&gt;0, fff是凹函数。 . Jensen 不等式: . 如果fff为凸函数, 则E[f(X)]≥f(E[X])E[f(X)] geq f(E[X])E[f(X)]≥f(E[X])。当且仅当x是常数时，E[f(x)]=f(E[x])E[f(x)]=f(E[x])E[f(x)]=f(E[x])。 | 如果fff是凹函数, 则E[f(X)]≤f(E[X])E[f(X)] leq f(E[X])E[f(X)]≤f(E[X])。 | 引入隐变量后，变换对数似然函数: . L(θ;X)=∑i=1Nlog⁡p(xi∣θ)=∑i=1Nlog⁡∑zip(xi,zi∣θ)=∑i=1Nlog⁡∑ziQ(zi)p(xi,zi∣θ)Q(zi)≥∑i=1N∑ziQ(zi)log⁡p(xi,zi∣θ)Q(zi) begin{aligned} mathcal{L}( theta ; X) &amp;= sum_{i=1}^{N} log p left(x_{i} | theta right) &amp;= sum_{i=1}^{N} log sum_{z_{i}} p left(x_{i}, z_{i} | theta right) &amp;= sum_{i=1}^{N} log sum_{z_{i}} Q left(z_{i} right) frac{p left(x_{i}, z_{i} | theta right)}{Q left(z_{i} right)} &amp; geq sum_{i=1}^{N} sum_{z_{i}} Q left(z_{i} right) log frac{p left(x_{i}, z_{i} | theta right)}{Q left(z_{i} right)} end{aligned}L(θ;X)​=i=1∑N​logp(xi​∣θ)=i=1∑N​logzi​∑​p(xi​,zi​∣θ)=i=1∑N​logzi​∑​Q(zi​)Q(zi​)p(xi​,zi​∣θ)​≥i=1∑N​zi​∑​Q(zi​)logQ(zi​)p(xi​,zi​∣θ)​​ . 推导: . 把式中的log函数体看成是一个整体，由于log(x)的二阶导数为−1x2- frac{1}{x^2}−x21​, 小于0，为凹函数。所以使用Jensen不等式时，应用第二条准则：f(E[X])≥E[f(x)]f(E[X]) geq E[f(x)]f(E[X])≥E[f(x)]。 | f(Ezi∼Q[p(xi,zi∣θ)Q(zi)])≥Ezi∼Q[f(p(xi,zi∣θ)Q(zi))]f left(E_{z_{i} sim Q} left[ frac{p left(x_{i}, z_{i} | theta right)}{Q left(z_{i} right)} right] right) geq E_{z_{i} sim Q} left[f left( frac{p left(x_{i}, z_{i} | theta right)}{Q left(z_{i} right)} right) right]f(Ezi​∼Q​[Q(zi​)p(xi​,zi​∣θ)​])≥Ezi​∼Q​[f(Q(zi​)p(xi​,zi​∣θ)​)] . 这里，Q(zi)Q left(z_{i} right)Q(zi​)是ziz_{i}zi​的函数， 且∑ziQ(zi)=1 sum_{z_{i}} Q left(z_{i} right)=1∑zi​​Q(zi​)=1。 | 由数学期望Ex∼p[g(X)]=∑xg(x)p(x)E_{x sim p}[g(X)]= sum_{x} g(x) p(x)Ex∼p​[g(X)]=∑x​g(x)p(x)，上式可以理解为: p(x)p(x)p(x)对应Q(zi)Q left(z_{i} right)Q(zi​), g(x)对应log⁡p(xi,zi∣θ)Q(zi) log frac{p left(x_{i}, z_{i} vert theta right)}{Q left(z_{i} right)}logQ(zi​)p(xi​,zi​∣θ)​表示ziz_{i}zi​的函数。 . | 似然函数: L(θ)≥J(z,Q) mathcal{L}( theta) geq mathcal{J}(z,Q)L(θ)≥J(z,Q)（zzz为隐含变量），那么我们可以通过不断的最大化J mathcal{J}J的下界，来使得L(θ) mathcal{L}( theta)L(θ)不断提高，最终达到它的最大值。 | 最大化L(θ) mathcal{L}( theta)L(θ)函数的下界，即让g(x)g(x)g(x)为常数c: . p(xi,zi∥θ)Q(zi)=c frac{p left(x_{i}, z_{i} | theta right)}{Q left(z_{i} right)}=cQ(zi​)p(xi​,zi​∥θ)​=c . Jensen不等式中说到，当自变量X=E(X)时，即为常数的时候，等式成立! . 变换公式, 对所有zzz求和得: . p(xi,zi∣θ)=c⋅Q(zi)∑zip(xi,zi∣θ)=c⋅∑ziQ(zi)c=∑zip(xi,zi∣θ) begin{aligned} p left(x_{i}, z_{i} | theta right) &amp;=c cdot Q left(z_{i} right) sum_{z_{i}} p left(x_{i}, z_{i} | theta right) &amp;=c cdot sum_{z_{i}} Q left(z_{i} right) c &amp;= sum_{z_{i}} p left(x_{i}, z_{i} | theta right) end{aligned}p(xi​,zi​∣θ)zi​∑​p(xi​,zi​∣θ)c​=c⋅Q(zi​)=c⋅zi​∑​Q(zi​)=zi​∑​p(xi​,zi​∣θ)​ . 其中，∑ziQ(zi)=1 sum_{z_{i}} Q left(z_{i} right) = 1∑zi​​Q(zi​)=1, 也得： . Q(zi)=p(xi,zi∣θ)∑zip(xi,zi∣θ)=p(zi∣xi,θ) begin{aligned} Q left(z_{i} right) &amp;= frac{p left(x_{i}, z_{i} | theta right)}{ sum_{z_{i}} p left(x_{i}, z_{i} | theta right)} &amp;=p left(z_{i} | x_{i}, theta right) end{aligned}Q(zi​)​=∑zi​​p(xi​,zi​∣θ)p(xi​,zi​∣θ)​=p(zi​∣xi​,θ)​ . 至此，我们推出了在固定参数θ后，使下界拉升的Q(z)Q(z)Q(z)的计算公式就是后验概率（条件概率），一并解决了Q(z)Q(z)Q(z)如何选择的问题。此步就是EM算法的E-step。 . 执行E-Step后与下界重合，此时似然变为： . L(θ(t);X)=∑i=1N∑ziQ(t)(zi)log⁡p(xi,zi∣θ(t))Q(t)(zi) mathcal{L} left( theta^{(t)} ; X right)= sum_{i=1}^{N} sum_{z_{i}} Q^{(t)} left(z_{i} right) log frac{p left(x_{i}, z_{i} | theta^{(t)} right)}{Q^{(t)} left(z_{i} right)}L(θ(t);X)=i=1∑N​zi​∑​Q(t)(zi​)logQ(t)(zi​)p(xi​,zi​∣θ(t))​ . 这时，对公式求导 . θ(t+1)=arg⁡max⁡θL(θ;X) theta^{(t+1)}= underset{ theta}{ arg max } mathcal{L}( theta ; X)θ(t+1)=θargmax​L(θ;X) . 得到t+1步的似然函数L(θ(t+1);X) mathcal{L} left( theta^{(t+1)} ; X right)L(θ(t+1);X)。 通过不断的迭代，可以得到使似然函数L(θ) mathcal{L}( theta)L(θ)最大化的参数θ，直至函数收敛。 只需要证明L(θ(t+1);X)≥L(θ(t);X) mathcal{L} left( theta^{(t+1)} ; X right) geq mathcal{L} left( theta^{(t)} ; X right)L(θ(t+1);X)≥L(θ(t);X), 则可证明EM的收敛性: . L(θ(t+1);X)=∑i=1Nlog⁡∑ziQ(t)(zi)p(xi,zi∣θ(t+1))Q(t)(zi)≥∑i=1N∑ziQ(t)(zi)log⁡p(xi,zi∣θ(t+1))Q(t)(zi)≥∑i=1N∑ziQ(t)(zi)log⁡p(xi,zi∣θ(t))Q(t)(zi)=L(θ(t);X) begin{aligned} mathcal{L} left( theta^{(t+1)} ; X right) &amp;= sum_{i=1}^{N} log sum_{z_{i}} Q^{(t)} left(z_{i} right) frac{p left(x_{i}, z_{i} | theta^{(t+1)} right)}{Q^{(t)} left(z_{i} right)} &amp; geq sum_{i=1}^{N} sum_{z_{i}} Q^{(t)} left(z_{i} right) log frac{p left(x_{i}, z_{i} | theta^{(t+1)} right)}{Q^{(t)} left(z_{i} right)} &amp; geq sum_{i=1}^{N} sum_{z_{i}} Q^{(t)} left(z_{i} right) log frac{p left(x_{i}, z_{i} | theta^{(t)} right)}{Q^{(t)} left(z_{i} right)} &amp;= mathcal{L} left( theta^{(t)} ; X right) end{aligned}L(θ(t+1);X)​=i=1∑N​logzi​∑​Q(t)(zi​)Q(t)(zi​)p(xi​,zi​∣θ(t+1))​≥i=1∑N​zi​∑​Q(t)(zi​)logQ(t)(zi​)p(xi​,zi​∣θ(t+1))​≥i=1∑N​zi​∑​Q(t)(zi​)logQ(t)(zi​)p(xi​,zi​∣θ(t))​=L(θ(t);X)​ . 5. 求解GMM . (1) GMM E-Step: . 已知θ(t) theta^{(t)}θ(t), 求Q(t+1)(zi)Q^{(t+1)} left(z_{i} right)Q(t+1)(zi​): . Q(t+1)(zi)=p(xi,zi∣θ(t))p(xi∣θ(t))=p(xi,zi∣θ(t))∑l∈zip(xi,l∣θ(t))=p(xi∣zi,θ(t))p(zi∣θ(t))∑l∈zip(xi∣l,θ(t))p(l∣θ(t))=N(μzi,Σzi)πzi∑l∈ziN(μl,Σl)πl begin{aligned} Q^{(t+1)} left(z_{i} right) &amp;= frac{p left(x_{i}, z_{i} | theta^{(t)} right)}{p left(x_{i} | theta^{(t)} right)} &amp;= frac{p left(x_{i}, z_{i} | theta^{(t)} right)}{ sum_{l in z_{i}} p left(x_{i}, l | theta^{(t)} right)} &amp;= frac{p left(x_{i} | z_{i}, theta^{(t)} right) p left(z_{i} | theta^{(t)} right)}{ sum_{l in z_{i}} p left(x_{i} | l, theta^{(t)} right) p left(l | theta^{(t)} right)} &amp;= frac{ mathcal{N} left( mu_{z_{i}}, Sigma_{z_{i}} right) pi_{z_{i}}}{ sum_{l in z_{i}} mathcal{N} left( mu_{l}, Sigma_{l} right) pi_{l}} end{aligned}Q(t+1)(zi​)​=p(xi​∣θ(t))p(xi​,zi​∣θ(t))​=∑l∈zi​​p(xi​,l∣θ(t))p(xi​,zi​∣θ(t))​=∑l∈zi​​p(xi​∣l,θ(t))p(l∣θ(t))p(xi​∣zi​,θ(t))p(zi​∣θ(t))​=∑l∈zi​​N(μl​,Σl​)πl​N(μzi​​,Σzi​​)πzi​​​​ . (2) GMM M-Step: . 已知Q(t+1)(zi)Q^{(t+1)} left(z_{i} right)Q(t+1)(zi​), 求 θ(t+1) theta^{(t+1)}θ(t+1): . L(θ;X)=∑iN∑lKQi(l)log⁡p(xi,l∣θ)Qi(l)=∑iN∑lKQi(l)log⁡p(xi,l∣θ)−∑iN∑lKQi(l)log⁡Qi(l)=∑iN∑lKQi(l)log⁡p(xi,l∣θ)−Constant =∑iN∑lKQi(l)log⁡πlN(μl,Σl)−Constant =∑iN∑lKQi(l)log⁡πl+∑iN∑lKQi(l)log⁡N(μl,Σl)−Constant begin{aligned} mathcal{L}( theta ; X) &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log frac{p left(x_{i}, l | theta right)}{Q_{i}(l)} &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log p left(x_{i}, l | theta right)- sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log Q_{i}(l) &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log p left(x_{i}, l | theta right)- text {Constant } &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log pi_{l} mathcal{N} left( mu_{l}, Sigma_{l} right)- text {Constant } &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log pi_{l}+ sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log mathcal{N} left( mu_{l}, Sigma_{l} right)- text {Constant} end{aligned}L(θ;X)​=i∑N​l∑K​Qi​(l)logQi​(l)p(xi​,l∣θ)​=i∑N​l∑K​Qi​(l)logp(xi​,l∣θ)−i∑N​l∑K​Qi​(l)logQi​(l)=i∑N​l∑K​Qi​(l)logp(xi​,l∣θ)−Constant =i∑N​l∑K​Qi​(l)logπl​N(μl​,Σl​)−Constant =i∑N​l∑K​Qi​(l)logπl​+i∑N​l∑K​Qi​(l)logN(μl​,Σl​)−Constant​ . (3) 求 π piπ: . ∀l∈{1,⋯ ,K},∂L(θ;X)∂πl=0 s.t. ∑lKπl=1 begin{aligned} forall_{l in {1, cdots, K }}, &amp; frac{ partial mathcal{L}( theta ; X)}{ partial pi_{l}}=0 &amp; text { s.t. } sum_{l}^{K} pi_{l}=1 end{aligned}∀l∈{1,⋯,K}​,​∂πl​∂L(θ;X)​=0 s.t. l∑K​πl​=1​ . 拉格朗日乘法约束 . {Lπl=∂L(θ;X)∂πl+λ(∑lKπl−1)=0Lλ=∑lKπl−1=0 left { begin{aligned} L_{ pi_{l}} &amp;= frac{ partial mathcal{L}( theta ; X)}{ partial pi_{l}}+ lambda left( sum_{l}^{K} pi_{l}-1 right)=0 L_{ lambda} &amp;= sum_{l}^{K} pi_{l}-1=0 end{aligned} right.⎩⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎧​Lπl​​Lλ​​=∂πl​∂L(θ;X)​+λ(l∑K​πl​−1)=0=l∑K​πl​−1=0​ . 求导： . {1π1∑iNQi(1)−λ=0⋮1πl∑iNQi(l)−λ=0 left { begin{array}{c} frac{1}{ pi_{1}} sum_{i}^{N} Q_{i}(1)- lambda=0 vdots frac{1}{ pi_{l}} sum_{i}^{N} Q_{i}(l)- lambda=0 end{array} right.⎩⎪⎪⎪⎨⎪⎪⎪⎧​π1​1​∑iN​Qi​(1)−λ=0⋮πl​1​∑iN​Qi​(l)−λ=0​ . 相加得： . ∑lK∑iNQi(l)=λ∑lKπl=λ sum_{l}^{K} sum_{i}^{N} Q_{i}(l)= lambda sum_{l}^{K} pi_{l}= lambdal∑K​i∑N​Qi​(l)=λl∑K​πl​=λ . 由 Qi(l)=p(l∣xi,θ)Q_{i}(l)=p left(l vert x_{i}, theta right)Qi​(l)=p(l∣xi​,θ), 得 . ∑lK∑iNQi(l)=∑iN∑lKQi(l)=∑iN∑lKp(l∣xi,θ)=∑iN1=N begin{aligned} sum_{l}^{K} sum_{i}^{N} Q_{i}(l) &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) &amp;= sum_{i}^{N} sum_{l}^{K} p left(l | x_{i}, theta right) &amp;= sum_{i}^{N} 1 &amp;=N end{aligned}l∑K​i∑N​Qi​(l)​=i∑N​l∑K​Qi​(l)=i∑N​l∑K​p(l∣xi​,θ)=i∑N​1=N​ . 则 . πl=1λ∑iNQi(l)=1N∑iNQi(l)=1N∑iNp(l∣xi,θ) begin{aligned} pi_{l} &amp;= frac{1}{ lambda} sum_{i}^{N} Q_{i}(l) &amp;= frac{1}{N} sum_{i}^{N} Q_{i}(l) &amp;= frac{1}{N} sum_{i}^{N} p left(l | x_{i}, theta right) end{aligned}πl​​=λ1​i∑N​Qi​(l)=N1​i∑N​Qi​(l)=N1​i∑N​p(l∣xi​,θ)​ . (4) 计算μ muμ . ∑iN∑lKQi(l)log⁡N(μl,Σl)=∑iN∑lKQi(l)log⁡12πσle−(xi−μl)22σl2=∑iN∑lKQi(l){−12log⁡2π−12log⁡σl2−(xi−μl)22σl2} begin{aligned} &amp; sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log mathcal{N} left( mu_{l}, Sigma_{l} right) &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) log frac{1}{ sqrt{2 pi} sigma_{l}} e^{- frac{ left(x_{i}- mu_{l} right)^{2}}{2 sigma_{l}^{2}}} &amp;= sum_{i}^{N} sum_{l}^{K} Q_{i}(l) left {- frac{1}{2} log 2 pi- frac{1}{2} log sigma_{l}^{2}- frac{ left(x_{i}- mu_{l} right)^{2}}{2 sigma_{l}^{2}} right } end{aligned}​i∑N​l∑K​Qi​(l)logN(μl​,Σl​)=i∑N​l∑K​Qi​(l)log2π . ​σl​1​e−2σl2​(xi​−μl​)2​=i∑N​l∑K​Qi​(l){−21​log2π−21​logσl2​−2σl2​(xi​−μl​)2​}​ . 求偏导： . ∂L(θ;X)∂μl=∑iNQi(l)xi−μlσ2=0 begin{aligned} frac{ partial mathcal{L}( theta ; X)}{ partial mu_{l}} &amp;= sum_{i}^{N} Q_{i}(l) frac{x_{i}- mu_{l}}{ sigma^{2}} &amp;=0 end{aligned}∂μl​∂L(θ;X)​​=i∑N​Qi​(l)σ2xi​−μl​​=0​ . 得μ muμ： . μl=∑iNQi(l)xi∑iNQi(l) mu_{l}= frac{ sum_{i}^{N} Q_{i}(l) x_{i}}{ sum_{i}^{N} Q_{i}(l)}μl​=∑iN​Qi​(l)∑iN​Qi​(l)xi​​ . ∂L(θ;X)∂σl2=∑iNQi(l){−12σl2+(xi−μl)22σl4}=0 begin{aligned} &amp; frac{ partial mathcal{L}( theta ; X)}{ partial sigma_{l}^{2}}= sum_{i}^{N} Q_{i}(l) left {- frac{1}{2 sigma_{l}^{2}}+ frac{ left(x_{i}- mu_{l} right)^{2}}{2 sigma_{l}^{4}} right } &amp;=0 end{aligned}​∂σl2​∂L(θ;X)​=i∑N​Qi​(l){−2σl2​1​+2σl4​(xi​−μl​)2​}=0​ . (5) 计算σ sigmaσ . ∂L(θ;X)∂σl2=∑iNQi(l){−12σl2+(xi−μl)22σl4}=0 begin{aligned} frac{ partial mathcal{L}( theta ; X)}{ partial sigma_{l}^{2}} &amp;= sum_{i}^{N} Q_{i}(l) left {- frac{1}{2 sigma_{l}^{2}}+ frac{ left(x_{i}- mu_{l} right)^{2}}{2 sigma_{l}^{4}} right } &amp;=0 end{aligned}∂σl2​∂L(θ;X)​​=i∑N​Qi​(l){−2σl2​1​+2σl4​(xi​−μl​)2​}=0​ . 得到 . σl=∑iNQi(l)(xi−μl)2∑iNQi(l) sigma_{l}= frac{ sum_{i}^{N} Q_{i}(l) left(x_{i}- mu_{l} right)^{2}}{ sum_{i}^{N} Q_{i}(l)}σl​=∑iN​Qi​(l)∑iN​Qi​(l)(xi​−μl​)2​ . 6 从KL散度角度解释EM . KL(q∥p)=∑zq(z)log⁡q(z)p(z∣x,θ)=∑zq(z)log⁡q(z)p(x∣θ)p(z,x∣θ)=−∑zq(z)log⁡p(z,x∣θ)q(z)+∑zq(z)log⁡p(x∣θ)=−∑zq(z)log⁡p(z,x∣θ)q(z)+log⁡p(x∣θ)∑zq(z)=−∑zq(z)log⁡p(z,x∣θ)q(z)+log⁡p(x∣θ)log⁡p(x∣θ)=KL(q∥p)+∑zq(z)log⁡p(z,x∣θ)q(z)=KL(q∥p)+L(q,θ) begin{aligned} K L(q | p) &amp;= sum_{z} q(z) log frac{q(z)}{p(z | x, theta)} &amp;= sum_{z} q(z) log frac{q(z) p(x | theta)}{p(z, x | theta)} &amp;=- sum_{z} q(z) log frac{p(z, x | theta)}{q(z)}+ sum_{z} q(z) log p(x | theta) &amp;=- sum_{z} q(z) log frac{p(z, x | theta)}{q(z)}+ log p(x | theta) sum_{z} q(z) &amp;=- sum_{z} q(z) log frac{p(z, x | theta)}{q(z)}+ log p(x | theta) log p(x | theta) &amp;=K L(q | p)+ sum_{z} q(z) log frac{p(z, x | theta)}{q(z)} &amp;=K L(q | p)+ mathcal{L}(q, theta) end{aligned}KL(q∥p)logp(x∣θ)​=z∑​q(z)logp(z∣x,θ)q(z)​=z∑​q(z)logp(z,x∣θ)q(z)p(x∣θ)​=−z∑​q(z)logq(z)p(z,x∣θ)​+z∑​q(z)logp(x∣θ)=−z∑​q(z)logq(z)p(z,x∣θ)​+logp(x∣θ)z∑​q(z)=−z∑​q(z)logq(z)p(z,x∣θ)​+logp(x∣θ)=KL(q∥p)+z∑​q(z)logq(z)p(z,x∣θ)​=KL(q∥p)+L(q,θ)​ . 参考： . 徐亦达-机器学习-EM .",
            "url": "https://zqfang.github.io/machine%20learning/2020/01/26/ML-EM.html",
            "relUrl": "/machine%20learning/2020/01/26/ML-EM.html",
            "date": " • Jan 26, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "SuperFast RNA-seq",
            "content": "salmon-tximport-deseq2 . Step 0: install salmon and download transcriptome cdna from gencode . conda install salmon wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_32/gencode.v32.transcripts.fa.gz . Step 1. build salmon index . salmon index -p 8 --gencode -t gencode.v32.transcripts.fa.gz -i salmonIndex_hg38 . Step 2: quantification . salmon quant -i salmonIndex_hg38 -l A -1 ${fn}/${samp}_1.fastq.gz -2 ${fn}/${samp}_2.fastq.gz -p 8 --validateMappings -o quants/${samp}_quant . Step 3: merge quantification outputs . use tximport in R . # R code library(tximport) library(readr) suppressMessages(library(&#39;EnsDb.Hsapiens.v86&#39;)) txdb &lt;- EnsDb.Hsapiens.v86 k &lt;- keys(txdb, keytype = &quot;GENEID&quot;) df &lt;- select(txdb, keys = k, keytype = &quot;GENEID&quot;, columns = c(&quot;TXID&quot;,&quot;GENEID&quot;)) tx2gene &lt;- df[, 2:1] # tx ID, then gene ID #tx2gene &lt;- read.table(tx2gene, header= T, sep=&quot; t&quot;, stringsAsFactors = F) samples &lt;- unlist(strsplit(sample_ids,&quot;,&quot;)) salmon.files &lt;- file.path(&#39;salmon&#39;,samples, &quot;quant.sf&quot;) names(salmon.files) &lt;- samples all(file.exists(salmon.files)) # get transcript level results txi.transcripts &lt;- tximport(salmon.files, type = &quot;salmon&quot;, txOut = TRUE, tx2gene = tx2gene,) # ignoreTxVersion = TRUE) # get gene level results txi.salmon &lt;- summarizeToGene(txi.transcripts, tx2gene) #save raw counts salmon.counts&lt;- txi.salmon$counts salmon.counts&lt;- as.data.frame(salmon.counts) write.table(salmon.counts, out_counts, sep=&quot; t&quot;, quote=F) #save gene tpms salmon.TPM&lt;- txi.salmon$abundance salmon.TPM&lt;- as.data.frame(salmon.TPM) write.table(salmon.TPM, out_tpm, sep=&quot; t&quot;, quote=F) #save transcripts tpms salmon.trans.TPM&lt;- txi.transcripts$abundance salmon.trans.TPM&lt;- as.data.frame(salmon.trans.TPM) write.table(salmon.trans.TPM, outTrans_tpm, sep=&quot; t&quot;, quote=F) save(txi.salmon, file=&quot;txi.salmon.RData&quot;) . Step 4: Differentially expressed gene analysis . DESeq2 pipeline demo . load(&quot;txi.salmon.RData&quot;) dds &lt;- DESeqDataSetFromTximport(txi.salmon, sampleTable, ~condition) dds$condition &lt;- relevel(dds$condition, ref=ctrl) dds &lt;- DESeq(dds, parallel=TRUE) res &lt;- results(dds, contrast=c(&quot;condition&quot;, treat, ctrl)) resOrdered &lt;- res[order(res$padj),] resOrdered = as.data.frame(resOrdered) write.table(resOrdered, file=&quot;degs.txt&quot;, quote=F, sep=&quot; t&quot;) .",
            "url": "https://zqfang.github.io/make%20bioinfo%20uncool%20again/2020/01/20/RNAseqTurbo.html",
            "relUrl": "/make%20bioinfo%20uncool%20again/2020/01/20/RNAseqTurbo.html",
            "date": " • Jan 20, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Shortcut to bulk RNA-seq analysis",
            "content": "hisat2-htseq-deseq2 . 3.1 transcriptom mapping . step 0: install tools . conda install htseq hisat2 stringtie . step 1: build index and extract splice sites . build index . hisat2-build -p {threads} genome/hg38.fa hisat2_index/hg38 . extract known splice sites for alignmnet . hisat2_extract_splice_sites.py gencode.gtf &gt; hisat2_index/splicesites.txt hisat2_extract_exons.py gencode.gtf &gt; histat2_index/exon.txt . step2: mapping . hisat2 --dta --threads ${threads} -x hisat2_index/hg38 --known-splicesite-infile hisat2_index/splicesites.txt -1 R1.fq.gz -2 R2.fq.gz -S output.sam . step 3: sam to bam . samtools view -Sbh -q 25 -@ ${threads} -o ouput.bam input.sam . step 4: bam sort and index . samtools sort -@ ${threads} input.bam &gt; output.sorted.bam samtools index input.sorted.bam #generate input.sorted.bam.bai . step 5: bam to bigwig . bamCoverage -p ${threads} --normalizeUsing RPKM # note: other normalization options -b input.sorted.bam -o output.bw . 3.2 Differentially expressed genes analysis . step 1: count reads . htseq-count -r pos -s no --additional-attr gene_name --additional-attr gene_type -f bam input.sorted.bam gencode.gtf &gt; output.count . step2: differentially expressed genes analysis . (1) construct read count table . option 1: HTSeq count file input . library(&quot;DESeq2&quot;) directory &lt;- &quot;/path/to/your/readCountFiles/&quot; sampleFiles &lt;- grep(&quot;count&quot;, list.files(directory), value=TRUE) condition &lt;- factor(c(&quot;KO&quot;,&quot;KO&quot;, &quot;WT&quot;,&quot;WT&quot;), levels = c(&quot;WT&quot;, &quot;KO&quot;)) # phenotable sampleTable &lt;- data.frame(sampleName = sampleFiles, fileName = sampleFiles, condition = condition) # construct read count table ddsHTSeq &lt;- DESeqDataSetFromHTSeqCount(sampleTable = sampleTable, directory = directory, design= ~ condition) . option 2: combined read count file into a single table first, then run . library(DESeq2) # read count table database &lt;- read.table(file = &quot;raw.counts.csv&quot;, sep = &quot;,&quot;, header = TRUE, row.names = 1) database &lt;- round(as.matrix(database)) # set level condition &lt;- factor(c(&quot;KO&quot;,&quot;KO&quot;, &quot;WT&quot;,&quot;WT&quot;), levels = c(&quot;WT&quot;, &quot;KO&quot;)) # build DESeq object coldata &lt;- data.frame(row.names = colnames(database), condition) dds &lt;- DESeqDataSetFromMatrix(countData=database, colData=coldata, design=~condition + treatmement) . (2) run DESeq2 and get output . library(DESeq2) dds &lt;- dds[ rowSums(counts(dds)) &gt; 1, ] # run statistical test dds &lt;- DESeq(dds) # get results res &lt;- results(dds) # summary(res) count_r &lt;- counts(dds, normalized=T) #normalized count matrix # export results res &lt;- res[order(res$padj),] diff_gene &lt;- subset(res, padj &lt; 0.05 &amp; (log2FoldChange &gt; 1 | log2FoldChange &lt; -1)) diff_gene &lt;- row.names(diff_gene) resdata &lt;- merge(as.data.frame(res), as.data.frame(counts(dds, normalized=TRUE)), by=&quot;row.names&quot;, sort=FALSE) write.csv(resdata, file = &quot;DEGs.csv&quot;, row.names = FALSE) . 3.3 Gene set enrichrment analysis . GO . clusterprofiler | Enrichr (GSEApy) | GSEA | . 3.4 Alternative splicing analysis . rMATS .",
            "url": "https://zqfang.github.io/make%20bioinfo%20uncool%20again/2020/01/20/RNAseq.html",
            "relUrl": "/make%20bioinfo%20uncool%20again/2020/01/20/RNAseq.html",
            "date": " • Jan 20, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Shortcut to ChIP-seq analysis",
            "content": "bowtie2-macs2-deeptools . 2.1 Genome mapping . Step 0: install software . # install miniconda, then call conda conda install -c bioconda bowtie2 hisat2 samtools deeptools . step 1: build index . bowtie2-build hg38.fa bowtie2_index/hg38 . step 2: mapping . Unpaired data . bowtie2 -p ${threads} -x index/hg38 -U input.fastq.gz -S ouput.sam . Paired data . bowtie2 -p 4 -x index/hg38 -1 input_R1.fastq.gz -2 input_R2.fastq.gz -S ouput.sam . step 3: sam to bam . samtools view -Sbh -q 25 -@ ${threads} -o ouput.bam input.sam . step 4: bam sort and index . samtools sort -@ ${threads} input.bam &gt; output.sorted.bam samtools index input.sorted.bam #generate input.sorted.bam.bai . step 5: bam to bigwig . bamCoverage -p ${threads} --normalizeUsing RPKM # note: other normalization options --centerReads -e 200 -b input.sorted.bam -o output.bw . 2.2 Peaks analysis . note:: macs2 (&gt;v2.2.x) supports python 3. . step 0: install tools . conda install macs2 bedtools pygenometracks . step 1: callpeaks (1) narrow peaks, e.g. TFs, h3k4m3 . # bam file input macs2 callpeak -t ChIP.elute.sorted.bam -c ChIP.input.sorted.bam -f BAM -g hs # organism -B -q 0.05 -n ${outFileName} --outdir macs_out . (2) Broad peaks, e.g. h3k27me3 . # sam file also works fine macs2 callpeak -t ./bowtie_out/WTme2ChIP.sam -c ./bowtie_out/ESCInput.sam -f SAM -g mm -B --SPMR --nomodel --extsize 147 --broad -n WTme2ChIP --outdir macs_out . step 2: advanced analysis . tools: bedtools, deeptools, pyGenomeTracks, igv | genome algebra overlap with other peaks: bedtools | | . | visualization heatmap: deeptools | signal tracks: pyGenomeTracks, igv | . | .",
            "url": "https://zqfang.github.io/make%20bioinfo%20uncool%20again/2020/01/20/ChIP.html",
            "relUrl": "/make%20bioinfo%20uncool%20again/2020/01/20/ChIP.html",
            "date": " • Jan 20, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Cheatsheet for command line",
            "content": "usefull tools for linux command line . a. Make terminal cool, install OhMyZsh . # install zsh sudo apt-get install zsh # ubuntu # change default shell to zsh chsh -s /usr/bin/zsh # install ohmyzsh sh -c &quot;$(wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)&quot; source ~/.zshrc . b. Terminal keyboard short cuts . Jump to head: Ctrl + a | Jump to end: Ctrl + e | Delete strings ahead: Ctrl + u | Delete strings follow: Ctrl + k | . c. Program keeps running in the background . 1. Run cmd using nohup . nohup command [options] &amp; . 2. Run cmd using Tmux . Outside Tmux: Typically these are run outside, but you can also run them inside an existing session . a. Start New Session . tmux new -s myname . b. Attach To Existing Session . tmux attach -t myname #by name tmux attach 4 #by number (in this case 4) . c. List Sessions . tmux ls . d. Kill Session . tmux kill-session -t myname . Inside Tmux Session: Start each command with CTRL + b, release, then press one of the following: . Panes   . % | vertical split | . ” | horizontal split | . d | detach from session (it keeps running in the background) | . x | kill pane | . Up/Down/Left/Right | move between panes | . PageUP/PageDown | CTRL+c to exit the PageUp/Down mode | . Fn+Up/Down | PageUp/Down: Mac keyboard | . : + resize-pane -D | Resizes the current pane down | . : + resize-pane -U | Resizes the current pane upward | . : + resize-pane -L | Resizes the current pane left | . : + resize-pane -R | Resizes the current pane right | . : + resize-pane -D 20 | Resizes the current pane down by 20 cells | . d. File compression and decompression . Decompression . File type Cmd e.g. . *.tar | tar -xvf |   | . *.tar.gz or *.tgz | tar -xvzf |   | . *bz2 | bzip2 -d or bunzip2 |   | . *.tar.bz2 | tar -xjf |   | . *.Z | uncompress |   | . *.tar.Z | tar -xZf |   | . *.rar | unrar e or rar x | unrar e file.rar | . *.zip | unzip |   | . *.gz | gunzip |   | . Compression . File type Cmd e.g. . *.tar | tar -cvf |   | . *.tar.gz or *.tgz | tar -cvzf |   | . *bz2 | bzip2 -z |   | . *.tar.bz2 | tar -cjf |   | . *.Z | compress |   | . *.tar.Z | tar -cZf |   | . *.rar | rar a | rar a -ep1 newname /home/user/cpp | . *.zip | zip |   | . *.gz | gzip |   | . For rar installation . sudo apt-get install rar . e. Handy tricks for handling filepath . very useful to strip file sufix, path et.al. . # e.g. var=./home/fastq/filename_R1.fq.gz # extract filename ${var#*/} # -&gt; home/fastq/filename_R1.fq.gz var1=${var##*/} # -&gt; filename_R1.fq.gz # remove file suffix ${var1%.*} # -&gt; filename_R1.fq ${var1%%.*} # -&gt; filename_R1 # get basebame var2=$(basename &quot;${var}&quot; .fq.gz) #-&gt; filename_R1 .",
            "url": "https://zqfang.github.io/make%20bioinfo%20uncool%20again/2020/01/20/Begin.html",
            "relUrl": "/make%20bioinfo%20uncool%20again/2020/01/20/Begin.html",
            "date": " • Jan 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a computational biologist working on genomics. I’m interested in bioimaging genomics, single-cell genomics and deep learning. I open this blog to keep track of my machine learning notes. .",
          "url": "https://zqfang.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
      ,"page4": {
          "title": "Publications",
          "content": "SOX21 Ensures Rostral Forebrain Identity by Suppression of WNT8B during Neural Regionalization of Human Embryonic Stem Cells . Published in Stem Cell Reports, 2019 . . My PhD thesis . . Download here . CDK11 safeguards the identity of human embryonic stem cells via fine‐tuning signaling pathways . Published in Journal of Cellular Physiology, 2019 . . Ding J, Fang Z, Liu X, Zhu Z, Wen C, Wang H, et al. CDK11 safeguards the identity of human embryonic stem cells via fine-tuning signaling pathways. J Cell Physiol. 2019 . . Download here . Stk40 deletion elevates c-JUN protein level and impairs mesoderm differentiation . Published in Journal of Biological Chemistry, 2019 . . Hu J, Li S, Sun X, Fang Z, Wang L, Xiao F, et al. Stk40 deletion elevates c-JUN protein level and impairs mesoderm differentiation. J Biol Chem. 2019;294(25):9959-72. . . Download here . Transcription coactivator Cited1 acts as an inducer of trophoblast-like state from mouse embryonic stem cells through the activation of BMP signaling . Published in Cell Death &amp; Disease, 2018 . . Xu Y, Luo X, Fang Z, Zheng X, Zeng Y, Zhu C, et al. Transcription coactivator Cited1 acts as an inducer of trophoblast-like state from mouse embryonic stem cells through the activation of BMP signaling. Cell Death Dis. 2018;9(9):924. . . Download here . Bioconda: sustainable and comprehensive software distribution for the life sciences . Published in Nature Methods, 2018 . . I’m honored to be one of the contributors. It is definitely worthy. Thanks for the Bioconda team, so we could install bioinformatic tools so easy! The open source Bioconda project is a milestone in computational biology. It saves lots of time when trying to install bioinfo tools. . . Download here . Single-cell analysis reveals lineage segregation in early post-implantation mouse embryos . Published in Journal of Biological Chemistry, 2017 . . Single cell analysis of mouse E5.5, E6.5 embryos. In this work, I dissected the embryos and prepared the single cell cDNA libraries. . . Download here . Deletion of Stk40 impairs definitive erythropoiesis in the mouse fetal liver . Published in Cell Death &amp; Disease, 2017 . . Wang L, Yu H, Cheng H, He K, Fang Z, et al. Deletion of Stk40 impairs definitive erythropoiesis in the mouse fetal liver. Cell Death Dis. 2017;8:e2722 . . Download here . Itch, an E3 ligase of Oct4, is required for embryonic stem cell self-renewal and pluripotency induction . Published in Journal of Cellular Physiology, 2012 . . Liao B, Zhong XM, Xu HM, Xiao F, Fang ZQ, et al. Itch, an E3 ligase of Oct4, is required for embryonic stem cell self-renewal and pluripotency induction. Journal of Cellular Physiology 2013;228:1443-51 . . Download here .",
          "url": "https://zqfang.github.io/publications/",
          "relUrl": "/publications/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  

  

  
  

  
      ,"page14": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://zqfang.github.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}