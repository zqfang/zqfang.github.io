<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Nature Language Processing - Category - My Computational Genomic Playground</title><link>https://zqfang.github.io/categories/nature-language-processing/</link><description>Nature Language Processing - Category - My Computational Genomic Playground</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Fri, 31 Jul 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://zqfang.github.io/categories/nature-language-processing/" rel="self" type="application/rss+xml"/><item><title>NLP: RNN and Self-attension</title><link>https://zqfang.github.io/2020-07-31-nlp-rnn/</link><pubDate>Fri, 31 Jul 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-07-31-nlp-rnn/</guid><description>Backpropagation Through Time Long Short-Term Memory Delete information from the context that is no longer needed: Forget Gate f $$ f_t = \sigma (U_f h_{t-1} + W_f X_t) $$
$$ k_t = c_{t-1} \odot f_t $$
Compute the actual information we need to extract from the previous hidden stat and current inputs $$ g_t = \tanh (U_g h_{t-1} + W_g x_t) $$</description></item><item><title>NLP: A short guide for biologist</title><link>https://zqfang.github.io/2020-06-20-nlp-basics/</link><pubDate>Sat, 20 Jun 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-06-20-nlp-basics/</guid><description>NLP Basics for the newbies like me
Languwage model Models that assigns probabilities to sequences of words are called languwage models.
Count-based Representation 1. one-hot representation 2. BoW: Bag of words Blow describes the occurrence of words within a document. including
A Vocabulary of known words A measure of the presence of known words, e.</description></item></channel></rss>