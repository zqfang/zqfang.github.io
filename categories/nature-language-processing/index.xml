<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Nature Language Processing - Category - My Computational Genomic Playground</title><link>https://zqfang.github.io/categories/nature-language-processing/</link><description>Nature Language Processing - Category - My Computational Genomic Playground</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 06 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://zqfang.github.io/categories/nature-language-processing/" rel="self" type="application/rss+xml"/><item><title>Graph Embedding: Node2Vec</title><link>https://zqfang.github.io/2020-12-06-ml-node2vec/</link><pubDate>Sun, 06 Dec 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-12-06-ml-node2vec/</guid><description><![CDATA[Node Embedings are learnt in the same way as word2vec (skip-gram model)
However, graphs could be (un)directed, (un)weighted, (a)cyclic and are basically much more complex than the strucure of a sequence&hellip;
So how do we generate &ldquo;corpus&rdquo; from a graph ?
Sampling strategy Node2vec&rsquo;s sampling strategy accepts 4 argument:
 Number of walks: number of random walks to be generated from each node in the graph Walk length: how many nodes are in each random walk P: return hyperparameter Q: Inout hyperparameter (DFS biased or BFS baised control)  Also, the standard skip-gram parameters]]></description></item><item><title>NLP: Word2Vec</title><link>https://zqfang.github.io/2020-12-05-nlp-word2vec/</link><pubDate>Sat, 05 Dec 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-12-05-nlp-word2vec/</guid><description>Word2Vec
CBOW Continuous Bag of Words Model (CBOW)
When trainning, use N-gram language model. That&amp;rsquo;s for a target word, select $m$ (window) words before and after.
Model one-hot encoding get $2m$ vectors: $$X = (x^{c-m}, \cdots, x^{c-1}, x^{c+1}, \cdots, x^{c+m})$$
Embeding Vector $\mathcal{V} \in R^{n \times \mathcal{V}}$,</description></item><item><title>NLP: Parse trees</title><link>https://zqfang.github.io/2020-08-01-nlp-constituency-and-depenency-parsing/</link><pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-08-01-nlp-constituency-and-depenency-parsing/</guid><description>Structure of Sentences: Parse trees Shallow parsing identifies phrasal units, the task of identifying the relationship between them is called parsing.
Parse trees indicate how different grammatical units in a sentence are related hierachically. (aslo refer to constituent parse, chart-based )
dependency parsing: directed graph (graph-based)</description></item><item><title>NLP: RNN and Transformers</title><link>https://zqfang.github.io/2020-07-31-nlp-rnn/</link><pubDate>Fri, 31 Jul 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-07-31-nlp-rnn/</guid><description>Backpropagation Through Time Long Short-Term Memory Delete information from the context that is no longer needed: Forget Gate f $$ f_t = \sigma (U_f h_{t-1} + W_f X_t) $$ $$ k_t = c_{t-1} \odot f_t $$ Compute the actual information we need to extract from the previous hidden stat and</description></item><item><title>NLP: Start point for biologist</title><link>https://zqfang.github.io/2020-06-20-nlp-basics/</link><pubDate>Sat, 20 Jun 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-06-20-nlp-basics/</guid><description>NLP Basics for the newbies like me
Languwage model Models that assigns probabilities to sequences of words are called languwage models.
Count-based Representation 1. one-hot representation 2. BoW: Bag of words Blow describes the occurrence of words within a document. including
A Vocabulary of known words A measure of the presence of known words, e.</description></item></channel></rss>