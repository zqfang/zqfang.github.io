
---
title: "RoPE: Rotary Positional Embeddings"
date: 2024-07-29
categories: ["Nature Language Processing"]
comments: true
tags: ["Deep Learning", "LLM", "NLP"]
math: true
---

Rotary Positional Embeddings, proposed in 2022, this innovation is swiftly making its way into prominent language models like Google's PaLM and Meta's LLaMa. 


RoPE is a new type of positional encoding that unifies absolute and relative positional encoding approaches


Rotary Positional Encoding is a type of position encoding that encodes **absolute positional information** with a rotation matrix and naturally **incorporates explicit relative position dependency** in self-attention formulation


RoPE provide a flexible mechanism to include positional context into tokens, without modifying the original embedding. the core principle revolves around rotating the queries and keys in the attention mechanism, where each position in the sequence receives a unique rotation. This way, the dot product between queries and keys gradually diminishes for tokens that are distant from one another in the sequence, providing an effective way to encode relative positions.


This approach tends to maintain more fo the original token information while still providing the model with an effective way to understand sequence positions.

The implementation looks like tthis
```python
def rotary_positional_embedding(position, d_model):
    freqs = np.exp(np.linspace(0., -1., d_model /2) * np.log(10000.)) #1
    angles = position * freqs # 2
    rotary_matrix - np.stack([np.sin(angels), np.cos(angles)], axis =-1) # 3
    return rotary_matrix.reshape(-1, d_model) #4
```



{{< youtube o29P0Kpobz0&t=530s >}}


## reference 

1. [The large language model playbook](https://cyrilzakka.github.io/llm-playbook/nested/rot-pos-embed.html)
2. [Efficient NLP](https://www.youtube.com/watch?v=o29P0Kpobz0&t=530s)
3. [让研究人员绞尽脑汁的Transformer位置编码](https://kexue.fm/archives/8130) 