[{"categories":["Nature Language Processing"],"content":"Basic concepts in NLP ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:0:0","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Traditional NLP ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:0","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Count-based Representation one-hot representation BoW: Bag of words Bow describes the occurrence of words within a document. including A Vocabulary of known words A measure of the presence of known words, e.g. count TF or TF-IDF representation: Term Frequency Inverse Document Frequency TF: the sum of the one-hot representation of a phrase, sentence or document’s constituent words $$ TF (w) = \\frac { \\text{ Number of the term w appears in the document }} { \\text{Number of terms in the document}} $$ IDF: penalizes common tokens and rewards rare tokens $$ IDF(w) = \\log \\frac{\\text{Number of documents}}{\\text{Number of documents with term w}} $$ TF-IDF: $TF(w) \\times IDF(w)$ ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:1","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Corpora, Tokens, and Types corpus (plural: corpora): a text dataset tokens (English): words and numeric sequences separated by white-spaces characters or punctuation instance or data point: the text along with its metatdata dataset: a collection of instances types: unique tokens present in a corpus. vocabulary or lexicon: the set of all types in a corpus ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:2","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Unigram, Bigrams, Trigrams, … , N-grams N-grams are fixed-length consecutive token sequence occurring in the text ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:3","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Lemmas and Stems Lemmas are the root forms of words. e.g. the root form of the word fly, can be inflected into other words – flow, flew, flies, flown, flowing … Stemming: use handcrafted rules to strip endings of words to reduce them to a common form called stems ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:4","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Word Senses and Semantics Senses: the different meanings of a word ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:5","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Categorizing Words: POS Tagging part-of-speech (POS) tagging: labeling individual words or tokens ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:6","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Categorizing Spans: Chunking and Named Entity Recognition a span of text: a contiguous multi-token boundary. chunking or shallow parsing: identify the noun phrases (NP) and verb pharses (VP) in a span of text. A named entity is a string mention of a real-world concept like a person, location, organization, drug name, et. al. ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:7","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Structure of Sentences: Parse trees Shallow parsing identifies phrasal units, the task of identifying the relationship between them is called parsing. Parse trees indicate how different grammatical units in a sentence are related hierachically. (aslo refer to constituent parse, chart-based ) dependency parsing: directed graph (graph-based) node -\u003e word edge -\u003e relation all the words have one incoming edge, except ROOT there is a unique path from each word to ROOT maximum spanning tree SyntaxNet: ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:8","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Reference Hung-yi Lee: Deep Learning for human language processing ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:2:0","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Statistic"],"content":"样本量、效应量、显著水平和统计功效的统计原理和计算 效应量通常用三种方式来衡量：标准均差（standardized mean difference），几率（odd ratio），(3) 相关系数（correlation coefficient）。 ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:0:0","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"统计原理 可视化样本量、效应量、α和统计功效的关系 Significance The probability of a type I error is usually denoted by $\\alpha$ and is commonly referred to as the signiﬁcance level of a test. \\The probability of a type II error is usually denoted by $\\beta$. ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:1:0","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"显著水平 α Type Error 定义 表示 举例 Ⅰ 型错误 拒绝实际上成立的$H_0$ Ⅰ 型错误的概率用显著水平 α 表示, 假阳性、误诊 II 型错误 拒绝（“接受”）实际上不成立的$H_0$ II 型错误概率用 β 表示 假阴性、漏诊 α 常取值0.05、0.01，α可以取单尾、双尾。需假设检验前预先设定。 β 只取单尾 形象化理解 ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:1:1","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"功效（power） 功效（power）：正确拒绝原假设的概率，记作1-β, 即 $$ 1 - \\beta = \\operatorname{Pr} ( \\text{rejecting } H_0 | H_1 true) $$ 假设检验的功效受以下三个因素影响： 样本量 (n)：其他条件保持不变，样本量越大，功效就越大。 显著性水平 (α)： 其他条件保持不变，显著性水平越低，功效就越小。 两总体之间的差异：其他条件保持不变，总体参数的真实值和估计值之间的差异越大，功效就越大。也可以说，效应量（effect size）越大，功效就越大。 ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:1:2","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"效应量（effect size） 效应量： 样本间差异或相关程度的量化指标。效应量越大，表示两个总体重叠的程度越小，效应越明显。 效应量通常用三种方式来衡量：(1) 标准均差（standardized mean difference），(2) 几率（odd ratio），(3) 相关系数（correlation coefficient）。 Difference family: Effect sizes based on differences between means 标准均差（standardized mean difference） Standardized mean difference: 基于总体均值和方差， 效应量为 $$ \\theta = \\frac{\\mu_1 - \\mu_2}{\\sigma} $$ Cohen’s d : 两总体均值之间的标准差异。适用于两组样本的样本量和方差相似的情况。 $$ d = \\frac{ \\bar{x}_1 - \\bar{x}_2}{s} = \\frac{\\mu_1 - \\mu_2}{\\sigma} $$ s是样本方差 $$ s = \\sqrt{\\frac{(n_1 - 1)s^2_1 + (n_2 - 1)s^2_2}{n_1 + n_2}} $$ $$ s_1 = \\frac{1}{n_1 -1} \\sum_{i=1}^{n_1} (x_{1,i} - \\bar{x}_1)^2 $$ d = 0.01 to 2.0 Effect size d Reference Very small 0.01 Sawilowsky, 2009 Small 0.20 Cohen, 1988 Medium 0.50 Cohen, 1988 Large 0.80 Cohen, 1988 Very large 1.20 Sawilowsky, 2009 Huge 2.0 Sawilowsky, 2009 Hedges’ g: 是cohen的方法的改进，适用于两组样本的样本量不同的情况。 $$ g = \\frac{ \\bar{x}_1 - \\bar{x}_2}{s^*} $$ 而$s^{*}$是 $$ s^* = \\sqrt{\\frac{(n_1 - 1)s^2_1 + (n_2 - 1)s^2_2}{n_1 + n_2 -2 }} $$ Glass’s Δ （delta）: 和cohen的方法类似，但是只除以控制组(control)的标准差。适用于两组样本的方差不同的情况。 $$ \\Delta = \\frac{ \\bar{x}_1 - \\bar{x}_2}{s_2} $$ Categorical family: Effect sizes for associations among categorical variables Odd ratio (OR) The odds ratio is the odds of success in the treatment group relative to the odds of success in the control group. 适用于binary数据。 Relative risk (RR) or risk ratio the risk (probability) of an event relative to some independent variable. Risk difference or absolute risk reduction the difference in risk (probability) of an event between two groups Cramer’s φ (Phi) or Cramer’s V: 用于测算类别型数据 (nominal data) 的效应量。当类别型变量包含2个类别时，使用Cramer’s phi，如果超过2个类别，那么使用Cramer’s V。 Cohen’s w … Correlation family: Effect sizes based on “variance explained” Pearson r correlation Effect size r small ~ 0.1 medium ~ 0.3 large r \u003e 0.5 Cohen’s $f^2$: 用于测算方差分析ANOVA，多元回归之类的效应量。 多元回归的效应量 $$ f^2 = \\frac{R^2}{1-R^2} $$ where $R^2$ is the squared multiple correlation (Coefficient of determination) ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:1:3","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"功效、效应量和样本量计算 ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:2:0","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"计算样本量 determining-sample-size ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:2:1","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"计算效应量 用statsmodels库计算功效，效应量和样本量的函数都是同一个，只要把需要计算的那个值仍然设为None，把其他想要达到的数值填上即可 单样本t检验： statsmodels.stats.power.tt_solve_power(effect_size=None, nobs=None, alpha=None, power=None, alternative='two-sided') 独立样本t检验： statsmodels.stats.power.tt_ind_solve_power(effect_size=None, nobs1=None, alpha=None, power=None, ratio=1.0, alternative='two-sided') 卡方拟合优度检验： statsmodels.stats.power.GofChisquarePower.solve_power(effect_size=None, nobs=None, alpha=None, power=None, n_bins=2) F方差齐性检验： statsmodels.stats.power.FTestPower.solve_power(effect_size=None, df_num=None, df_denom=None, nobs=None, alpha=None, power=None, ncc=1) 方差分析： statsmodels.stats.power.FTestAnovaPower.solve_power(effect_size=None, nobs=None, alpha=None, power=None, k_groups=2) ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:2:2","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"StatQuest Sample size Power Analysis 参考： https://en.wikipedia.org/wiki/Effect_size https://www.cnblogs.com/HuZihu/p/12009535.html ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:3:0","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"Quantile normalization is frequently used in microarray data analysis. It was introduced as quantile standardization and then renamed as quantile normalization. ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:0:0","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"Quantile, quartile, percentile ??? Quantiles are just the lines that divide data into equally sized groups. percentiles are just quantiles that divide the data into 100 equally sized groups Example: 0 quartile = 0 quantile = 0th percentile 1 quartile = 0.25 quantile = 25th percentile 2 quartile = .5 quantile = 50th percentile (median) 3 quartile = .75 quantile = 75th percentile 4 quartile = 1 quantile = 100th percentile ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:1:0","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"Quantile normalization Quantile normalization transform the statistical distributions across samples to be the same. ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:2:0","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"Assumptions The roughly same distribution of values across samples Most genes are not differentially expressed Assume global differences in the distribution are induced by only technical variation! ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:2:1","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"How Q-normalization work row: genes column: samples/Arrays Procedure: order values within each sample determine a rank from lowest to highest and record the order within each sample Average across rows and substitute value with average re-order averaged values in the original order recorded in 2. Tied rank entries ? Average the tied rank entries’ mean values and substitute. ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:2:2","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"When NOT to normalize Consider a dilution experiment. In which distributions are supposed to decrease (left plot), Q-normalization does the totally wrong thing (right plot). When you expect a real difference in distributions, Q-normalization will create weird artifacts. ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:2:3","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"Smooth quantile ormalizaiton ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:3:0","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"Assumptions the statistical distribution of each sample should be the same ( or have the same distributional shape) within biological groups or conditions, but allowing that they may differ between groups ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:3:1","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"How to At each quantile, a weight is computed comparing the variability between groups relative to the total variability between and within groups Let gene(g) denote the $g^{th}$ row after sorting each column in the data. For each row, gene(g), we compute the weight $w(g)$ ∈ [0,1], where a weight of 0 implies quantile normalization within groups is applied and a weight of 1 indicates quantile normalization is applied. The weight at each row depends on the between group sum of squares SSB(g) and total sum of squares SST(g), as follows: $$ w_{(g)} = \\operatorname{median} \\bigg\\lbrace 1- \\frac{SSB_{(i)}}{SST_{(i)}} \\bigg\\rbrace \\text{for } i = g -k, \\cdots, g, \\cdots, g+k $$ where $k$ = floor(Total number of genes * 0.05). The number 0.05 is a flexible parameter that can be altered to change the window of the number of genes considered. ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:3:2","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"StatQuest: Quantile Normalization ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:4:0","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"reference https://en.wikipedia.org/wiki/Quantile_normalization BIOMEDIN 245: Statistical and Machine Learning Methods for Genomics, Stanford Hicks SC, Okrah K, Paulson JN, Quackenbush J, Irizarry RA, Bravo HC. Smooth quantile normalization. Biostatistics. 2018;19(2):185‐198. doi:10.1093/biostatistics/kxx028 ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:5:0","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"What’s Multilevel models, and how to deal with it ","date":"2020-06-13","objectID":"/2020-06-13-stats-hierachial-models/:0:0","tags":null,"title":"Multilevel (Hierachical) Models","uri":"/2020-06-13-stats-hierachial-models/"},{"categories":["Statistic"],"content":"What is multilevel model Multilevel model AKA: multilevel Models random-effects models hierarchical models variance-components models random-coefficient models mixed models Many kinds of data, including observational data collected in the human and biological sciences, have a hierarchical or clustered structure, or non-hierarchical structures.. ","date":"2020-06-13","objectID":"/2020-06-13-stats-hierachial-models/:1:0","tags":null,"title":"Multilevel (Hierachical) Models","uri":"/2020-06-13-stats-hierachial-models/"},{"categories":["Statistic"],"content":"A Simple Example Given a set of repeated measures data giving growth patterns for a sample of 26 boys in Oxford, England. The height of each boy is measured on nine different occasions. We could try modelling the growth pattern with a simple linear regression $$ H = \\beta_0 + \\beta_1 A + \\epsilon $$ where H: height A: age $\\epsilon$: the variation in height that cannot be explained by the linear relationship with age. However, If we try to use the model above for the complete set of data, the fit will be very poor (see figure above) To make model more realistic, we allow the intercept in the model above to vary from subject to subject. New multilevel model: $$ H_{ij} = \\beta_{0j} + \\beta_1 A_{ji} + \\epsilon_{ij} $$ Now, assume that the individual intercepts follow a normal distribution with variance $\\tau_{0}$, $$ \\beta_{0j} = \\beta_{0} + \\mu_{0j} $$ where, $\\mu_{0j} \\sim \\mathcal{N} (0, \\tau_0)$, $\\mu_{0j}$ accounts for the variation from one subject to another Fitting the multilevel model to the data, and obtain much better predictions ","date":"2020-06-13","objectID":"/2020-06-13-stats-hierachial-models/:2:0","tags":null,"title":"Multilevel (Hierachical) Models","uri":"/2020-06-13-stats-hierachial-models/"},{"categories":["Statistic"],"content":"How do multilevel models differ from regression models? $$ H_{ij} = (\\beta_0 + \\mu_{0j}) + \\beta_1 A_{ij} + \\epsilon_{ij} = \\beta_0 + \\beta_1 A_{ij} + \\mu_{0j} + \\epsilon_{ij} $$ The feature that distinguishes this model from an ordinary regression model is the presence of two random variables the measurement level random variable $\\epsilon_{ij}$ the subject level random variable $\\mu_{0j}$ Because multilevel models contain a mix of fixed effects and random effects, they are sometimes known as mixed-effects models. ","date":"2020-06-13","objectID":"/2020-06-13-stats-hierachial-models/:3:0","tags":null,"title":"Multilevel (Hierachical) Models","uri":"/2020-06-13-stats-hierachial-models/"},{"categories":["Statistic"],"content":"Benefits of multilevel modelling Generalize to a wider population Fewer parameters are needed Information can be shared between groups ","date":"2020-06-13","objectID":"/2020-06-13-stats-hierachial-models/:4:0","tags":null,"title":"Multilevel (Hierachical) Models","uri":"/2020-06-13-stats-hierachial-models/"},{"categories":["Statistic"],"content":"How to deal with hierachical structures Hierarchical structures : model all levels simultaneously Non- Hierarchical structures cross-classified structure multiple membership with weights … 参考： https://en.wikipedia.org/wiki/Multilevel_model http://www.statstutor.ac.uk/resources/uploaded/multilevelmodelling.pdf https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/hierarchical-models.pdf ","date":"2020-06-13","objectID":"/2020-06-13-stats-hierachial-models/:5:0","tags":null,"title":"Multilevel (Hierachical) Models","uri":"/2020-06-13-stats-hierachial-models/"},{"categories":["Machine Learning"],"content":"蒙特卡罗方法，又称统计模拟方法(statistical simulation method), 通过概率模型的随机抽样进行进行近似数值计算的方法。 马可夫蒙特卡罗法（Markov Chain Monte Carlo, MCMC）则是以马可夫链为概率模型的蒙特卡罗方法。 Metropolis-Hastings算法是最基本的MCMC。 Gibbs sampling是更简单、使用更广泛的MCMC。 Markov Chain Monte Carlo (MCMC) ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:0:0","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"蒙特卡罗法（Monte Carlo） 蒙特卡罗法要解决的问题是，假设概率分布的定义己知，通过抽样获得概率分布的随机样本，并通过得到的随机样本对概率分布的特征进行分析 ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:1:0","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"1. 蒙特卡罗方法的核心 蒙特卡罗方法的核心是随机抽样(random sampling) 直接抽样 接受-拒绝抽样： 适用于概率密度函数复杂，不能直接抽样的情况 重要性抽样： 适用于概率密度函数复杂，不能直接抽样的情况 接受-拒绝抽样思想：找一个可以直接抽样的建议分布（proposal distribution），其概率密度函数为$q(x)$, 并且$q(x)$的$c$倍一定大于$p(x)$， 其中$c \u003e 0$,按照$q(x)$进行抽样，假设得到结果$x^\\ast$， 再按照$\\frac{p(x^\\ast)}{cq(x^\\ast )}$的比例随机决定是否接受$x^\\ast$。落到$p(x)$范围内的就接受，落到$p(x)$范围外的就拒绝❌。 这些抽样方法的缺点： 抽样效率低， 比如 $p(x)$ 占 $cq(x)$ 涵盖体积比例很低 当x为高维数据时，很难寻找合适的建议分布 一个解决办法就是MCMC. ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:1:1","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"2. 数学期望估计(Estimation of mathematical expectation) 按照概率分布 $p(x)$ 独立抽取n个样本后计算函数的样本均值 $$ \\hat f_{n}=\\frac{1}{n} \\sum_{i=1}^{n} f\\left(x_{i}\\right) $$ 作为数学期望的近似值。 根据大数定律可知，当样本容量增大是，样本均值以概率1收敛性于数学期望 $$ \\hat f_{n} \\rightarrow E_{p(x)}[f(x)], \\quad n \\rightarrow \\infty $$ 于是，得到数学期望的近似计算方法 $$ E_{p(x)}[f(x)] \\approx \\frac{1}{n} \\sum_{i=1}^{n} f\\left(x_{i}\\right) $$ ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:1:2","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"3. 蒙特卡罗积分（Monte carlo intergration） 计算函数 $h(x)$ 积分 $$ \\int_{\\mathcal{X}} h(x) \\mathrm{d} x $$ 将 $h(x)$ 分解成 $f(x)$ 和概率密度函数 $p(x)$ 的乘积，即函数 $h(x)$ 的积分可以表示为函数 $f(x)$ 关于概率密度函数 $p(x)$ 的数学期望： $$ \\int_{\\mathcal{X}} h(x) \\mathrm{d} x=\\int_{\\mathcal{X}} f(x) p(x) \\mathrm{d} x=E_{p(x)}[f(x)] $$ 因此，可利用样本均值计算近似积分： $$ \\int_{\\mathcal{X}} h(x) \\mathrm{d} x=E_{p(x)}[f(x)] \\approx \\frac{1}{n} \\sum_{i=1}^{n} f\\left(x_{i}\\right) $$ 更进一步 $$ \\begin{aligned} E_{p(z)}[f(z)] \u0026= \\int f(z) p(z) dz \\cr \u0026= \\int \\underbrace{f(z) \\frac{p(z)}{q(z)}}_{new \\tilde{f} (z)} q(z) dz \\cr \u0026 \\approx \\frac{1}{N} \\sum_{n=1}^{N} f(z^{i}) \\frac{p(z^{i})}{q(z^{i})} \\end{aligned} $$ ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:1:3","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"Markov Chain ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:2:0","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"定义 马可夫性： 随机变量$X_t$只依赖$X_{t-1}$，而不依赖过去的随机变量 $\\lbrace X_{0}, X_{1}, \\cdots, X_{t-2} \\rbrace$。即 $$ P\\left(X_{t} | X_{0}, X_{1}, \\cdots, X_{t-1}\\right)=P\\left(X_{t} | X_{t-1}\\right), \\quad t=1,2, \\cdots $$ 马可夫链或马可夫过程（markov process）指： 具有马可夫性的随机序列 $X=\\lbrace X_{0}, X_{1}, \\cdots, X_{t}, \\cdots \\rbrace$。 马可夫链的转移条件概率分布为 $P(X_t | X_{t-1})$ 。转移概率分布决定马可夫链的特性。 时间齐次马可夫链（time homogenous Markov Chain）是指转移状态分布于t无关的马可夫链 离散状态马可夫链 状态转移矩阵 平稳分布： 马可夫链 $X$， 其状态空间为$\\mathcal{S}$， 转移矩阵为 $P = (p_{ij})$， 如果存在状态空间 $\\mathcal{S}$ 上的一个分布 $$ \\pi = \\left[\\begin{array}{c} \\pi_1 \\cr \\pi_2 \\cr \\vdots \\end{array}\\right] $$ 使得 $\\pi=P\\pi$, 则称$\\pi$为马可夫链$X = {X_0, X_1, \\cdots, X_t, \\cdots }$ 的平稳分布 连续状态马可夫链 定义在连续状态空间，转移概率分布有概率转移核（trainsition kernel）表示 $$ P(x, A) = \\int_{A} p(x, y) dy $$ 转移核$P(x, A)$表示转移概率 $$ P (X_t = A | X_{t-1} = x) = P (x, A) $$ 马可夫链的性质 不可约 (irreducible): 时刻 0 从状态 $j$ 出发，时刻 $t$ 到达状态 $i$ 的概率大于 0 ，则称此马尔可夫链 $X$ 是不可约的 $$ P(X_t = i | X_0 = j) \u003e 0 $$ 非周期：不纯在一个状态，使得再返回到这个状态所经历的时间长呈周期性 正常返(positive recurrent): 任意一个状态$i$，从其他任意状态 $j$ 出发，当时间趋近无穷时，首次转移到这个状态$i$的概率 $p^t_{ij}$ 不为0 $$ \\lim_{t \\rightarrow \\infty} p^t_{ij} \u003e 0 $$ 遍历定理：满足相应条件的马尔可夫链，当时间趋于无穷时，马尔可 夫链的状态分布趋近于平稳分布，随机变量的函数的样本均值以概率 1 收敛于该函数 的数学期望 马可夫链 $X$， 其状态空间为$\\mathcal{S}$， 若马可夫链 $X$ 不可约、非周期且正常返， 则马可夫链有唯一的平稳分布 $\\pi = (\\pi_1, \\pi_2, \\cdots)^T$， 并且转移概率的极限分布是马可夫链的平稳分布 $$ \\lim_{t \\rightarrow \\infty} P(X_t = i | X_0 = j) = \\pi_i, i = 1,2, \\cdots ; j = 1,2,\\cdots $$ 若 $f(X)$是定义在状态空间上的函数 $E_{pi}[ | f(X) | ] \u003c \\infty$, 则 $$ P { \\hat{f_t} \\rightarrow E_{pi}[ f(X) ] } = 1 $$ 这里， $$ \\hat{f_t} = \\frac{1}{t} \\sum^t_{s=1} f(x_s) $$ 关于平稳分布 $\\pi = (\\pi_1, \\pi_2, \\cdots)^T$ 的数学期望 $E_{pi}[f(X)] = \\sum f(i)\\pi_i$, 有 $$ \\hat{f_t} \\rightarrow E_{pi}[ f(X) ], t \\rightarrow \\infty $$ 处处成立或以概率1成立。 ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:2:1","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"Markov Chain Monte Carlo 马可夫蒙特卡罗法更适合随机变量是多元的、密度函数是非标准形式的、随机变量各分量不独立等情况 基本思想： 在随机变量$x$的状态空间$\\mathcal{S}$上定一个满足遍历定理的马可夫链，使其平稳分布就是抽样的目标分布 $p(x)$， 然后在这个马可夫链上进行随机游走，每个时刻得到一个样本。根据遍历定理，当时间趋于无穷是，样本的分布趋近平稳分布，样本函数均值趋近函数的数学期望 $$ \\hat{E}f = \\frac{1}{n-m} \\sum^{n}_{i=m+1} f(x_i) $$ 马尔可夫链蒙特卡罗法的关键是如何构建转移核函数或转移矩阵， 包括： Metropolis-Hastings 和 Gibbs sampling。 马尔可夫链蒙特卡罗法中得到的样本序列，相邻的样本点是相关的，而不是独立的 马尔可夫链蒙特卡罗法的收敛性的判断通常是经验性的 基本步骤： 在随机变量$x$的状态空间$\\mathcal{S}$上构建一个满足遍历定理的马可夫链，使其平稳分布为目标分布 $p(x)$ 从状态空间的某一点 $X_0$ 出发，用构造的马可夫链进行随机游走，产生样本序列 ${x_0, x_1, \\cdots, x_t, \\cdots }$。 应用马可夫链的遍历定理， 确定正整数 $m$ 和 $n$， $m \u003c n$， 得到样本集合 ${x_{m+1}, x_{m+2}， \\cdots, x_n }$ 求得函数f的（遍历）均值 $$ \\hat{E}f = \\frac{1}{n-m} \\sum^{n}_{i=m+1} f(x_i) $$ 几个重要问题需要注意： 如何定义马可夫链，保证MCMC成立 如何确定收敛步数m，保证抽样无偏性 如何确定迭代步数n， 保证遍历均值的计算精度 ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:3:0","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"Metropolis-Hastings Metropolis-Hasting是马尔可夫链蒙特卡罗法的代表算法 可以对多元变量的每一变量的条件分布依次分别进行抽样， 从而实现对整个多元变量的一次抽样，这就是单分量 Metropolis- Hastings (singlecomponent Metropolis- Hastings) 算法。 ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:3:1","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"Gibbs Sampling 吉布斯抽样，可以认为是 Metropolis-Hastings 算法的特殊情况，但是更容易实现，因而被广泛使用。 吉布斯抽样用于多元变量联合分布的抽样和估计。 其基本做法是，从联合概率分布定义满条件概率分布，依次对满条件概率分布进行抽样，得到样本的序列。 吉布斯抽样适合于满条件概率分布 容易抽样 的情况，而单分量MetropolisHastings 算法适合于满条件概率分布不容易抽样的情况，这时使用容易抽样的条件分布作建议分布。 参考： 李航《统计学习方法》 ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:3:2","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"奇异值分解(SVD)是一种矩阵因子分解方法，在线性代数中，被广泛应用。 奇异值分解也是一种矩阵近似的方法，这个近似是在弗罗贝尼乌斯范数（Frobenius norm) 意义下的近似。 奇异值分解是在平方损失(弗罗贝尼乌斯范数)意义下对矩阵的最优近似，即数据压缩。 ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:0:0","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"定义 ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:1:0","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"1. 奇异值分解 将一个 $m \\times n$ 的实矩阵$A$，$A \\in \\mathbf{R}^{m \\times n}$ 表示为以下三个实矩阵乘积形式的运算，即矩阵因子分解： $$ A=U \\Sigma V^{\\mathrm{T}} $$ 其中， $U$是$m$阶正交矩阵: $UU^T = I$ $V$为$n$阶正交矩阵: $VV^T = I$ $\\Sigma$是由降序排列的非负的对角线元素组成的$m \\times n$的对角矩阵: $\\Sigma = diag(\\sigma_1, \\sigma_2, \\cdots, \\sigma_p)$ $\\sigma_{1} \\geqslant \\sigma_{2} \\geqslant \\cdots \\geqslant \\sigma_{p} \\geqslant 0$ $p = \\min(m, n)$ 那么，称 $U \\Sigma V^{\\mathrm{T}}$ : 矩阵A的奇异值分解 $\\sigma_i$ 为A的奇异值(singluar value) $U$ 的列向量为左奇异向量(left singular vector) $V$ 的列向量为右奇异向量(right singular vector) ⚠️注意，矩阵的奇异值分解不唯一。 实际常用的是： ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:1:1","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"2. 紧奇异值分解(compact singular value decomposition)： 无损压缩 设$m \\times n$的实矩阵$A$，其秩为$\\operatorname{rank}(A)=r$, $r \\leqslant \\min (m, n)$, 则紧奇异值分解为： $$ A=U_r \\Sigma_r V^{\\mathrm{T}}_r $$ 其中， $U_r$ 是 $m \\times r$ 矩阵 $V_r$ 是 $n \\times r$ 矩阵 $\\Sigma_r$ 是 $r$ 阶对角阵 $r = \\operatorname{rank}(A)$ ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:1:2","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"3. 截断奇异值分解(truncated singular value decomposition)： 有损压缩 一般讲奇异值分解，实际上多指截断奇异值分解 在奇异值分解中， 只取最大的$k$个奇异值($k \u003c r, r= \\operatorname{rank}(A)$ )对应的部分，就得到截断奇异值分解 设 $m \\times n$ 的实矩阵 $A$，其秩为 $\\operatorname{rank}(A)=r$, 且 $0 \u003c k \u003c r$, 则截断奇异值分解为： $$ A \\approx U_{k} \\Sigma_{k} V_{k}^{\\mathrm{T}} $$ 其中， $U_k$ 是$m \\times k$ 矩阵(前 $k$列) $V_k$ 是$n \\times k$ 矩阵(前 $k$列) $\\Sigma_k$ 是 $k$ 阶对角阵(前 $k$个) $r = \\operatorname{rank}(A)$ ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:1:3","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"几何解释 从线性变换的角度理解奇异值分解： 将 $m \\times n$ 的实矩阵 $A$表示为从 $n$ 维空间 $R_n$ 到 $m$ 维空间 $R_m$的一个线性变换： $$ T: x \\rightarrow A x $$ $x$, $Ax$为各自空间的向量。 那么线性变换可以理解为： 一个坐标系的旋转或反射变换 一个坐标轴的缩放变换 另一个坐标系的旋转或反射变换 对A进行奇异值分解，U和V都是正交矩阵 V的列向量构成Rn空间的一组标准正交基，表示Rn空间中正交坐标系的旋转或反射变换 U的列向量都成Rm空间的一组标准正交基，表示Rm空间中正交坐标系的旋转或反射变换 $\\Sigma$的对角元素是一组非负实数，表示Rn中的原始正交坐标系坐标轴的缩放变换 ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:2:0","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"奇异值计算 矩阵$A$的奇异值分解可以通过求对阵矩阵$A^TA$的特征值和特征向量得到。 $A^TA$的特征向量构成正交矩阵$V$的列 $A^TA$的特征值$\\lambda_j$的平方根为奇异值$\\sigma_i$，即 $$ \\sigma_{j}=\\sqrt{\\lambda_{j}}, \\quad j=1,2, \\cdots, n $$ 对$\\sigma_i$从大到小排列，得到对角矩阵 $\\Sigma$ 求正奇异值对应的左奇异向量，再扩充的 $A^T$ 的标准正交基，构成正交矩阵 $U$ 的列 ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:3:0","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"求值过程 求对阵矩阵$A^TA$的特征值和特征向量 计算对称矩阵 $W= A^TA$ 求解特征方程 $(W - \\lambda I)x = 0$ 得到特征值$\\lambda_i$，并将之降序排列 $$ \\lambda_{1} \\geqslant \\lambda_{2} \\geqslant \\cdots \\geqslant \\lambda_{n} \\geqslant 0 $$ 将特征值 $\\lambda_i$ 代入特征方程求的对应特征向量 求 $n$ 阶正交矩阵 $V$ 将特征向量单位化， 得到单位特征向量构成 $n$ 阶正交矩阵V $$ V=\\left[\\begin{array}{llll} v_{1} \u0026 v_{2} \u0026 \\cdots \u0026 v_{n} \\end{array}\\right] $$ 求 $m \\times n$ 对角矩阵 $\\Sigma$ 计算A的奇异值 $$ \\sigma_{j}=\\sqrt{\\lambda_{j}}, \\quad j=1,2, \\cdots, n $$ 构造 $m \\times n$ 矩阵对角矩阵 $\\Sigma$， 主对角线元素是奇异值， 其余元素为 0 $$ \\Sigma=\\operatorname{diag}\\left(\\sigma_{1}, \\sigma_{2}, \\cdots, \\sigma_{n}\\right) $$ 求 $m$ 阶正交矩阵 $U$ 对 $A$ 的前 $r$个正奇异值， 令 $$ u_{j}=\\frac{1}{\\sigma_{j}} A v_{j}, \\quad j=1,2, \\cdots, r $$ 得到 $$ U_{1}=\\left[\\begin{array}{llll} u_{1} \u0026 u_{2} \u0026 \\cdots \u0026 u_{r} \\end{array}\\right] $$ 求 $A^T$ 的零空间的一组标准正交基 $$ \\lbrace u_{r+1}, u_{r+2}, \\cdots, u_{m} \\rbrace $$ 令 $$ U_{2}=\\left[\\begin{array}{llll} u_{r+1} \u0026 u_{r+2} \u0026 \\cdots \u0026 u_{m} \\end{array}\\right] $$ 且令 $$ U=\\left[\\begin{array}{ll} U_{1} \u0026 U_{2} \\end{array}\\right] $$ 得到奇异值分解 $$ A=U \\Sigma V^{\\mathrm{T}} $$ ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:3:1","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"举例 求: 矩阵 $A$ 的奇异值分解 $$ A=\\left[\\begin{array}{ll} 1 \u0026 1 \\cr 2 \u0026 2 \\cr 0 \u0026 0 \\end{array}\\right] $$ 解： 求 $A^TA$ 的特征值和特征向量 $$ A^{\\mathrm{T}} A=\\left[\\begin{array}{lll} 1 \u0026 2 \u0026 0 \\cr 1 \u0026 2 \u0026 0 \\end{array}\\right]\\left[\\begin{array}{ll} 1 \u0026 1 \\cr 2 \u0026 2 \\cr 0 \u0026 0 \\end{array}\\right]=\\left[\\begin{array}{ll} 5 \u0026 5 \\cr 5 \u0026 5 \\end{array}\\right] $$ 满足特征方程 $$ \\left(A^{\\mathrm{T}} A-\\lambda I\\right) x=0 $$ 得到齐次线性方程组 $$ \\begin{cases} (5-\\lambda) x_{1} + \u0026 5 x_{2}=0 \\cr 5 x_{1} + \u0026 (5-\\lambda) x_{2}=0 \\end{cases} $$ 该方程组有非零解的充要条件是 $$ \\left|\\begin{array}{cc} 5-\\lambda \u0026 5 \\cr 5 \u0026 5-\\lambda \\end{array}\\right|=0 $$ 即 $$ \\lambda^{2}-10 \\lambda=0 $$ 得到 $\\lambda_1 = 10 $, $\\lambda_2 = 0$. 特征值代入线性方程组，分别得到对应单位特征向量 $$ v_{1}=\\left[\\begin{array}{c} \\frac{1}{\\sqrt{2}} \\cr \\frac{1}{\\sqrt{2}} \\end{array}\\right], v_{2}=\\left[\\begin{array}{c} \\frac{1}{\\sqrt{2}} \\cr -\\frac{1}{\\sqrt{2}} \\end{array}\\right] $$ 求正交矩阵 $V$ 构造正交矩阵 $$ V=\\left[\\begin{array}{cc} \\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{2}} \\cr \\frac{1}{\\sqrt{2}} \u0026 -\\frac{1}{\\sqrt{2}} \\end{array}\\right] $$ 求对角矩阵 $\\Sigma$ 奇异值为 $\\sigma_1 = \\sqrt{\\lambda_1} = \\sqrt{10}$, $\\sigma_2 = 0$, 那么 $$ \\Sigma=\\left[\\begin{array}{cc} \\sqrt{10} \u0026 0 \\cr 0 \u0026 0 \\cr 0 \u0026 0 \\end{array}\\right] $$ ⚠️注意： 为了 $\\Sigma$ 能与$U$和$V$进行矩阵乘法， $\\Sigma$要加上零行向量 求正交矩阵 $U$ 基于 $A$ 的奇异值计算得到列向量 $u_1$ $$ u_{1}=\\frac{1}{\\sigma_{1}} A v_{1}=\\frac{1}{\\sqrt{10}}\\left[\\begin{array}{cc} 1 \u0026 1 \\cr 2 \u0026 2 \\cr 0 \u0026 0 \\end{array}\\right]\\left[\\begin{array}{c} \\frac{1}{\\sqrt{2}} \\cr \\frac{1}{\\sqrt{2}} \\end{array}\\right]=\\left[\\begin{array}{c} \\frac{1}{\\sqrt{5}} \\cr \\frac{2}{\\sqrt{5}} \\cr 0 \\end{array}\\right] $$ 列向量$u_2$， $u_3$是 $A^T$的零空间 $N(A^T)$ 的一组标准正交基，故而求解以下线性方程组 $$ A^{\\mathrm{T}} x=\\left[\\begin{array}{lll} 1 \u0026 2 \u0026 0 \\cr 1 \u0026 2 \u0026 0 \\end{array}\\right]\\left[\\begin{array}{l} x_{1} \\cr x_{2} \\cr x_{3} \\end{array}\\right]=\\left[\\begin{array}{l} 0 \\cr 0 \\end{array}\\right] $$ 即 $$ \\begin{array}{c} x_{1}+2 x_{2}+0 x_{3}=0 \\cr x_{1}=-2 x_{2}+0 x_{3} \\end{array} $$ 分别取 $x_2$，$x_3$ 为 $(1,0)$ 和 $(0,1)$ 得到 $N(A^T)$的基 $$ (-2,1,0)^{\\mathrm{T}}, \\quad(0,0,1)^{\\mathrm{T}} $$ 得 $N(A^T)$ 的一组标准正交基是 $$ u_{2}=\\left(-\\frac{2}{\\sqrt{5}}, \\frac{1}{\\sqrt{5}}, 0\\right)^{\\mathrm{T}}, \\quad u_{3}=(0,0,1)^{\\mathrm{T}} $$ 最后，构造正交矩阵 $U$ $$ U=\\left[\\begin{array}{ccc} \\frac{1}{\\sqrt{5}} \u0026 -\\frac{2}{\\sqrt{5}} \u0026 0 \\cr \\frac{2}{\\sqrt{5}} \u0026 \\frac{1}{\\sqrt{5}} \u0026 0 \\cr 0 \u0026 0 \u0026 1 \\end{array}\\right] $$ 矩阵 $A$ 的奇异值分解 $$ A=U \\Sigma V^{\\mathrm{T}}=\\left[\\begin{array}{ccc} \\frac{1}{\\sqrt{5}} \u0026 -\\frac{2}{\\sqrt{5}} \u0026 0 \\cr \\frac{2}{\\sqrt{5}} \u0026 \\frac{1}{\\sqrt{5}} \u0026 0 \\cr 0 \u0026 0 \u0026 1 \\end{array}\\right]\\left[\\begin{array}{cc} \\sqrt{10} \u0026 0 \\cr 0 \u0026 0 \\cr 0 \u0026 0 \\end{array}\\right]\\left[\\begin{array}{cc} \\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{2}} \\cr \\frac{1}{\\sqrt{2}} \u0026 -\\frac{1}{\\sqrt{2}} \\end{array}\\right] $$ ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:3:2","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"矩阵的外积展开式表示 将A的奇异值分解看成矩阵 $U\\Sigma$ 和 $V^T$ 的乘积， 将 $U\\Sigma$ 按列向量分块， $$ U \\Sigma=\\left[\\begin{array}{llll} \\sigma_{1} u_{1} \u0026 \\sigma_{2} u_{2} \u0026 \\cdots \u0026 \\sigma_{n} u_{n} \\end{array}\\right] $$ $V^T$ 按行向量分块 $$ V^{\\mathrm{T}}=\\left[\\begin{array}{c} v_{1}^{\\mathrm{T}} \\cr v_{2}^{\\mathrm{T}} \\cr \\vdots \\cr v_{n}^{\\mathrm{T}} \\end{array}\\right] $$ 那么外积展开式为 $$ A=\\sigma_{1} u_{1} v_{1}^{\\mathrm{T}}+\\sigma_{2} u_{2} v_{2}^{\\mathrm{T}}+\\cdots+\\sigma_{n} u_{n} v_{n}^{\\mathrm{T}} $$ 或者 $$ A=\\sum_{k=1}^{n} A_{k}=\\sum_{k=1}^{n} \\sigma_{k} u_{k} v_{k}^{\\mathrm{T}} $$ 其中 $A_{k}=\\sigma_{k} u_{k} v_{k}^{\\mathrm{T}}$ 是 $m \\times n$ 矩阵 而 $$ u_{i} v_{j}^{\\mathrm{T}}=\\left[\\begin{array}{c} u_{1 i} \\cr u_{2 i} \\cr \\vdots \\cr u_{m i} \\end{array}\\right]\\left[\\begin{array}{cccc} v_{1 j} \u0026 v_{2 j} \u0026 \\cdots \u0026 v_{n j} \\end{array}\\right] = \\left[\\begin{array}{cccc} u_{1 i} v_{1 j} \u0026 u_{1 i} v_{2 j} \u0026 \\cdots \u0026 u_{1 i} v_{n j} \\cr u_{2 i} v_{1 j} \u0026 u_{2 i} v_{2 j} \u0026 \\cdots \u0026 u_{2 i} v_{n j} \\cr \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\cr u_{m i} v_{1 j} \u0026 u_{m i} v_{2 j} \u0026 \\cdots \u0026 u_{m i} v_{n j} \\end{array}\\right] $$ ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:4:0","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"补充： ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:5:0","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"Determinant 行列式 Determinant: The determinant of a square matrix is a scalar that provides information about the matrix. e.g. Invertibility Geometrically, it can be viewed as the volume scaling factor of the linear transformation described by the matrix. 一个矩阵的行列式就是一个超平行多面体的（有向的）面积/体积，这个多面体的每条边对应着对应矩阵的列； 矩阵 $A$ 的行列式 $det(A)$ 就是线性变换 $A$ 下的图形面积或体积的伸缩因子。 矩阵的行列式的几何意义是矩阵对应的线性变换前后的面积比 Property: $Det(I) = 1$ Exchanging rows only reverse the sign of det Determinant is “linear” for each row $det(A) \\neq 0$, $A$ is invertible Cramer’s rule: $A^{-1} = \\frac{1}{det(A)}C^T$ $det(A)$: scalar $C$: cofactors of $A$ (C has the same size as $A$) $C^T$ is adjugate of $A$ (伴随矩阵) ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:5:1","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"Eigenvalue and Eigenvector Eigen (German word): “unique to” or “belonging to” if $Av = \\lambda v$ ($v$ is a vector , $\\lambda$ is a scalar) $A$ must be square $v$ is an eigenvector of $A$, exluding zero vector $\\lambda$ is an eigenvalue of $A$ that correponds to $v$ $T$ is a linear operator if $T(v) = \\lambda v$ ( $v$ is a vector, $\\lambda$ is a scalar) $v$ is an eigenvector of $T$, exluding zero vector $\\lambda$ is an eigenvalue of $T$ that correponds to $v$ An eigenvector of A corresponds to a unique eigenvalue. An eigenvalue of A has infinitely many eigenvectors. =\u003e how to find eigenvalues t : $$ det(A - tI_n) = 0 $$ 参考： 李航《统计学习方法》 ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:5:2","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Coding"],"content":"Get answers for Swift within 10s","date":"2020-05-30","objectID":"/2020-06-03-code-unsafe-pointer-swift/","tags":["Swift"],"title":"Pointer in UnSafe Swift","uri":"/2020-06-03-code-unsafe-pointer-swift/"},{"categories":["Coding"],"content":"Pointers Unsafe Swift pointers use a predictable naming scheme: Unsafe [Mutable][Raw][Buffer]Pointer[\u003cT\u003e] Explain: Pointers are just memory addresses. Direct memory access is Unsafe. Mutable means you can write to it. Raw means it points to a blob of bytes. Buffer means that is works like a collection. Generic [\u003cT\u003e] pointers are typed. ","date":"2020-05-30","objectID":"/2020-06-03-code-unsafe-pointer-swift/:1:0","tags":["Swift"],"title":"Pointer in UnSafe Swift","uri":"/2020-06-03-code-unsafe-pointer-swift/"},{"categories":["Coding"],"content":"Working with Pointers C Pointer Swift Type int * UnsaftMutablePointer const int * UnsafePointer NSDate ** AutoreleasingUnsafeMutablePointer struct UnknowType * OpaquePointer void * UnsafeMutableRawPointer const void * UnsafeRawPointer Explain: see here ","date":"2020-05-30","objectID":"/2020-06-03-code-unsafe-pointer-swift/:2:0","tags":["Swift"],"title":"Pointer in UnSafe Swift","uri":"/2020-06-03-code-unsafe-pointer-swift/"},{"categories":["Coding"],"content":"Usage see here and here ","date":"2020-05-30","objectID":"/2020-06-03-code-unsafe-pointer-swift/:3:0","tags":["Swift"],"title":"Pointer in UnSafe Swift","uri":"/2020-06-03-code-unsafe-pointer-swift/"},{"categories":["Coding"],"content":"Get answers for Swift within 10s","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Swift Cheat Sheet Stolen from iwasrobbed. I simplify it and add some more. It’s a high level and a quick reference to Swift. The purpose of this cheat sheet is to teach myself and get answers within 10s. ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:0:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Table of Contents Code Document Data Types Operators Operator Overloading Declaring Classes Declarations Lazy Property Property Observer Literals Functions Constants and Variables Naming Conventions Closures Generics Control Statements Extension Protocol Protocol Extension Error Handling Passing Information User Defaults Common Patterns Unicode Support File IO ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:0:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Code Documentation Two ways of commenting: // /* … */ Two ways of documenting with markdown (Reconigzed by xcode): /// /** … */ ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:1:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Markdown a few keywords that xcode can recognized automatically, with the format like - . The most common: Prameters, Throws, Returns /** - Prameters: - argument1: This is arg1 - argument2: This is arg2 - Returns: The results string. - Throws: `Error` if nil */ Other keywords /** - Precondition: - Postcondition: - Requires: All the information in the object should be sorted - Invariant: The object will maintained sorted - Complexity: O(n^2) - Important: - Warning: Very computation consuming - Attention: Same as Warning - Note: something to keep in mind - Remark: Same as note */ Metadata /** - Author: - Authors: - Copyright: - Date: - Since: - Version: */ Back to top ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:1:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"MARK Using MARK to organize your code: // MARK:- Use mark to logically organize your code // Declare some functions or variables here // MARK:- They also show up nicely in the properties/functions list in Xcode // Declare some more functions or variables here ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:1:2","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"FIXME Using FIXME to remember to fix your code: // Some broken code might be here // FIXME:Use fixme to create a reminder to fix broken code later FIXME works a lot like MARK because it makes organizing code easier, but it’s used exclusively when you need to remember to fix something. ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:1:3","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"TODO Using TODO to remember to add, delete, or generally refactor your code: // Some incomplete code might be here // TODO:Use todo to create a reminder to finish things up later TODO is very similar to FIXME and MARK, but it’s used exclusively when you need to remember to add, delete, or change your code later. Auto-generating method documentation: In a method’s preceding line, press ⌥ Option + ⌘ Command + / to automatically generate a documentation stub for your method. Back to top ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:1:4","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Data Types ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:2:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Size Permissible sizes of data types are determined by how many bytes of memory are allocated for that specific type and whether it’s a 32-bit or 64-bit environment. In a 32-bit environment, long is given 4 bytes, which equates to a total range of 2^(4*8) (with 8 bits in a byte) or 4294967295. In a 64-bit environment, long is given 8 bytes, which equates to 2^(8*8) or 1.84467440737096e19. For a complete guide to 64-bit changes, please see the transition document. ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:2:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"C Primitives Unless you have a good reason to use C primitives, you should just use the Swift types to ensure compability going foward. In fact, Swift just aliases C types to a Swift equivalent: // C char is aliased as an Int8 and unsigned as UInt8 let aChar = CChar() let anUnsignedChar = CUnsignedChar() print(\"C char size: \\(MemoryLayout.size(ofValue: aChar))with min: \\(Int8.min)and max: \\(Int8.max)\") // C char size: 1 with min: -128 and max: 127 print(\"C unsigned char size: \\(MemoryLayout.size(ofValue: anUnsignedChar))with min: \\(UInt8.min)and max: \\(UInt8.max)\") // C unsigned char size: 1 with min: 0 and max: 255 // C short is aliased as an Int16 and unsigned as UInt16 let aShort = CShort() let unsignedShort = CUnsignedShort() print(\"C short size: \\(MemoryLayout.size(ofValue: aShort))with min: \\(Int16.min)and max: \\(Int16.max)\") // C short size: 2 with min: -32768 and max: 32767 print(\"C unsigned short size: \\(MemoryLayout.size(ofValue: unsignedShort))with min: \\(UInt16.min)and max: \\(UInt16.max)\") // C unsigned short size: 2 with min: 0 and max: 65535 // C int is aliased as an Int32 and unsigned as UInt32 let anInt = CInt() let unsignedInt = CUnsignedInt() print(\"C int size: \\(MemoryLayout.size(ofValue: anInt))with min: \\(Int32.min)and max: \\(Int32.max)\") // C int size: 4 with min: -2147483648 and max: 2147483647 print(\"C unsigned int size: \\(MemoryLayout.size(ofValue: unsignedInt))with min: \\(UInt32.min)and max: \\(UInt32.max)\") // C unsigned int size: 4 with min: 0 and max: 4294967295 // C long is aliased as an Int and unsigned as UInt let aLong = CLong() let unsignedLong = CUnsignedLong() print(\"C long size: \\(MemoryLayout.size(ofValue: aLong))with min: \\(Int.min)and max: \\(Int.max)\") // C long size: 8 with min: -9223372036854775808 and max: 9223372036854775807 print(\"C unsigned long size: \\(MemoryLayout.size(ofValue: unsignedLong))with min: \\(UInt.min)and max: \\(UInt.max)\") // C unsigned long size: 8 with min: 0 and max: 18446744073709551615 // C long long is aliased as an Int64 and unsigned as UInt64 let aLongLong = CLongLong() let unsignedLongLong = CUnsignedLongLong() print(\"C long long size: \\(MemoryLayout.size(ofValue: aLongLong))with min: \\(Int64.min)and max: \\(Int64.max)\") // C long long size: 8 with min: -9223372036854775808 and max: 9223372036854775807 print(\"C unsigned long long size: \\(MemoryLayout.size(ofValue: unsignedLongLong))with min: \\(UInt64.min)and max: \\(UInt64.max)\") // C unsigned long long size: 8 with min: 0 and max: 18446744073709551615 From the docs: C Type Swift Type bool CBool char, signed char CChar unsigned char CUnsignedChar short CShort unsigned short CUnsignedShort int CInt unsigned int CUnsignedInt long CLong unsigned long CUnsignedLong long long CLongLong unsigned long long CUnsignedLongLong wchar_t CWideChar char16_t CChar16 char32_t CChar32 float CFloat double CDouble Integers Integers can be signed or unsigned. When signed, they can be either positive or negative and when unsigned, they can only be positive. Apple states: Unless you need to work with a specific size of integer, always use Int for integer values in your code. This aids code consistency and interoperability. Even on 32-bit platforms, Int […] is large enough for many integer ranges. Fixed width integer types with their accompanying byte sizes as the variable names: // Exact integer types let aOneByteInt: Int8 = 127 let aOneByteUnsignedInt: UInt8 = 255 let aTwoByteInt: Int16 = 32767 let aTwoByteUnsignedInt: UInt16 = 65535 let aFourByteInt: Int32 = 2147483647 let aFourByteUnsignedInt: UInt32 = 4294967295 let anEightByteInt: Int64 = 9223372036854775807 let anEightByteUnsignedInt: UInt64 = 18446744073709551615 // Minimum integer types let aTinyInt: Int8 = 127 let aTinyUnsignedInt: UInt8 = 255 let aMediumInt: Int16 = 32767 let aMediumUnsignedInt: UInt16 = 65535 let aNormalInt: Int32 = 2147483647 let aNormalUnsignedInt: UInt32 = 4294967295 let aBigInt: Int64 = 9223372036854775807 let aBigUnsignedInt: UInt64 = 1844674407370955","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:2:2","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Enum \u0026 Bitmask Types Enumeration types can be defined as follows: // Specifying a typed enum with a name (recommended way) enum UITableViewCellStyle: Int { case default, valueOne, valueTwo, subtitle } // Accessing it: let cellStyle: UITableViewCellStyle = .default As of Swift 3, all enum options should be named in lowerCamelCased. Working with Bitmasks Newer Swift versions have a nice substitute for the old NS_OPTIONS macro for creating bitmasks to compare to. An example for posterity: struct Options: OptionSet { let rawValue: Int init(rawValue: Int) { self.rawValue = rawValue } init(number: Int) { self.init(rawValue: 1 \u003c\u003c number) } static let OptionOne = Options(number: 0) static let OptionTwo = Options(number: 1) static let OptionThree = Options(number: 2) } let options: Options = [.OptionOne, .OptionTwo] options.contains(.OptionOne) // true options.contains(.OptionThree) // false ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:2:3","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Type Casting Sometimes it is necessary to cast an object into a specific class or data type. Examples of this would be casting from a Float to an Int or from a UITableViewCell to a subclass such as RPTableViewCell. Checking Types Swift uses is and as both for checking object types as well as conformance to a given protocol. Operator: is Checking object type using is: if item is Movie { movieCount += 1 print(\"It is a movie.\") } else if item is Song { songCount += 1 print(\"It is a song.\") } The is operator returns true if an instance is of that object type, or conforms to the specified protocol, and returns false if it does not. Operators: as? and as! If you want to be able to easily access the data during one of these checks, you can use as? to optionally (or as! to force) unwrap the object when necessary: for item in library { if let movie = item as? Movie { print(\"Director: \\(movie.director)\") } else if let song = item as? Song { print(\"Artist: \\(song.artist)\") } } The as? version of the downcast operator returns an optional value of the object or protocol’s type, and this value is nil if the downcast fails or this instance does not conform to the specified protocol. The as! version of the downcast operator forces the downcast to the specified object or protocol type and triggers a runtime error if the downcast does not succeed. Casting from Generic Types If you’re working with AnyObject objects given from the Cocoa API, you can use: for movie in someObjects as! [Movie] { // do stuff } If given an array with Any objects, you can use a switch statement with the type defined for each case: var things = [Any]() for thing in things { switch thing { case 0 as Int: print(\"Zero as an Int\") case let someString as! String: print(\"S string value of \\\"\\(someString)\\\"\") case let (x, y) as! (Double, Double): print(\"An (x, y) point at \\(x), \\(y)\") case let movie as! Movie: print(\"A movie called '\\(movie.name)' by director \\(movie.director)\") default: print(\"Didn't match any of the cases specified\") } } Basic Casting Swift also offers some simple methods of casting between it’s given data types. // Example 1: let aDifferentDataType: Float = 3.14 let anInt: Int = Int(aDifferentDataType) // Example 2: let aString: String = String(anInt) Back to top ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:2:4","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Operators Swift supports most standard C operators and improves several capabilities to eliminate common coding errors. The assignment operator = does not return a value, to prevent it from being mistakenly used when the equal to operator == is intended. Arithmetic operators (+, -, *, /, %) detect and disallow value overflow, to avoid unexpected results when working with numbers that become larger or smaller than the allowed value range of the type that stores them. Arithmetic Operators Operator Purpose + Addition - Subtraction * Multiplication / Division % Remainder Comparative Operators Operator Purpose == Equal to === Identical to != Not equal to !== Not identical to ~= Pattern match \u003e Greater than \u003c Less than \u003e= Greater than or equal to \u003c= Less than or equal to Assignment Operators Operator Purpose = Assign += Addition -= Subtraction *= Multiplication /= Division %= Remainder \u0026= Bitwise AND |= Bitwise Inclusive OR ^= Exclusive OR «= Shift Left »= Shift Right Logical Operators Operator Purpose ! NOT \u0026\u0026 Logical AND || Logical OR Range Operators Operator Purpose ..\u003c Half-open range … Closed range Bitwise Operators Operator Purpose \u0026 Bitwise AND | Bitwise Inclusive OR ^ Exclusive OR ~ Unary complement (bit inversion) « Shift Left » Shift Right Overflow and Underflow Operators Typically, assigning or incrementing an integer, float, or double past it’s range would result in a runtime error. However, if you’d instead prefer to safely truncate the number of available bits, you can opt-in to have the variable overflow or underflow using the following operators: Operator Purpose \u0026+ Addition \u0026- Subtraction \u0026* Multiplication Example for unsigned integers (works similarly for signed): var willOverflow = UInt8.max // willOverflow equals 255, which is the largest value a UInt8 can hold willOverflow = willOverflow \u0026+ 1 // willOverflow is now equal to 0 var willUnderflow = UInt8.min // willUnderflow equals 0, which is the smallest value a UInt8 can hold willUnderflow = willUnderflow \u0026- 1 // willUnderflow is now equal to 255 Other Operators Operator Purpose ?? Nil coalescing ?: Ternary conditional ! Force unwrap object value ? Safely unwrap object value Back to top ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:3:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Operator Overloading Swift allows you to overwrite existing operators or define new operators for existing or custom types. For example, this is why in Swift you can join strings using the + operator, even though it is typically used for math. Operator overloading is limited to the following symbols, / = - + * % \u003c \u003e ! \u0026 | ^ . ~, however you cannot overload the = operator by itself (it must be combined with another symbol). Operators can be specified as: prefix: goes before an object such as -negativeNumber infix: goes between two objects, such as a + b postfix: goes after an object, such as unwrapMe! ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:4:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Custom operators associativity: defines how operators of the same precedence are grouped together (left, right) precedence: gives some operators higher priority than others; these operators are applied first. Refer Operator Declarations to see full details about operator associativity and precedence. Example: DefaultPrecedence group // declare first and set rules with a precedence group infix operator ** // use DefaultPrecedence group Custom Precedence group // define a custom precedence group precedencegroup ExponentiationPrecedence { higherThan: MultiplicationPrecedence associativity: right // none, left, right //assignment: false } // now, replace original declaration of ** with infix operator **: ExponentiationPrecedence That’s it. // impelment infix func ** (x: Double, p: Double) -\u003e Double { return pow(x, p) } 2**3 // 8 2**3**2 // 512 1+2**3**2 // 513 5*2**3**2 // 2560 see also docs Back to top ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:4:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Declaring Classes Classes are typically declared using separate .swift files, but multiple classes can also be created within the same file if you’d like to organize it that way. Unlike Objective-C, there’s no need for an interface file (.h) in Swift. The implementation file should contain (in this order): Any needed import statements A class declaration which contains any constants or variables necessary for the class All public and private functions Example: MyClass.swift import UIKit class MyClass { // Declare any constants or variables at the top let kRPErrorDomain = \"com.myIncredibleApp.errors\" var x: Int, y: Int // MARK:- Class Methods, e.g. MyClass.functionName() class func alert() { print(\"This is a class function.\") } // MARK:- Instance Methods, e.g. myClass.functionName() init(x: Int, y: Int) { self.x = x self.y = y } // MARK:- Private Methods private func pointLocation() -\u003e String { return \"x: \\(x), y: \\(y)\" } } Instantiation When you want to create a new instance of a class, you use the syntax: let myClass = MyClass(x: 1, y: 2) where x and y are variables that are passed in at the time of instantiation. Back to top ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:5:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Declarations More info here in the docs. Preprocessor Swift doesn’t come with a preprocessor so it only supports a limited number of statements for build time. Things like #define have been replaced with global constants defined outside of a class. Directive Purpose #if An if conditional statement #elif An else if conditional statement #else An else conditional statement #endif An end if conditional statement Imports Directive Purpose import Imports a framework Constants \u0026 Variables Directive Purpose let Declares local or global constant var Declares a local or global variable class Declares a class-level constant or variable static Declares a static type Classes, Structure, Functions and Protocols Directive Purpose typealias Introduces a named alias of an existing type enum Introduces a named enumeration struct Introduces a named structure class Begins the declaration of a class init Introduces an initializer for a class, struct or enum init? Produces an optional instance or an implicitly unwrapped optional instance; can return nil deinit Declares a function called automatically when there are no longer any references to a class object, just before the class object is deallocated func Begins the declaration of a function protocol Begins the declaration of a formal protocol static Defines as type-level within struct or enum convenience Delegate the init process to another initializer or to one of the class’s designated initializers extension Extend the behavior of class, struct, or enum subscript Adds subscripting support for objects of a particular type, normally for providing a convenient syntax for accessing elements in a collective, list or sequence override Marks overriden initializers Operators Directive Purpose operator Introduces a new infix, prefix, or postfix operator Declaration Modifiers Directive Purpose dynamic Marks a member declaration so that access is always dynamically dispatched using the Objective-C runtime and never inlined or devirtualized by the compiler final Specifies that a class can’t be subclassed, or that a property, function, or subscript of a class can’t be overridden in any subclass lazy Indicates that the property’s initial value is calculated and stored at most once, when the property is first accessed optional Specifies that a protocol’s property, function, or subscript isn’t required to be implemented by conforming members required Marks the initializer so that every subclass must implement it weak Indicates that the variable or property has a weak reference to the object stored as its value Access Control Directive Purpose open Can be subclassed outside of its own module and its methods overridden as well; truly open to modification by others and useful for framework builders public Can only be subclassed by its own module or have its methods overridden by others within the same module internal (Default) Indicates the entities are only available to the entire module that includes the definition, e.g. an app or framework target fileprivate Indicates the entities are available only from within the source file where they are defined private Indicates the entities are available only from within the declaring scope within the file where they are defined (e.g. within the { } brackets only) public class AccessLevelsShowCase { // Property accessible for other modules public var somePublicProperty = 0 // Property accessible from the module var someInternelProperty = 1 // Property accessible from its own defining source file fileprivate func someFilePrivateMethod() {} // Property accessible fro its enclosing declaration private func somePrivateMethod() {} } Back to top ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:6:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Literals Literals are compiler directives which provide a shorthand notation for creating common objects. Syntax What it does \"string\" Returns a String object 28 Returns an Int 3.14, 0xFp2, 1.25e2 Returns a Double object true, false Returns a Bool object [] Returns an Array object [keyName:value] Returns a Dictionary object 0b Returns a binary digit 0o Returns an octal digit 0x Returns a hexadecimal digit Strings Special characters can be included: Null Character: \\0 Backslash: \\\\ (can be used to escape a double quote) Horizontal Tab: \\t Line Feed: \\n Carriage Return: \\r Double Quote: \\\" Single Quote: \\' Unicode scalar: \\u{n} where n is between one and eight hexadecimal digits Multiline string literal let json = \"\"\" { \"username\": \"David\", \"loginCount\": 2} \"\"\" Array Access Syntax let example = [ \"hi\", \"there\", 23, true ] print(\"item at index 0: \\(example[0])\") Dictionary Access Syntax let example = [ \"hi\" : \"there\", \"iOS\" : \"people\" ] if let value = example[\"hi\"] { print(\"hi \\(value)\") } Mutability For mutable literals, declare it with var; immutable with let. Back to top ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:7:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Functions Declaration Syntax Functions without a return type use this format: // Does not return anything or take any arguments func doWork() { // Code } class precedes declarations of class functions: // Call on a class, e.g. MyClass.someClassFunction() class func someClassFunction() { // Code } static is similar to class functions where you don’t need an instance of the class or struct in order to call a method on it: // Call on a class/struct, e.g. MyStruct.someStaticFunction() static func someStaticFunction() { // Code } Declare instance functions: // Called on an instance of a class, e.g. myClass.someInstanceFunction() func doMoreWork() { // Code } Function arguments are declared within the parentheses: // Draws a point func draw(point: CGPoint) Return types are declared as follows: // Returns a String object for the given String argument func sayHelloToMyLilFriend(lilFriendsName: String) -\u003e String { return \"Oh hello, \\(lilFriendsName). Cup of tea?\" } You can have multiple return values, referred to as a tuple: // Returns multiple objects func sayHelloToMyLilFriend(lilFriendsName: String) -\u003e (msg: String, nameLength: Int) { return (\"Oh hello, \\(lilFriendsName). Cup of tea?\", countElements(lilFriendsName)) } var hello = sayHelloToMyLilFriend(\"Rob\") print(hello.msg) // \"Oh hello, Rob. Cup of tea?\" print(hello.nameLength) // 3 And those multiple return values can be optional: func sayHelloToMyLilFriend(lilFriendsName: String) -\u003e (msg: String, nameLength: Int)? By default, external parameter names are given when you call the function, but you can specify that one or more are not shown in the method signature by putting a _ symbol in front of the parameter name: func sayHelloToMyLilFriend(_ lilFriendsName: String) { // Code } sayHelloToMyLilFriend(\"Rob\") or you can rename the variable once within the method scope: func sayHelloToMyLilFriend(friendsName lilFriendsName: String) { // Code } sayHelloToMyLilFriend(friendsName: \"Rob\") // and local variable is `lilFriendsName` You can also specify default values for the parameters: func sayHelloToMyLilFriend(_ lilFriendsName: String = \"Rob\") { // Code } sayHelloToMyLilFriend() // \"Oh hello, Rob. Cup of tea?\" sayHelloToMyLilFriend(\"Jimbob\") // \"Oh hello, Jimbob. Cup of tea?\" Swift also supports variadic parameters so you can have an open-ended number of parameters passed in: func sayHelloToMyLilFriends(_ lilFriendsName: String...) { // Code } sayHelloToMyLilFriends(\"Rob\", \"Jimbob\", \"Cletus\") // \"Oh hello, Rob, Jimbob and Cletus. Cup of tea?\" And lastly, you can also use a prefix to declare input parameters as inout. An in-out parameter has a value that is passed in to the function, is modified by the function, and is passed back out of the function to replace the original value. You may remember inout parameters from Objective-C where you had to sometimes pass in an \u0026error parameter to certain methods, where the \u0026 symbol specifies that you’re actually passing in a pointer to the object instead of the object itself. The same applies to Swift’s inout parameters now as well. Calling Functions Functions are called using dot syntax: myClass.doWork() or self.sayHelloToMyLilFriend(\"Rob Phillips\") self is a reference to the function’s containing class. At times, it is necessary to call a function in the superclass using super.someMethod(). Back to top ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:8:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Constants and Variables Declaring a constant or variable allows you to maintain a reference to an object within a class or to pass objects between classes. Constants are defined with let and variables with var. By nature, constants are obviously immutable (i.e. cannot be changed once they are instantiated) and variables are mutable. class MyClass { let text = \"Hello\" // Constant var isComplete: Bool // Variable } There are many ways to declare properties in Swift, so here are a few examples: var myInt = 1 // inferred type var myExplicitInt: Int = 1 // explicit type var x = 1, y = 2, z = 3 // declare multiple variables let (a,b) = (1,2) // declare multiple constants Getters and Setters In Objective-C, variables were backed by getters, setters, and private instance variables created at build time. However, in Swift getters and setters are only used for computed properties and constants actually don’t have a getter or setter at all. The getter is used to read the value, and the setter is used to write the value. The setter clause is optional, and when only a getter is needed, you can omit both clauses and simply return the requested value directly. However, if you provide a setter clause, you must also provide a getter clause. You can overrride the getter and setter of a property to create the illusion of the Objective-C property behavior, but you’d need to store them as a private property with a different name (not recommended for most scenarios): private var _x: Int = 0 var x: Int { get { print(\"Accessing x...\") return _x } set { print(\"Setting x...\") _x = newValue } } Property Observer Swift also has callbacks for when a property will be or was set using willSet and didSet shown below: willset: before assignment didSet: after assignment class LightBulb { static let maxCurrent = 30 var current = 0 { willSet(newCurrent) { // do something before value assignment // newValue -\u003e newCurrent print(\"Current value changed, the change is \\(abs(current- newCurrent))\") } didSet { // do somthing afther value assignment if current == LightBulb.maxCurrent { print(\"current get to maximum point\") } // oldValue } } } let bulb = LightBulb() bulb.current = 20 bulb.current = 30 bulb.current = 40 Back to top Lazy Property lazy: only compute once and remember the value, won’t re-compute if called again. class ClosedRange { let start: Int let end: Int var width: Int { return end - start +1 } // note the = lazy var sum: Int = { var res = 0 print(\"run\") for i in self.start...self.end{ res += 1 } return }() // don't forget () init?(start: Int, end: Int){ if start \u003e end { return nil } self.start = start self.end = end } } // example if let range = ClosedRange(start: 0, end: 10_000) { range.width //1001 range.sum // will print out \"run\" range.sum // range.sum // } Back to top Accessing Local Variables Local variables and constants only exist within the scope of a function. func doWork() { let localStringVariable = \"Some local string variable.\" self.doSomething(string: localStringVariable) } Back to top ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:9:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Naming Conventions The general rule of thumb: Clarity and brevity are both important, but clarity should never be sacrificed for brevity. Functions and Properties These both use camelCase where the first letter of the first word is lowercase and the first letter of each additional word is capitalized. Class names and Protocols These both use CapitalCase where the first letter of every word is capitalized. ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:10:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Enums The options in an enum should be lowerCamelCased Functions These should use verbs if they perform some action (e.g. performInBackground). You should be able to infer what is happening, what arguments a function takes, or what is being returned just by reading a function signature. Example: // Correct func move(from start: Point, to end: Point) {} // Incorrect (likely too expressive, but arguable) func moveBetweenPoints(from start: Point, to end: Point) {} // Incorrect (not expressive enough and lacking argument clarity) func move(x: Point, y: Point) {} Back to top ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:10:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Closures Closures in Swift are similar to blocks in Objective-C and are essentially chunks of code, typically organized within a {} clause, that are passed between functions or to execute code as a callback within a function. Swift’s func functions are actually just a special case of a closure in use. Syntax { (params) -\u003e returnType in statements } Examples // Map just iterates over the array and performs whatever is in the closure on each item let people = [\"Rob\", \"Jimbob\", \"Cletus\"] people.map({ (person: String) -\u003e String in \"Oh hai, \\(person)...\" }) // Oh hai, Rob // Oh hai, Jimbob // Oh hai, Cletus // Closure for alphabetically reversing an array of names, where sorted is a Swift library function let names = [\"Francesca\", \"Joe\", \"Bill\", \"Sally\", ] var reversed = names.sorted { (s1: String, s2: String) -\u003e Bool in return s1 \u003e s2 } // Or on a single line: reversed = names.sorted{ (s1: String, s2: String) -\u003e Bool in return s1 \u003e s2 } // Or because Swift can infer the Bool type: reversed = names.sorted { s1, s2 in return s1 \u003e s2 } // Or because the return statement is implied: reversed = names.sorted { s1, s2 in s1 \u003e s2 } // Or even shorter using shorthand argument names, such as $0, $1, $2, etc.: reversed = names.sorted { $0 \u003e $1 } // Or just ridiculously short because Swift's String greater-than operator implementation exactly matches this function definition: reversed = names.sorted(by: \u003e) If the closure is the last parameter to the function, you can also use the trailing closure pattern. This is especially useful when the closure code is especially long and you’d like some extra space to organize it: func someFunctionThatTakesAClosure(closure: () -\u003e ()) { // function body goes here } // Instead of calling like this: someFunctionThatTakesAClosure({ // closure's body goes here }) // You can use trailing closure like this: someFunctionThatTakesAClosure() { // trailing closure's body goes here } Capturing Values A closure can capture constants and variables from the surrounding context in which it is defined. The closure can then refer to and modify the values of those constants and variables from within its body, even if the original scope that defined the constants and variables no longer exists. In Swift, the simplest form of a closure that can capture values is a nested function, written within the body of another function. A nested function can capture any of its outer function’s arguments and can also capture any constants and variables defined within the outer function. func makeIncrementor(forIncrement amount: Int) -\u003e () -\u003e Int { var runningTotal = 0 func incrementor() -\u003e Int { runningTotal += amount return runningTotal } return incrementor } Swift determines what should be captured by reference and what should be copied by value. You don’t need to annotate a variable to say that they can be used within the nested function. Swift also handles all memory management involved in disposing of variables when they are no longer needed by the function. Capturing Self If you create a closure that references self.* it will capture self and retain a strong reference to it. This is sometimes the intended behavior, but often could lead to retain cycles where both objects won’t get deallocated at the end of their lifecycles. The two best options are to use unowned or weak. This might look a bit messy, but saves a lot of headache. Use unowned when you know the closure will only be called if self still exists, but you don’t want to create a strong (retain) reference. Use weak if there is a chance that self will not exist, or if the closure is not dependent upon self and will run without it. If you do use weak also remember that self will be an optional variable and should be checked for existence. typealias SomeClosureType = (_ value: String) -\u003e () class SomeClass { fileprivate var currentValue = \"\" init() { someMethod { (value) in // Retained self self.currentValue = value } someMethod { [unowned self] (value) in // Not retained, but ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:11:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Generics Coming soon… Back to top ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:12:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Control Statements Swift uses all of the same control statements that other languages have: If-Else If-Else if someTestCondition { // Code to execute if the condition is true } else if someOtherTestCondition { // Code to execute if the other test condition is true } else { // Code to execute if the prior conditions are false } As you can see, parentheses are optional. Ternary Operators The shorthand notation for an if-else statement is a ternary operator of the form: someTestCondition ? doIfTrue : doIfFalse Example: func stringForTrueOrFalse(trueOrFalse: Bool) -\u003e String { return trueOrFalse ? \"True\" : \"False\" } Nil Coalescing Operators In Swift, we need to consider the use of optional values. One very basic way to handle nil cases is with an if-else statement: func stringForOptionalExistence(optionalValue: String?) -\u003e String { if optionalValue != nil { return optionalValue } else { return \"Empty\" } } In this particular case, we are returning optionalValue if it is not nil, and \"Empty\" if optionalValue is nil. The shorthand notation for this type of if(!=nil)-else statement is a nil coalescing operator of the form: optionalValue ?? nonOptionalValue Example: func stringForOptionalExistence(optionalValue: String?) -\u003e String { return optionalValue ?? \"Empty\" } For Loops Swift enables you to use ranges inside of for loops now: for index in 1...5 { print(\"\\(index)times 5 is \\(index * 5)\") } // Or if you don't need the value of the index let base = 3, power = 10 var answer = 1 for _ in 1...power { answer *= base } print(\"\\(base)to the power of \\(power)is \\(answer)\") // prints \"3 to the power of 10 is 59049\" Enumerating arrays \u0026 dictionaries // We explicitly cast to the Movie class from AnyObject class for movie in someObjects as [Movie] { // Code to execute each time } // Enumerating simple array let names = [\"Anna\", \"Alex\", \"Brian\", \"Jack\"] for name in names { print(\"Hello, \\(name)!\") } // Enumerating simple dictionary let numberOfLegs = [\"spider\": 8, \"ant\": 6, \"cat\": 4] for (animalName, legCount) in numberOfLegs { print(\"\\(animalName)s have \\(legCount)legs\") } If you need to cast to a certain object type, see the earlier discussion about the as! and as? keywords. While Loop while someTestCondition { // Code to execute while the condition is true } Repeat While Loop repeat { // Code to execute while the condition is true } while someTestCondition Switch Switch statements are often used in place of if statements if there is a need to test if a certain variable matches the value of another constant or variable. For example, you may want to test if an error code integer you received matches an existing constant value or if it’s a new error code. switch errorStatusCode { case .network: // Code to execute if it matches case .wifi: // Code to execute if it matches default: // Code to execute if nothing else matched } Switch statements in Swift do not fall through the bottom of each case and into the next one by default. Instead, the entire switch statement finishes its execution as soon as the first matching switch case is completed, without requiring an explicit break statement. This makes the switch statement safer and easier to use than in C, and avoids executing more than one switch case by mistake. Exiting Loops Although break is not required in Swift, you can still use a break statement to match and ignore a particular case, or to break out of a matched case before that case has completed its execution. return : Stops execution and returns to the calling function. It can also be used to return a value from a function. break : Used to stop the execution of a loop. Back to top ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:13:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Extension Extensions add new functionality to an existing class, structure, enumeration or protocol type extension String { // Extending String type to calculate if a String instance is truthy of falsy var boolValue:Bool { if self == \"1\" return true } return false } let isTrue = \"0\".boolValue Back to top ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:14:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Protocol Define protocol Codable { // definitions var description: String var mustBeSettable: Int { get set } var doesNotNeedToBeSettable: Int { get } func dance () -\u003e Double static func someTypeMethod() mutating func toggle() // modify (or mutate) the instance it belongs to init(someParameter: Int) // require specific initializers } Usage import Foundation struct UserInfo: Codable { let username: String let loginCount: Int } extension UserInfo: CustomStringConvertible { var description: String { return \"\\(username)has tried to login \\(loginCount)time(s)\" } } ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:15:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Protocol Extension Protocols can be extended to provide method, initializer, subscript, and computed property implementations to conforming types. Very Important and Useful: Implementation to any method or computed property requirement of that protocol can only be in extension extension RandomNumberGenerator { func randomBool() -\u003e Bool { return random() \u003e 0.5 } } By creating an extension on the protocol, all conforming types automatically gain this method implementation without any additional modification. You can use protocol extensions to provide a default implementation to any method or computed property requirement of that protocol. extension PrettyTextRepresentable { var prettyTextualDescription: String { return textualDescription } } Back to top ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:15:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Error Handling Representing an Error enum BeverageMachineError: Error { case invalidSelection case insufficientFunds case outOfStock } func selectBeverage (_ selection: Int) throws -\u003e String { // do something return \"Waiting for beverage...\" } // us do...catch to handle error throwed by func let message:String do { message = try selectBeverage(20) } catch BeverageMachineError.invalidSelection { print(\"Invalid selection\") } catch BeverageMachineError.insufficientFunds { print(\"Insufficient Funds\") } catch BeverageMachineError.outOfStock { print(\"Out of Stock\") } catch { print (\"Generic error\") } // if throw error, return nil let message = try? selectBeverage(10) // if throw error, get a runtime error let message = try! selectBeverage(10) Back to top ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:16:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Passing Information Coming soon… Back to top ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:17:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"User Defaults User defaults are basically a way of storing simple preference values which can be saved and restored across app launches. It is not meant to be used as a data storage layer, like Core Data or sqlite. ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:18:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Storing Values let userDefaults = UserDefaults.standard userDefaults.setValue(\"Some Value\", forKey: \"RPSomeUserPreference\") ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:18:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Retrieving Values let userDefaults = UserDefaults.standard let someValue = userDefaults.value(forKey: \"RPSomeUserPreference\") as AnyObject? There are also other convenience functions on UserDefaults instances such as bool(forKey:...), string(forKey:...), etc. Back to top ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:18:2","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Common Patterns For a comprehensive list of design patterns, as established by the Gang of Four, look here: Design Patterns in Swift ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:19:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Singletons Singleton’s are a special kind of class where only one instance of the class exists for the current process. They are a convenient way to share data between different parts of an app without creating global variables or having to pass the data around manually, but they should be used sparingly since they often create tighter coupling between classes. To turn a class into a singleton, you use the following implementation where the function name is prefixed with shared plus another word which best describes your class. For example, if the class is a network or location manager, you would name the function sharedManager instead of sharedInstance. class MyClass { // MARK:- Instantiation // Naming convention: // sharedInstance, sharedManager, sharedController, etc. // depending on the class type static let sharedInstance = MyClass() // This prevents others from using the default '()' initializer for this class. private init() {} var isReady = true // More class code here } Explanation: The static constant sharedInstance is run as dispatch_once the first time that variable is accessed to make sure the initialization is atomic. This ensures it is thread safe, fast, lazy, and also bridged to ObjC for free. More from here. Usage: You would get a reference to that singleton class in another class with the following code: // Now you could do let myClass = MyClass.sharedInstance let answer = myClass.isReady ? \"Yep!\" : \"Nope!\" print(\"Are you ready to rock and roll? \\(answer)\") Back to top ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:19:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Unicode Support Although I don’t recommend this, Swift will compile even if you use emoji’s in your code since it offers Unicode support. More info from Apple here Back to top ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:20:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"FileIO ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:21:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"C style FileIO let fd = fopen(\"aFile.txt\", \"w\") fwrite(\"Hello Swift!\", 12, 1, fd) let res = fclose(file) if res != 0 { print(strerror(errno)) } let fd = fopen(\"aFile.txt\", \"r\") var array = [Int8](count: 13, repeatedValue: 0) fread(\u0026array, 12, 1, fd) fclose(fd) let str = String.fromCString(array) print(str) // Hello Swift! ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:21:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Swift Style FileIO let path = Bundle.main.path(forResource:\"test\", ofType: \"txt\") // read let lines = try? String(contentsOfFile: path!) .split{$0 == \"\\n\"} .map(String.init) // write do { let lines = self._outlines.joined(separator: \"\\n\") try lines.write(to: url, atomically: false, encoding: .utf8) } catch{} Back to top ","date":"2020-05-30","objectID":"/2020-05-30-code-swift-cheat-sheet/:21:2","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-code-swift-cheat-sheet/"},{"categories":["Machine Learning"],"content":"提升（Boosting）方法： 通过改变训练样本的权重（概率分布），学习过个分类器，并将这些分类器线性组合，提高分类性能。 ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:0:0","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"1. AdaBoost AdaBoost通过提高被前一轮弱分类器错误分类样本的权值，从而降低被正确分类样本的权值，并采取甲醛多数表决的方法达到分类目的。 输入：训练数据集$T={(x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)}$, $\\mathcal{Y} = {-1,+1}$; 输出：分类器$G(x)$ 1). 初始化训练数据权值分布 $$D_1 = (w_{11}, \\cdots, w_{1i}, \\cdots, w_{1N}), w_{1i} = \\frac{1}{N}, i = 1,2,\\cdots,N$$ 2). 对 $m = 1，2，\\cdots, M$ a.对权值分布$D_m$的训练数据集学习，得到基本分类器 $$ G_{m}(x): \\mathcal{X} \\rightarrow{-1,+1} $$ b.计算$G(x)$在训练数据集上的分类误差率 $$ e_{m}=\\sum_{i=1}^{N} P\\left(G_{m}\\left(x_{i}\\right) \\neq y_{i}\\right)=\\sum_{i=1}^{N} w_{m i} I\\left(G_{m}\\left(x_{i}\\right) \\neq y_{i}\\right) $$ c. 计算$G(x)$的系数 $$ \\alpha_{m}=\\frac{1}{2} \\log \\frac{1-e_{m}}{e_{m}} $$ d. 更新训练数据的权值分布 $$ D_{m+1}=\\left(w_{m+1,1}, \\cdots, w_{m+1, i}, \\cdots, w_{m+1, N}\\right) $$ $$ w_{m+1, i} = \\frac{w_{m i}}{Z_{m}} \\exp \\left(-\\alpha_{m} y_{i} G_{m}\\left(x_{i}\\right)\\right), \\quad i=1,2, \\cdots, N $$ 其中， $$ Z_{m}=\\sum_{i=1}^{N} w_{m i} \\exp \\left(-\\alpha_{m} y_{i} G_{m}\\left(x_{i}\\right)\\right) $$ 3）构建基本线性分类器组合 $$ f(x)=\\sum_{m=1}^{M} \\alpha_{m} G_{m}(x) $$ 得到最终分类器 $$ \\begin{aligned} G(x) \u0026=\\operatorname{sign}(f(x)) \\cr \u0026=\\operatorname{sign}\\left(\\sum_{m=1}^{M} \\alpha_{m} G_{m}(x)\\right) \\end{aligned} $$ ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:1:0","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"1.1 AdaBoost算法误差 AdaBoost算法最终分类器训练误差界为 $$ \\frac{1}{N} \\sum_{i=1}^{N} I\\left(G\\left(x_{i}\\right) \\neq y_{i}\\right) \\leqslant \\frac{1}{N} \\sum_{i} \\exp \\left(-y_{i} f\\left(x_{i}\\right)\\right)=\\prod_{m} Z_{m} $$ 这一定理说明，每一轮选取适当的$G_m$使$Z_m$最小，从而使训练误差下降最快。 对于二分类问题： $$ \\begin{aligned} \\prod_{m=1}^{M} Z_{m} \u0026=\\prod_{m=1}^{M}[2 \\sqrt{e_{m}\\left(1-e_{m}\\right)}] \\cr \u0026=\\prod_{m=1}^{M} \\sqrt{\\left(1-4 \\gamma_{m}^{2}\\right)} \\cr \u0026 \\leqslant \\exp \\left(-2 \\sum_{m=1}^{M} \\gamma_{m}^{2}\\right) \\end{aligned} $$ 其中， $\\gamma_{m}=\\frac{1}{2}-e_{m}$ ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:1:1","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"1.2 AdaBoost算法解释 AdaBooost可以认为：模型为加法模型，损失函数为指数函数，学习算法为前向分布算法的二分类学习方法 1.2.1 前向分步算法 考虑加法模型（additive model） $$ f(x)=\\sum_{m=1}^{M} \\beta_{m} b\\left(x ; \\gamma_{m}\\right) $$ 其中，$b(x; \\gamma_m)$为基函数，$gamma_m$为参数， $\\beta_m$为系数。 在给定训练集和损失函数$L(y,f(x))$的条件下，学习加法模型$f(x)$成为经验风险极小化（损失函数极小化）问题： $$ \\min_{\\beta_{m}, \\gamma_{m}} \\sum_{i=1}^{N} L\\left(y_{i}, \\sum_{m=1}^{M} \\beta_{m} b\\left(x_{i} ; \\gamma_{m}\\right)\\right) $$ 前向分布算法思想是： 从前向后，每一步只学一个基函数及其系数，逐步逼近优化目标函数，达到优化步骤简化的目的。 因此，每一步只需优化如下损失函数： $$ \\min_{\\beta, \\gamma} \\sum_{i=1}^{N} L\\left(y_{i}, \\beta b\\left(x_{i} ; \\gamma\\right)\\right) $$ 算法步骤 输入：训练数据集$T=\\lbrace (x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)\\rbrace$, 损失函数$L(y,f(x))$;基函数集$\\lbrace b(x;\\gamma) \\rbrace$; 输出：加法模型$f(x)$ 1）初始化$f_0(x) = 0$ 2) 对$m = 1,2,\\cdots, M$ a.极小化损失函数 $$ \\left(\\beta_{m}, \\gamma_{m}\\right)=\\arg \\min _{\\beta, \\gamma} \\sum_{i=1}^{N} L\\left(y_{i}, f_{m-1}\\left(x_{i}\\right)+\\beta b\\left(x_{i} ; \\gamma\\right)\\right) $$ 得到参数$\\beta_m$, $\\gamma_m$。 b.更新 $$ f_{m}(x)=f_{m-1}(x)+\\beta_{m} b\\left(x ; \\gamma_{m}\\right) $$ 3）得到加法模型 $$ f(x)=f_{M}(x)=\\sum_{m=1}^{M} \\beta_{m} b\\left(x ; \\gamma_{m}\\right) $$ ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:1:2","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"2. Boosting Tree 提升树🌲是以决策树为基本分类器的提升方法 ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:2:0","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"2.1 提升树模型 采用加法模型（基函数的线性组合）与前向分布算法： $$ f_{M}(x)=\\sum_{m=1}^{M} T\\left(x ; \\Theta_{m}\\right) $$ 其中 $T\\left(x ; \\Theta_{m}\\right)$表示决策树，$\\Theta_{m}$决策树参数， $M$为树的个数 ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:2:1","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"2.2 提升树算法 采用加法模型和前向分布算法实现学习优化的过程。 首先确定提升树$f_{0}(x)=0$， 第$m$步的模型是 $$ f_{m}(x)=f_{m-1}(x)+T\\left(x ; \\Theta_{m}\\right) $$ 其中， $f_{m-1}(x)$为当前模型，通过经验风险极小化确定下一刻决策树的参数$\\Theta_{m}$： $$ \\hat \\Theta_m = \\arg \\min_{\\Theta_{m}} \\sum_{i=1}^{N} L(y_{i}, f_{m-1} (x_{i})+T (x_{i} ; \\Theta_{m} )) $$ 2.2.1 回归问题提升树 训练数据集: $T=\\lbrace (x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)\\rbrace$, $x_{i} \\in \\mathcal{X} \\subseteq \\mathbf{R}^{n}$, $\\mathcal{X}$为输入空间， $\\mathcal{Y} \\subseteq \\mathbf{R}$; 将输入空间划分为$J$个互不相交的区域$R1，R2, \\cdots, R_J$， 并且每个区域上确定输出的常量$c_j$，那么树可以表示为： $$ T(x ; \\Theta)=\\sum_{j=1}^{J} c_{j} I\\left(x \\in R_{j}\\right) $$ 其中， $$ \\Theta=\\lbrace \\left(R_{1}, c_{1}\\right),\\left(R_{2}, c_{2}\\right), \\cdots,\\left(R_{J}, c_{J}\\right)\\rbrace $$ 表示树的却与划分和各个取悦是那个的常数。 采用一下前向分布算法 $$ \\begin{aligned} \u0026f_{0}(x)=0\\cr \u0026\\begin{array}{l} f_{m}(x)=f_{m-1}(x)+T\\left(x ; \\Theta_{m}\\right), \\quad m=1,2, \\cdots, M \\cr f_{M}(x)=\\sum_{m=1}^{M} T\\left(x ; \\Theta_{m}\\right) \\end{array} \\end{aligned} $$ 求解$\\hat \\Theta_{m}$， 若用平方误差损失函数： $$ L(y, f(x))=(y-f(x))^{2} $$ 则损失函数为： $$ \\begin{aligned} L\\left(y, f_{m-1}(x)+T\\left(x ; \\Theta_{m}\\right)\\right) \u0026=\\left[y-f_{m-1}(x)-T\\left(x ; \\Theta_{m}\\right)\\right]^{2} \\cr \u0026=\\left[r-T\\left(x ; \\Theta_{m}\\right)\\right]^{2} \\end{aligned} $$ 这里， $$ r=y-f_{m-1}(x) $$ 是当前模型拟合数据的残差（residual）。因此对于回归问题提升树，只需拟合当前模型残差。得到$T\\left(x ; \\Theta_{m}\\right)$，更新模型，得到$f_m(x)$。 ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:2:2","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"3. 梯度提升 当损失函数不是简单的平方损失、指数损失时，提升树的优化就很难。梯度提升算法利用最速下降法的近似方法，计算损失函数的负梯度在当前模型的值 $$ -\\left[\\frac{\\partial L\\left(y, f\\left(x_{i}\\right)\\right)}{\\partial f\\left(x_{i}\\right)}\\right]_{f(x)=f_{m-1}(x)} $$ 并将其作为回归问题提升树算法中的残差近似值，拟合一个回归树。 输入： 训练数据集$T=\\lbrace (x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)\\rbrace$, $x_{i} \\in \\mathcal{X} \\subseteq \\mathbf{R}^{n}$,$\\mathcal{X}$为输入空间， $\\mathcal{Y} \\subseteq \\mathbf{R}$; 损失函数$L(y,f(x))$ 输出： 回归树$\\hat f(x)$ 初始化 $$ f_{0}(x)=\\arg \\min _{c} \\sum_{i=1}^{N} L\\left(y_{i}, c\\right) $$ 对 $m=1，2，\\cdots, M$ (1) 对 $i=1，2，\\cdots, N$计算 $$ r_{m i}=-\\left[\\frac{\\partial L\\left(y_{i}, f\\left(x_{i}\\right)\\right)}{\\partial f\\left(x_{i}\\right)}\\right]_{f(x)=f_{m-1}(x)} $$ (2) 对$r_{mi}$拟合一个回归树，得到第$m$颗树的节点区域$R_{mj}$ (3) 对$j=1,2,\\cdots, J$, 计算 $$ c_{m j}=\\arg \\min _{c} \\sum_{x_{i} \\in R_{m j}} L\\left(y_{i}, f_{m-1}\\left(x_{i}\\right)+c\\right) $$ (4)更新 $$ f_{m}(x)=f_{m-1}(x)+\\sum_{j=1}^{J} c_{m j} I\\left(x \\in R_{m j}\\right) $$ 得到回归树 $$ \\hat{f}(x)=f_{M}(x)=\\sum_{m=1}^{M} \\sum_{j=1}^{J} c_{m j} I\\left(x \\in R_{m j}\\right) $$ 参考： 李航《统计学习方法》 ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:3:0","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"隐马可夫模型（HMM）描述隐藏的马可夫链随机生成观测序列的过程，属于生成模型。 HMM在语音识别、自然语言处理、生物信息、模式识别等领域由广泛应用。 ","date":"2020-05-03","objectID":"/2020-05-03-ml-hmm/:0:0","tags":["Hidden Markov Model","Expectation Maximization","Probabilistic Graphical Model","Statistical Learning"],"title":"Hidden Markov Model (HMM)","uri":"/2020-05-03-ml-hmm/"},{"categories":["Machine Learning"],"content":"1. HMM的定义 隐马可夫模型是关于时序的概率模型， 描述由一个隐藏的马可夫链随机生成不可观测的状态，再由各个状态生成一个观测，从而产生观测随机序列的过程。 简而言之，隐马可夫链随机成状态序列（state sequence），而每个状态生成观测，产生观测序列（observation sequence）。序列的一个位置可以看作一个时刻。 令$Q$ 表示所有可能状态的集合：$Q = { q_1, q_2, \\cdots, q_N }$; 令$V$ 表示所有可能的观测集合：$V = {v_1, v_2, \\cdots, v_M }$; 令$I$ 表示长度为T的状态序列： $I = (i_1, i_2, \\cdots, i_T)$; 令$O$ 表示对应的是观测序列： $O = (o_1, o_2, \\cdots, o_T)$. 令$A$是转移概率矩阵： $$A = [a_{ij}]_{N \\times N}$$ 其中， $$a_{ij} = P(i_{t+1} = q_j | i_t = q_j), i=1,2, \\cdots, N; j = 1,2, \\cdots, N$$ 是在时刻$t$处于状态$q_i$的条件下生成观测$t +1$转移到状态$q_j$的概率。 令$B$是观测概率矩阵： $$B = [b_j(k)]_{N \\times M}$$ 其中， $$b_j(k) = P(o_t = v_k | i_t = q_j), k=1,2,\\cdots, M; j=1,2,\\cdots, N$$ 是在时刻$t$处于状态$q_j$的条件下生成观测$v_k$ 的概率。 令$\\pi$是初始状态概率向量： $$\\pi = (\\pi_i)$$ 其中， $$\\pi_{i} = P(i_1 = q_i),i=1,2,\\cdots, N$$ 是时刻t=1处于状态$q_i$的概率. 隐马可夫模型$\\lambda$由$\\pi$， $A$，$B$决定。 $$\\lambda = (A, B, \\pi)$$ 其中，$\\pi$和$A$决定状态序列，$B$决定观测序列。 隐马可夫模型的两个基本假设 齐次马可夫性 隐马可夫链在任意时刻t的状态前一时刻状态，与其他时刻的隐状态和观测无关， 也与时刻t无关： $$P(i_t | i_{t-1}, O_{t-1}, \\cdots, i_1, o_1) = P(i_t | i_{t-1}), t = 1,2,\\cdots,T$$ 观测独立性 任意时刻的观测只依赖改时刻的马可夫链状态，与其他观测和状态无关: $$P(o_t | i_{T}, O_{T}, i_{T-1}, o_{T-1}\\cdots, i_{t+1}, O_{t+1}, i_{t-1}, O_{t-1}, i_1, o_1) = P(o_t | i_{t})$$ ","date":"2020-05-03","objectID":"/2020-05-03-ml-hmm/:1:0","tags":["Hidden Markov Model","Expectation Maximization","Probabilistic Graphical Model","Statistical Learning"],"title":"Hidden Markov Model (HMM)","uri":"/2020-05-03-ml-hmm/"},{"categories":["Machine Learning"],"content":"2. HMM的3个基本问题 概率计算：给定模型$\\lambda = (A, B, \\pi)$和观测序列 $O = (o_1, o_2, \\cdots, o_T)$， 求概率$P(O | \\lambda)$ 学习: 已知观测序列$O = (o_1, o_2, \\cdots, o_T)$，估计模型参数$\\lambda = (A, B, \\pi)$， 使概率$P(O \\vert \\lambda)$最大（用极大似然估计）。 预测：给定模型$\\lambda = (A, B, \\pi)$和观测序列 $O = (o_1, o_2, \\cdots, o_T)$，求条件概率$P(I | O)$最大的状态序列 $I = (i_1, i_2, \\cdots, i_T)$. ","date":"2020-05-03","objectID":"/2020-05-03-ml-hmm/:2:0","tags":["Hidden Markov Model","Expectation Maximization","Probabilistic Graphical Model","Statistical Learning"],"title":"Hidden Markov Model (HMM)","uri":"/2020-05-03-ml-hmm/"},{"categories":["Machine Learning"],"content":"2.1 概率计算前向（forward）和后向（backward）算法 2.1.1 前向算法 给定模型$\\lambda$，当时刻$t$时，状态为$q_i$，部分观测序列为$o_1, o_2, \\cdots, o_t$，记： $$\\alpha_{t}(i) = P(o_1, o_2, \\cdots, o_t, i_t = q_i | \\lambda)$$ 输入： 隐马可夫模型 $\\lambda$， 观测序列$O$; 输出： 观测序列概率$P(O | \\lambda)$ （1）初值 $$ \\alpha_{1}(i)=\\pi_{i} b_{i}\\left(o_{1}\\right), \\quad i=1,2, \\cdots, N $$ （2）递推 对 $t = 1,2, \\cdots, T-1,$ $$ \\alpha_{t+1}(i)=\\left[\\sum_{j=1}^{N} \\alpha_{t}(j) a_{j i}\\right] b_{i}\\left(o_{t+1}\\right), \\quad i=1,2, \\cdots, N $$ （3）终止 $$ P(O | \\lambda)=\\sum_{i=1}^{N} \\alpha_{T}(i) $$ 2.1.2 后向算法 给定模型$\\lambda$，当时刻$t$时，状态为$q_i$，部分观测序列为$o_1, o_2, \\cdots, o_t$，记： $$\\beta_{t}(i) = P(o_{t+1}, o_{t+2}, \\cdots, o_T | i_t = q_i, \\lambda)$$ 输入： 隐马可夫模型 $\\lambda$， 观测序列$O$; 输出： 观测序列概率$P(O | \\lambda)$ （1）初始 令最终时刻所有状态$q_i$ $$\\beta_T(i) = 1, i=1,2,\\cdots, N$$ （2）递推 对$t=T-1, T-2, \\cdots, 1$ $$ \\beta_{t}(i)=\\sum_{j=1}^{N} a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j), \\quad i=1,2, \\cdots, N $$ （3）终止 $$ P(O | \\lambda)=\\sum_{i=1}^{N} \\pi_{i} b_{i}\\left(o_{1}\\right) \\beta_{1}(i) $$ 利用前后向概率定义，可以将观测序列概率$P(O \\vert \\lambda)$统一写成 $$ P(O | \\lambda)=\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{t}(i) a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j), \\quad t=1,2, \\cdots, T-1 $$ ","date":"2020-05-03","objectID":"/2020-05-03-ml-hmm/:2:1","tags":["Hidden Markov Model","Expectation Maximization","Probabilistic Graphical Model","Statistical Learning"],"title":"Hidden Markov Model (HMM)","uri":"/2020-05-03-ml-hmm/"},{"categories":["Machine Learning"],"content":"2.2 学习问题的EM算法 实质上求一个隐变量的概率模型的参数估计： $$ P(O | \\lambda)=\\sum_{I} P(O | I, \\lambda) P(I | \\lambda) $$ 参数估计由EM算法实现: (待续) 输入：观测数据$O = (o_1, o_2, \\cdots, o_T)$; 输出：隐马可夫模型参数 （1）初始化 对 n=0， 选取$a_{ij}^{(0)}$, $b_{j}(k)^{(0)}$, $\\pi_{i}^{(0)}$, 得到模型$\\lambda = (A^{(0)}, B^{(0)}, \\pi^{(0)})$. （2）递推 对$n=1,2, \\cdots,$, 有 $$ a_{i j}^{(n+1)}=\\frac{\\sum_{t=1}^{T-1} \\xi_{t}(i, j)}{\\sum_{t=1}^{T-1} \\gamma_{t}(i)} $$ 另， $$ b_{j}(k)^{(n+1)}=\\frac{\\sum_{t=1, o_{t}=v_{k}}^{T} \\gamma_{t}(j)}{\\sum_{t=1}^{T} \\gamma_{t}(j)} $$ $$ \\pi_{i}^{(n+1)}=\\gamma_{1}(i) $$ 其中，时刻$t$处于$q_i$，且时刻$t+1$处于状态$q_j$的概率, 记 $$ \\xi_{t}(i, j)=P\\left(i_{t}=q_{i}, i_{t+1}=q_{j} | O, \\lambda\\right) $$ 那么 $$ \\xi_{t}(i, j)=\\frac{P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O | \\lambda\\right)}{P(O | \\lambda)}=\\frac{P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O | \\lambda\\right)}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O | \\lambda\\right)} $$ 和时刻$t$处于$q_i$的概率， 有 $$ \\gamma_{t}(i)=P\\left(i_{t}=q_{i} | O, \\lambda\\right)=\\frac{P\\left(i_{t}=q_{i}, O | \\lambda\\right)}{P(O | \\lambda)} $$ （3）终止 得到模型参数$\\lambda^{(n+1)} = (A^{(n+1)}, B^{(n+1)}, \\pi^{(n+1)})$ ","date":"2020-05-03","objectID":"/2020-05-03-ml-hmm/:2:2","tags":["Hidden Markov Model","Expectation Maximization","Probabilistic Graphical Model","Statistical Learning"],"title":"Hidden Markov Model (HMM)","uri":"/2020-05-03-ml-hmm/"},{"categories":["Machine Learning"],"content":"2.3 预测算法 包括近似算法和维特比算法（Viterbi algorithm） 2.3.1 近似算法 在每个时刻$t$， 选择在该时刻最可能出现的状态 $i^*_t$从而得到一个状态序列 $I^* = (i^*_i, i^*_i, \\cdots, i^*_T)$，将它最为预测结果。 给定模型$\\lambda$和观测序列$O$， 在时刻$t$处于状态$q_i$的概率$\\gamma_t(i)$是 $$ \\gamma_{t}(i)=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{P(O | \\lambda)}=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{\\sum_{j=1}^{N} \\alpha_{t}(j) \\beta_{t}(j)} $$ 而每一时刻$t$最有可能的状态$i_{t}^{*}$是 $$ i_{t}^{*}=\\arg \\max _{1 \\leqslant i \\leqslant N}\\left[\\gamma_{t}(i)\\right], \\quad t=1,2, \\cdots, T $$ 从而得到状态序列 $$I^* = (i^*_i, i^*_i, \\cdots, i^*_T)$$ 缺点： 不能保证预测状态序列整体是最有可能的状态序列，因为预测的状态序列实际可能由不发生的部分。 2.3.2 维特比算法 实质是运用动态规划求概率最大路径，从而解决HMM的预测问题 维特比算法: 只需从时刻$t=1$开始，递推地计算在时刻$t$状态为$q_i$的各条部分路径的最大概率，直至得到时刻$t = T$状态为$i$的各条路径的最大概率。时刻 $t = T$ 的最大概率即为最优路径的概率 $P^\\ast$, 最优路径的终结点$i^*_T$ 也同时得到。之后，为了找出最优路径的各个结点，从终结点$i^*_T$开始，由后向前逐步求得结点 $i^*_{T-1}, \\cdots, i^*_1$，得到最优路径$I^* = (i^*_i, i^*_i, \\cdots, i^*_T)$。 定义在时刻$t$状态$i$的所有单个路径中概率最大值为 $$ \\delta_{t}(i)=\\max_{i_{1}, i_{2}, \\cdots, i_{t-1}} P\\left(i_{t}=i, i_{t-1}, \\cdots, i_{1}, o_{t}, \\cdots, o_{1} | \\lambda\\right), \\quad i=1,2, \\cdots, N $$ 因此 $$ \\begin{aligned} \\delta_{t+1}(i) \u0026=\\max _{i_{1}, i_{2}, \\cdots, i_{t}} P\\left(i_{t+1}=i, i_{t}, \\cdots, i_{1}, o_{t+1}, \\cdots, o_{1} | \\lambda\\right) \\cr \u0026= \\max _{1 \\leqslant j \\leqslant N}\\left[\\delta_{t}(j) a_{j i}\\right] b_{i}\\left(o_{t+1}\\right), \\quad i=1,2, \\cdots, N ; \\quad t=1,2, \\cdots, T-1 \\end{aligned} $$ 定义在时刻t状态i的所有单个路径中概率最大路径的第$t-1$个节点为 $$ \\Psi_{t}(i)=\\arg \\max _{1 \\leqslant j \\leqslant N}\\left[\\delta_{t-1}(j) a_{j i}\\right], \\quad i=1,2, \\cdots, N $$ 输入： 隐马可夫模型 $\\lambda$， 观测序列$O$; 输出： 最优路径$$I^* = (i^*_i, i^*_i, \\cdots, i^*_T)$$ (1) 初始化: $$ \\begin{array}{c} \\delta_{1}(i)=\\pi_{i} b_{i}\\left(o_{1}\\right), \\quad i=1,2, \\cdots, N \\cr \\Psi_{1}(i)=0, \\quad i=1,2, \\cdots, N \\end{array} $$ (2) 递推: $$ \\begin{array}{c} \\delta_{t}(i)=\\max _{1 \\leqslant j \\leqslant N}\\left[\\delta_{t-1}(j) a_{j i}\\right] b_{i}\\left(o_{t}\\right), \\quad i=1,2, \\cdots, N \\cr \\Psi_{t}(i)=\\arg \\max _{1 \\leqslant j \\leqslant N}\\left[\\delta_{t-1}(j) a_{j i}\\right], \\quad i=1,2, \\cdots, N \\end{array} $$ (3) 终止 $$ \\begin{array}{c} P^* = \\max _{1 \\leqslant i \\leqslant N} \\delta_T(i) \\cr i^*_T = \\arg \\max _{1 \\leqslant i \\leqslant N} [ \\delta_T(i)] \\end{array} $$ (4) 最优路径回溯 对$t=T-1, T-2, \\cdots, 1$, $$i^*_t = \\Psi_{t+1}(i^*_{t+1})$$ 参考： 李航《统计学习方法》 ","date":"2020-05-03","objectID":"/2020-05-03-ml-hmm/:2:3","tags":["Hidden Markov Model","Expectation Maximization","Probabilistic Graphical Model","Statistical Learning"],"title":"Hidden Markov Model (HMM)","uri":"/2020-05-03-ml-hmm/"},{"categories":["Machine Learning"],"content":"潜在语义分析（LSA）是一种非监督学习方法，用于文本话题分析。其特点是通过矩阵分解发现文本于单词之间的基于话题的语义关系。 潜在语义分析是一种非概率话题分析模型。步骤为： 先将文本集合表示为单词-文本矩阵 对单词-文本矩阵进行奇异值分解（SVD）或非负矩阵分解（NMF），从而得到话题向量空间，以及文本在话题向量空间的表示 ","date":"2020-04-30","objectID":"/2020-04-30-ml-lsa/:0:0","tags":["TF-IDF","Latent Semantic Analysis","Statistical Learning"],"title":"Latent semantic analysis (LSA)","uri":"/2020-04-30-ml-lsa/"},{"categories":["Machine Learning"],"content":"1. 单词向量空间 给定由n个文本的集合 $D = \\lbrace d_1, d_2, \\cdots, d_n \\rbrace$ ,以及所有文本中单词集 $W = \\lbrace w_1, w_2, \\cdots, w_m \\rbrace$, 则单词在文本中出现的数据用单词-文本矩阵(word-document matrix)表示, 记为： $$ X=\\left[\\begin{array}{cccc} x_{11} \u0026 x_{12} \u0026 \\cdots \u0026 x_{1 n} \\cr x_{21} \u0026 x_{22} \u0026 \\cdots \u0026 x_{2 n} \\cr \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\cr x_{m 1} \u0026 x_{m 2} \u0026 \\cdots \u0026 x_{m n} \\end{array}\\right] $$ 其中， $x_{ij}$表示单词$w_i$在文本$d_j$中出现的频数或权值。由于单词种类多，而每个文本中单词种类通常少，所以为单词-文本矩阵是稀疏矩阵。 权值常用单词频率-逆文本频率(term frequeny-inverse document frequency, TF-IDF)表示: $$ \\mathrm{TFIDF_{ij}}=\\frac{\\mathrm {tf_{ij}}}{\\mathrm {tf_{\\cdot j}}} \\log \\frac{\\mathrm{df}}{\\mathrm {df_i}}, \\quad i=1,2, \\cdots, m ; \\quad j=1,2, \\cdots, n $$ 其中， $\\mathrm{tf}_{ij}$ 是单词 $w_i$ 在文本 $d_j$ 中的频数; $\\mathrm{tf}_{\\cdot j}$ 是 $d_j$ 中出现的所有单词频数之和; $\\mathrm{df}_{i}$ 是含有单词$w_i$的文本数; $\\mathrm{df}$ 是文本集合$D$的全部文本数。 单词-文本矩阵的第j列向量$x_j$表示文本$d_j$： $$ x_{j}=\\left[\\begin{array}{c} x_{1 j} \\cr x_{2 j} \\cr \\vdots \\cr x_{m j} \\end{array}\\right], \\quad j=1,2, \\cdots, n $$ 两个单词向量内积或者标准化内积表示对应文本之间的语义相似度， 因此文本$d_i$与$d_j$的之间的相似度： $$ x_{i} \\cdot x_{j}, \\quad \\frac{x_{i} \\cdot x_{j}}{\\left|x_{i}\\right|\\left|x_{j}\\right|} $$ 单词向量空间模型的优缺点 优点： 简单，计算高效 缺点： 一词多义(polysemy)和多词一义(synonymy)不能很好处理， 存在相似度计算不准确的问题 ","date":"2020-04-30","objectID":"/2020-04-30-ml-lsa/:0:1","tags":["TF-IDF","Latent Semantic Analysis","Statistical Learning"],"title":"Latent semantic analysis (LSA)","uri":"/2020-04-30-ml-lsa/"},{"categories":["Machine Learning"],"content":"2. 话题（topic）向量空间 话题没有严格的定义，是指文本讨论的内容或主题。而基于话题的模型是为了解决单词向量不能很好处理一词多义(polysemy)和多词一义(synonymy)的问题。 给定由n个文本的集合 $D = \\lbrace d_1, d_2, \\cdots, d_n \\rbrace$, 以及所有文本中单词集 $W = \\lbrace w_1, w_2, \\cdots, w_m \\rbrace$, 则单词在文本中出现的数据用单词-文本矩阵(word-document matrix)表示, 记为： $$ X=\\left[\\begin{array}{cccc} x_{11} \u0026 x_{12} \u0026 \\cdots \u0026 x_{1 n} \\cr x_{21} \u0026 x_{22} \u0026 \\cdots \u0026 x_{2 n} \\cr \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\cr x_{m 1} \u0026 x_{m 2} \u0026 \\cdots \u0026 x_{m n} \\end{array}\\right] $$ 假设所有文本含有k个话题， 每个话题l由一个定义在单词集合W傻姑娘的m维向量： $$ t_{l}=\\left[\\begin{array}{c} t_{1 l} \\cr t_{2 l} \\cr \\vdots \\cr t_{m l} \\end{array}\\right], \\quad l=1,2, \\cdots, k $$ 则有话题向量矩阵T： $$ T=\\left[\\begin{array}{cccc} t_{11} \u0026 t_{12} \u0026 \\cdots \u0026 t_{1 k} \\cr t_{21} \u0026 t_{22} \u0026 \\cdots \u0026 t_{2 k} \\cr \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\cr t_{m 1} \u0026 t_{m 2} \u0026 \\cdots \u0026 t_{m k} \\end{array}\\right] $$ ","date":"2020-04-30","objectID":"/2020-04-30-ml-lsa/:0:2","tags":["TF-IDF","Latent Semantic Analysis","Statistical Learning"],"title":"Latent semantic analysis (LSA)","uri":"/2020-04-30-ml-lsa/"},{"categories":["Machine Learning"],"content":"3. 文本在话题向量空间的表示 文本$d_j$在单词向量空间用$x_j$表示，将$x_j$投影到话题向量空间$T$，得到话题向量空间$y_j$，$y_j$是个k维向量: $$ y_{j}=\\left[\\begin{array}{c} y_{1 j} \\cr y_{2 j} \\cr \\vdots \\cr y_{k j} \\end{array}\\right], \\quad j=1,2, \\cdots, n $$ 因此，话题-文本矩阵$Y$： $$ Y=\\left[\\begin{array}{cccc} y_{11} \u0026 y_{12} \u0026 \\cdots \u0026 y_{1 n} \\cr y_{21} \u0026 y_{22} \u0026 \\cdots \u0026 y_{2 n} \\cr \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\cr y_{k 1} \u0026 y_{k 2} \u0026 \\cdots \u0026 y_{k n} \\end{array}\\right] $$ ","date":"2020-04-30","objectID":"/2020-04-30-ml-lsa/:0:3","tags":["TF-IDF","Latent Semantic Analysis","Statistical Learning"],"title":"Latent semantic analysis (LSA)","uri":"/2020-04-30-ml-lsa/"},{"categories":["Machine Learning"],"content":"4. 单词向量空间到话题向量空间转换 因此，单词向量空间的文本向量$x_j$，可以用话题向量空间$y_j$近似表示，即以k个话题向量以$y_j$为系数的线性组合： $$ x_{j} \\approx y_{1 j} t_{1}+y_{2 j} t_{2}+\\cdots+y_{k j} t_{k}, \\quad j=1,2, \\cdots, n $$ 矩阵形式： $$ X \\approx TY $$ 这就是潜在语义分析。 参考： 李航《统计学习方法》 ","date":"2020-04-30","objectID":"/2020-04-30-ml-lsa/:0:4","tags":["TF-IDF","Latent Semantic Analysis","Statistical Learning"],"title":"Latent semantic analysis (LSA)","uri":"/2020-04-30-ml-lsa/"},{"categories":["Machine Learning"],"content":"CRF条件随机场，可应用于标注问题 概率无向图模型Probabilistic undirected graphical model(Markov random field) 是一个可以由无向图表示的联合概率分布 ","date":"2020-04-29","objectID":"/2020-04-29-ml-crf/:0:0","tags":["Probabilistic Graphical Model","Statistical Learning"],"title":"Conditional random field (CRF)","uri":"/2020-04-29-ml-crf/"},{"categories":["Machine Learning"],"content":"0. 概率图模型分类 概率图模型（probabilistic graphical model, PGM），是一种学习任务的框架描述，它将学习任务归结为计算变量的概率分布。 按照概率图中变量关系的不同，概率图模型可以大致分为两类： 贝叶斯网络：有向图模型，使用有向无环图表达关系（通常，变量间存在显式的因果关系） 马尔科夫网络：无向图模型，使用无图表达关系（通常，变量间存有关系，但是难以显式表达） 同时存有有向边和无向边的模型，如条件随机场（conditional random field）和链图（chain graph），单独看做一类局部有向模型。 贝叶斯网络 可以分为静态贝叶斯网络和动态贝叶斯网络。相比于静态贝叶斯网络，动态（dynamic）贝叶斯网络主要用于时序数据建模（如语音识别、自然语言处理、轨迹数据挖掘等）。其中，一种结构最简单的动态贝叶斯网络就是隐马尔可夫模型（hidden markov model, HMM）。一般来说，贝叶斯网络中每一个结点都对应于一个先验概率分布或者条件概率分布，因此整体的联合分布可以直接分解为所有单个结点所对应的分布的乘积。 马尔可夫网 由于变量之间没有明确的因果关系，它的联合概率分布通常会表达为一系列势函数（potential function）的乘积。通常情况下，这些乘积的积分并不等于1，因此，还要对其进行归一化才能形成一个有效的概率分布——这一点往往在实际应用中给参数估计造成非常大的困难。 按照表示的抽象级别不同，概率图模型可以分为： 基于随机变量的概率图模型，如贝叶斯网、马尔可夫网、条件随机场和链图等 基于模板的概率图模型．这类模型根据应用场景不同又可分为两种： 暂态模型，包括动态贝叶斯网（Dynamic Bayesian Network, DBN）和状态观测模型，其中状态观测模型又包括线性动态系统（Linear Dynamic System, LDS）如卡尔曼滤波器，还有隐马尔可夫模型（Hidden Markov Model, HMM）； 对象关系领域的概率图模型，包括盘模型（Plate Model，PM）、概率关系模型（Probabilistic Relational Model, PRM）和关系马尔可夫网（Relational Markov Network, RMN）。 参考：概率图模型总览 ","date":"2020-04-29","objectID":"/2020-04-29-ml-crf/:0:1","tags":["Probabilistic Graphical Model","Statistical Learning"],"title":"Conditional random field (CRF)","uri":"/2020-04-29-ml-crf/"},{"categories":["Machine Learning"],"content":"1. 模型定义 概率图模型：由图（Graph）表示的概率分布。 令无向图 G = (V, E) 表示联合概率分布P(Y)，即G中， 节点$v \\in V$ 表示随机变量$Y_{v}, Y=\\left(Y_{v}\\right)_{v \\in V}$； 边$e \\in E$表示随机变量之间的概率依赖关系 无向图表示的随机变量存在: 成对马可夫性 pariwise Markov property 指任意两个没有边连接的节点，在给定随机变量组（其他所有节点）条件下，该两节点是条件独立 局部马可夫性 local Markov property 全局马可夫性 global Markov property 概率无向图模型：无向图 $G = (V, E)$ 表示联合概率分布 $P(Y)$，如果联合概率分布 $P(Y)$ 满足成对、局部或全局马可夫性，就称此联合概率分布 $P(Y)$ 为概率无向图模型，或马可夫随机场 团（clique）：图G中任何两个节点均有边连接的节点子集 最大团（maximal clique）：团C中不能再加任何一个节点使它成为更大的团，则称最大团 ","date":"2020-04-29","objectID":"/2020-04-29-ml-crf/:0:2","tags":["Probabilistic Graphical Model","Statistical Learning"],"title":"Conditional random field (CRF)","uri":"/2020-04-29-ml-crf/"},{"categories":["Machine Learning"],"content":"2. 条件随机场 条件随机场指给定随机变量X条件下， 随机变量Y的马可夫随机场。 2.1 条件随机场： 若随机变量$Y$构成一个由无向图$G = (V, E)$表示的马可夫随机场，即 $$ P\\left(Y_{v} | X, Y_{w}, w \\neq v\\right)=P\\left(Y_{v} | X, Y_{w}, w \\sim v\\right) $$ 对于任意节点$v$成立， 则称条件概率分布$P(Y\\vert X)$为条件随机场。其中$w \\sim v$表示在图$G = (V, E)$中与节点$v$有边连接的所有节点$w$， $w \\neq v$表示节点v以外的所有节点。 2.2 线性链条件随机场（ linear chain conditional random field） 线性链条件随机场也是对数线性模型(log linear model)，定义为： $$ P\\left(Y_{i} | X, Y_{1}, \\cdots, Y_{i-1}, Y_{i+1}, \\cdots, Y_{n}\\right)=P\\left(Y_{i} | X, Y_{i-1}, Y_{i+1}\\right) $$ 在条件概率模型$P(Y | X)$中， $Y$是输出变量，表示标记序列（状态序列，参见HMM）；$X$使输入变量，表示需要标注的观测序列。利用训练集，通过极大似然估计或正则化的极大似然估计得到条件概率模型$\\hat{P}(Y | X)$;预测时，对于给定输入序列$x$，求条件概率$\\hat{P}(Y | X)$最大的输出序列$\\hat{y}$。 2.3 条件随机场的参数化形式 设$P(Y\\vert X)$为线性链条件随机场，X取值为x， Y取值为y的条件概率具有如下形式： $$ P(y | x)=\\frac{1}{Z(x)} \\exp \\left(\\sum_{i, k} \\lambda_{k} t_{k}\\left(y_{i-1}, y_{i}, x, i\\right)+\\sum_{i, l} \\mu_{l} s_{l}\\left(y_{i}, x, i\\right)\\right) $$ 其中， $$ Z(x)=\\sum_{y} \\exp \\left(\\sum_{i, k} \\lambda_{k} t_{k}\\left(y_{i-1}, y_{i}, x, i\\right)+\\sum_{i, l} \\mu_{l} s_{l}\\left(y_{i}, x, i\\right)\\right) $$ 式中，$t_{k}$和$s_{l}$是特征函数, $\\lambda_{k}$和$\\mu_{l}$是对应的权值。 $Z(x)$是规范化因子。在所有可能输出的序列上进行求和操作。 关于特征函数： 令$t_{k}$是定义在边上的特征函数，称为转移特征，依赖当前和前一个位置 令$s_{l}$是定义在节点上的特征函数，称为状态特征，依赖当前位置 特征函数$t_{k}$和$s_{l}$取值0或1；满足条件取1，反之0 条件随机长完全由特征函数$t_{k}$和$s_{l}$， 和对应的权值$\\lambda_{k}$和$\\mu_{l}$确定。 2.4 条件随机场的矩阵形式 对于观测序列x的每个位置，y在m个标记中取值，可以定义一个m阶的矩阵随机变量： $$ M_{i}(x) = [ M_{i}(y_{i-1}, y_{i}|x) ] $$ 矩阵随机变量元素为 $$ \\begin{aligned} \u0026M_{i}\\left(y_{i-1}, y_{i} | x\\right)=\\exp \\left(W_{i}\\left(y_{i-1}, y_{i} | x\\right)\\right)\\cr \u0026W_{i}\\left(y_{i-1}, y_{i} | x\\right)=\\sum_{k=1}^{K} w_{k} f_{k}\\left(y_{i-1}, y_{i}, x, i\\right) \\end{aligned} $$ 这里$w_k$为 $$ w_{k}=\\begin{cases} \\lambda_{k}, \u0026 k=1,2, \\cdots, K_{1} \\cr \\mu_{l}, \u0026 k=K_{1}+l ; l=1,2, \\cdots, K_{2} \\end{cases} $$ 和$f_k$为 $$ f_{k}\\left(y_{i-1}, y_{i}, x, i\\right)=\\begin{cases} t_{k}\\left(y_{i-1}, y_{i}, x, i\\right), \u0026 k=1,2, \\cdots, K_{1} \\cr s_{l}\\left(y_{i}, x, i\\right), \u0026 k=K_{1}+l ; l=1,2, \\cdots, K_{2} \\end{cases} $$ 于是，条件概率$P_{w}(y \\vert x)$: $$ P_{w}(y | x)=\\frac{1}{Z_{w}(x)} \\prod_{i=1}^{n+1} M_{i}\\left(y_{i-1}, y_{i} | x\\right) $$ 其中， $$ Z_{w}(x)=\\left[M_{1}(x) M_{2}(x) \\cdots M_{n+1}(x)\\right]_{\\mathrm{start}, \\mathrm{stop}} $$ 注， $y_{0} = \\mathrm{start}$，表示开始状态； $y_{n+1} = \\mathrm{stop}$， 表示终止状态 参考： 李航《统计学习方法》 ","date":"2020-04-29","objectID":"/2020-04-29-ml-crf/:0:3","tags":["Probabilistic Graphical Model","Statistical Learning"],"title":"Conditional random field (CRF)","uri":"/2020-04-29-ml-crf/"},{"categories":["Statistic"],"content":"Probability, P-value, Likelihood ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:0:0","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"Probability and likelihood likehood \u0026 maximum likehood 在非正式场合似然（likelihood）和概率（Probability）几乎是一对同义词，但是在统计学中似然和概率却是两个不同的概念。 概率: 在特定环境下某件事情发生的可能性，也就是结果没有产生之前依据环境所对应的参数来预测某件事情发生的可能性。 比如抛硬币，抛之前我们不知道最后是哪一面朝上，但是根据硬币的性质我们可以推测任何一面朝上的可能性均为50%，这个概率只有在抛硬币之前才是有意义的，抛完硬币后的结果便是确定的； 似然: 刚好相反，是在确定的结果下去推测产生这个结果的可能环境（参数）。 假设随机抛掷一枚硬币1,000次，结果500次人头朝上，500次数字朝上，那么两面朝上的概率均为50%。运用出现的结果来判断这个事情本身的性质（参数），也就是似然。 当结果和参数相互对应，似然和概率在数值上相等。 用 θ 表示环境对应的参数，x 表示结果，那么概率可以表示为： $$P(x | \\theta )$$ $p(x \\vert θ)$ 是条件概率的表示方法。θ 是前置条件，理解为在 θ 的前提下，事件 x 发生的概率，相对应的似然可以表示为: $$\\mathcal{L}(\\theta | x)$$ 可以理解为已知结果为 x ，参数为 θ (似然函数里 θ 是变量，这里说的参数和变量是相对与概率而言的)对应的概率，即： $$\\mathcal{L}(\\theta | x)=P(x | \\theta)$$ 两者在数值上相等，但是意义并不相同, $\\mathcal{L}$ 是关于 θ 的函数，而 P 则是关于 x 的函数。 ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:1:0","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"Probability and P-value A p-value is the probability that random chance generated the data, or something else that is equal or rarer. A p-value is composed of three parts: The probability random chance would result in the observation. The probability of observing something else that is equally rare. The probability of observing something rarer or more extreme. But probability $$ \\text{Probalibility} = \\frac{ \\text{Number of outcomes of interest}} { \\text{The total number of outcomes}}$$ In hypothesis testing, p-values are numbers, between 0 and 1, that, how small does a p-value have to be before we are confident that interested A is different from B. Statquest: P Values, clearly explained ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:2:0","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"PDF (probability density function) PDF：概率密度函数（probability density function）, 连续型随机变量的概率密度函数是一个描述某个确定的取值点附近的可能性的函数。 数学表示：用PDF在某一区间上的积分来刻画随机变量落在这个区间中的概率 $$ \\operatorname{Pr}(a \\leq X \\leq b)=\\int_{a}^{b} f_{X}(x) d x $$ ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:2:1","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"PMF (probability mass function) PMF : 概率质量函数（probability mass function), 在概率论中，概率质量函数是离散随机变量在各特定取值上的概率。 数学表示： PMF其实就是高中所学的离散型随机变量的分布律。 $$ f_{X}(x)=\\operatorname{Pr}(X=x) $$ ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:2:2","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"CDF (cumulative distribution function) CDF : 累积分布函数 (cumulative distribution function)，是概率密度函数的积分，能完整描述一个实随机变量X的概率分布。 CDF是PDF的（从负无穷$-\\infty$到当前值的）积分，PDF是CDF的导数．（为了便于概率的计算，引入CDF的概念） CDF相当于其左侧的面积，也相当于小于该值的概率，负无穷的CDF值为０，正无穷的CDF值总为１． 对于连续变量，有 $$ F_{X}(x)=\\operatorname{Pr}(X \\leq x)=\\int_{-\\infty}^{x} f_{X}(t) dt $$ 对于离散型变量，有如 $$ F_{X}(x)=\\operatorname{Pr}(X \\leq x)= \\begin{cases} 0 \\text { if } x\u003c0 \\cr \\frac{1}{2} \\text { if } 0 \\leq x\u003c1 \\cr 1 \\text { if } x \\geq 1 \\end{cases} $$ ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:2:3","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"Central Limit Theorem 中心极限定理（Central Limit Theorem） 给定一个任意分布的总体， 每次从这些总体中随机抽取 n 个抽样，一共抽 m 次， 然后把这 m 组抽样分别求出平均值， 当m足够大时，这m次的平均值的分布（称为抽样分布）接近正态分布。 独立同分布的中心极限定理 $$ \\lim_{n \\rightarrow \\infty} F_{\\mathcal{X}}(x) = \\lim_{n \\rightarrow \\infty} P \\Bigg\\lbrace \\frac{\\sum_{k=1}^{n}X_k - n\\mu}{\\sqrt{n}\\sigma} \\leq x \\Bigg\\rbrace = \\int_{-\\infty}^{x} \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^2}{2}}dt $$ 德莫佛－拉普拉斯定理: 设随机变量序列$\\lbrace \\eta_1, \\eta_2,\\cdots, \\eta_n \\rbrace$ 服从参数为$n, p (0 \u003c p \u003c 1)$ 的二项分布 $$ \\lim_{n \\rightarrow +\\infty} P \\Bigg\\lbrace \\frac{\\eta_n - np }{\\sqrt{np(1-p)}} \\leq x \\Bigg\\rbrace = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^2}{2}}dt $$ ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:2:4","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"Law of Large Numbers 当样本数据无限大时，样本均值趋于总体均值 $$ \\bar{X} = \\frac{1}{n} \\sum_{k=1}^n X_k \\xrightarrow{p} \\mu $$ 大数定律告诉我们能用频率近似代替概率；能用样本均值近似代替总体均值。 辛钦大数定律: 设$X_1, X_2, \\cdots, X_n$是相互独立且服从同分布的随机变量序列， 具有数学期望$E(X_k) = \\mu$, $k=1,2,3,\\cdots$。对于任意 $\\epsilon \u003e 0$, 有 $$ \\lim_{n \\rightarrow +\\infty} P \\bigg\\lbrace | \\frac{1}{n} \\sum_{k=1}^n X_k - \\mu | \u003c \\epsilon \\bigg\\rbrace = 1 $$ 切比雪夫大数定律: 随机变量序列$X$具有相同期望和方差, 样本均值依概率$p$收敛于 $\\mu$ $$ \\frac{1}{n} \\sum_{k=1}^n X_k \\xrightarrow{p} \\mu $$ 伯努利大数定律: $n_A$是n次独立重复试验中事件A发生的次数， p是事件A在每次试验中发生的概率，任意$\\epsilon \u003e 0$ $$ \\lim_{n \\rightarrow +\\infty} P \\bigg\\lbrace | \\frac{n_A}{n} - p | \u003c \\epsilon \\bigg\\rbrace = 1 $$ 比较 定律 分布 期望 方差 结论 辛钦大数定律 相互独立且同分布 存在 估算期望 切比雪夫大数定律 相互独立 相同 相同 估算期望 伯努利大数定律 二项分布 相同 相同 频率=概率 相同点：$n \\rightarrow +\\infty$, 依概率趋近 条件组件变得严格 ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:2:5","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"Confidence interval 置信区间（confidence interval） 置信区间是指由样本统计量所构造的总体参数的估计区间。 置信区间展现的是这个参数的真实值落在测量值（推测值）的周围的可信程度。 ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:2:6","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"Additional: StatQuest How to calculate P-value ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:3:0","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Make bioinfo uncool again"],"content":"The correct way to convert seurat Robj to Scanpy h5ad","date":"2020-04-28","objectID":"/2020-04-28-seurat2scanpy/","tags":["scRNA-seq","Scanpy","Seurat"],"title":"Convert Seurat Robj to Scanpy h5ad","uri":"/2020-04-28-seurat2scanpy/"},{"categories":["Make bioinfo uncool again"],"content":"It costed me a lot of time to convert seurat objects to scanpy. It’s not a pleasant experience. Finally, I solved it. ","date":"2020-04-28","objectID":"/2020-04-28-seurat2scanpy/:0:0","tags":["scRNA-seq","Scanpy","Seurat"],"title":"Convert Seurat Robj to Scanpy h5ad","uri":"/2020-04-28-seurat2scanpy/"},{"categories":["Make bioinfo uncool again"],"content":"1. Install Seurat v3.0.2, or python kernel will always died!!! Don’t know why latest seurat not work. ","date":"2020-04-28","objectID":"/2020-04-28-seurat2scanpy/:0:1","tags":["scRNA-seq","Scanpy","Seurat"],"title":"Convert Seurat Robj to Scanpy h5ad","uri":"/2020-04-28-seurat2scanpy/"},{"categories":["Make bioinfo uncool again"],"content":"2. Set the R version for rpy2 # user defined R installation import os # path to your libR.so, only Seurat v3.0.2 works! # create a conda R env for seurat 3.0.2 first os.environ['R_HOME'] = '/home/fangzq/miniconda/envs/seurat/lib/R' # path depends on where you installed Python. os.environ['R_USER'] = '/home/fangzq/miniconda/lib/python3.7/site-packages/rpy2' ","date":"2020-04-28","objectID":"/2020-04-28-seurat2scanpy/:0:2","tags":["scRNA-seq","Scanpy","Seurat"],"title":"Convert Seurat Robj to Scanpy h5ad","uri":"/2020-04-28-seurat2scanpy/"},{"categories":["Make bioinfo uncool again"],"content":"3. Now, you’er good to go import scanpy as sc import glob Install anndata2ri first import anndata2ri from rpy2.robjects import r from rpy2.robjects.conversion import localconverter # activate rpy2 env anndata2ri.activate() robjs = glob.glob(\"data/*Robj\") Convert to h5ad r('library(Seurat)') for robj in robjs: r(f'x\u003c-load(\"{robj}\")') r('y=get(x)') r('rm(x)') r('DefaultAssay(y) \u003c- \"RNA\"') # get raw count matrix to save # seurat2 object # adata = r('as.SingleCellExperiment(UpdateSeuratObject(y))') adata = r('as.SingleCellExperiment(y)') adata.write_h5ad(filename=robj.replace(\"Robj\",\"h5ad\")) ","date":"2020-04-28","objectID":"/2020-04-28-seurat2scanpy/:0:3","tags":["scRNA-seq","Scanpy","Seurat"],"title":"Convert Seurat Robj to Scanpy h5ad","uri":"/2020-04-28-seurat2scanpy/"},{"categories":["Make bioinfo uncool again"],"content":"4. other way seurat -\u003e loom -\u003e scanpy It’s much easier, but I did not test. save to loom format fist. pbmc.loom \u003c- as.loom(pbmc.seurat, filename = \"../output/pbmc3k.loom\", verbose = FALSE) pbmc.loom read into scanpy pbmc3k = sc.read_loom(\"../output/pbmc3k.loom\") use sceasy to save h5ad. ","date":"2020-04-28","objectID":"/2020-04-28-seurat2scanpy/:0:4","tags":["scRNA-seq","Scanpy","Seurat"],"title":"Convert Seurat Robj to Scanpy h5ad","uri":"/2020-04-28-seurat2scanpy/"},{"categories":["Make bioinfo uncool again"],"content":"GATK is design for human genetics, it works not so good on homogeneous inbred mouse. But we could use bcftools instead. One of my colleague who studies mouse genetics, said, I tried the haplotype caller from GATK. But it seems that the haplotype caller is designed for heterogeneous genome like human than for mice. Therefore, the result coming out of HC is worse than samtools, as I manually inspected a few regions that HC calls didn’t make sense. In addition, in one of their mouse genomic paper that we reviewed, they even skipped the second recalibration step. We asked them why and they said it was because of the same reason: good for human but not that good for the homogeneous inbred mouse. But we still could collect the resource bundle for mouse. I found a workflow here. However, the script is out of date. Also, see discussion here For GATK4, we have ","date":"2020-03-10","objectID":"/2020-03-10-gatk4-mm10-bundle/:0:0","tags":["GATK","Genetics"],"title":"GATK for inbred mouse is not good","uri":"/2020-03-10-gatk4-mm10-bundle/"},{"categories":["Make bioinfo uncool again"],"content":"1. Genome Download from NCBI (mm10) or Sanger Mouse Genetics Programme # NCBI wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/635/GCF_000001635.26_GRCm38.p6/GCF_000001635.26_GRCm38.p6_genomic.fna.gz -O GRCm38_68.fa.gz # or Sanger MGP wget ftp://ftp-mouse.sanger.ac.uk/ref/GRCm38_68.fa -O GRCm38_68.fa ","date":"2020-03-10","objectID":"/2020-03-10-gatk4-mm10-bundle/:0:1","tags":["GATK","Genetics"],"title":"GATK for inbred mouse is not good","uri":"/2020-03-10-gatk4-mm10-bundle/"},{"categories":["Make bioinfo uncool again"],"content":"2. dbSNP Depends on your study design. Download All in one vcf file from NCBI wget ftp://ftp.ncbi.nih.gov/snp/organisms/archive/mouse_10090/VCF/00-All.vcf.gz \\ -O mouse.dbsnp.vcf.gz Download from the Sanger Mouse Genetics Programme (Sanger MGP) wget ftp://ftp-mouse.sanger.ac.uk/REL-1505-SNPs_Indels/mgp.v5.merged.snps_all.dbSNP142.vcf.gz ","date":"2020-03-10","objectID":"/2020-03-10-gatk4-mm10-bundle/:0:2","tags":["GATK","Genetics"],"title":"GATK for inbred mouse is not good","uri":"/2020-03-10-gatk4-mm10-bundle/"},{"categories":["Make bioinfo uncool again"],"content":"3. Known Indels For mouse indels, the Sanger Mouse Genetics Programme (Sanger MGP) is probably the best resource. Download all MGP indels (5/2015 release): wget ftp://ftp-mouse.sanger.ac.uk/REL-1505-SNPs_Indels/mgp.v5.merged.indels.dbSNP142.normed.vcf.gz \\ -O mgp.v5.indels.vcf.gz Filter for passing variants # take header first zcat mgp.v5.indels.vcf.gz | head -1000 | grep \"^#\" | cut -f 1-8 \\ \u003e mgp.v5.indels.pass.chr.vcf # keep only passing and append zcat mgp.v5.indels.vcf.gz | grep -v \"^#\" | cut -f 1-8 \\ | grep -w \"PASS\" \u003e\u003e mgp.v5.indels.pass.chr.vcf Sort VCF (automatically generated index has to be deleted due to a known bug -\u003e No anymore): gatk SortVcf -SD GRCm38_68.dict -I mgp.v5.indels.pass.chr.vcf -O mgp.v5.indels.pass.chr.sort.vcf # rm .idx # rm mgp.v5.indels.pass.chr.sort.vcf.idx ","date":"2020-03-10","objectID":"/2020-03-10-gatk4-mm10-bundle/:0:3","tags":["GATK","Genetics"],"title":"GATK for inbred mouse is not good","uri":"/2020-03-10-gatk4-mm10-bundle/"},{"categories":["Coding"],"content":"Get answers for C/C++ within ? s","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes2/","tags":["C++"],"title":"C++ Notes: Advanced 1","uri":"/2020-02-10-code-cpp-notes2/"},{"categories":["Coding"],"content":"Just some advanced C/C++ code snippets to keep in mind. ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes2/:0:0","tags":["C++"],"title":"C++ Notes: Advanced 1","uri":"/2020-02-10-code-cpp-notes2/"},{"categories":["Coding"],"content":"default and delete in class special class member: default constructor deconstructor copy constructor operater = when use default and deleted default class X { public: X()=default; // with this, you could declare like this: X x; X(int){}; }; delete: prohibit func call marked by delete class X { public: X(); X(const X\u0026) = delete; X\u0026 operator = (const X \u0026) = delete; }; // example X x1; X x2=x1; // Error, copy constructor is prohibited ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes2/:1:0","tags":["C++"],"title":"C++ Notes: Advanced 1","uri":"/2020-02-10-code-cpp-notes2/"},{"categories":["Coding"],"content":"Pointer and Smart Pointer cast #include \u003cmemory\u003e // smart pointer header // [pomter to smart pointer struct Base {}; struct Derived: Base {}; // pointer to smart Base *p1 = new Derived(); // upcast std::shared_ptr\u003cBase\u003e sp(p1); // a polymorphic type Base *p = new Derived(); // upcast, dynamic_cast is unnecessary Derived* dp = dynamic_cast\u003cDerived*\u003e (p); // downcast // smart pointer convert to pointer std::shared_ptr\u003cBase\u003e smart = std::make_shared\u003cDerived\u003e(); Base* p2 = smart.get(); // .get() // smart pointer cast // downcast std::shared_ptr\u003cDerived\u003e dsmart = std::dynamic_pointer_cast\u003cDerived\u003e(smart); // upcast // case 1 std::shared_ptr\u003cBase\u003e foo(new Derived()); // case 2 std::shared_ptr\u003cDerived\u003e bar = std::make_shared\u003cBase\u003e(); std::shared_ptr\u003cBase\u003e foo = std::dynamic_pointer_cast\u003cA\u003e(bar); ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes2/:2:0","tags":["C++"],"title":"C++ Notes: Advanced 1","uri":"/2020-02-10-code-cpp-notes2/"},{"categories":["Coding"],"content":"FileIO The simplest example #include \u003ciostream\u003e#include \u003cfstream\u003e // output file std::ofstream output; output.open(\"test.compact.txt\"); // read input file string line; std::ifstream input(\"test.chrX.vcf\"); if (input.is_open()) { while (getline(input, line)) output \u003c\u003c line \u003c\u003c'\\n'; } input.close(); output.close(); ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes2/:3:0","tags":["C++"],"title":"C++ Notes: Advanced 1","uri":"/2020-02-10-code-cpp-notes2/"},{"categories":["Coding"],"content":"strings: split and strip split string by delimiter std::vector\u003cstd::string\u003e split(const std::string\u0026 s, char delimiter) { std::vector\u003cstd::string\u003e tokens; std::string token; std::istringstream tokenStream(s); while (std::getline(tokenStream, token, delimiter)) { tokens.push_back(token); } return tokens; } strip strings std::string trim(const std::string\u0026 str, const std::string delimiter = \" \\n\\r\\t\") { // std::string s; // s.erase(s.find_last_not_of(\" \\n\\r\\t\")+1); size_t first = str.find_first_not_of(delimiter); if (std::string::npos == first) { return str; } size_t last = str.find_last_not_of(delimiter); return str.substr(first, (last - first + 1)); } ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes2/:4:0","tags":["C++"],"title":"C++ Notes: Advanced 1","uri":"/2020-02-10-code-cpp-notes2/"},{"categories":["Coding"],"content":"Design Pattern ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes2/:5:0","tags":["C++"],"title":"C++ Notes: Advanced 1","uri":"/2020-02-10-code-cpp-notes2/"},{"categories":["Coding"],"content":"Singleton Define class Singleton { private: /* Here will be the instance stored. */ static Singleton* instance; /* Private constructor to prevent instancing. */ Singleton() {}; public: /* Static access method. */ static Singleton* getInstance() { if (instance == 0) instance = new Singleton(); return instance; } }; /* NULL, because instance will be initialized on demand. */ Singleton* Singleton::instance = 0; Usage #include \u003ciostream\u003eint main() { //new Singleton(); // Won't work Singleton* s = Singleton::getInstance(); // Ok Singleton* r = Singleton::getInstance(); /* The addresses will be the same. */ std::cout \u003c\u003c s \u003c\u003c std::endl; std::cout \u003c\u003c r \u003c\u003c std::endl; } ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes2/:5:1","tags":["C++"],"title":"C++ Notes: Advanced 1","uri":"/2020-02-10-code-cpp-notes2/"},{"categories":["Coding"],"content":"Delegate Don’t confuse with delegate constructor!!! A delegate is a class that wraps a pointer or reference to an object instance, a member method of that object’s class to be called on that object instance, and provides a method to trigger that call. Example 1 #include \u003ciostream\u003eusing namespace std; class RealPrinter { public: void print() { std::cout \u003c\u003c \"real-printer\" \u003c\u003c std::endl; } }; class Printer { public: Printer() : p(RealPrinter()) {} void print() { p.print(); } private: RealPrinter p; }; int main() { Printer* printer = new Printer(); printer-\u003eprint(); } Example 2: #include \u003ciostream\u003eclass I //interface { public: virtual void f() = 0; virtual void g() = 0; }; class A : public I { public: void f(){std::cout \u003c\u003c \"A::f()\" \u003c\u003c std::endl;} void g(){std::cout \u003c\u003c \"A::g()\" \u003c\u003c std::endl;} }; class B : public I { public: void f(){std::cout \u003c\u003c \"B::f()\" \u003c\u003c std::endl;} void g(){std::cout \u003c\u003c \"B::g()\" \u003c\u003c std::endl;} }; class C : public I { public: C() { m_i = new A();/*delegation*/ } void f(){ m_i-\u003ef(); } void g(){ m_i-\u003eg(); } // normal attributes void toA(){ m_i = new A(); } void toB(){ m_i = new B(); } private: I* m_i; } int main() { C cc = C(); cc.f(); // output: A::f() cc.g(); // output: A::g() cc.toB(); cc.f(); // output: B::f() cc.g(); // output: B::g() } ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes2/:5:2","tags":["C++"],"title":"C++ Notes: Advanced 1","uri":"/2020-02-10-code-cpp-notes2/"},{"categories":["Coding"],"content":"Composite Composite is a structural design pattern that allows composing objects into a tree-like structure and work with the it as if it was a singular object. ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes2/:5:3","tags":["C++"],"title":"C++ Notes: Advanced 1","uri":"/2020-02-10-code-cpp-notes2/"},{"categories":["Coding"],"content":"Get answers for C/C++ within ? s","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"C++ Notes Just some C/C++ code snippets to keep in mind. C/C++ is tremendous complicated, but it’s still the most powerful programming language. ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:0:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"Table of Contents char to Int Pointer Return Reference/Pointer Array as Argument Operator that can’t be overloaded Object Instantization Object Relationship Virtual Function and Ploymorphism Friend Const Constexpr Extern and static ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:1:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"Char to Int ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:2:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"1. Char is ASCII C store Char as ASCII (Int) by default. So, Char is equal to ASCII code. char a = 'A'; // 65 int c = a; //c = 65 ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:2:1","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"2. Char to Int, to String, and vice versa char and int char c='5' int res = c -'0' ; // 5 int i=5; char res = I + '0'; // '5' char*,int and string // char * to string const char * str_c = \"hello\"; std::string str = str_c; // string to char* str.c_str(); // return const char* // int to string std::to_string() // string to int std::stoi() ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:2:2","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"3. Char as subscript of array: legal! This is useful when create hashmap. e.g. counting chars int test[200] = {0}; test['A'] = 1; // legal test['b'] = 2; // legal Back to top ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:2:3","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"Operator Could not be Overloaded operator memo . Member access or dot operator .* Pointer-to-member Operator :: Scope Resolution operator ? : conditional operator, ternary operator sizeof object size operator, built-in operations typeid object type operator, built-in operations OK, What’s .* ? //we have a class struct X { void f() {} void g() {} }; typedef void (X::*pointer)(); //ok, let's take a pointer and assign f to it. pointer somePointer = \u0026X::f; //now I want to call somePointer. But for that, I need an object X x; //now I call the member function on x like this (x.*somePointer)(); //will call x.f() //now, suppose x is not an object but a pointer to object X* px = new X; //I want to call the memfun pointer on px. I use -\u003e* (px -\u003e* somePointer)(); //will call px-\u003ef(); Now, you can’t use x.somePointer(), or px-\u003esomePointer() because there is no such member in class X. see here Back to top ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:3:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"Pointer Pointer syntax Rule: read from right to left int a; // an int int *a; // a pointer point to int int **a; // secondary int pointer, point to another int pointer int a[10]; // int array int *a[10]; // a poiter array, point to int int (*a)[10]; // a int pointer point to an int array int (*a)(int); // a pointer point to a function, will return an int int (*a[10])(int); // a poiter array, point to a function，will return an int Declare two pointers int* a, b; // equal to int* a; int b; int *a, *b; // correct way Back to top ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:4:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"Return Reference Return Reference when define function return ref/pointer only if array or given object’s ref/pointer. return value only if temp object created // ref RMB\u0026 RMB::operator++() { yuan++; return *this; // already existed object, created outside } // value RMB RMB::operator++(int) { RMB temp(yuan);//create object yuan++; return temp;//return a copy of temp } Back to top ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:5:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"Array ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:6:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"Array as formal arguments An Array could not copy to anther Array (Copy pointer is not allowed!), so call-by-value is not allowed. So, use array pointer: //these are same void print(const int*); void print(const int[]); void print(const int[5]); multi-dimension array void print(const int(*p)[3], int rowsize); void print(const int p[][5], int rowsize); When use pointer to an Array, the dimension is unknown. So, need an extra argument to specify it explicitly. Example: void print1(int (*p)[3]) { cout\u003c\u003cp[1][1]\u003c\u003cendl; } void print2(int p[][3]) { cout\u003c\u003cp[0][0]\u003c\u003cendl; } int a[2][3]={ {1,2},{3,4} }; print1(a); // 4 print2(a); // 1 int b[2][4]={ {1,2,5,6},{3,4,7,8} }; print1(b); // error Back to top ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:6:1","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"Object Instantization ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:7:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"1. without new stack ClassName object(param); // A a(1); ClassName object2 = ClassName(param); // A b = A(1); ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:7:1","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"2. with new heap ClassName *object = new ClassName(param);//A *a = new A(); delete object; ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:7:2","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"3. copy constructor // ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:7:3","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"4. Smart Pointer std::unique_ptr\u003cClassName\u003e object (new ClassName(param)); // recommend this way of instantization std::unique_ptr\u003cClassName\u003e object = std::make_unique\u003cClassName\u003e(param); Back to top ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:7:4","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"Friend The friend declaration appears in a class body and grants a function or another class access to private and protected members of the class where the friend declaration appears. ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:8:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"1. friend function Declare anywhere inside a class, but define outside // function friend \u003ctype\u003e \u003cName\u003e(\u003carguments\u003e); Example: class A { public: A(int _a):a(_a){}; // non-member function friend int getA_a(A \u0026_classA); private: int a; }; // without the friend keyword int getA_a(A \u0026_classA) { //access member by formal arguments return _classA.a; } A _classA(3); std::cout\u003c\u003cgetA_a(_classA); // 3 Back to top ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:8:1","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"2. friend class Delare inside class, define outside // class friend class \u003cName\u003e; Note: friend class X {}; is an error Example: class B { public: B(int _b):b(_b){}; friend class C; // friend class private: int b; }; class C { public: int getB_b(B _classB){ //access member by formal arguments return _classB.b; }; }; B _classB(3); C _classC; // an instance of a friend class _classC.getB_b(_classB); Back to top ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:8:2","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"3. Others: friend ostream, friend template … class Y { int data; // the non-member function operator\u003c\u003c will have access to Y's private members friend std::ostream\u0026 operator\u003c\u003c(std::ostream\u0026 out, const Y\u0026 o); friend char* X::foo(int); // members of other classes can be friends too friend X::X(char), X::~X(); // constructors and destructors can be friends }; // this operator\u003c\u003c still needs to be defined, as a non-member std::ostream\u0026 operator\u003c\u003c(std::ostream\u0026 out, const Y\u0026 y) { // can access private member Y::data return out \u003c\u003c y.data; } Back to top ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:8:3","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"Object Relationships relation types “is-a” “has-a” “uses-a” “depends-on” Property Composition Aggregation Association Relationship type Whole/part Whole/part Otherwise unrelated Members can belong to multiple classes No Yes Yes Members existence managed by class Yes No No Directionality Unidirectional Unidirectional Unidirectional or bidirectional Relationship verb Part-of Has-a Uses-a ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:9:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"Composition: has a data member Building complex objects from simpler ones is called object composition . object composition models a “has-a” relationship between two objects. In C++, It means structs and classes can have data members of various types. class A； class B { public: B(){} ~B(){} private: A a; int b; }； Summary: Typically use normal member variables Can use pointer members if the class handles object allocation/deallocation itself Responsible for creation/destruction of parts ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:9:1","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"Aggregation: “has a” Unlike a composition, parts can belong to more than one object at a time, and the whole object is not responsible for the existence and lifespan of the parts. Summary: Typically use pointer or reference members that point to or reference objects that live outside the scope of the aggregate class Not responsible for creating/destroying parts ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:9:2","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"Association: “uses a” Association models as “uses-a” relationship. The doctor “uses” the patient (to earn income). The patient uses the doctor (for whatever health purposes they need). ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:9:3","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"Delegate: “has a” or called pImpl(Pointer to IMPLementation) Delegate: Composition by reference has a pointer of another object class A； class B { public: B(){} ~B(){} private: A *a; int b; }； ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:9:4","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"Inheritance: “is a” public, protected, private class A { public: A(){} virtual ~A(){} } class B : public A { }; Back to top ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:9:5","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"Virtual Functions and Runtime Polymorphism Declare: vitrual keyword class TestA { public: virtual void func() { cout \u003c\u003c \"virtual function\" \u003c\u003c endl; } }; class Test : public TestA { public: virtual void func() { // virtual could be omited cout \u003c\u003c \"Test virtual function\" \u003c\u003c endl; } ~Test() { } }; TestA* t = new Test; // parent pointer point to child (Ploymorphism) t-\u003efunc(); // Results：Test virtual function delete t; Member Could not be virtual inline function constructor non-member function static function: only one copy of all objects. friend function: it’s non-member function member function template ! Pure virtual function declare virtual void fun() = 0; class with pure virtual functoin could not be instantized! a derived class of virtual class has to define pure virtual function. then the derived class could be instantized. abstract class: class with pure virtual function virtual deconstrutor A parent pointer point to it’s child. When delete the parent pointer, only parent constuctor is called. if declared a virtual deconstuctor, child’s deconstuctor is called first, then the parent deconsturctor. virtural keyword could be omited if a parent deconstructor is declared. delete a pointer will only called object’s deconstructor where the pointer point to. Back to top ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:10:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"Const ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:11:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"1. const before or behind type/class, the syntax semantic are same // they are same const int x; // (int x) is const/inmutable int const x; // (const x) has type int ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:11:1","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"2. Pointer with const: const int* p, int const* p and int *const p Dirty trick: use * as a separator, const restrict the type according to the side where it belong to point to const: These two expression are same // -\u003e (const int) | p; p : a mutable pointer points to a const/immutable int const int * p; // -\u003e (int const) | p; p2: a mutable pointer points a const which has type int int const * p2; const pointer: But these two not the same // -\u003e int | (const p); p3: a const pointer, point to an mutable int int * const p3; // -\u003e (const int) | const p; p4: a const pointer, pointing to an immutable/const int const int * const p4; ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:11:2","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"3. class member func with const: () const a. const object could not change class variable could not call non-const function class Number { public: void set(int num) { number = num; } int get() { return number; } int get2() const {return number;} int number = 0; }; // Example const Number n; n.number = 1; // Error, n is const n.set(1); // Error, n is const, non-const `set()` n.get(); // Error, non-const `get()` n.get2(); // OK b. () const could not change class variable, except static could get variable class Number { private: int a; static int b; const int c = 20; public: void set() { a = 10; // error when `this` argument has type 'const' void set2() const { b = 20; // OK } int get() const { // OK return a; // did not change a } }; const Number n; n.set(); // Error n.set2(); // OK n.get(); // OK Easy to understand, when pointer this is const void Number::set(const Number *const this, int num) { number = num; } // illegal -\u003e const this c. () const overloading Back to top ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:11:3","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"constexpr The constexpr specifier declares that it is possible to evaluate the value of the function or variable at compile time. Back to top ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:12:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Coding"],"content":"extern, static global variable defined outside all functions and available to all functions. unaffected by scopes and are always available (exists until the program ends) extern declare a global variable (exists on the whole project): variable could be used in multi- .cpp files extern \"C\" {/* c code */}: compile c code. static declare a local global variable (file scope): only be accessed in its translation unit or .o file, that’s, in the file where it is created. declare a static class member (class scope): initialization should be outside class body static data member static function: no this pointer: only access to other static member/function could declare as private Back to top ","date":"2020-02-10","objectID":"/2020-02-10-code-cpp-notes/:13:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-code-cpp-notes/"},{"categories":["Machine Learning"],"content":"A recipe for interactive computing using custom Jupyter kernels on Stanford's Sherlock.","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"A recipe for interactive computing using custom Jupyter kernels on Stanford’s Sherlock. ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:0:0","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"Setting up custom conda environment on Sherlock’s login node ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:0","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"1. Download and install Miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh # install bash Miniconda3-latest-Linux-x86_64.sh conda config --set always_yes yes ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:1","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"2. Install jupyter notebook/lab and secure your notebooks with a password # install the default py3 kernel for jupyter notebook conda install ipython jupyter notebook jupyterlab # add password jupyter notebook password ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:2","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"3. (Optional) Add custom conda environment. i.e. fastai conda create -n fastai ipython ipykernel # add the custom to Jupyter notebook conda activate fastai python -m ipykernel install --user --name fastai --display-name FastAI you could also add R, Julia etc kernel. ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:3","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"4. Install pytorch/tensorflow You should select the existed cuda version which installed in Sherlock conda install -c pytorch pytorch torchvision cudatoolkit=10.1 tensorflow conda install tensorflow-gpu cudatoolkit=10.1 ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:4","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"5. Load gpu modules. Select the corresponding cuda version you’ve just installed # this is my version module load cuda/10.1.168 module load cudnn/7.6.4 module load nccl ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:5","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"6. now, open ipython, run import torch print(torch.cuda.is_avilable()) if print out is True, then you’er OK to use GPUs. Follow these steps on your local machine see details here. ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:6","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"7. Download the forward repo git clone https://github.com/vsoch/forward cd forward ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:7","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"8. Generate your parameters bash setup.sh Select Sherlock partition: gpu ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:8","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"9. SSH Credentials bash hosts/sherlock_ssh.sh \u003e\u003e ~/.ssh/config ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:9","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"10. create a sbatch script in forward/sbatches/sherlock and save as jupyter-gpu.sbatch #!/bin/bash PORT=$1 NOTEBOOK_DIR=$2 if [ -z \"$NOTEBOOK_DIR\" ]; then cd $SCRATCH else cd $NOTEBOOK_DIR fi ## to compile libtorch C++ code, load these modules # module load gcc/7.3.0 # module load gdb # module load cmake # export CC=$(which gcc) # export CXX=$(which g++) # select cuda version you need module load cuda/10.1.168 module load cudnn/7.6.4 module load nccl # activate fastai env source activate fastai jupyter lab --no-browser --port=$PORT ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:10","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"11. Start a session The default working directory is $SCRATCH bash start.sh jupyter-gpu change the working directory bash start.sh jupyter /path/to/dir ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:11","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"12. open your browser in local machine and type if your port is 51888, then http://localhost:51888/ here is my jupyter lab computing environment. Have fun! fastai kernel Test GPUs ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:12","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"13. Resume a session bash resume.sh jupyter-gpu # or bash resume.sh jupyter-gpu /path/to/dir ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:13","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"14. Stop a session bash end.sh jupyter-gpu # or bash end.sh jupyter-gpu /path/to/dir ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:14","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Algorithm and data structure"],"content":"Graphs ","date":"2020-02-09","objectID":"/2020-02-09-cs-graph/:0:0","tags":["C++"],"title":"Graph","uri":"/2020-02-09-cs-graph/"},{"categories":["Algorithm and data structure"],"content":"Some data structures to keep in my mind. BinaryHeap: Complete binary tree MaxHeap: Parent \u003e Both Children IndexMaxHeap MinHeap: Parent \u003c Both Children IndexMinHeap Priority queue (MaxHeap) BinarySearchTree Not always complete binary tree Value: leftChild \u003c Parent \u003c rightChild DenseGraph SparseGraph Code snippets take from Play with Algorithm ","date":"2020-02-09","objectID":"/2020-02-09-cs-graph/:0:1","tags":["C++"],"title":"Graph","uri":"/2020-02-09-cs-graph/"},{"categories":["Algorithm and data structure"],"content":"1. Dense Graph #include \u003ciostream\u003e#include \u003cvector\u003e#include \u003ccassert\u003e using namespace std; // 稠密图 - 邻接矩阵 class DenseGraph{ private: int n, m; bool directed; vector\u003cvector\u003cbool\u003e\u003e g; public: DenseGraph( int n , bool directed ){ this-\u003en = n; this-\u003em = 0; this-\u003edirected = directed; for( int i = 0 ; i \u003c n ; i ++ ) g.push_back( vector\u003cbool\u003e(n, false) ); } ~DenseGraph(){ } int V(){ return n;} int E(){ return m;} void addEdge( int v , int w ){ assert( v \u003e= 0 \u0026\u0026 v \u003c n ); assert( w \u003e= 0 \u0026\u0026 w \u003c n ); if( hasEdge( v , w ) ) return; g[v][w] = true; if( !directed ) g[w][v] = true; m ++; } bool hasEdge( int v , int w ){ assert( v \u003e= 0 \u0026\u0026 v \u003c n ); assert( w \u003e= 0 \u0026\u0026 w \u003c n ); return g[v][w]; } void show(){ for( int i = 0 ; i \u003c n ; i ++ ){ for( int j = 0 ; j \u003c n ; j ++ ) cout\u003c\u003cg[i][j]\u003c\u003c\"\\t\"; cout\u003c\u003cendl; } } class adjIterator{ private: DenseGraph \u0026G; int v; int index; public: adjIterator(DenseGraph \u0026graph, int v): G(graph){ this-\u003ev = v; this-\u003eindex = -1; } int begin(){ index = -1; return next(); } int next(){ for( index += 1 ; index \u003c G.V() ; index ++ ) if( G.g[v][index] ) return index; return -1; } bool end(){ return index \u003e= G.V(); } }; }; ","date":"2020-02-09","objectID":"/2020-02-09-cs-graph/:0:2","tags":["C++"],"title":"Graph","uri":"/2020-02-09-cs-graph/"},{"categories":["Algorithm and data structure"],"content":"2. Sparse Graph #include \u003ciostream\u003e#include \u003cvector\u003e#include \u003ccassert\u003e using namespace std; // 稀疏图 - 邻接表 class SparseGraph{ private: int n, m; bool directed; vector\u003cvector\u003cint\u003e\u003e g; public: SparseGraph( int n , bool directed ){ this-\u003en = n; this-\u003em = 0; this-\u003edirected = directed; for( int i = 0 ; i \u003c n ; i ++ ) g.push_back( vector\u003cint\u003e() ); } ~SparseGraph(){ } int V(){ return n;} int E(){ return m;} void addEdge( int v, int w ){ assert( v \u003e= 0 \u0026\u0026 v \u003c n ); assert( w \u003e= 0 \u0026\u0026 w \u003c n ); g[v].push_back(w); if( v != w \u0026\u0026 !directed ) g[w].push_back(v); m ++; } bool hasEdge( int v , int w ){ assert( v \u003e= 0 \u0026\u0026 v \u003c n ); assert( w \u003e= 0 \u0026\u0026 w \u003c n ); for( int i = 0 ; i \u003c g[v].size() ; i ++ ) if( g[v][i] == w ) return true; return false; } void show(){ for( int i = 0 ; i \u003c n ; i ++ ){ cout\u003c\u003c\"vertex \"\u003c\u003ci\u003c\u003c\":\\t\"; for( int j = 0 ; j \u003c g[i].size() ; j ++ ) cout\u003c\u003cg[i][j]\u003c\u003c\"\\t\"; cout\u003c\u003cendl; } } class adjIterator{ private: SparseGraph \u0026G; int v; int index; public: adjIterator(SparseGraph \u0026graph, int v): G(graph){ this-\u003ev = v; this-\u003eindex = 0; } int begin(){ index = 0; if( G.g[v].size() ) return G.g[v][index]; return -1; } int next(){ index ++; if( index \u003c G.g[v].size() ) return G.g[v][index]; return -1; } bool end(){ return index \u003e= G.g[v].size(); } }; }; ","date":"2020-02-09","objectID":"/2020-02-09-cs-graph/:0:3","tags":["C++"],"title":"Graph","uri":"/2020-02-09-cs-graph/"},{"categories":["Algorithm and data structure"],"content":"Binary trees Difference ","date":"2020-02-08","objectID":"/2020-02-08-cs-binarytree/:0:0","tags":["C++"],"title":"Binary tree","uri":"/2020-02-08-cs-binarytree/"},{"categories":["Algorithm and data structure"],"content":"Some data structures to keep in my mind. BinaryHeap: Complete binary tree MaxHeap: Parent \u003e Both Children IndexMaxHeap MinHeap: Parent \u003c Both Children IndexMinHeap Priority queue (MaxHeap) BinarySearchTree Not always complete binary tree Value: leftChild \u003c Parent \u003c rightChild DenseGraph SparseGraph Code snippets take from Play with Algorithm ","date":"2020-02-08","objectID":"/2020-02-08-cs-binarytree/:0:1","tags":["C++"],"title":"Binary tree","uri":"/2020-02-08-cs-binarytree/"},{"categories":["Algorithm and data structure"],"content":"1. MaxHeap #include \u003calgorithm\u003e#include \u003ccassert\u003e using namespace std; template\u003ctypename Item\u003e class MaxHeap{ private: Item *data; int count; int capacity; void shiftUp(int k){ while( k \u003e 1 \u0026\u0026 data[k/2] \u003c data[k] ){ swap( data[k/2], data[k] ); k /= 2; } } void shiftDown(int k){ while( 2*k \u003c= count ){ int j = 2*k; // which child is larger, then swap if( j+1 \u003c= count \u0026\u0026 data[j+1] \u003e data[j] ) j ++; if( data[k] \u003e= data[j] ) break; swap( data[k] , data[j] ); k = j; } } public: MaxHeap(int capacity){ data = new Item[capacity+1]; count = 0; this-\u003ecapacity = capacity; } MaxHeap(Item arr[], int n){ data = new Item[n+1]; capacity = n; for( int i = 0 ; i \u003c n ; i ++ ) data[i+1] = arr[i]; count = n; for( int i = count/2 ; i \u003e= 1 ; i -- ) shiftDown(i); } ~MaxHeap(){ delete[] data; } int size(){ return count; } bool isEmpty(){ return count == 0; } void insert(Item item){ assert( count + 1 \u003c= capacity ); data[count+1] = item; shiftUp(count+1); count ++; } Item extractMax(){ assert( count \u003e 0 ); Item ret = data[1]; swap( data[1] , data[count] ); count --; shiftDown(1); return ret; } Item getMax(){ assert( count \u003e 0 ); return data[1]; } }; ","date":"2020-02-08","objectID":"/2020-02-08-cs-binarytree/:0:2","tags":["C++"],"title":"Binary tree","uri":"/2020-02-08-cs-binarytree/"},{"categories":["Algorithm and data structure"],"content":"2. IndexMaxHeap Need 3 vector: data, indexes, reverse Code #include \u003calgorithm\u003e#include \u003ccassert\u003e using namespace std; template\u003ctypename Item\u003e class IndexMaxHeap{ private: Item *data; int *indexes; int *reverse; int count; int capacity; void shiftUp( int k ){ while( k \u003e 1 \u0026\u0026 data[indexes[k/2]] \u003c data[indexes[k]] ){ swap( indexes[k/2] , indexes[k] ); reverse[indexes[k/2]] = k/2; reverse[indexes[k]] = k; k /= 2; } } void shiftDown( int k ){ while( 2*k \u003c= count ){ int j = 2*k; if( j + 1 \u003c= count \u0026\u0026 data[indexes[j+1]] \u003e data[indexes[j]] ) j += 1; if( data[indexes[k]] \u003e= data[indexes[j]] ) break; swap( indexes[k] , indexes[j] ); reverse[indexes[k]] = k; reverse[indexes[j]] = j; k = j; } } public: IndexMaxHeap(int capacity){ data = new Item[capacity+1]; indexes = new int[capacity+1]; reverse = new int[capacity+1]; for( int i = 0 ; i \u003c= capacity ; i ++ ) reverse[i] = 0; count = 0; this-\u003ecapacity = capacity; } ~IndexMaxHeap(){ delete[] data; delete[] indexes; delete[] reverse; } int size(){ return count; } bool isEmpty(){ return count == 0; } // 传入的i对用户而言,是从0索引的 void insert(int i, Item item){ assert( count + 1 \u003c= capacity ); assert( i + 1 \u003e= 1 \u0026\u0026 i + 1 \u003c= capacity ); i += 1; data[i] = item; indexes[count+1] = i; reverse[i] = count+1; count++; shiftUp(count); } Item extractMax(){ assert( count \u003e 0 ); Item ret = data[indexes[1]]; swap( indexes[1] , indexes[count] ); reverse[indexes[count]] = 0; reverse[indexes[1]] = 1; count--; shiftDown(1); return ret; } int extractMaxIndex(){ assert( count \u003e 0 ); int ret = indexes[1] - 1; swap( indexes[1] , indexes[count] ); reverse[indexes[count]] = 0; reverse[indexes[1]] = 1; count--; shiftDown(1); return ret; } Item getMax(){ assert( count \u003e 0 ); return data[indexes[1]]; } int getMaxIndex(){ assert( count \u003e 0 ); return indexes[1]-1; } bool contain( int i ){ assert( i + 1 \u003e= 1 \u0026\u0026 i + 1 \u003c= capacity ); return reverse[i+1] != 0; } Item getItem( int i ){ assert( contain(i) ); return data[i+1]; } void change( int i , Item newItem ){ assert( contain(i) ); i += 1; data[i] = newItem; // 找到indexes[j] = i, j表示data[i]在堆中的位置 // 之后shiftUp(j), 再shiftDown(j) // for( int j = 1 ; j \u003c= count ; j ++ ) // if( indexes[j] == i ){ // shiftUp(j); // shiftDown(j); // return; // } int j = reverse[i]; shiftUp( j ); shiftDown( j ); } // test reverse index bool testReverseIndex(){ int *copyIndexes = new int[count+1]; int *copyReverseIndexes = new int[count+1]; for( int i = 0 ; i \u003c= count ; i ++ ){ copyIndexes[i] = indexes[i]; copyReverseIndexes[i] = reverse[i]; } copyIndexes[0] = copyReverseIndexes[0] = 0; std::sort(copyIndexes, copyIndexes + count + 1); std::sort(copyReverseIndexes, copyReverseIndexes + count + 1); bool res = true; for( int i = 1 ; i \u003c= count ; i ++ ) if( copyIndexes[i-1] + 1 != copyIndexes[i] || copyReverseIndexes[i-1] + 1 != copyReverseIndexes[i] ) res = res || false; delete[] copyIndexes; delete[] copyReverseIndexes; if( !res ){ cout\u003c\u003c\"Error 1\"\u003c\u003cendl; return res; } for( int i = 1 ; i \u003c= count ; i ++ ) if( reverse[ indexes[i] ] != i ){ cout\u003c\u003c\"Error 2\"\u003c\u003cendl; return false; } return true; } }; ","date":"2020-02-08","objectID":"/2020-02-08-cs-binarytree/:0:3","tags":["C++"],"title":"Binary tree","uri":"/2020-02-08-cs-binarytree/"},{"categories":["Algorithm and data structure"],"content":"3. BinarySearchTree #include \u003ciostream\u003e#include \u003cqueue\u003e#include \u003ccassert\u003e using namespace std; template \u003ctypename Key, typename Value\u003e class BST{ private: struct Node{ Key key; Value value; Node *left; Node *right; Node(Key key, Value value){ this-\u003ekey = key; this-\u003evalue = value; this-\u003eleft = this-\u003eright = NULL; } Node(Node *node){ this-\u003ekey = node-\u003ekey; this-\u003evalue = node-\u003evalue; this-\u003eleft = node-\u003eleft; this-\u003eright = node-\u003eright; } }; Node *root; int count; public: BST(){ root = NULL; count = 0; } ~BST(){ destroy( root ); } int size(){ return count; } bool isEmpty(){ return count == 0; } void insert(Key key, Value value){ root = insert(root, key, value); } bool contain(Key key){ return contain(root, key); } Value* search(Key key){ return search( root , key ); } // 前序遍历 void preOrder(){ preOrder(root); } // 中序遍历 void inOrder(){ inOrder(root); } // 后序遍历 void postOrder(){ postOrder(root); } // 层序遍历 void levelOrder(){ queue\u003cNode*\u003e q; q.push(root); while( !q.empty() ){ Node *node = q.front(); q.pop(); cout\u003c\u003cnode-\u003ekey\u003c\u003cendl; if( node-\u003eleft ) q.push( node-\u003eleft ); if( node-\u003eright ) q.push( node-\u003eright ); } } // 寻找最小的键值 Key minimum(){ assert( count != 0 ); Node* minNode = minimum( root ); return minNode-\u003ekey; } // 寻找最大的键值 Key maximum(){ assert( count != 0 ); Node* maxNode = maximum(root); return maxNode-\u003ekey; } // 从二叉树中删除最小值所在节点 void removeMin(){ if( root ) root = removeMin( root ); } // 从二叉树中删除最大值所在节点 void removeMax(){ if( root ) root = removeMax( root ); } // 从二叉树中删除键值为key的节点 void remove(Key key){ root = remove(root, key); } private: // 向以node为根的二叉搜索树中,插入节点(key, value) // 返回插入新节点后的二叉搜索树的根 Node* insert(Node *node, Key key, Value value){ if( node == NULL ){ count ++; return new Node(key, value); } if( key == node-\u003ekey ) node-\u003evalue = value; else if( key \u003c node-\u003ekey ) node-\u003eleft = insert( node-\u003eleft , key, value); else // key \u003e node-\u003ekey node-\u003eright = insert( node-\u003eright, key, value); return node; } // 查看以node为根的二叉搜索树中是否包含键值为key的节点 bool contain(Node* node, Key key){ if( node == NULL ) return false; if( key == node-\u003ekey ) return true; else if( key \u003c node-\u003ekey ) return contain( node-\u003eleft , key ); else // key \u003e node-\u003ekey return contain( node-\u003eright , key ); } // 在以node为根的二叉搜索树中查找key所对应的value Value* search(Node* node, Key key){ if( node == NULL ) return NULL; if( key == node-\u003ekey ) return \u0026(node-\u003evalue); else if( key \u003c node-\u003ekey ) return search( node-\u003eleft , key ); else // key \u003e node-\u003ekey return search( node-\u003eright, key ); } // 对以node为根的二叉搜索树进行前序遍历 void preOrder(Node* node){ if( node != NULL ){ cout\u003c\u003cnode-\u003ekey\u003c\u003cendl; preOrder(node-\u003eleft); preOrder(node-\u003eright); } } // 对以node为根的二叉搜索树进行中序遍历 void inOrder(Node* node){ if( node != NULL ){ inOrder(node-\u003eleft); cout\u003c\u003cnode-\u003ekey\u003c\u003cendl; inOrder(node-\u003eright); } } // 对以node为根的二叉搜索树进行后序遍历 void postOrder(Node* node){ if( node != NULL ){ postOrder(node-\u003eleft); postOrder(node-\u003eright); cout\u003c\u003cnode-\u003ekey\u003c\u003cendl; } } void destroy(Node* node){ if( node != NULL ){ destroy( node-\u003eleft ); destroy( node-\u003eright ); delete node; count --; } } // 在以node为根的二叉搜索树中,返回最小键值的节点 Node* minimum(Node* node){ if( node-\u003eleft == NULL ) return node; return minimum(node-\u003eleft); } // 在以node为根的二叉搜索树中,返回最大键值的节点 Node* maximum(Node* node){ if( node-\u003eright == NULL ) return node; return maximum(node-\u003eright); } // 删除掉以node为根的二分搜索树中的最小节点 // 返回删除节点后新的二分搜索树的根 Node* removeMin(Node* node){ if( node-\u003eleft == NULL ){ Node* rightNode = node-\u003eright; delete node; count --; return rightNode; } node-\u003eleft = removeMin(node-\u003eleft); return node; } // 删除掉以node为根的二分搜索树中的最大节点 // 返回删除节点后新的二分搜索树的根 Node* removeMax(Node* node){ if( node-\u003eright == NULL ){ Node* leftNode = node-\u003eleft; delete node; count --; return leftNode; } node-\u003eright = removeMax(node-\u003eright); return node; } // 删除掉以node为根的二分搜索树中键值为key的节点 // 返回删除节点后新的二分搜索树的根 Node* remove(Node* node, Key key){ if( node == NULL ) return NULL; if( key \u003c node-\u003ekey ){ node-\u003eleft = remove( node-\u003eleft , key ); return node; } else if( key \u003e node-\u003ekey ){ node-\u003eright = remove( node-\u003eright, key ); return node; } else{ // ","date":"2020-02-08","objectID":"/2020-02-08-cs-binarytree/:0:4","tags":["C++"],"title":"Binary tree","uri":"/2020-02-08-cs-binarytree/"},{"categories":["Algorithm and data structure"],"content":"Sort algorithms Code snippets are taken from Play with Algorithm ","date":"2020-02-08","objectID":"/2020-02-08-cs-sort/:0:0","tags":["C++"],"title":"Sort","uri":"/2020-02-08-cs-sort/"},{"categories":["Algorithm and data structure"],"content":"Some algorithm to keep in my mind. selectionSort insertionSort mergeSort quickSort two way three way heapSort ","date":"2020-02-08","objectID":"/2020-02-08-cs-sort/:0:1","tags":["C++"],"title":"Sort","uri":"/2020-02-08-cs-sort/"},{"categories":["Algorithm and data structure"],"content":"1. insertionSort #include \u003ciostream\u003e#include \u003calgorithm\u003e using namespace std; template\u003ctypename T\u003e void insertionSort(T arr[], int n){ for( int i = 1 ; i \u003c n ; i ++ ) { T e = arr[i]; int j; for (j = i; j \u003e 0 \u0026\u0026 arr[j-1] \u003e e; j--) arr[j] = arr[j-1]; arr[j] = e; } return; } // 对arr[l...r]范围的数组进行插入排序 template\u003ctypename T\u003e void insertionSort(T arr[], int l, int r){ for( int i = l+1 ; i \u003c= r ; i ++ ) { T e = arr[i]; int j; for (j = i; j \u003e l \u0026\u0026 arr[j-1] \u003e e; j--) arr[j] = arr[j-1]; arr[j] = e; } return; } ","date":"2020-02-08","objectID":"/2020-02-08-cs-sort/:0:2","tags":["C++"],"title":"Sort","uri":"/2020-02-08-cs-sort/"},{"categories":["Algorithm and data structure"],"content":"2. mergeSort #include \u003ciostream\u003e#include \u003calgorithm\u003e#include \"InsertionSort.h\" using namespace std; template\u003ctypename T\u003e void __merge(T arr[], int l, int mid, int r){ T aux[r-l+1]; for( int i = l ; i \u003c= r; i ++ ) aux[i-l] = arr[i]; int i = l, j = mid+1; for( int k = l ; k \u003c= r; k ++ ){ if( i \u003e mid ) { arr[k] = aux[j-l]; j ++;} else if( j \u003e r ){ arr[k] = aux[i-l]; i ++;} else if( aux[i-l] \u003c aux[j-l] ){ arr[k] = aux[i-l]; i ++;} else { arr[k] = aux[j-l]; j ++;} } } template\u003ctypename T\u003e void __mergeSort(T arr[], int l, int r){ if( r - l \u003c= 15 ){ insertionSort(arr, l, r); return; } int mid = (l+r)/2; __mergeSort(arr, l, mid); __mergeSort(arr, mid+1, r); if( arr[mid] \u003e arr[mid+1] ) __merge(arr, l, mid, r); } template\u003ctypename T\u003e void mergeSort(T arr[], int n){ __mergeSort( arr , 0 , n-1 ); } template \u003ctypename T\u003e void mergeSortBU(T arr[], int n){ for( int i = 0 ; i \u003c n ; i += 16 ) insertionSort(arr,i,min(i+15,n-1)); for( int sz = 16; sz \u003c= n ; sz += sz ) for( int i = 0 ; i \u003c n - sz ; i += sz+sz ) if( arr[i+sz-1] \u003e arr[i+sz] ) __merge(arr, i, i+sz-1, min(i+sz+sz-1,n-1) ); } ","date":"2020-02-08","objectID":"/2020-02-08-cs-sort/:0:3","tags":["C++"],"title":"Sort","uri":"/2020-02-08-cs-sort/"},{"categories":["Algorithm and data structure"],"content":"3. qucikSort #include \u003ciostream\u003e#include \u003cctime\u003e#include \u003calgorithm\u003e#include \"InsertionSort.h\" using namespace std; template \u003ctypename T\u003e int _partition(T arr[], int l, int r){ // select a rand index in arr, and swap swap( arr[l] , arr[rand()%(r-l+1)+l] ); T v = arr[l]; int i = l+1, j = r; while( true ){ while( i \u003c= r \u0026\u0026 arr[i] \u003c v ) i ++; while( j \u003e= l+1 \u0026\u0026 arr[j] \u003e v ) j --; if( i \u003e j ) break; swap( arr[i] , arr[j] ); i ++; j --; } swap( arr[l] , arr[j]); return j; } template \u003ctypename T\u003e void _quickSort(T arr[], int l, int r){ if( r - l \u003c= 15 ){ insertionSort(arr,l,r); return; } int p = _partition(arr, l, r); _quickSort(arr, l, p-1 ); _quickSort(arr, p+1, r); } template \u003ctypename T\u003e void quickSort(T arr[], int n){ srand(time(NULL)); _quickSort(arr, 0, n-1); } template \u003ctypename T\u003e void __quickSort3Ways(T arr[], int l, int r){ if( r - l \u003c= 15 ){ insertionSort(arr,l,r); return; } swap( arr[l], arr[rand()%(r-l+1)+l ] ); T v = arr[l]; int lt = l; // arr[l+1...lt] \u003c v int gt = r + 1; // arr[gt...r] \u003e v int i = l+1; // arr[lt+1...i) == v while( i \u003c gt ){ if( arr[i] \u003c v ){ swap( arr[i], arr[lt+1]); i ++; lt ++; } else if( arr[i] \u003e v ){ swap( arr[i], arr[gt-1]); gt --; } else{ // arr[i] == v i ++; } } swap( arr[l] , arr[lt] ); __quickSort3Ways(arr, l, lt-1); __quickSort3Ways(arr, gt, r); } template \u003ctypename T\u003e void quickSort3Ways(T arr[], int n){ srand(time(NULL)); __quickSort3Ways( arr, 0, n-1); } ","date":"2020-02-08","objectID":"/2020-02-08-cs-sort/:0:4","tags":["C++"],"title":"Sort","uri":"/2020-02-08-cs-sort/"},{"categories":["Algorithm and data structure"],"content":"4. heapSort #include \"Heap.h\" using namespace std; template\u003ctypename T\u003e void heapSort2(T arr[], int n){ MaxHeap\u003cT\u003e maxheap = MaxHeap\u003cT\u003e(arr,n); for( int i = n-1 ; i \u003e= 0 ; i-- ) arr[i] = maxheap.extractMax(); } template\u003ctypename T\u003e void heapSort1(T arr[], int n){ MaxHeap\u003cT\u003e maxheap = MaxHeap\u003cT\u003e(n); for( int i = 0 ; i \u003c n ; i ++ ) maxheap.insert(arr[i]); for( int i = n-1 ; i \u003e= 0 ; i-- ) arr[i] = maxheap.extractMax(); } ","date":"2020-02-08","objectID":"/2020-02-08-cs-sort/:0:5","tags":["C++"],"title":"Sort","uri":"/2020-02-08-cs-sort/"},{"categories":["Algorithm and data structure"],"content":"What on earth is Big O? Time complexity and space complexity ","date":"2020-02-01","objectID":"/2020-02-01-cs-algo/:0:1","tags":["C++"],"title":"What is Big O","uri":"/2020-02-01-cs-algo/"},{"categories":["Algorithm and data structure"],"content":"Time complexity O(f(n)): number of commands need to execute. proportional to f(n). 表示运行算法所需要执行的指令数，和f(n)成正。 严格来讲，O(f(n))表示算法执行的上界。业界默认为算法执行的最低上界(最坏情况）。 n represents the data scale 数据规模 when n is a large number, the constant is usually ignored. algorithm n of cmd Binary reserach $O(logn)$ $a*logn$ Max/min in an array $O(n)$ b*n merge sort $O(nlogn)$ $c*nlogn$ select sort $O(n^2)$ $d*n^2$ quick sort $O(nlogn)$ e*nlogn adjacent graph $O(V+E)$ Lazy Prim $O(ElogE)$ Prim $O(ElogV)$ Kruskal $O(ElogE)$ Dijkstra $O(ElogV)$ Bellman-Ford $O(EV)$ minimum span tree Shortest path tree (Single source shortest path) ","date":"2020-02-01","objectID":"/2020-02-01-cs-algo/:0:2","tags":["C++"],"title":"What is Big O","uri":"/2020-02-01-cs-algo/"},{"categories":["Algorithm and data structure"],"content":"Space complexity cmd complexity new an array $O(n)$ new 2d array $O(n^2)$ new an constant space $O(1)$ recursive function: the depth (n) of a recursive function, the extra space need $O(n)$. ","date":"2020-02-01","objectID":"/2020-02-01-cs-algo/:0:3","tags":["C++"],"title":"What is Big O","uri":"/2020-02-01-cs-algo/"},{"categories":["Algorithm and data structure"],"content":"Make sense of n If you want to solve the problem in 1 second, then an algorithm of complexity cmds n $O(n^2)$ could exec cmd n = $10^4$ $O(n)$ could exec cmd n = $10^8$ $O(nlogn)$ could exec cmd n = $10^7$ ","date":"2020-02-01","objectID":"/2020-02-01-cs-algo/:0:4","tags":["C++"],"title":"What is Big O","uri":"/2020-02-01-cs-algo/"},{"categories":["Algorithm and data structure"],"content":"Example binarySearch find from n element find from $n/2$ element find from $n/4$ element ... find from 1 That’s, need how many steps of search when n = 1? $log_{2}n = O(logn)$. int2string. Set num \u003e 0 string int2string(int num) { string s=\"\"; while(num) { s += '0' + num%10; num /= 10; } reverse(s); // O(n) return s; } That is, how many “/10” steps when num = 0? $log_{10}n = O(logn)$. Case: two nested for loop, not always $O(n^2)$ void hello(int n){ for (int sz =1; sz \u003c n; sz ++ sz) // logn here for( int i=1; i \u003c n;; i++) //n cout\u003c\u003c\"hello, complex\" \u003c\u003cendl; } So, should be $O(nlogn)$ isPrime: $O(\\sqrt{n})$ // set n \u003e 1 bool isPrime(int n){ for( int x =2; x*x \u003c= n; x++){ if( n%x == 0) return false; return true; } } recursive function single recursive function call int binarySearch(int arr[], int l, int r, int target) { if (l\u003er) return -1; int mid = l +(r-l)/2; if (arr[mid] == target) return mid; else if (arr[mid] \u003e target) return binarySearch(arr, ;, mid-1, target); else return binarySearch(arr, mid+1, r, target); } each step need O(1), so overall complexity depend on recursive exec depth. That is, if each function call needs time T, then time complexity: O(T*depth) -\u003e O(n). Another example: recursion depth logn, them time complexity O(logn). double pow( double x, int n){ assert(n \u003e=0); if (n==0) return 1.0; double t = pow(x, n/2); if( n%2) return x*t*t; return t*t; } multi recursive exec how many exec step? int f(int n) { assert(n \u003e=0); if(n == 0) return 1; return f(n-1) + f(n); } that’s, count how many nodes on a full binary tree. $2^{n+1} -1 = O(2^n)$ how to think about this? void mergeSort(int arr[]. int l. int r){ if (l \u003e=r) return; int mid = (l+r) /2; mergeSort(arr, l, mid); mergeSort(arr, mid+1, r); merge(arr, l, mid, r); } For binary tree, complexity for each level O(n), while tree depth O(logn). Overall, O(nlogn) Amortized time i.e. dynamic vector/stack/deque template\u003cT\u003e class MyVector{ private: T* data; int capacity; int size; //O(n) void resize(int newCapacity){ assert(newCapacity \u003e= size); T* newData = new T[newCapacity]; for(int i = 0; i \u003c size; i++ ){ newData[i] = data[i]; } delete[] data; data = newData; capacity = newCapacity; } public: MyVector() { data = new T[10]; capacity = 10; size = 0; } ~MyVector() { delete[] data; } // Average: O(1) void push_back(T e){ //assert(size \u003c capacity) if (size == capacity) resize (2 *capacity); data[size++] = e; } // Average O(1) T pop_back(){ assert (size \u003e0); T ret = data[size-1]; size --; // note the denominator here. To advoid ossilation of space complexity if(size == capacity / 4) resize(capacity /2); return ret; } }; ","date":"2020-02-01","objectID":"/2020-02-01-cs-algo/:0:5","tags":["C++"],"title":"What is Big O","uri":"/2020-02-01-cs-algo/"},{"categories":["Machine Learning"],"content":"A breif review over the foundations of statistical inference ","date":"2020-01-30","objectID":"/2020-01-30-ml-bayes/:0:0","tags":["Naive Bayes","Bayesian Inference"],"title":"Statistical Modeling and Inference","uri":"/2020-01-30-ml-bayes/"},{"categories":["Machine Learning"],"content":"Statistical Models and Inference statistical inference: a formal approach to characterizing a random phenomenon using observations, either by providing a description of a past phenomenon or by giving some predictions about future phenomenon of similar nature. ","date":"2020-01-30","objectID":"/2020-01-30-ml-bayes/:1:0","tags":["Naive Bayes","Bayesian Inference"],"title":"Statistical Modeling and Inference","uri":"/2020-01-30-ml-bayes/"},{"categories":["Machine Learning"],"content":"1. Statistical Models The first step in statistical inference is to specify a statistical model, under some simplifying assumptions (i.e. independence assumptions). Hierarchical models: the probability distribution of one parameter is dependent on the values of other hierachical paramters (i.e. conditional independent). Steps: Set assumptions (i.e. independent), parameter and model. Make explicit assumptions on the probability distributions. focus on parametric modeling, because of limited data nonparametric not consider here, used for hypothesis testing or when sample size is very large. Once the model is specified, then choose a method of inference, as well as an algorithm to obtain estimates. Most commonly use: Maximum likelihood inference Bayesian inference ","date":"2020-01-30","objectID":"/2020-01-30-ml-bayes/:1:1","tags":["Naive Bayes","Bayesian Inference"],"title":"Statistical Modeling and Inference","uri":"/2020-01-30-ml-bayes/"},{"categories":["Machine Learning"],"content":"2. Maximum likelihood inference Quantifying confidence: the Fisher Information Matrix Newton’s algorithm Approximate Techniques Monte Carlo Sampling for intractable likelihoods Composite likelihood ","date":"2020-01-30","objectID":"/2020-01-30-ml-bayes/:1:2","tags":["Naive Bayes","Bayesian Inference"],"title":"Statistical Modeling and Inference","uri":"/2020-01-30-ml-bayes/"},{"categories":["Machine Learning"],"content":"3. Bayesian Inference A statistical model describes the uncertainty about how the data was produced. The ultimate aim for statistical inference is to obtain information about the unknown parameter $\\theta$ given the data $\\mathcal{D}$. Frequentist: $\\theta$ is fixed but unknown quantity. Bayesian: use a fully probabilistic model and treat $\\theta$ as a random quantity. To do so, chose an appropriate prior distribution $\\mathbb{P}(\\theta)$, which reflects the knowledge (i.e. uncertainty) about $\\theta$ prior to the experiment the goal is to update the knowledge given the information contained in the data $\\mathcal{D}$. the updated knowledge (i.e. reduced uncertainty) is encapsulated in the posterior distribution $\\mathbb{P}(\\theta \\vert \\mathcal{D})$, which is calculated via Bayes’theorem. $$ \\mathbb{P}(\\boldsymbol{\\theta} | \\mathcal{D})=\\frac{\\mathbb{P}(\\mathcal{D} | \\boldsymbol{\\theta}) \\mathbb{P}(\\boldsymbol{\\theta})}{\\mathbb{P}(\\mathcal{D})} $$ The bayesian paradigm boils down to the slogan: posterior $\\propto$ likelihood $\\times$ prior 3.1 Choice of prior distributions Conjugate priors the prior and the posterior lie in the same class of distributions. often chosen, because it leads to a well-known form of the posterior, which simplifies the calculations choose a prior that contains as little information about the parameter as possible at first choice would, of course, be a locally uniform prior. Under a uniform prior we have $\\mathbb{P}(\\boldsymbol{\\theta} \\vert \\boldsymbol{D}) \\propto \\mathcal{L}(\\boldsymbol{\\theta})$. Jeffrey’s prior, but often hard to come by 3.2 Bayesian point estimates and confidence intervals Bayesian point estimates: the posterior mean, mode and median $$ \\hat{\\theta}=\\mathbb{E}[\\theta | D]=\\int \\theta \\mathbb{P}(\\theta | \\mathcal{D}) \\mathrm{d} \\theta $$ confidence: highest posterior density (HPD) region for a threshold value $\\pi$, the region $\\mathcal{C}_{\\alpha}={\\theta: \\mathbb{P}(\\theta \\vert \\mathcal{D})\u003e\\pi}$, we get $$ \\int_{C_{\\alpha}} \\mathbb{P}(\\theta | \\mathcal{D}) \\mathrm{d} \\theta=1-\\alpha $$ This region $\\mathcal{C}_{\\alpha}$ is the HPD region. 3.3 Markov Chain Monte Carlo A common challenge in Bayesian inference is that the integral $$ \\mathbb{P}(D)=\\int \\mathbb{P}(D | \\theta) \\mathbb{P}(\\theta) d_{\\theta} $$ can’t be solved analytically. to be continued… 3.4 Empirical Bayes for Latent Variable Problems The first step is to infer point estimates for the parameters at higher levels by integrating out those at lower levels, and the infer posterior distributions for lower level parameters while setting those at a higher level to their point estimate. 3.5 Approximate Bayesian Computation Approximate Bayesian computation (ABC) is a class of simulation-based techniques to conduct Bayesian inference under models with intractable likelihoods 3.6 Model selection how to compare the several candidate models explaining the data $\\mathcal{D}$? the most commonly used methods: likelihood ratio statistic model posterior probabilities Bayes factors others: cross-validation, Akaike’s information criterion (AIC), Bayesian information criterion (BIC) ","date":"2020-01-30","objectID":"/2020-01-30-ml-bayes/:1:3","tags":["Naive Bayes","Bayesian Inference"],"title":"Statistical Modeling and Inference","uri":"/2020-01-30-ml-bayes/"},{"categories":["Machine Learning"],"content":"4. Naive Bayes and Bayesian estimation Naive Bayes and Bayesian estimation are two different concepts! Naive Bayes is a statistical learning method. For a give training set, learn the join probability distribution of $P(X,Y)$. Based on this model, for a given input $x$, output a $y$ with maximal posterior probability (Bayes theorem). Set prior prob distribution: $$ P\\left(Y=c_{k}\\right), \\quad k=1,2, \\cdots, K $$ Conditional prob distribution: $$ P\\left(X=x | Y=c_{k}\\right)=P\\left(X^{(1)}=x^{(1)}, \\cdots, X^{(n)}=x^{(n)} | Y=c_{k}\\right), \\quad k=1,2, \\cdots, K $$ Naive Bayes make a strong assumption that conditional prob distribution are all conditional independent, which is: $$ \\begin{aligned} P\\left(X=x | Y=c_{k}\\right) \u0026=P\\left(X^{(1)}=x^{(1)}, \\cdots, X^{(n)}=x^{(n)} | Y=c_{k}\\right) \\cr \u0026=\\prod_{j=1}^{n} P\\left(X^{(j)}=x^{(j)} | Y=c_{k}\\right) \\end{aligned} $$ then, posterior prob is: $$ P\\left(Y=c_{k} | X=x\\right)=\\frac{P\\left(X=x | Y=c_{k}\\right) P\\left(Y=c_{k}\\right)}{\\sum_{k} P\\left(X=x | Y=c_{k}\\right) P\\left(Y=c_{k}\\right)} $$ If conditional probability of each input variable is not independent, then model become Baysian Network! Naive Bayes Classifier is: $$ y=f(x)=\\arg \\max_{c_{k}} \\frac{P\\left(Y=c_{k}\\right) \\prod_{j} P\\left(X^{(j)}=x^{(j)} | Y=c_{k}\\right) }{\\sum_{k} P\\left(Y=c_{k}\\right) \\prod_{j} P\\left(X^{(j)}=x^{(j)} | Y=c_{k}\\right)} $$ and it’s short form: $$ y=f(x)= \\arg \\max_{c_{k}} P\\left(Y=c_{k}\\right) \\prod_{j} P\\left(X^{(j)}=x^{(j)} | Y=c_{k}\\right) $$ ","date":"2020-01-30","objectID":"/2020-01-30-ml-bayes/:1:4","tags":["Naive Bayes","Bayesian Inference"],"title":"Statistical Modeling and Inference","uri":"/2020-01-30-ml-bayes/"},{"categories":["Machine Learning"],"content":"Multi-label classification, tasks commonly be seen on health record data (multi symptoms). Loss function design: Multi binary cross-entropy each class has a binary output Label smoothing, another regularization technique It’s designed to make the model a little bit less certain of it’s decision by changing a little bit its target: instead of wanting to predict 1 for the correct class and 0 for all the others, we ask it to predict 1-ε for the correct class and ε for all the others, with ε a (small) positive number and N the number of classes. This can be written as: $$ \\text {loss}=(1-\\varepsilon) c e(i)+\\varepsilon \\sum c e(j) / N $$ where ce(x) is cross-entropy of x (i.e. −log(px)), and i is the correct class. finally, for multi-label loss function: $$ (1-\\epsilon) \\sum_{i}\\left(-\\frac{\\log p_{i}}{n}\\right)+\\frac{\\epsilon}{N} \\sum\\left(-\\log p_{i}\\right) $$ See the fastai implementation here: LabelSmoothingCrossEntropy about line 285: class LabelSmoothingCrossEntropy(Module): y_int = True def __init__(self, eps:float=0.1, reduction='mean'): self.eps,self.reduction = eps,reduction def forward(self, output, target): c = output.size()[-1] log_preds = F.log_softmax(output, dim=-1) if self.reduction=='sum': loss = -log_preds.sum() else: loss = -log_preds.sum(dim=-1) if self.reduction=='mean': loss = loss.mean() return loss*self.eps/c + (1-self.eps) * F.nll_loss(log_preds, target.long(), reduction=self.reduction) def activation(self, out): return F.softmax(out, dim=-1) def decodes(self, out): return out.argmax(dim=-1) ","date":"2020-01-29","objectID":"/2020-01-29-ml-lossfunc/:0:0","tags":["Pytorch"],"title":"Loss function for multi-label classification","uri":"/2020-01-29-ml-lossfunc/"},{"categories":["Machine Learning"],"content":"Maximum Likelihood Estimation Gaussian Mixture Model Expectation Maximization ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:0:0","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"1. Probability and likelihood likehood \u0026 maximum likehood 在非正式场合似然（likelihood）和概率（Probability）几乎是一对同义词，但是在统计学中似然和概率却是两个不同的概念。 概率: 在特定环境下某件事情发生的可能性，也就是结果没有产生之前依据环境所对应的参数来预测某件事情发生的可能性。 比如抛硬币，抛之前我们不知道最后是哪一面朝上，但是根据硬币的性质我们可以推测任何一面朝上的可能性均为50%，这个概率只有在抛硬币之前才是有意义的，抛完硬币后的结果便是确定的； 似然: 刚好相反，是在确定的结果下去推测产生这个结果的可能环境（参数）。 假设随机抛掷一枚硬币1,000次，结果500次人头朝上，500次数字朝上，那么两面朝上的概率均为50%。运用出现的结果来判断这个事情本身的性质（参数），也就是似然。 当结果和参数相互对应，似然和概率在数值上相等。 用 θ 表示环境对应的参数，x 表示结果，那么概率可以表示为： $$P(x | \\theta )$$ $p(x \\vert θ)$ 是条件概率的表示方法。θ 是前置条件，理解为在 θ 的前提下，事件 x 发生的概率，相对应的似然可以表示为: $$\\mathcal{L}(\\theta | x)$$ 可以理解为已知结果为 x ，参数为 θ (似然函数里 θ 是变量，这里说的参数和变量是相对与概率而言的)对应的概率，即： $$\\mathcal{L}(\\theta | x)=P(x | \\theta)$$ 两者在数值上相等，但是意义并不相同, $\\mathcal{L}$ 是关于 θ 的函数，而 P 则是关于 x 的函数。 ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:1:0","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"2. Maximum Likelihood Estimation 单高斯模型 $x \\sim \\mathcal{N}(\\mu, \\Sigma)$, $x_{i} \\in \\mathcal{D}$, 那么对参数 $\\mu$和 $\\Sigma$ 进行估计，只需要最大化log-likelihood函数： $$ \\begin{aligned} \\log p(X) \u0026=\\sum_{i=1}^{N} \\log \\mathcal{N}\\left(x_{i} | \\mu, \\Sigma\\right) \\cr \u0026=\\sum_{i=1}^{N} \\log \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{\\left(x_{i}-\\mu\\right)^{2}}{2 \\sigma^{2}}} \\cr \u0026=\\sum_{i=1}^{N} \\log \\frac{1}{\\sqrt{2 \\pi} \\sigma}+\\sum_{i=1}^{N}-\\frac{\\left(x_{i}-\\mu\\right)^{2}}{2 \\sigma^{2}} \\cr \u0026=-\\frac{N}{2} \\log 2 \\pi-\\frac{N}{2} \\log \\sigma^{2}-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{N}\\left(x_{i}-\\mu\\right)^{2} \\end{aligned} $$ 求偏导数，得到参数估计： $$ \\begin{aligned} \\frac{\\partial \\log p(X)}{\\partial \\mu} \u0026=\\frac{1}{\\sigma^{2}} \\sum_{i=1}^{N}\\left(x_{i}-\\mu\\right)=0 \\cr \u0026 \\Rightarrow \\mu=\\frac{1}{N} \\sum_{i=1}^{N} x_{i} \\cr \\frac{\\partial \\log p(X)}{\\partial \\sigma^{2}} \u0026=-\\frac{N}{2 \\sigma^{2}}+\\frac{1}{2 \\sigma^{4}} \\sum_{i=1}^{N}\\left(x_{i}-\\mu\\right)^{2}=0 \\cr \u0026 \\Rightarrow \\sigma^{2}=\\frac{1}{N} \\sum_{i=1}^{N}\\left(x_{i}-\\mu\\right)^{2} \\end{aligned} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:2:0","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"3. Gaussian Mixture Model 如果有K个高斯线性叠加: $$ \\begin{aligned} p(x)=\u0026 \\sum_{k=1}^{K} \\pi_{k} \\mathcal{N}\\left(x | \\mu_{k}, \\Sigma_{k}\\right) \\cr \u0026 \\text { s.t. } \\sum_{k=1}^{K} \\pi_{k}=1 \\cr \u0026 0 \\leq \\pi_{k} \\leq 1 \\end{aligned} $$ 那么对数似然函数为 $$ \\log p(X)=\\sum_{i=1}^{N} \\log \\lbrace \\sum_{k=1}^{K} \\pi_{k} \\mathcal{N} (x_{i} | \\mu_{k}, \\Sigma_{k}) \\rbrace $$ 因为对数里有求和，因此无法无法直接通过最大似然估计方法进行参数估计。 其中，如果$\\pi_{k}$是每个高斯出现的概率$p(k)$，则高斯混合模型分解为以$p(k)$获得一个高斯分布，然后在分布中获得$x$，因此$x$边缘概率分布为： $$ p(x)=\\sum_{k=1}^{K} p(k) p(x | k) $$ 后验概率$p(k\\vert x)$表示$x$属于每个高斯的概率（离散值）: $$ \\begin{aligned} p(k | x) \u0026=\\frac{p(x | k) p(k)}{\\sum_{l} p(x | l) p(l)} \\cr \u0026=\\frac{\\pi_{k} \\mathcal{N}\\left(x | \\mu_{k}, \\Sigma_{k}\\right)}{\\sum_{l} \\pi_{l} \\mathcal{N}\\left(x | \\mu_{l}, \\Sigma_{l}\\right)} \\end{aligned} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:3:0","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"4. Expectation Maximization 思想： 通过引入隐变量，运用迭代方法，求解混合高斯模型 $$ \\theta^{(t+1)}=\\underset{\\theta}{\\arg \\max } \\mathcal{L}(\\theta ; X) $$ 引入隐变量Zi(状态i）， z服从多项分布，选择zi的概率为p(zi),则高斯混合模型为： $$ \\begin{aligned} z_{i} \u0026 \\sim \\operatorname{Multinoimal}\\left(\\pi_{1}, \\cdots, \\pi_{k}\\right) \\cr x_{i} | z_{i} \u0026 \\sim \\mathcal{N}\\left(\\mu_{z_{i}}, \\Sigma_{z_{i}}\\right) \\end{aligned} $$ 步骤： E-Step: 在现有$\\theta^{(t)}$下最大化似然下界, 计算隐变量$z$的期望$Q\\left(z_{i}\\right)=p\\left(z_{i} \\vert x_{i}, \\theta\\right)$ 作为其下界 M-Step: 在上面$Q(z_{i})$下计算参数列表$\\theta$来最大化似然 ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:4:0","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"(0) 理解EM的前提 凹凸函数: $\\forall_{x \\in \\mathbb{R}}, f^{\\prime \\prime}(x) \\geq 0$,则$f$为凸函数。 当$x$为向量，如果其hessian矩阵 $H$ 是半正定的($H \\geq 0$),则$f$为凸函数 如果$f^{\\prime \\prime}(x)\u003e0$或者$H\u003e0$, $f$是严格凸函数。 如果$f^{\\prime \\prime}(x)\u003c0$或者$H\u003e0$, $f$是凹函数。 Jensen 不等式: 如果$f$为凸函数, 则$E[f(X)] \\geq f(E [ X ])$。当且仅当$x$是常数时，$E[f(x)]=f(E[ x ])$。 如果$f$是凹函数, 则$E[f(X)] \\leq f(E[ X ])$。 引入隐变量后，变换对数似然函数: $$ \\begin{aligned} \\mathcal{L}(\\theta ; X) \u0026=\\sum_{i=1}^{N} \\log p\\left(x_{i} | \\theta\\right) \\cr \u0026=\\sum_{i=1}^{N} \\log \\sum_{z_{i}} p\\left(x_{i}, z_{i} | \\theta\\right) \\cr \u0026=\\sum_{i=1}^{N} \\log \\sum_{z_{i}} Q\\left(z_{i}\\right) \\frac{p\\left(x_{i}, z_{i} | \\theta\\right)}{Q\\left(z_{i}\\right)} \\cr \u0026 \\geq \\sum_{i=1}^{N} \\sum_{z_{i}} Q\\left(z_{i}\\right) \\log \\frac{p\\left(x_{i}, z_{i} | \\theta\\right)}{Q\\left(z_{i}\\right)} \\end{aligned} $$ 推导: 把式中的log函数体看成是一个整体，由于$\\log (x)$的二阶导数为$-\\frac{1}{x^2}$, 小于0，为凹函数。所以使用Jensen不等式时，应用第二条准则：$f(E [ X ] ) \\geq E[f(x)]$。 $$ f\\left(E_{z_{i} \\sim Q}\\left[\\frac{p\\left(x_{i}, z_{i} | \\theta\\right)}{Q\\left(z_{i}\\right)}\\right]\\right) \\geq E_{z_{i} \\sim Q}\\left[f\\left(\\frac{p\\left(x_{i}, z_{i} | \\theta\\right)}{Q\\left(z_{i}\\right)}\\right)\\right] $$ 这里，$Q\\left(z_{i}\\right)$是$z_{i}$的函数， 且$\\sum_{z_{i}} Q\\left(z_{i}\\right)=1$。 由数学期望$E_{x \\sim p}[g(X)]=\\sum_{x} g(x) p(x)$，上式可以理解为: $p(x)$对应$Q\\left(z_{i}\\right)$, g(x)对应$\\log \\frac{p\\left(x_{i}, z_{i} \\vert \\theta\\right)}{Q\\left(z_{i}\\right)}$表示$z_{i}$的函数。 似然函数: $\\mathcal{L}(\\theta) \\geq \\mathcal{J}(z,Q)$（$z$为隐含变量），那么我们可以通过不断的最大化$\\mathcal{J}$的下界，来使得$\\mathcal{L}(\\theta)$不断提高，最终达到它的最大值。 最大化$\\mathcal{L}(\\theta)$函数的下界，即让$g(x)$为常数c: $$ \\frac{p\\left(x_{i}, z_{i} | \\theta\\right)}{Q\\left(z_{i}\\right)}=c $$ Jensen不等式中说到，当自变量$X=E(X)$时，即为常数的时候，等式成立! 变换公式, 对所有$z$求和得: $$ \\begin{aligned} p\\left(x_{i}, z_{i} | \\theta\\right) \u0026=c \\cdot Q\\left(z_{i}\\right) \\cr \\sum_{z_{i}} p\\left(x_{i}, z_{i} | \\theta\\right) \u0026=c \\cdot \\sum_{z_{i}} Q\\left(z_{i}\\right) \\cr c \u0026=\\sum_{z_{i}} p\\left(x_{i}, z_{i} | \\theta\\right) \\end{aligned} $$ 其中，$\\sum_{z_{i}} Q\\left(z_{i}\\right) = 1$, 也得： $$ \\begin{aligned} Q\\left(z_{i}\\right) \u0026=\\frac{p\\left(x_{i}, z_{i} | \\theta\\right)}{\\sum_{z_{i}} p\\left(x_{i}, z_{i} | \\theta\\right)} \\cr \u0026=p\\left(z_{i} | x_{i}, \\theta\\right) \\end{aligned} $$ 至此，我们推出了在固定参数θ后，使下界拉升的$Q(z)$的计算公式就是后验概率（条件概率），一并解决了$Q(z)$如何选择的问题。此步就是EM算法的E-step。 执行E-Step后与下界重合，此时似然变为： $$ \\mathcal{L}\\left(\\theta^{(t)} ; X\\right)=\\sum_{i=1}^{N} \\sum_{z_{i}} Q^{(t)}\\left(z_{i}\\right) \\log \\frac{p\\left(x_{i}, z_{i} | \\theta^{(t)}\\right)}{Q^{(t)}\\left(z_{i}\\right)} $$ 这时，对公式求导 $$ \\theta^{(t+1)}=\\underset{\\theta}{\\arg \\max } \\mathcal{L}(\\theta ; X) $$ 得到 $t+1$ 步的似然函数 $\\mathcal{L}\\left(\\theta^{(t+1)} ; X\\right)$。 通过不断的迭代，可以得到使似然函数$\\mathcal{L}(\\theta)$最大化的参数 $\\theta$，直至函数收敛。 只需要证明$\\mathcal{L}\\left(\\theta^{(t+1)} ; X\\right) \\geq \\mathcal{L}\\left(\\theta^{(t)} ; X\\right)$, 则可证明EM的收敛性: $$ \\begin{aligned} \\mathcal{L}\\left(\\theta^{(t+1)} ; X\\right) \u0026=\\sum_{i=1}^{N} \\log \\sum_{z_{i}} Q^{(t)}\\left(z_{i}\\right) \\frac{p\\left(x_{i}, z_{i} | \\theta^{(t+1)}\\right)}{Q^{(t)}\\left(z_{i}\\right)} \\cr \u0026 \\geq \\sum_{i=1}^{N} \\sum_{z_{i}} Q^{(t)}\\left(z_{i}\\right) \\log \\frac{p\\left(x_{i}, z_{i} | \\theta^{(t+1)}\\right)}{Q^{(t)}\\left(z_{i}\\right)} \\cr \u0026 \\geq \\sum_{i=1}^{N} \\sum_{z_{i}} Q^{(t)}\\left(z_{i}\\right) \\log \\frac{p\\left(x_{i}, z_{i} | \\theta^{(t)}\\right)}{Q^{(t)}\\left(z_{i}\\right)} \\cr \u0026=\\mathcal{L}\\left(\\theta^{(t)} ; X\\right) \\end{aligned} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:4:1","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"5. 求解GMM ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:5:0","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"(1) GMM E-Step: 已知$\\theta^{(t)}$, 求$Q^{(t+1)}\\left(z_{i}\\right)$: $$ \\begin{aligned} Q^{(t+1)}\\left(z_{i}\\right) \u0026=\\frac{p\\left(x_{i}, z_{i} | \\theta^{(t)}\\right)}{p\\left(x_{i} | \\theta^{(t)}\\right)} \\cr \u0026=\\frac{p\\left(x_{i}, z_{i} | \\theta^{(t)}\\right)}{\\sum_{l \\in z_{i}} p\\left(x_{i}, l | \\theta^{(t)}\\right)} \\cr \u0026=\\frac{p\\left(x_{i} | z_{i}, \\theta^{(t)}\\right) p\\left(z_{i} | \\theta^{(t)}\\right)}{\\sum_{l \\in z_{i}} p\\left(x_{i} | l, \\theta^{(t)}\\right) p\\left(l | \\theta^{(t)}\\right)} \\cr \u0026=\\frac{\\mathcal{N}\\left(\\mu_{z_{i}}, \\Sigma_{z_{i}}\\right) \\pi_{z_{i}}}{\\sum_{l \\in z_{i}} \\mathcal{N}\\left(\\mu_{l}, \\Sigma_{l}\\right) \\pi_{l}} \\end{aligned} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:5:1","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"(2) GMM M-Step: 已知$Q^{(t+1)}\\left(z_{i}\\right)$, 求 $\\theta^{(t+1)}$: $$ \\begin{aligned} \\mathcal{L}(\\theta ; X) \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log \\frac{p\\left(x_{i}, l | \\theta\\right)}{Q_{i}(l)} \\cr \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log p\\left(x_{i}, l | \\theta\\right)-\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log Q_{i}(l) \\cr \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log p\\left(x_{i}, l | \\theta\\right)-\\text {Constant } \\cr \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log \\pi_{l} \\mathcal{N}\\left(\\mu_{l}, \\Sigma_{l}\\right)-\\text {Constant } \\cr \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log \\pi_{l}+\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log \\mathcal{N}\\left(\\mu_{l}, \\Sigma_{l}\\right)-\\text {Constant} \\end{aligned} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:5:2","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"(3) 求 $\\pi$: 令 $\\forall_{l \\in{1, \\cdots, K}}$ $$ \\begin{aligned} \\frac{\\partial \\mathcal{L}(\\theta ; X)}{\\partial \\pi_{l}} \u0026=0 \\cr \\text { s.t. } \\sum_{l}^{K} \\pi_{l} \u0026= 1 \\end{aligned} $$ 拉格朗日乘法约束 $$ \\begin{cases}\\begin{aligned} L_{\\pi_{l}} \u0026=\\frac{\\partial \\mathcal{L}(\\theta ; X)}{\\partial \\pi_{l}}+\\lambda(\\sum_{l}^{K} \\pi_{l}-1)=0 \\cr L_{\\lambda} \u0026=\\sum_{l}^{K} \\pi_{l}-1=0 \\end{aligned}\\end{cases} $$ 求导： $$ \\begin{cases}\\begin{array}{c} \\frac{1}{\\pi_{1}} \\sum_{i}^{N} Q_{i}(1)-\\lambda=0 \\cr \\vdots \\cr \\frac{1}{\\pi_{l}} \\sum_{i}^{N} Q_{i}(l)-\\lambda=0 \\end{array}\\end{cases} $$ 相加得： $$ \\sum_{l}^{K} \\sum_{i}^{N} Q_{i}(l)=\\lambda \\sum_{l}^{K} \\pi_{l}=\\lambda $$ 由 $Q_{i}(l)=p\\left(l \\vert x_{i}, \\theta\\right)$, 得 $$ \\begin{aligned} \\sum_{l}^{K} \\sum_{i}^{N} Q_{i}(l) \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\cr \u0026=\\sum_{i}^{N} \\sum_{l}^{K} p\\left(l | x_{i}, \\theta\\right) \\cr \u0026=\\sum_{i}^{N} 1 \\cr \u0026=N \\end{aligned} $$ 则 $$ \\begin{aligned} \\pi_{l} \u0026=\\frac{1}{\\lambda} \\sum_{i}^{N} Q_{i}(l) \\cr \u0026=\\frac{1}{N} \\sum_{i}^{N} Q_{i}(l) \\cr \u0026=\\frac{1}{N} \\sum_{i}^{N} p\\left(l | x_{i}, \\theta\\right) \\end{aligned} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:5:3","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"(4) 计算$\\mu$ $$ \\begin{aligned} \u0026\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log \\mathcal{N}\\left(\\mu_{l}, \\Sigma_{l}\\right)\\cr \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log \\frac{1}{\\sqrt{2 \\pi} \\sigma_{l}} e^{-\\frac{\\left(x_{i}-\\mu_{l}\\right)^{2}}{2 \\sigma_{l}^{2}}}\\cr \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\lbrace -\\frac{1}{2} \\log 2 \\pi-\\frac{1}{2} \\log \\sigma_{l}^{2}-\\frac{\\left(x_{i}-\\mu_{l}\\right)^{2}}{2 \\sigma_{l}^{2}}\\rbrace \\end{aligned} $$ 求偏导： $$ \\begin{aligned} \\frac{\\partial \\mathcal{L}(\\theta ; X)}{\\partial \\mu_{l}} \u0026=\\sum_{i}^{N} Q_{i}(l) \\frac{x_{i}-\\mu_{l}}{\\sigma^{2}} \\cr \u0026=0 \\end{aligned} $$ 得$\\mu$： $$ \\mu_{l}=\\frac{\\sum_{i}^{N} Q_{i}(l) x_{i}}{\\sum_{i}^{N} Q_{i}(l)} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:5:4","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"(5) 计算$\\sigma$ $$ \\begin{aligned} \\frac{\\partial \\mathcal{L}(\\theta ; X)}{\\partial \\sigma_{l}^{2}} \u0026=\\sum_{i}^{N} Q_{i}(l) \\bigg\\lbrace -\\frac{1}{2 \\sigma_{l}^{2}}+\\frac{\\left(x_{i}-\\mu_{l}\\right)^{2}}{2 \\sigma_{l}^{4}} \\bigg\\rbrace \\cr \u0026=0 \\end{aligned} $$ 得到 $$ \\sigma_{l}=\\frac{\\sum_{i}^{N} Q_{i}(l)\\left(x_{i}-\\mu_{l}\\right)^{2}}{\\sum_{i}^{N} Q_{i}(l)} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:5:5","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"6 从KL散度角度解释EM $$ \\begin{aligned} K L(q | p) \u0026=\\sum_{z} q(z) \\log \\frac{q(z)}{p(z | x, \\theta)} \\cr \u0026=\\sum_{z} q(z) \\log \\frac{q(z) p(x | \\theta)}{p(z, x | \\theta)} \\cr \u0026=-\\sum_{z} q(z) \\log \\frac{p(z, x | \\theta)}{q(z)}+\\sum_{z} q(z) \\log p(x | \\theta) \\cr \u0026=-\\sum_{z} q(z) \\log \\frac{p(z, x | \\theta)}{q(z)}+\\log p(x | \\theta) \\sum_{z} q(z) \\cr \u0026=-\\sum_{z} q(z) \\log \\frac{p(z, x | \\theta)}{q(z)}+\\log p(x | \\theta) \\cr \\log p(x | \\theta) \u0026=K L(q | p)+\\sum_{z} q(z) \\log \\frac{p(z, x | \\theta)}{q(z)} \\cr \u0026=K L(q | p)+\\mathcal{L}(q, \\theta) \\end{aligned} $$ 参考： 徐亦达-机器学习-EM ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:6:0","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Make bioinfo uncool again"],"content":"Usefull tools","date":"2020-01-20","objectID":"/2020-01-20-begin/","tags":["Bioinformatics","Linux"],"title":"Cheatsheet for command line","uri":"/2020-01-20-begin/"},{"categories":["Make bioinfo uncool again"],"content":"usefull tools for linux command line ","date":"2020-01-20","objectID":"/2020-01-20-begin/:0:0","tags":["Bioinformatics","Linux"],"title":"Cheatsheet for command line","uri":"/2020-01-20-begin/"},{"categories":["Make bioinfo uncool again"],"content":"a. Make terminal cool, install OhMyZsh # install zsh sudo apt-get install zsh # ubuntu # change default shell to zsh chsh -s /usr/bin/zsh # install ohmyzsh sh -c \"$(wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)\" source ~/.zshrc ","date":"2020-01-20","objectID":"/2020-01-20-begin/:0:1","tags":["Bioinformatics","Linux"],"title":"Cheatsheet for command line","uri":"/2020-01-20-begin/"},{"categories":["Make bioinfo uncool again"],"content":"b. Terminal keyboard short cuts Jump to head: Ctrl + a Jump to end: Ctrl + e Delete strings ahead: Ctrl + u Delete strings follow: Ctrl + k ","date":"2020-01-20","objectID":"/2020-01-20-begin/:0:2","tags":["Bioinformatics","Linux"],"title":"Cheatsheet for command line","uri":"/2020-01-20-begin/"},{"categories":["Make bioinfo uncool again"],"content":"c. Program keeps running in the background 1. Run cmd using nohup nohup command [options] \u0026 2. Run cmd using Tmux Outside Tmux: Typically these are run outside, but you can also run them inside an existing session a. Start New Session tmux new -s myname b. Attach To Existing Session tmux attach -t myname #by name tmux attach 4 #by number (in this case 4) c. List Sessions tmux ls d. Kill Session tmux kill-session -t myname Inside Tmux Session: Start each command with CTRL + b, release, then press one of the following: Panes % vertical split \" horizontal split d detach from session (it keeps running in the background) x kill pane Up/Down/Left/Right move between panes PageUP/PageDown CTRL+c to exit the PageUp/Down mode Fn+Up/Down PageUp/Down: Mac keyboard : + resize-pane -D Resizes the current pane down : + resize-pane -U Resizes the current pane upward : + resize-pane -L Resizes the current pane left : + resize-pane -R Resizes the current pane right : + resize-pane -D 20 Resizes the current pane down by 20 cells ","date":"2020-01-20","objectID":"/2020-01-20-begin/:0:3","tags":["Bioinformatics","Linux"],"title":"Cheatsheet for command line","uri":"/2020-01-20-begin/"},{"categories":["Make bioinfo uncool again"],"content":"d. File compression and decompression Decompression File type Cmd e.g. *.tar tar -xvf *.tar.gz or *.tgz tar -xvzf *bz2 bzip2 -d or bunzip2 *.tar.bz2 tar -xjf *.Z uncompress *.tar.Z tar -xZf *.rar unrar e or rar x unrar e file.rar *.zip unzip *.gz gunzip Compression File type Cmd e.g. *.tar tar -cvf *.tar.gz or *.tgz tar -cvzf *bz2 bzip2 -z *.tar.bz2 tar -cjf *.Z compress *.tar.Z tar -cZf *.rar rar a rar a -ep1 newname /home/user/cpp *.zip zip *.gz gzip For rar installation sudo apt-get install rar ","date":"2020-01-20","objectID":"/2020-01-20-begin/:0:4","tags":["Bioinformatics","Linux"],"title":"Cheatsheet for command line","uri":"/2020-01-20-begin/"},{"categories":["Make bioinfo uncool again"],"content":"e. Handy tricks for handling filepath very useful to strip file sufix, path et.al. # e.g. var=./home/fastq/filename_R1.fq.gz # extract filename ${var#*/} # -\u003e home/fastq/filename_R1.fq.gz var1=${var##*/} # -\u003e filename_R1.fq.gz # remove file suffix ${var1%.*} # -\u003e filename_R1.fq ${var1%%.*} # -\u003e filename_R1 # get basebame var2=$(basename \"${var}\" .fq.gz) #-\u003e filename_R1 ","date":"2020-01-20","objectID":"/2020-01-20-begin/:0:5","tags":["Bioinformatics","Linux"],"title":"Cheatsheet for command line","uri":"/2020-01-20-begin/"},{"categories":["Make bioinfo uncool again"],"content":"hisat2-htseq-deseq2","date":"2020-01-20","objectID":"/2020-01-20-rnaseq/","tags":["RNA-seq"],"title":"Shortcut to bulk RNA-seq analysis","uri":"/2020-01-20-rnaseq/"},{"categories":["Make bioinfo uncool again"],"content":"hisat2-htseq-deseq2 ","date":"2020-01-20","objectID":"/2020-01-20-rnaseq/:0:0","tags":["RNA-seq"],"title":"Shortcut to bulk RNA-seq analysis","uri":"/2020-01-20-rnaseq/"},{"categories":["Make bioinfo uncool again"],"content":"3.1 transcriptom mapping step 0: install tools conda install htseq hisat2 stringtie step 1: build index and extract splice sites build index hisat2-build -p {threads} genome/hg38.fa hisat2_index/hg38 extract known splice sites for alignmnet hisat2_extract_splice_sites.py gencode.gtf \u003e hisat2_index/splicesites.txt hisat2_extract_exons.py gencode.gtf \u003e histat2_index/exon.txt step2: mapping hisat2 --dta --threads ${threads} \\ -x hisat2_index/hg38 \\ --known-splicesite-infile hisat2_index/splicesites.txt \\ -1 R1.fq.gz \\ -2 R2.fq.gz \\ -S output.sam step 3: sam to bam samtools view -Sbh -q 25 \\ -@ ${threads} \\ -o ouput.bam \\ input.sam step 4: bam sort and index samtools sort -@ ${threads} input.bam \u003e output.sorted.bam samtools index input.sorted.bam #generate input.sorted.bam.bai step 5: bam to bigwig bamCoverage -p ${threads} \\ --normalizeUsing RPKM \\ # note: other normalization options -b input.sorted.bam \\ -o output.bw ","date":"2020-01-20","objectID":"/2020-01-20-rnaseq/:0:1","tags":["RNA-seq"],"title":"Shortcut to bulk RNA-seq analysis","uri":"/2020-01-20-rnaseq/"},{"categories":["Make bioinfo uncool again"],"content":"3.2 Differentially expressed genes analysis step 1: count reads htseq-count -r pos -s no \\ --additional-attr gene_name \\ --additional-attr gene_type \\ -f bam input.sorted.bam gencode.gtf \u003e output.count step2: differentially expressed genes analysis (1) construct read count table option 1: HTSeq count file input library(\"DESeq2\") directory \u003c- \"/path/to/your/readCountFiles/\" sampleFiles \u003c- grep(\"count\", list.files(directory), value=TRUE) condition \u003c- factor(c(\"KO\",\"KO\", \"WT\",\"WT\"), levels = c(\"WT\", \"KO\")) # phenotable sampleTable \u003c- data.frame(sampleName = sampleFiles, fileName = sampleFiles, condition = condition) # construct read count table ddsHTSeq \u003c- DESeqDataSetFromHTSeqCount(sampleTable = sampleTable, directory = directory, design= ~ condition) option 2: combined read count file into a single table first, then run library(DESeq2) # read count table database \u003c- read.table(file = \"raw.counts.csv\", sep = \",\", header = TRUE, row.names = 1) database \u003c- round(as.matrix(database)) # set level condition \u003c- factor(c(\"KO\",\"KO\", \"WT\",\"WT\"), levels = c(\"WT\", \"KO\")) # build DESeq object coldata \u003c- data.frame(row.names = colnames(database), condition) dds \u003c- DESeqDataSetFromMatrix(countData=database, colData=coldata, design=~condition + treatmement) (2) run DESeq2 and get output library(DESeq2) dds \u003c- dds[ rowSums(counts(dds)) \u003e 1, ] # run statistical test dds \u003c- DESeq(dds) # get results res \u003c- results(dds) # summary(res) count_r \u003c- counts(dds, normalized=T) #normalized count matrix # export results res \u003c- res[order(res$padj),] diff_gene \u003c- subset(res, padj \u003c 0.05 \u0026 (log2FoldChange \u003e 1 | log2FoldChange \u003c -1)) diff_gene \u003c- row.names(diff_gene) resdata \u003c- merge(as.data.frame(res), as.data.frame(counts(dds, normalized=TRUE)), by=\"row.names\", sort=FALSE) write.csv(resdata, file = \"DEGs.csv\", row.names = FALSE) 3.3 Gene set enrichrment analysis GO clusterprofiler Enrichr (GSEApy) GSEA ","date":"2020-01-20","objectID":"/2020-01-20-rnaseq/:0:2","tags":["RNA-seq"],"title":"Shortcut to bulk RNA-seq analysis","uri":"/2020-01-20-rnaseq/"},{"categories":["Make bioinfo uncool again"],"content":"3.4 Alternative splicing analysis rMATS ","date":"2020-01-20","objectID":"/2020-01-20-rnaseq/:0:3","tags":["RNA-seq"],"title":"Shortcut to bulk RNA-seq analysis","uri":"/2020-01-20-rnaseq/"},{"categories":["Make bioinfo uncool again"],"content":"bowtie2-macs2-deeptools","date":"2020-01-20","objectID":"/2020-01-20-chip/","tags":["ChIP-seq"],"title":"Shortcut to ChIP-seq analysis","uri":"/2020-01-20-chip/"},{"categories":["Make bioinfo uncool again"],"content":"bowtie2-macs2-deeptools ","date":"2020-01-20","objectID":"/2020-01-20-chip/:0:0","tags":["ChIP-seq"],"title":"Shortcut to ChIP-seq analysis","uri":"/2020-01-20-chip/"},{"categories":["Make bioinfo uncool again"],"content":"2.1 Genome mapping Step 0: install software # install miniconda, then call conda conda install -c bioconda bowtie2 hisat2 samtools deeptools step 1: build index bowtie2-build hg38.fa bowtie2_index/hg38 step 2: mapping Unpaired data bowtie2 -p ${threads} -x index/hg38 \\ -U input.fastq.gz \\ -S ouput.sam Paired data bowtie2 -p 4 -x index/hg38 \\ -1 input_R1.fastq.gz \\ -2 input_R2.fastq.gz \\ -S ouput.sam step 3: sam to bam samtools view -Sbh -q 25 \\ -@ ${threads} \\ -o ouput.bam \\ input.sam step 4: bam sort and index samtools sort -@ ${threads} input.bam \u003e output.sorted.bam samtools index input.sorted.bam #generate input.sorted.bam.bai step 5: bam to bigwig bamCoverage -p ${threads} \\ --normalizeUsing RPKM \\ # note: other normalization options --centerReads \\ -e 200 \\ -b input.sorted.bam \\ -o output.bw ","date":"2020-01-20","objectID":"/2020-01-20-chip/:0:1","tags":["ChIP-seq"],"title":"Shortcut to ChIP-seq analysis","uri":"/2020-01-20-chip/"},{"categories":["Make bioinfo uncool again"],"content":"2.2 Peaks analysis note:: macs2 (\u003ev2.2.x) supports python 3. step 0: install tools conda install macs2 bedtools pygenometracks step 1: callpeaks (1) narrow peaks, e.g. TFs, h3k4m3 # bam file input macs2 callpeak -t ChIP.elute.sorted.bam \\ -c ChIP.input.sorted.bam \\ -f BAM \\ -g hs # organism \\ -B -q 0.05 \\ -n ${outFileName}\\ --outdir macs_out (2) Broad peaks, e.g. h3k27me3 # sam file also works fine macs2 callpeak -t ./bowtie_out/WTme2ChIP.sam \\ -c ./bowtie_out/ESCInput.sam \\ -f SAM \\ -g mm \\ -B --SPMR \\ --nomodel --extsize 147 \\ --broad -n WTme2ChIP --outdir macs_out step 2: advanced analysis tools: bedtools, deeptools, pyGenomeTracks, igv genome algebra overlap with other peaks: bedtools visualization heatmap: deeptools signal tracks: pyGenomeTracks, igv ","date":"2020-01-20","objectID":"/2020-01-20-chip/:0:2","tags":["ChIP-seq"],"title":"Shortcut to ChIP-seq analysis","uri":"/2020-01-20-chip/"},{"categories":["Make bioinfo uncool again"],"content":"salmon-tximport-deseq2","date":"2020-01-20","objectID":"/2020-01-23-rnaseqturbo/","tags":["RNA-seq"],"title":"SuperFast RNA-seq","uri":"/2020-01-23-rnaseqturbo/"},{"categories":["Make bioinfo uncool again"],"content":"salmon-tximport-deseq2 ","date":"2020-01-20","objectID":"/2020-01-23-rnaseqturbo/:0:0","tags":["RNA-seq"],"title":"SuperFast RNA-seq","uri":"/2020-01-23-rnaseqturbo/"},{"categories":["Make bioinfo uncool again"],"content":"Step 0: install salmon and download transcriptome cdna from gencode conda install salmon wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_32/gencode.v32.transcripts.fa.gz ","date":"2020-01-20","objectID":"/2020-01-23-rnaseqturbo/:0:1","tags":["RNA-seq"],"title":"SuperFast RNA-seq","uri":"/2020-01-23-rnaseqturbo/"},{"categories":["Make bioinfo uncool again"],"content":"Step 1. build salmon index salmon index -p 8 --gencode -t gencode.v32.transcripts.fa.gz -i salmonIndex_hg38 ","date":"2020-01-20","objectID":"/2020-01-23-rnaseqturbo/:0:2","tags":["RNA-seq"],"title":"SuperFast RNA-seq","uri":"/2020-01-23-rnaseqturbo/"},{"categories":["Make bioinfo uncool again"],"content":"Step 2: quantification salmon quant -i salmonIndex_hg38 -l A \\ -1 ${fn}/${samp}_1.fastq.gz \\ -2 ${fn}/${samp}_2.fastq.gz \\ -p 8 --validateMappings -o quants/${samp}_quant ","date":"2020-01-20","objectID":"/2020-01-23-rnaseqturbo/:0:3","tags":["RNA-seq"],"title":"SuperFast RNA-seq","uri":"/2020-01-23-rnaseqturbo/"},{"categories":["Make bioinfo uncool again"],"content":"Step 3: merge quantification outputs use tximport in R # R code library(tximport) library(readr) suppressMessages(library('EnsDb.Hsapiens.v86')) txdb \u003c- EnsDb.Hsapiens.v86 k \u003c- keys(txdb, keytype = \"GENEID\") df \u003c- select(txdb, keys = k, keytype = \"GENEID\", columns = c(\"TXID\",\"GENEID\")) tx2gene \u003c- df[, 2:1] # tx ID, then gene ID #tx2gene \u003c- read.table(tx2gene, header= T, sep=\"\\t\", stringsAsFactors = F) samples \u003c- unlist(strsplit(sample_ids,\",\")) salmon.files \u003c- file.path('salmon',samples, \"quant.sf\") names(salmon.files) \u003c- samples all(file.exists(salmon.files)) # get transcript level results txi.transcripts \u003c- tximport(salmon.files, type = \"salmon\", txOut = TRUE, tx2gene = tx2gene,) # ignoreTxVersion = TRUE) # get gene level results txi.salmon \u003c- summarizeToGene(txi.transcripts, tx2gene) #save raw counts salmon.counts\u003c- txi.salmon$counts salmon.counts\u003c- as.data.frame(salmon.counts) write.table(salmon.counts, out_counts, sep=\"\\t\", quote=F) #save gene tpms salmon.TPM\u003c- txi.salmon$abundance salmon.TPM\u003c- as.data.frame(salmon.TPM) write.table(salmon.TPM, out_tpm, sep=\"\\t\", quote=F) #save transcripts tpms salmon.trans.TPM\u003c- txi.transcripts$abundance salmon.trans.TPM\u003c- as.data.frame(salmon.trans.TPM) write.table(salmon.trans.TPM, outTrans_tpm, sep=\"\\t\", quote=F) save(txi.salmon, file=\"txi.salmon.RData\") ","date":"2020-01-20","objectID":"/2020-01-23-rnaseqturbo/:0:4","tags":["RNA-seq"],"title":"SuperFast RNA-seq","uri":"/2020-01-23-rnaseqturbo/"},{"categories":["Make bioinfo uncool again"],"content":"Step 4: Differentially expressed gene analysis DESeq2 pipeline demo load(\"txi.salmon.RData\") dds \u003c- DESeqDataSetFromTximport(txi.salmon, sampleTable, ~condition) dds$condition \u003c- relevel(dds$condition, ref=ctrl) dds \u003c- DESeq(dds, parallel=TRUE) res \u003c- results(dds, contrast=c(\"condition\", treat, ctrl)) resOrdered \u003c- res[order(res$padj),] resOrdered = as.data.frame(resOrdered) write.table(resOrdered, file=\"degs.txt\", quote=F, sep=\"\\t\") ","date":"2020-01-20","objectID":"/2020-01-23-rnaseqturbo/:0:5","tags":["RNA-seq"],"title":"SuperFast RNA-seq","uri":"/2020-01-23-rnaseqturbo/"},{"categories":null,"content":"My PhD thesis","date":"2019-12-10","objectID":"/publication/2019-12-10-sox21/","tags":null,"title":"SOX21 Ensures Rostral Forebrain Identity by Suppression of WNT8B during Neural Regionalization of Human Embryonic Stem Cells","uri":"/publication/2019-12-10-sox21/"},{"categories":null,"content":"My PhD thesis. Published in Stem Cell Reports, 2019 Highlights The transcriptomic analysis of rostrocaudal patterning of hESC-derived NPCs SOX21 KO leads to caudalized regional identity in rostral forebrain progenitors SOX21 represses Wnt signaling to ensure the rostral forebrain identity WNT8B is a major downstream target of SOX21 Download here ","date":"2019-12-10","objectID":"/publication/2019-12-10-sox21/:0:0","tags":null,"title":"SOX21 Ensures Rostral Forebrain Identity by Suppression of WNT8B during Neural Regionalization of Human Embryonic Stem Cells","uri":"/publication/2019-12-10-sox21/"},{"categories":null,"content":"Ding J, Fang Z, Liu X, Zhu Z, Wen C, Wang H, et al. CDK11 safeguards the identity of human embryonic stem cells via fine-tuning signaling pathways. J Cell Physiol. 2019 Download here ","date":"2019-10-15","objectID":"/publication/2019-10-15/:0:0","tags":null,"title":"CDK11 safeguards the identity of human embryonic stem cells via fine‐tuning signaling pathways","uri":"/publication/2019-10-15/"},{"categories":null,"content":"Hu J, Li S, Sun X, Fang Z, Wang L, Xiao F, et al. Stk40 deletion elevates c-JUN protein level and impairs mesoderm differentiation. J Biol Chem. 2019;294(25):9959-72. Download here ","date":"2019-06-25","objectID":"/publication/2019-06-21/:0:0","tags":null,"title":"Stk40 deletion elevates c-JUN protein level and impairs mesoderm differentiation","uri":"/publication/2019-06-21/"},{"categories":null,"content":"Xu Y, Luo X, Fang Z, Zheng X, Zeng Y, Zhu C, et al. Transcription coactivator Cited1 acts as an inducer of trophoblast-like state from mouse embryonic stem cells through the activation of BMP signaling. Cell Death Dis. 2018;9(9):924. Download here ","date":"2018-09-11","objectID":"/publication/2018-09-11/:0:0","tags":null,"title":"Transcription coactivator Cited1 acts as an inducer of trophoblast-like state from mouse embryonic stem cells through the activation of BMP signaling","uri":"/publication/2018-09-11/"},{"categories":null,"content":"I’m honored to be one of the contributors. It is definitely worthy. Thanks for the Bioconda team, so we could install bioinformatic tools so easy! The open source Bioconda project is a milestone in computational biology. It saves lots of time when trying to install bioinfo tools. Download here ","date":"2018-07-02","objectID":"/publication/2018-07-02/:0:0","tags":null,"title":"Bioconda: sustainable and comprehensive software distribution for the life sciences","uri":"/publication/2018-07-02/"},{"categories":null,"content":"Single cell analysis of mouse E5.5, E6.5 embryos. In this work, I dissected the embryos and prepared the single cell cDNA libraries.","date":"2017-06-09","objectID":"/publication/2016-06-09/","tags":null,"title":"Single-cell analysis reveals lineage segregation in early post-implantation mouse embryos","uri":"/publication/2016-06-09/"},{"categories":null,"content":"Wen J, Zeng Y, Fang Z, Gu J, Ge L, et al. Single-cell analysis reveals lineage segregation in early post-implantation mouse embryos. The Journal of biological chemistry 2017;292:9840-54 Download here ","date":"2017-06-09","objectID":"/publication/2016-06-09/:0:0","tags":null,"title":"Single-cell analysis reveals lineage segregation in early post-implantation mouse embryos","uri":"/publication/2016-06-09/"},{"categories":null,"content":"Wang L, Yu H, Cheng H, He K, Fang Z, et al. Deletion of Stk40 impairs definitive erythropoiesis in the mouse fetal liver. Cell Death Dis. 2017;8:e2722 Download here ","date":"2017-03-30","objectID":"/publication/2017-03-30/:0:0","tags":null,"title":"Deletion of Stk40 impairs definitive erythropoiesis in the mouse fetal liver","uri":"/publication/2017-03-30/"},{"categories":null,"content":"Liao B, Zhong XM, Xu HM, Xiao F, Fang ZQ, et al. Itch, an E3 ligase of Oct4, is required for embryonic stem cell self-renewal and pluripotency induction. Journal of Cellular Physiology 2013;228:1443-51 Download here ","date":"2012-12-18","objectID":"/publication/2012-12-18/:0:0","tags":null,"title":"Itch, an E3 ligase of Oct4, is required for embryonic stem cell self-renewal and pluripotency induction","uri":"/publication/2012-12-18/"},{"categories":null,"content":"I would like to use pandas to explore my data, but I did not find a convenient tool to do gene set enrichment analysis in python. I want something like this: Ability to run inside python console without switching to R. User friendly for both wet and dry lab users. Produce or reproduce publishable figures. Perform batch jobs easy. Command line support (Win, Mac, Linux). That’s why GSEApy comes to play. See code here ","date":"0001-01-01","objectID":"/portfolio/portfolio-1/:0:0","tags":null,"title":"GSEApy","uri":"/portfolio/portfolio-1/"},{"categories":null,"content":"My bioinformatic workflows using snakemake. I would update it if only I have more time. Why snakemake? Good: Community Easy to use and update Reusable workflows Scalable Support conda env Support slurm, qsub … python syntax Drawback: Hard to debug workflows. TODO: Integrate snakemake-wrappers to current workflows to make it more reproducible. See code here ","date":"0001-01-01","objectID":"/portfolio/portfolio-3/:0:0","tags":null,"title":"Snakeflow","uri":"/portfolio/portfolio-3/"},{"categories":null,"content":"My journey to object detection began with YOLOv3. I think it’s really a good starting point for someone like me without computer vison background to understand what’s going on behind the scence. While learning object detection, I made a simple modified C++ version (with LibTorch) based on others’ work 1 2 3. See code here ","date":"0001-01-01","objectID":"/portfolio/portfolio-2/:0:0","tags":null,"title":"YOLOv3 Inference Framework in C++","uri":"/portfolio/portfolio-2/"},{"categories":null,"content":"Performance test code: yolov3 models/yolov3.cfg models/yolov3.weights images Results: tested with CPU: Core i9 Windows MAC: average time (682 ms/image). tested with GPU: Tesla V100 Linux: average time (22 ms/image). ","date":"0001-01-01","objectID":"/portfolio/portfolio-2/:1:0","tags":null,"title":"YOLOv3 Inference Framework in C++","uri":"/portfolio/portfolio-2/"},{"categories":null,"content":"Features Supports NMS Soft NMS Weighted NMS ","date":"0001-01-01","objectID":"/portfolio/portfolio-2/:2:0","tags":null,"title":"YOLOv3 Inference Framework in C++","uri":"/portfolio/portfolio-2/"},{"categories":null,"content":"TODO Support training … ","date":"0001-01-01","objectID":"/portfolio/portfolio-2/:3:0","tags":null,"title":"YOLOv3 Inference Framework in C++","uri":"/portfolio/portfolio-2/"},{"categories":null,"content":"Credits YOLOv3_CPP repo is created based on the implementations below: weixu000. ↩︎ PyTorch-YOLOv3. ↩︎ YOLO_v3_tutorial_from_scratch. ↩︎ ","date":"0001-01-01","objectID":"/portfolio/portfolio-2/:4:0","tags":null,"title":"YOLOv3 Inference Framework in C++","uri":"/portfolio/portfolio-2/"}]