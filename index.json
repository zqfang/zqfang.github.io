[{"categories":["Machine Learning"],"content":"More about Graph Neural Network ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:0:0","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"Algebra presentation of Graphs ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:1:0","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"1. Adjacency matrix $$ A_{i j}= \\begin{cases} 1 \u0026 \\text { if }\\lbrace v_{i}, v_{j}\\rbrace \\in E \\text { and } i \\neq j \\cr 0 \u0026 \\text { otherwise } \\end{cases} $$ ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:1:1","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"2. Degree matrix: D is a diagonal matrix, where $$ D_{ii} = d(v_i) $$ ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:1:2","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"3. Laplacian matrix What and why Laplacian matrix if we consider all edges in graph $G$ to be undirected, then Laplacian matrix $L$ could be defined as $$ L = D-A $$ Thus, we have the elements: $$ L_{i j}=\\begin{cases} d\\left(v_{i}\\right) \u0026 \\text { if } i=j \\cr -1 \u0026 \\text { if }\\lbrace v_{i}, v_{j}\\rbrace \\in E \\text { and } i \\neq j \\cr 0 \u0026 \\text { otherwise. } \\end{cases}. $$ ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:1:3","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"4. Symmetric normalized Laplacian the symmetric normalized Laplacian is define as: $$ \\begin{aligned} L^{sym} \u0026=D^{-\\frac{1}{2}} L D^{-\\frac{1}{2}} \\cr \u0026=I-D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}} \\end{aligned} $$ The elements are given by: $$ L_{i j}^{s y m}=\\begin{cases} 1 \u0026 \\text { if } i=j \\text { and } d\\left(v_{i}\\right) \\neq 0 \\cr -\\frac{1}{\\sqrt{d\\left(v_{i}\\right) d\\left(v_{j}\\right)}} \u0026 \\text { if } \\lbrace v_{i}, v_{j} \\rbrace \\in E \\text { and } i \\neq j \\cr 0 \u0026 \\text { otherwise. } \\end{cases} $$ ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:1:4","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"5. Random walk nmormalized Laplacian $$ L^{rw} = D^{-1}L = I - D^{-1}A $$ The elements can be computed by: $$ L_{i j}^{r w}= \\begin{cases} 1 \u0026 \\text { if } i=j \\text { and } d\\left(v_{i}\\right) \\neq 0 \\cr -\\frac{1}{d\\left(v_{i}\\right)} \u0026 \\text { if }\\lbrace v_{i}, v_{j}\\rbrace \\in E \\text { and } i \\neq j \\cr 0 \u0026 \\text { otherwise } \\end{cases} $$ ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:1:5","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"6. Incidence Matrix $$ M_{i j}= \\begin{cases} 1 \u0026 \\text { if } \\exists k \\text { s.t } e_{j}=\\lbrace v_{i}, v_{k} \\rbrace \\cr -1 \u0026 \\text { if } \\exists k \\text { s.t } e_{j}=\\lbrace v_{k}, v_{i} \\rbrace \\cr 0 \u0026 \\text { otherwise. } \\end{cases} $$ for undrected graph, the corresponding incidence matrix statisfies that $$ M_{i j}=\\begin{cases} 1 \u0026 \\text { if } \\exists k \\text { s.t } e_{j}=\\lbrace v_{i}, v_{k}\\rbrace \\cr 0 \u0026 \\text { otherwise. } \\end{cases}. $$ ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:1:6","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"Vanilla Graph Neural Networks A node is defined by its features and related nodes in the graph. The aim of GNN is to lean a state embedding $h_v \\in R^s$, which encodes the information of the neighborhood, for each node. The state embedding $h_v$ is used to produce an output $O_v$, such as the distribution of the predicted node lable. ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:2:0","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"Model $$ h_v = f(X_v, X_{co[v]}, h_{ne[v]}, X_{ne[v]}) $$ $$ o_v = g(h_v, X_v) $$ $f$: local transition function, shared amoing all nodes $g$: local output function $X$: the input feature $h$: hidden state $co[v]$: the set of edges connected to node v $ne[v]$: the set of neighbors of node $v$ Now, we have a compact form as $$ H = F(H,X) \\\\ O = G(H, X_N) $$ $F$: the global transition function $G$: the global output funciton GNN use the classic iterative scheme to compute the state $$ H^{t+1} = F(H^t, X) $$ Next question is how to learn parameters of $f$ and $g$. The loss can be written as $$ loss = \\sum_{i = 1}^p(t_i - o_i) $$ where $p$: the number of supervised nodes the state $h_{v}^{t}$ are iteratively updated until a time step $T$. ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:2:1","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"Graph Convolutional Networks Spectral approaches and spatial approaches. Four calssic models (Spectral Network, ChebNet, GCN, and AGCN) ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:3:0","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"Spectral approaches Spectral Network the convolution operation is defined in the Fourier domain by computing the eigendecomposition of the graph Laplacian. The operation can be defined as multiplication of a signal x (a scalar for each node) with a filter $g_{\\theta} = \\text{diag}(\\theta)$: $$ \\mathbf{g_{\\theta}} \\star \\mathbf{x}=\\mathbf{U g}_{\\theta}(\\Lambda) \\mathbf{U}^{T} \\mathbf{x} $$ $\\mathbf{U}$: the matrix of eigenvectors of the normalized graph Laplacian $L = I_N - D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} = \\mathbf{U}\\Lambda\\mathbf{U}^T$ This operation results in potentially intense computations and non-spatially localized filters CHEBNET the operation is $$ \\mathbf{g_\\theta} \\star \\mathbf{x} \\approx \\sum_{k=0}^{K} \\boldsymbol{\\theta}_{k} \\mathbf{T}_{k}(\\tilde{\\mathbf{L}}) \\mathbf{x} $$ $\\tilde{\\mathbf{L}} = \\frac{2}{\\lambda_{max}}L - I_N$ $\\lambda_{max}$: the largest eigenvalue of $L$. $\\theta$: a vector of Chebyshev coefficients $T_{k}(x)$: the Chebyshev ploynomials $T_k(x) = 2xT_{k-1}(x) -T_{k-2}(x)$. $T_0(x) = 1$, $T_1(x) = x$. GCN $$ \\mathbf{g_\\theta^{\\prime}} \\star \\mathbf{x} \\approx \\theta_{0}^{\\prime} \\mathbf{x}+\\theta_{1}^{\\prime}\\left(\\mathbf{L}-\\mathbf{I}_{N}\\right) \\mathbf{x}=\\theta_{0}^{\\prime} \\mathbf{x}-\\theta_{1}^{\\prime} \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{A} \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{x} $$ then, constraining the number of parameters with $\\theta = \\theta_{0}^{\\prime} = - \\theta_{1}^{\\prime}$, get $$ \\mathbf{g_\\theta} \\star \\mathbf{x} \\approx \\theta\\left(\\mathbf{I}_{N}+\\mathbf{D}^{-\\frac{1}{2}} \\mathbf{A} \\mathbf{D}^{-\\frac{1}{2}}\\right) \\mathbf{x} $$ this operator could lead to numberical instabilities and exploding/vanishing gradients. Finally, introudce the renormalization trick: $I_N + \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{A} \\mathbf{D}^{-\\frac{1}{2}} \\rightarrow \\tilde{\\mathbf{D}}^{-\\frac{1}{2}} \\tilde{\\mathbf{A}} \\tilde{\\mathbf{D}}^{-\\frac{1}{2}}$, with $\\tilde{A} = A + I_N$, and $\\tilde{D} = \\sum_j\\tilde{A}_{ij}$. Finally $$ \\mathbf{Z}=\\tilde{\\mathbf{D}}^{-\\frac{1}{2}} \\tilde{\\mathbf{A}} \\tilde{\\mathbf{D}}^{-\\frac{1}{2}} \\mathbf{X} \\Theta $$ $\\Theta \\in \\mathbb{R}^{C \\times F}$: a matrix of filter paramters $\\mathbf{Z} \\in \\mathbb{R}^{N \\times F}$: the convolved signal matrix AGCN Adaptive Graph Convolution Network (AGCN) is proposed to learn the underlying relations. AGCN learns a â€œresidualâ€ graph Laplacian $\\mathbf{L}_{res}$ and add it to the original Lapalcian matrix $$ \\widehat{\\mathbf{L}}=\\mathbf{L}+\\alpha \\mathbf{L}_{r e s} $$ $\\mathbf{L}_{res}$ is computed by learned graph adjacency matrix $\\widehat{\\mathbf{A}}$ $$ \\begin{aligned} \\mathbf{L_{res}} \u0026=\\mathbf{I}-\\widehat{\\mathbf{D}}^{-\\frac{1}{2}} \\widehat{\\mathbf{A}} \\widehat{\\mathbf{D}}^{-\\frac{1}{2}} \\cr \\widehat{\\mathbf{D}} \u0026=\\operatorname{degree}(\\widehat{\\mathbf{A}}) \\end{aligned} $$ $\\widehat{\\mathbf{A}}$ is computed via a learned metric The idea behind the adaptive metric is that Euclidean distance is not suitatble for graph structured data and the metric should be adaptive the the task and input features. ACGN use the generalized Mahalanobis distance $$ D(\\mathbf x_{i}, \\mathbf x_{j})= \\sqrt{ (\\mathbf x_{i}-\\mathbf x_{j})^{T} \\mathbf{M}(\\mathbf x_{i}-\\mathbf x_{j})} $$ where M is a learned prameter taht statisfies $M = W_d W_{d}^T$. ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:3:1","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"Spatial Methods spatial approches defined convolutions directly on the graph, operating on spatially close neighbors. The major challenge of spatial approches is defining the convolution operation with differently sized neighborhoods and maintaining the local invariance of CNNs. Nerual FPS Patchy-SAV DCNN DGCN LGCN MONET GRAPHSAGE ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:3:2","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"Introduction of Graph Neural Networks ","date":"2020-07-25","objectID":"/2020-07-25-dl-geometric/:0:0","tags":["Deep Learning"],"title":"Geometric Deep Learning","uri":"/2020-07-25-dl-geometric/"},{"categories":["Machine Learning"],"content":"Data Eculidean Structure Data: image, video, voice â€¦ easy to find adjacent neighbors easy to define distance Non-Eculidean data: Graph, Manifold hard to define adjacent neighbors or the numbers of adjacent nodes varies. means hard to define distance, convolution â€¦ ","date":"2020-07-25","objectID":"/2020-07-25-dl-geometric/:1:0","tags":["Deep Learning"],"title":"Geometric Deep Learning","uri":"/2020-07-25-dl-geometric/"},{"categories":["Machine Learning"],"content":"Embed (project) Non-Eculidean Data into Eculidean Space using geometric deep learning ","date":"2020-07-25","objectID":"/2020-07-25-dl-geometric/:2:0","tags":["Deep Learning"],"title":"Geometric Deep Learning","uri":"/2020-07-25-dl-geometric/"},{"categories":["Machine Learning"],"content":"Graph Neural Network ","date":"2020-07-25","objectID":"/2020-07-25-dl-geometric/:3:0","tags":["Deep Learning"],"title":"Geometric Deep Learning","uri":"/2020-07-25-dl-geometric/"},{"categories":["Machine Learning"],"content":"Common tasks graph classification: classcify graphs according to its topology. each graph has a label. most common definition: graph $G = (A, F)$ Adjacency matrix (of G): $A \\in \\lbrace 0,1 \\rbrace ^{n \\times n}$ Feature matrix (of nodes): $F \\in R^{n \\times d}$, with n nodes, d features Given $\\mathcal{D} = \\lbrace (G_1, y_1), \\cdots, (G_n, y_n) \\rbrace$, learn $$ f: \\mathcal{G} \\rightarrow \\mathcal{Y} $$ node classification: each node has a label generative graph (models): e.g. virtual drug screen ","date":"2020-07-25","objectID":"/2020-07-25-dl-geometric/:3:1","tags":["Deep Learning"],"title":"Geometric Deep Learning","uri":"/2020-07-25-dl-geometric/"},{"categories":["Machine Learning"],"content":"Algebra Presentation of Graphs Adjacency matrix $$ A_{i j}= \\begin{cases} 1 \u0026 \\text { if }\\lbrace v_{i}, v_{j}\\rbrace \\in E \\text { and } i \\neq j \\cr 0 \u0026 \\text { otherwise } \\end{cases} $$ Degree matrix: D is a diagonal matrix, where $$ D_{ii} = d(v_i) $$ Laplacian matrix: if we consider all edges in graph $G$ to be undirected, then Laplacian matrix $L$ could be defined as $$ L = D-A $$ Thus, we have the elements: $$ L_{i j}=\\begin{cases} d\\left(v_{i}\\right) \u0026 \\text { if } i=j \\cr -1 \u0026 \\text { if }\\lbrace v_{i}, v_{j}\\rbrace \\in E \\text { and } i \\neq j \\cr 0 \u0026 \\text { otherwise. } \\end{cases}. $$ What and why Laplacian matrix Symmetric normalized Laplacian the symmetric normalized Laplacian is define as: $$ \\begin{aligned} L^{sym} \u0026=D^{-\\frac{1}{2}} L D^{-\\frac{1}{2}} \\cr \u0026=I-D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}} \\end{aligned} $$ The elements are given by: $$ L_{i j}^{s y m}=\\begin{cases} 1 \u0026 \\text { if } i=j \\text { and } d\\left(v_{i}\\right) \\neq 0 \\cr -\\frac{1}{\\sqrt{d\\left(v_{i}\\right) d\\left(v_{j}\\right)}} \u0026 \\text { if } \\lbrace v_{i}, v_{j} \\rbrace \\in E \\text { and } i \\neq j \\cr 0 \u0026 \\text { otherwise. } \\end{cases} $$ Random walk nmormalized Laplacian $$ L^{rw} = D^{-1}L = I - D^{-1}A $$ The elements can be computed by: $$ L_{i j}^{r w}= \\begin{cases} 1 \u0026 \\text { if } i=j \\text { and } d\\left(v_{i}\\right) \\neq 0 \\cr -\\frac{1}{d\\left(v_{i}\\right)} \u0026 \\text { if }\\lbrace v_{i}, v_{j}\\rbrace \\in E \\text { and } i \\neq j \\cr 0 \u0026 \\text { otherwise } \\end{cases} $$ Incidence Matrix $$ M_{i j}= \\begin{cases} 1 \u0026 \\text { if } \\exists k \\text { s.t } e_{j}=\\lbrace v_{i}, v_{k} \\rbrace \\cr -1 \u0026 \\text { if } \\exists k \\text { s.t } e_{j}=\\lbrace v_{k}, v_{i} \\rbrace \\cr 0 \u0026 \\text { otherwise. } \\end{cases} $$ for undrected graph, the corresponding incidence matrix statisfies that $$ M_{i j}=\\begin{cases} 1 \u0026 \\text { if } \\exists k \\text { s.t } e_{j}=\\lbrace v_{i}, v_{k}\\rbrace \\cr 0 \u0026 \\text { otherwise. } \\end{cases}. $$ ","date":"2020-07-25","objectID":"/2020-07-25-dl-geometric/:3:2","tags":["Deep Learning"],"title":"Geometric Deep Learning","uri":"/2020-07-25-dl-geometric/"},{"categories":["Machine Learning"],"content":"Convolution on Spectral The key to understand graph convolution: Laplacian matrix and Newton Coolâ€™s Law Now, the convolution operation is defined in the Fourier domain by computing the eigendecomposition of the Laplacian Matrix $$ L = U \\Lambda U^{-1} = U \\Lambda U^T $$ Note: $U$ is an orthognal matrix, $U^{-1} = U^T$ Then, given $x \\in R^n$, the fourier transform: $\\hat{x} = U^{T} x$ reverse fourier transform: $x = U \\hat{x}$ Finally, given signal $x$ and kernel $y$, the graph fourier transform ($*_{\\mathcal{g}}$) is $$ x *_{\\mathcal{G}} y=U\\left(\\left(U^{T} x\\right) \\odot\\left(U^{T} y\\right)\\right) $$ $\\odot$: element-wise multiplication As we have a kernel $g_{\\theta}(\\sdot)$, $$ y=g_{\\theta}(L)(x)=g_{\\theta}\\left(U \\Lambda U^{T}\\right) x=U g_{\\theta}(\\Lambda) U^{T} x $$ where $$ g_{\\theta}(\\Lambda)=\\operatorname{diag}(\\theta)=\\left[\\begin{array}{ccc} \\theta_{1} \u0026 \\cdots \u0026 0 \\cr \\vdots \u0026 \\ddots \u0026 \\vdots \\cr 0 \u0026 \\cdots \u0026 \\theta_{n-1} \\end{array}\\right] $$ The learned parameters are in $\\operatorname{diag}(\\theta)$ The problems: lost local connectivity on space (e.g. CNN on images preserve locality) computational complexity $O(n)$, not well generalized on large scale Graphs Need more knowledge of the Chebyshev ploynomials to get deeper. see my next post about GNN. ","date":"2020-07-25","objectID":"/2020-07-25-dl-geometric/:4:0","tags":["Deep Learning"],"title":"Geometric Deep Learning","uri":"/2020-07-25-dl-geometric/"},{"categories":["Machine Learning"],"content":"Convolution on Spatial Another way to understand graph convolution: Message Passing. ","date":"2020-07-25","objectID":"/2020-07-25-dl-geometric/:5:0","tags":["Deep Learning"],"title":"Geometric Deep Learning","uri":"/2020-07-25-dl-geometric/"},{"categories":["Machine Learning"],"content":"Message passing Message passing: node $\\mathcal{S}_1$ and its neigbor $\\mathcal{N}$ (B1, B2, B3), aggregate $\\mathcal{N}$'s message to $\\mathcal{S}_1$. for example, aggreate (sum) each nodesâ€™s features $H^{(l)} \\in R^d$, $$ \\sum_{u \\in \\mathcal{N}(v)} H^{(l)}(u) \\in \\mathbb{R}^{d_{i}} $$ node $v$'s neigbors: $\\mathcal{N(v)}$, layer: $l$ Generally, we add a linear transform matrix $W^{(l)} \\in R^{d_i \\times d_o}$ to change the feature dimension. $$ \\left(\\sum_{u \\in \\mathcal{N}(v)} H^{(l)}(u)\\right) W^{(l)} \\in \\mathbb{R}^{d_{o}} $$ After add activate function, get a more compact equation $$ f(H^{(l)}, A) = \\sigma ( A H^{(l)}W^{(l)}) $$ $A$: Adjacency Matrix ","date":"2020-07-25","objectID":"/2020-07-25-dl-geometric/:5:1","tags":["Deep Learning"],"title":"Geometric Deep Learning","uri":"/2020-07-25-dl-geometric/"},{"categories":["Machine Learning"],"content":"Example Given graph input feature (10 dim): $f_{in} \\in R^{10}$ output feature (20 dim): $f_{out} \\in R^{20}$ each nodeâ€™s feature: $H^{(l)} \\in R^{6 \\times 10}$ weight: $W^{(l)} \\in R^{10 \\times 20}$ adjcency matrix: $A \\in R^{6 \\times 6}$ Message passing step: feature dimension change: $HW \\in R^{6 \\times 20}$ select the neigborhood nodes: $AHW$ The problems: Each node have different degree, make the scale of output feature map will completely change (see each row of adjcency matrix). So, we have to normalize laplacian matrix. Each node did not include information from itself. So need to make a self connection. ","date":"2020-07-25","objectID":"/2020-07-25-dl-geometric/:5:2","tags":["Deep Learning"],"title":"Geometric Deep Learning","uri":"/2020-07-25-dl-geometric/"},{"categories":["Machine Learning"],"content":"Definition Adjcency Matrix: $$ \\tilde{A} = A + I_n $$ Degree Matrix: $$ \\tilde D_{ii} = \\sum_{j} \\tilde{A}_{ij} $$ Random Walk Normalization of A: make row sum equal to 1 $$ \\tilde{A} = D^{-1}A $$ Symmetric Normalization: used more in practice, more dynamic. $$ A = D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} $$ Laplacian matrix normliaztion: $$ \\begin{aligned} L^{sym} \u0026= D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}} \\cr \u0026= D^{-\\frac{1}{2}}(D-A)D^{-\\frac{1}{2}} \\cr \u0026= I_n - D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} \\end{aligned} $$ Finally, we have $$ H^{(l+1)} = \\sigma ( \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)}) $$ $\\tilde{A} = A+ I_n$ $\\tilde D_ii = \\sum_j \\tilde A_{ij}$ ","date":"2020-07-25","objectID":"/2020-07-25-dl-geometric/:5:3","tags":["Deep Learning"],"title":"Geometric Deep Learning","uri":"/2020-07-25-dl-geometric/"},{"categories":["Coding"],"content":"Get answers for C/C++ within ? s","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"Learn C++11 thread library. Code snippets from Concurrent Programming with C++11 ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:0:0","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"Process vs. Threads ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:1:0","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"Usage Summary A short summary of thread library in STL thread and async /* thread */ std::thread t1(factorial, 6); // create a new thread std::this_thread::sleep_for(chrono::milliseconds(3)); chrono::steady_clock::time_point tp = chrono::steady_clock::now() + chrono::microseconds(4); std::this_thread::sleep_until(tp); /* async() */ std::future\u003cint\u003e fu = async(factorial, 6); // create a new thread mutex /* Mutex */ std::mutex mu; std::lock_guard\u003cmutex\u003e locker(mu); std::unique_lock\u003cmutex\u003e ulocker(mu); ulocker.try_lock(); ulocker.try_lock_for(chrono::nanoseconds(500)); ulocker.try_lock_until(tp); condition variable /* Condition Variable */ std:condition_variable cond; cond.wait_for(ulocker, chrono::microseconds(2)); cond.wait_until(ulocker, tp); future and promise /* Future and Promise */ std::promise\u003cint\u003e p; std::future\u003cint\u003e f = p.get_future(); f.get(); f.wait(); f.wait_for(chrono::milliseconds(2)); f.wait_until(tp); Packaged task /* Packaged Task */ std::packaged_task\u003cint(int)\u003e t(factorial); std::future\u003cint\u003e fu2 = t.get_future(); t(6); ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:2:0","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"Cases ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:3:0","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"thread two way to create new thread std::thread t1(func); std::async(std::launch::async, func); exmample of thread #incldue \u003cthread\u003e void function1() { std::cout \u003c\u003c\"hello\"\u003c\u003cstd::endl; } std::tread t1(function1); // t1 start running // t1.join(); // main thread wait for t1 to finish t1.detach(); // t1 will freely on its own -- deamon process`` /// once detach, forever detach. if (t1.joinable()) t1.join(); // if detached, this line crashed. ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:3:1","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"thread managment class Fctor { public: void operator()(std::string \u0026 s) { std::cout\u003c\u003c\"this is thread\"\u003c\u003cstd::endl; } }; std::string = \"string int\" std::thread t1((Fctor()), s); // alway pass by value std::thread t2((Fctor()), std::ref(s)); // pass by ref std::thread t3((Fctor()), std::move(s)); // move s from main to thread std::thread t4 = std::move(t3); // thread could not be copy, only move try { std::cout\u003c\u003c\"this is main\"\u003c\u003cstd::endl; } catch (...) { t1.join(); t2.jion(); throw; } t1.join(); t2.join(); t4.join(); if oversubscription, limit threads with maximum cpu cores std:ğŸ§µ:hardware_concurrency(); // indication, number of cpu cores ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:3:2","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"race condition and mutex #include \u003cthread\u003e#include \u003cmutex\u003e std::mutex mu; void shared_print(std::string msg, int id) { std::lock_guard\u003cstd::mutex\u003e guard(mu); // RAII //mu.lock(); // +\u003e safeguard no two threads using cout at the same time, std::cout\u003c\u003cmsg \u003c\u003c id \u003c\u003c std::endl; //mu.unlock(); // if error thrown between .lock() and .unlock(), generate zombie process } void function_1() { for(int i=0; i \u003c 100; i++) shared_print(\"from functino t1\", i); } int main() { std::thread t1(function_1); for (int i=0; i \u003c 100; i++) shared_print(\"from main\", i); t1.join() } more practical example #include \u003cthread\u003e#include \u003cmutex\u003e class LogFile { std::mutex mu; std::ofstream f; public: LogFile() { f.open(\"log.txt\"); } void shared_print(std::string msg, int id) { std::lock_guard\u003cstd::mutex\u003e guard(mu); // RAII f \u003c\u003c msg \u003c\u003c id \u003c\u003c std::endl; } // never return f to outside wworld // never pass f as an argument for user }; void function_1(LogFile\u0026 log) { for(int i=0; i \u003c 100; i++) log.shared_print(\"from functino t1\", i); } int main() { LogFile log; std::thread t1(function_1, std::ref(log)); for (int i=0; i \u003c 100; i++) log.shared_print(\"from main\", i); t1.join() } ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:3:3","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"Advoid Deadlock prefer locking single mutex Advoid locking a mutex and then calling a user provded function use std::lock() to lock more than one mutex lock the mutexs in same order. ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:3:4","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"unique_lock and lazy initialization class LogFile { std::mutex mu; std::ofstream f; std::once_flag flag; public: LogFile() { f.open(\"log.txt\"); } void shared_print(std::string msg, int id) { // if you need to check whether a file is open in each call, use once_flag to rescue // std::call_once(flag, [\u0026](){ f.open(\"log.txt\");}) // file only open once std::unique_lock\u003cstd::mutex\u003e locker(mu, std::defer_lock); // note here // do something else locker.lock(); f \u003c\u003c msg \u003c\u003c id \u003c\u003c std::endl; locker.unlock(); // call again locker.lock(); // do something ... locker.unlock(); std::unique_lock\u003cstd::mutex\u003e locker2 = std::move(locker); // could change ownership } }; ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:3:5","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"condition variable Condition variable is to synchronize the execution order of threads std::condition_variable cond; // usage 1 std::unique_lock\u003cstd::mutex\u003e locker(mu); cond.wait(locker); // spurious wake cond.wait(locker, [](){return !q.empty();}) // cond.notify_one(); // notify one waiting thread cond.notify_all(); // ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:3:6","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"future and promise Future and promise provide a convenience way to communicate between threads. e.g. return value to main thread int factorial(int n) { int res = 1; return res+ n; } int x; std::future\u003cint\u003e fu = std::async(function, 4); // future, get something in future x = fu.get(); // fu.get(); //crash std::future\u003cint\u003e fu2 = std::async(std::launch::deferred, factorial, 4); // means not excuate unitl call .get() x = fu2.get(); // only excuate fu2 when called get std::future\u003cint\u003e fu3 = std::async(std::launch::async | std::launch::deferred , factorial, 4); // create new thread by calling async or not x = fu3.get(); // only excuate fu2 when called get usage of promise int factorial(std::future\u003cint\u003e \u0026f) { int res = 1; int N = f.get(); // note here return res + N; } int x; std::promise\u003cint\u003e p; std::future\u003cint\u003e f = p.get_future(); std::future\u003cint\u003e fu4 = std::async(std::launch::async, factorial, std::ref(f)); // do something else ... //// if p not set, throw error // p.set_exception(std::make_exception_ptr)(std::runtime_error(\"To err is human\")); // set p p.set_value(4); // get from child x = fu4.get(); shared_future for multi-threads int factorial(std::shared_future\u003cint\u003e \u0026f) { int res = 1; int N = f.get(); // note here return res + N; } int x; std::promise\u003cint\u003e p; std::future\u003cint\u003e f = p.get_future(); std::shared_future\u003cint\u003e sf = f.shared(); std::future\u003cint\u003e fu5 = std::async(std::launch::async, factorial, sf); std::future\u003cint\u003e fu6 = std::async(std::launch::async, factorial, sf); std::future\u003cint\u003e fu7 = std::async(std::launch::async, factorial, sf); p.set_value(4); // get from child x = fu4.get(); ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:3:7","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"using callable object class A { public: void f(int, char) {} long g(double x) {return 0;} int operator()(int n) {return 0;} }; A a; std::thread t1(a, 6); // copy of a() in a different thread std::thread t2(std::ref(a), 6) // a() in a different thread std::thread t3(A(), 6); // temp A std::thread t4([](int x){return x*x;}, 6); std::thread t5(\u0026A::f, a, 6, 'w'); // copy of a.f(6,'w') in a different thread // these feature could be used in // std::bind, std::async ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:3:8","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"packagee tasks packaged_task provides a way to implement a task pool. It can conveniently convey the returned value from a task to a different thread std::thread t1(factorial, 6); // could pass args std::packaged_task\u003cint(int)\u003e t(factorial); // could not pass additional args std::packaged_task\u003cint()\u003e t2(std::bind(factorial, 6)); // now we could pass args using std::bind // ... t(6); // in a different context, t alwaly return void, so int x = t.get_future().get(); // get value // call t2 by t2(); int factorial(int N) { int res =1; for (int i=N; i \u003e 1; i --) res *= i; return res; } std::deque\u003cstd::packaged_task\u003cint()\u003e\u003e task_q; std::mutex mu; std::condition_variable cond; void thread_1() { std::packaged_task\u003cint()\u003e t; { //std::lock_guard\u003cstd::mutex\u003e locker(mu); // advoid data race std::unique_lock\u003cstd::mutex\u003e locker(mu); cond.wait(locker, [](){return !taks_q.empty();}) t = std::move(task_q.front()); task_q.pop_front(); } t(); } int main() { std::thread t1(thread_1); // so, task_q run in t1; std::packaged_task\u003cint()\u003e t(std::bind(factorical, 6)); std::future\u003cint\u003e fu = t.get_future(); // get returned value to main thread { std::lock_guard\u003cstd::mutex\u003e locker(mu); task_q.push_bask(std::move(t)); } std::cout\u003c\u003cfu.get(); t1.join(); return 0; } Summary: 3 method to get a future promise::get_future() packaged_task::get_future() async() returns a future ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:3:9","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Statistic"],"content":"one-way ANOVA from scratch Calculate the Sum of Squares Total (SST): $$ SS_{total} = \\sum_{j=1}^k \\sum_{i=1}^l (X_{ij} - \\bar{X})^2 $$ Calculate the Sum of Squares Within Groups (SSW): $$ SS_{within} = \\sum_{j=1}^k \\sum_{i=1}^l (X_{ij} - \\bar{X_j})^2 $$ Calculate the Sum of Squares Between Groups (SSB): $$ SS_{between} = \\sum_{j=1}^k n_j ( \\bar X_{j} - \\bar{X}) ^2 $$ $n_j$: numbers of individual point in group j. Verify that $$ SS_{total} = SS_{between} + SS_{within} $$ Calculate the Degrees of Freedom (df) Calculate the Degrees of Freedom Total (DFT) $$ df_{total} = n -1 $$ Calculate the Degrees Between k Groups (DFB) $$ df_{bewteen} = k -1 $$ Calculate the Degrees of Freedom Within Groups (DFW) $$ df_{within} = n - k $$ Verify that $$ df_t = df_w + df_b $$ Calculate the Mean Squares Calculate the Mean Squares Between (MSB) $$ MS_{between} = \\frac{SS_{between}}{df_{between}} $$ Calculate the Mean Squares Within (MSW) $$ MS_{within} = \\frac{SS_{within}}{df_{within}} $$ Calculate the F Statistic $$ F = \\frac{ MS_{between}}{MS_{within}} $$ get pvalue import scipy.stats as stat pvalue = stat.f.sf(F, dfb, dfw) # sf: pvalue = 1 - stat.f.cdf() ","date":"2020-06-30","objectID":"/2020-06-30-stats-anova/:1:0","tags":null,"title":"ANOVA","uri":"/2020-06-30-stats-anova/"},{"categories":["Statistic"],"content":"ANOVA Effect size Omega squared (Ï‰2) is a measure of effect size, or the degree of association for a population. It is an estimate of how much variance in the response variables are accounted for by the explanatory variables. Omega squared is widely viewed as a lesser biased alternative to eta-squared, especially when sample sizes are small. MSerror: mean square error SSE/df(error) Formula $$ \\omega^2 = \\frac {SS_{Effect} - df_{Effect} MS_{error}}{SS_{total} + MS_{error}} $$ for multi-factor, completely randomized design, Formula $$ \\omega^2 = \\frac {SS_{Effect} - df_{Effect} MS_{errors}} {SS_{Effect} + (N-df_{Effect}) MS_{error}} $$ Interpreting Results Ï‰2 can have values between Â± 1. Zero indicates no effect. If the observed F is less than one, Ï‰2 will be negative. ","date":"2020-06-30","objectID":"/2020-06-30-stats-anova/:2:0","tags":null,"title":"ANOVA","uri":"/2020-06-30-stats-anova/"},{"categories":["Statistic"],"content":"ANOVA Post-hoc comparison ANOVA does not tell which group are significantly different from each other. To know the pairs of significant different groups, we could perform multiple pairwise comparison (Post-hoc comparison) analysis using Tukey HSD test. from statsmodels.stats.multicomp import pairwise_tukeyhsd # perform multiple pairwise comparison (Tukey HSD) m_comp = pairwise_tukeyhsd(endog=d_melt['value'], groups=d_melt['treatments'], alpha=0.05) ","date":"2020-06-30","objectID":"/2020-06-30-stats-anova/:3:0","tags":null,"title":"ANOVA","uri":"/2020-06-30-stats-anova/"},{"categories":["Statistic"],"content":"Two-way (two factor) ANOVA example # load packages import statsmodels.api as sm from statsmodels.formula.api import ols # Ordinary Least Squares (OLS) model # C(): as categorical # C(Genotype):C(years) represent interaction term model = ols('value ~ C(Genotype) + C(years) + C(Genotype):C(years)', data=d_melt).fit() anova_table = sm.stats.anova_lm(model, typ=2) anova_table see more about ANOVA in python here ","date":"2020-06-30","objectID":"/2020-06-30-stats-anova/:4:0","tags":null,"title":"ANOVA","uri":"/2020-06-30-stats-anova/"},{"categories":["Statistic"],"content":"Advanced ","date":"2020-06-30","objectID":"/2020-06-30-stats-anova/:5:0","tags":null,"title":"ANOVA","uri":"/2020-06-30-stats-anova/"},{"categories":["Statistic"],"content":"One way Anova is a multiple regression model $$ y = \\beta_{0} + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\cdots, H_0 : y = \\beta_0 $$ $x_i$ï¿¼ are indicators ( $x = \\lbraceï¿¼ 0,1\\rbrace$), where at most one $x_i = 1$ while all other $x_i = 0$. The Kruskal-wallis test (non-parametric test) is simply a one-way ANOVA on the rank-transformed y (value). $$ rank(y) = \\beta_{0} + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\cdots $$ ","date":"2020-06-30","objectID":"/2020-06-30-stats-anova/:5:1","tags":null,"title":"ANOVA","uri":"/2020-06-30-stats-anova/"},{"categories":["Statistic"],"content":"ANCOVA This is simply ANOVA with a continuous regressor added so that it now contains continuous and (dummy-coded) categorical predictors. $$ y = \\beta_{0} + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_n age $$ $\\beta_0$ is now the mean for the first group at age=0. ","date":"2020-06-30","objectID":"/2020-06-30-stats-anova/:5:2","tags":null,"title":"ANOVA","uri":"/2020-06-30-stats-anova/"},{"categories":["Statistic"],"content":"Reference See: Common statistical tests are linear models (or: how to teach stats) R version by Jonas Kristoffer LindelÃ¸v Python port by George Ho ","date":"2020-06-30","objectID":"/2020-06-30-stats-anova/:5:3","tags":null,"title":"ANOVA","uri":"/2020-06-30-stats-anova/"},{"categories":["Statistic"],"content":"Censoring Censoring Surivial without Censoring Surivial with Censoring ","date":"2020-06-22","objectID":"/2020-06-22-stats-survival-analysis/:1:0","tags":null,"title":"Survival Analysis","uri":"/2020-06-22-stats-survival-analysis/"},{"categories":["Statistic"],"content":"Kaplan Meier Curve More individual in each group, better sepration of the group, better p-value Takes censoring into account Estimates probabilitu of â€œsurvivalâ€ on a given day Conditional probability of surviving on a given day: $$ \\frac {N_{ \\text{â€œaliveâ€ day before}} - N_{ \\text{â€œdyingâ€ nextday}}} { \\text{â€œaliveâ€ day before}} $$ Kaplan-Meier survival curve Survival times $t_1 \\leq t_2 \\leq \\cdots \\leq t_n$ The proportion of subjects, $S(t)$, surviving beyoind any follow up time $t$ is estimated by (conditional probability): $$ S(t) = \\frac {r_1 - d_1}{r_1} \\times \\frac {r_2 - d_2}{r_2} \\times \\cdots \\times \\frac{r_p - d_p}{r_p} $$ where $t_p$ is the largest survival time less han or equal to $t$ $r_i$ is the number of subjects alive just before time $t_i$ $d_i$ = numebr who died at time $t_i$ for censored obeservations $d_i = 0$ ","date":"2020-06-22","objectID":"/2020-06-22-stats-survival-analysis/:2:0","tags":null,"title":"Survival Analysis","uri":"/2020-06-22-stats-survival-analysis/"},{"categories":["Statistic"],"content":"Statistic ","date":"2020-06-22","objectID":"/2020-06-22-stats-survival-analysis/:3:0","tags":null,"title":"Survival Analysis","uri":"/2020-06-22-stats-survival-analysis/"},{"categories":["Statistic"],"content":"Log Rank Test Compares survival times of two independent groups. Assumes that the relative risk of event (e.g. death) between the two groups is constant (proportional hazards) Ranks the survial times combined and compared observed and expected rates Null hypothesis: the rates of events (death) in the two groups are equal under $H_0$, $$ X^2 = \\frac { (O_A - E_A)^2}{E_A} + \\frac { (O_B - E_B)^2}{E_B} \\sim \\chi^2 $$ $O_A$: observed events in group A $E_A$: expected events in gorup A under null hypohesis expect = (proportion in risk set) * (# of failures over both groups) $$ e_{1j} = ( \\frac{ n_{1j}}{ n_{1j} + n_{2j}}) \\times ( m_{1j} + m_{2j}) $$ $$ e_{2j} = ( \\frac{ n_{2j}}{ n_{1j} + n_{2j}}) \\times ( m_{1j} + m_{2j}) $$ ","date":"2020-06-22","objectID":"/2020-06-22-stats-survival-analysis/:3:1","tags":null,"title":"Survival Analysis","uri":"/2020-06-22-stats-survival-analysis/"},{"categories":["Statistic"],"content":"Cox Regression Extends comparison of survial times to allow different predictors (estimate k variable together) Models the hazard: probability of dying at a point in time, given survival to that point in time $$ H(t) = H_0(t) \\times \\exp(b_1X_1 + b_2X_2 + \\cdots + b_kX_k ) $$ Model links to a baseline hazard, $H_0(t)$ Can accomodate many variables, both discrete and continuous measures of event times Proportional hazards assumption: the hazard for any individual is a fixed proportion of the hazard for any other individual Hazard ratio Exp(B) give the hazard ratio (or relative hazard/risk) ","date":"2020-06-22","objectID":"/2020-06-22-stats-survival-analysis/:3:2","tags":null,"title":"Survival Analysis","uri":"/2020-06-22-stats-survival-analysis/"},{"categories":["Nature Language Processing"],"content":"Basic concepts in NLP ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:0:0","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"NLP ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:0","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Languwage model MOdels that assigns probabilities to sequences of words are called languwage models. ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:1","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Count-based Representation one-hot representation BoW: Bag of words Bow describes the occurrence of words within a document. including A Vocabulary of known words A measure of the presence of known words, e.g. count TF or TF-IDF representation: Term Frequency Inverse Document Frequency TF: the sum of the one-hot representation of a phrase, sentence or documentâ€™s constituent words $$ TF (w) = \\frac { \\text{ Number of the term w appears in the document }} { \\text{Number of terms in the document}} $$ IDF: penalizes common tokens and rewards rare tokens $$ IDF(w) = \\log \\frac{\\text{Number of documents}}{\\text{Number of documents with term w}} $$ TF-IDF: $TF(w) \\times IDF(w)$ ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:2","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Corpora, Tokens, and Types corpus (plural: corpora): a text dataset tokens (English): words and numeric sequences separated by white-spaces characters or punctuation instance or data point: the text along with its metatdata dataset: a collection of instances types: unique tokens present in a corpus. vocabulary or lexicon: the set of all types in a corpus ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:3","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Unigram, Bigrams, Trigrams, â€¦ , N-grams N-grams are fixed-length consecutive token sequence occurring in the text ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:4","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Lemmas and Stems Lemmas are the root forms of words. e.g. the root form of the word fly, can be inflected into other words â€“ flow, flew, flies, flown, flowing â€¦ Stemming: use handcrafted rules to strip endings of words to reduce them to a common form called stems ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:5","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Word Senses and Semantics Senses: the different meanings of a word ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:6","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Categorizing Words: POS Tagging part-of-speech (POS) tagging: labeling individual words or tokens ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:7","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Categorizing Spans: Chunking and Named Entity Recognition a span of text: a contiguous multi-token boundary. chunking or shallow parsing: identify the noun phrases (NP) and verb pharses (VP) in a span of text. A named entity is a string mention of a real-world concept like a person, location, organization, drug name, et. al. ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:8","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Coreference The task of deciding whether two strings refer to same entity ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:9","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Minimumn Edit distance A way to quantify string similarity. ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:10","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Perplexity The perplexity (sometimes called PP) of a language model on a test set is the inverse probability of the test set, normalised by the number of words. $$\\begin{aligned} \\mathrm{PP}(W) \u0026=P(w_{1} w_{2} \\ldots w_{N})^{-\\frac{1}{N}} \\cr \u0026=\\sqrt[N]{\\frac{1}{P(w_{1} w_{2} \\ldots w_{N})}} \\cr \u0026= \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_{i} \\mid w_{1} \\ldots w_{i-1})}} \\end{aligned} $$ Another way to hink about perplexity: as the weighted average branching factor of a language. ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:11","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Entropy Entropy is a measure of information. The entropy of the random variable X is: $$ H(X)=-\\sum_{x \\in \\chi} p(x) \\log _{2} p(x) $$ the log can be computed in any base. If we use log base 2, the resulting value of entropy will mesured in bits. One intuitive way to think about entorpy is as a lower bound on the number of bits it would take to encode a certain desision or piece of information in the optimal coding scheme. ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:12","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Cross-entropy The cross-entropy is useful when we donâ€™t know the actual probability distribution p that generated some data. The cross-entropy of m (a model of p) on p is defined by $$ H(p,m) = \\lim_{n \\rightarrow \\infty} - \\frac{1}{n}\\sum_{W \\in L} p (w_1,\\cdots,w_n) \\log m (w_1, cdots, w_n) $$ the cross-entropy $H(p,m)$ is an upper bound on the entropy $H(p)$. For any model m: $$ H(p) \\leq H(p,m) $$ The more accurate m is, the closer the cross-entropy $H(p,m)$ will be to the true entropy $H(p)$. ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:13","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"The relation of perplexity and cross-entropy The approximation to the cross-entropy of a model $M = P(w_i | W_{i-N+1} \\cdots W_{i-1})$ on a sequence of words W is $$ H(W) = - \\frac{1}{N} \\log P(w_1 w_2 \\cdots w_N) $$ The perplexity of a model P on a seqence of words W is defined as exp of this cross-entropy $$ \\begin{aligned} \\operatorname{Perplexity}(W) \u0026=2^{H(W)} \\cr \u0026=P\\left(w_{1} w_{2} \\ldots w_{N}\\right)^{-\\frac{1}{N}} \\cr \u0026=\\sqrt[N]{\\frac{1}{P\\left(w_{1} w_{2} \\ldots w_{N}\\right)}} \\cr \u0026=\\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P\\left(w_{i} \\mid w_{1} \\ldots w_{i-1}\\right)}} \\end{aligned} $$ ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:14","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Structure of Sentences: Parse trees Shallow parsing identifies phrasal units, the task of identifying the relationship between them is called parsing. Parse trees indicate how different grammatical units in a sentence are related hierachically. (aslo refer to constituent parse, chart-based ) dependency parsing: directed graph (graph-based) node -\u003e word edge -\u003e relation all the words have one incoming edge, except ROOT there is a unique path from each word to ROOT maximum spanning tree SyntaxNet: ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:15","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"statistical testing The approach to computing p-values(x) in NLP is to use non-parametric tests. e.g. bootstrap test approximate randomization bootstrapping refers to repeated drawing large numbers of smaller samples with replacement from an orignial larger sample. the intuition of the bootstrap test is that we can create many virtual test sets from an observed test set by repeated sampling from it. the method only maks the assumption that sample is representative of the population ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:2:0","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Reference Hung-yi Lee: Deep Learning for human language processing ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:3:0","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Statistic"],"content":"æ ·æœ¬é‡ã€æ•ˆåº”é‡ã€æ˜¾è‘—æ°´å¹³å’Œç»Ÿè®¡åŠŸæ•ˆçš„ç»Ÿè®¡åŸç†å’Œè®¡ç®— æ•ˆåº”é‡é€šå¸¸ç”¨ä¸‰ç§æ–¹å¼æ¥è¡¡é‡ï¼šæ ‡å‡†å‡å·®ï¼ˆstandardized mean differenceï¼‰ï¼Œå‡ ç‡ï¼ˆodd ratioï¼‰ï¼Œ(3) ç›¸å…³ç³»æ•°ï¼ˆcorrelation coefficientï¼‰ã€‚ ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:0:0","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"ç»Ÿè®¡åŸç† å¯è§†åŒ–æ ·æœ¬é‡ã€æ•ˆåº”é‡ã€Î±å’Œç»Ÿè®¡åŠŸæ•ˆçš„å…³ç³» Significance The probability of a type I error is usually denoted by $\\alpha$ and is commonly referred to as the signiï¬cance level of a test. \\The probability of a type II error is usually denoted by $\\beta$. ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:1:0","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"æ˜¾è‘—æ°´å¹³ Î± Type Error å®šä¹‰ è¡¨ç¤º ä¸¾ä¾‹ â…  å‹é”™è¯¯ æ‹’ç»å®é™…ä¸Šæˆç«‹çš„$H_0$ â…  å‹é”™è¯¯çš„æ¦‚ç‡ç”¨æ˜¾è‘—æ°´å¹³ Î± è¡¨ç¤º, å‡é˜³æ€§ã€è¯¯è¯Š II å‹é”™è¯¯ æ‹’ç»ï¼ˆâ€œæ¥å—â€ï¼‰å®é™…ä¸Šä¸æˆç«‹çš„$H_0$ II å‹é”™è¯¯æ¦‚ç‡ç”¨ Î² è¡¨ç¤º å‡é˜´æ€§ã€æ¼è¯Š Î± å¸¸å–å€¼0.05ã€0.01ï¼ŒÎ±å¯ä»¥å–å•å°¾ã€åŒå°¾ã€‚éœ€å‡è®¾æ£€éªŒå‰é¢„å…ˆè®¾å®šã€‚ Î² åªå–å•å°¾ å½¢è±¡åŒ–ç†è§£ ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:1:1","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"åŠŸæ•ˆï¼ˆpowerï¼‰ åŠŸæ•ˆï¼ˆpowerï¼‰ï¼šæ­£ç¡®æ‹’ç»åŸå‡è®¾çš„æ¦‚ç‡ï¼Œè®°ä½œ1-Î², å³ $$ 1 - \\beta = \\operatorname{Pr} ( \\text{rejecting } H_0 | H_1 true) $$ å‡è®¾æ£€éªŒçš„åŠŸæ•ˆå—ä»¥ä¸‹ä¸‰ä¸ªå› ç´ å½±å“ï¼š æ ·æœ¬é‡ (n)ï¼šå…¶ä»–æ¡ä»¶ä¿æŒä¸å˜ï¼Œæ ·æœ¬é‡è¶Šå¤§ï¼ŒåŠŸæ•ˆå°±è¶Šå¤§ã€‚ æ˜¾è‘—æ€§æ°´å¹³ (Î±)ï¼š å…¶ä»–æ¡ä»¶ä¿æŒä¸å˜ï¼Œæ˜¾è‘—æ€§æ°´å¹³è¶Šä½ï¼ŒåŠŸæ•ˆå°±è¶Šå°ã€‚ ä¸¤æ€»ä½“ä¹‹é—´çš„å·®å¼‚ï¼šå…¶ä»–æ¡ä»¶ä¿æŒä¸å˜ï¼Œæ€»ä½“å‚æ•°çš„çœŸå®å€¼å’Œä¼°è®¡å€¼ä¹‹é—´çš„å·®å¼‚è¶Šå¤§ï¼ŒåŠŸæ•ˆå°±è¶Šå¤§ã€‚ä¹Ÿå¯ä»¥è¯´ï¼Œæ•ˆåº”é‡ï¼ˆeffect sizeï¼‰è¶Šå¤§ï¼ŒåŠŸæ•ˆå°±è¶Šå¤§ã€‚ ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:1:2","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"æ•ˆåº”é‡ï¼ˆeffect sizeï¼‰ æ•ˆåº”é‡ï¼š æ ·æœ¬é—´å·®å¼‚æˆ–ç›¸å…³ç¨‹åº¦çš„é‡åŒ–æŒ‡æ ‡ã€‚æ•ˆåº”é‡è¶Šå¤§ï¼Œè¡¨ç¤ºä¸¤ä¸ªæ€»ä½“é‡å çš„ç¨‹åº¦è¶Šå°ï¼Œæ•ˆåº”è¶Šæ˜æ˜¾ã€‚ æ•ˆåº”é‡é€šå¸¸ç”¨ä¸‰ç§æ–¹å¼æ¥è¡¡é‡ï¼š(1) æ ‡å‡†å‡å·®ï¼ˆstandardized mean differenceï¼‰ï¼Œ(2) å‡ ç‡ï¼ˆodd ratioï¼‰ï¼Œ(3) ç›¸å…³ç³»æ•°ï¼ˆcorrelation coefficientï¼‰ã€‚ Difference family: Effect sizes based on differences between means æ ‡å‡†å‡å·®ï¼ˆstandardized mean differenceï¼‰ Standardized mean difference: åŸºäºæ€»ä½“å‡å€¼å’Œæ–¹å·®ï¼Œ æ•ˆåº”é‡ä¸º $$ \\theta = \\frac{\\mu_1 - \\mu_2}{\\sigma} $$ Cohenâ€™s d : ä¸¤æ€»ä½“å‡å€¼ä¹‹é—´çš„æ ‡å‡†å·®å¼‚ã€‚é€‚ç”¨äºä¸¤ç»„æ ·æœ¬çš„æ ·æœ¬é‡å’Œæ–¹å·®ç›¸ä¼¼çš„æƒ…å†µã€‚ $$ d = \\frac{ \\bar{x}_1 - \\bar{x}_2}{s} = \\frac{\\mu_1 - \\mu_2}{\\sigma} $$ sæ˜¯æ ·æœ¬æ–¹å·® $$ s = \\sqrt{\\frac{(n_1 - 1)s^2_1 + (n_2 - 1)s^2_2}{n_1 + n_2}} $$ $$ s_1 = \\frac{1}{n_1 -1} \\sum_{i=1}^{n_1} (x_{1,i} - \\bar{x}_1)^2 $$ d = 0.01 to 2.0 Effect size d Reference Very small 0.01 Sawilowsky, 2009 Small 0.20 Cohen, 1988 Medium 0.50 Cohen, 1988 Large 0.80 Cohen, 1988 Very large 1.20 Sawilowsky, 2009 Huge 2.0 Sawilowsky, 2009 Hedgesâ€™ g: æ˜¯cohençš„æ–¹æ³•çš„æ”¹è¿›ï¼Œé€‚ç”¨äºä¸¤ç»„æ ·æœ¬çš„æ ·æœ¬é‡ä¸åŒçš„æƒ…å†µã€‚ $$ g = \\frac{ \\bar{x}_1 - \\bar{x}_2}{s^*} $$ è€Œ$s^{*}$æ˜¯ $$ s^* = \\sqrt{\\frac{(n_1 - 1)s^2_1 + (n_2 - 1)s^2_2}{n_1 + n_2 -2 }} $$ Glassâ€™s Î” ï¼ˆdeltaï¼‰: å’Œcohençš„æ–¹æ³•ç±»ä¼¼ï¼Œä½†æ˜¯åªé™¤ä»¥æ§åˆ¶ç»„(control)çš„æ ‡å‡†å·®ã€‚é€‚ç”¨äºä¸¤ç»„æ ·æœ¬çš„æ–¹å·®ä¸åŒçš„æƒ…å†µã€‚ $$ \\Delta = \\frac{ \\bar{x}_1 - \\bar{x}_2}{s_2} $$ Categorical family: Effect sizes for associations among categorical variables Odd ratio (OR) The odds ratio is the odds of success in the treatment group relative to the odds of success in the control group. é€‚ç”¨äºbinaryæ•°æ®ã€‚ Relative risk (RR) or risk ratio the risk (probability) of an event relative to some independent variable. Risk difference or absolute risk reduction the difference in risk (probability) of an event between two groups Cramerâ€™s Ï† (Phi) or Cramerâ€™s V: ç”¨äºæµ‹ç®—ç±»åˆ«å‹æ•°æ® (nominal data) çš„æ•ˆåº”é‡ã€‚å½“ç±»åˆ«å‹å˜é‡åŒ…å«2ä¸ªç±»åˆ«æ—¶ï¼Œä½¿ç”¨Cramerâ€™s phiï¼Œå¦‚æœè¶…è¿‡2ä¸ªç±»åˆ«ï¼Œé‚£ä¹ˆä½¿ç”¨Cramerâ€™s Vã€‚ Cohenâ€™s w â€¦ Correlation family: Effect sizes based on â€œvariance explainedâ€ Pearson r correlation Effect size r small ~ 0.1 medium ~ 0.3 large r \u003e 0.5 Cohenâ€™s $f^2$: ç”¨äºæµ‹ç®—æ–¹å·®åˆ†æANOVAï¼Œå¤šå…ƒå›å½’ä¹‹ç±»çš„æ•ˆåº”é‡ã€‚ å¤šå…ƒå›å½’çš„æ•ˆåº”é‡ $$ f^2 = \\frac{R^2}{1-R^2} $$ where $R^2$ is the squared multiple correlation (Coefficient of determination) ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:1:3","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"åŠŸæ•ˆã€æ•ˆåº”é‡å’Œæ ·æœ¬é‡è®¡ç®— ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:2:0","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"è®¡ç®—æ ·æœ¬é‡ determining-sample-size ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:2:1","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"è®¡ç®—æ•ˆåº”é‡ ç”¨statsmodelsåº“è®¡ç®—åŠŸæ•ˆï¼Œæ•ˆåº”é‡å’Œæ ·æœ¬é‡çš„å‡½æ•°éƒ½æ˜¯åŒä¸€ä¸ªï¼Œåªè¦æŠŠéœ€è¦è®¡ç®—çš„é‚£ä¸ªå€¼ä»ç„¶è®¾ä¸ºNoneï¼ŒæŠŠå…¶ä»–æƒ³è¦è¾¾åˆ°çš„æ•°å€¼å¡«ä¸Šå³å¯ å•æ ·æœ¬tæ£€éªŒï¼š statsmodels.stats.power.tt_solve_power(effect_size=None, nobs=None, alpha=None, power=None, alternative='two-sided') ç‹¬ç«‹æ ·æœ¬tæ£€éªŒï¼š statsmodels.stats.power.tt_ind_solve_power(effect_size=None, nobs1=None, alpha=None, power=None, ratio=1.0, alternative='two-sided') å¡æ–¹æ‹Ÿåˆä¼˜åº¦æ£€éªŒï¼š statsmodels.stats.power.GofChisquarePower.solve_power(effect_size=None, nobs=None, alpha=None, power=None, n_bins=2) Fæ–¹å·®é½æ€§æ£€éªŒï¼š statsmodels.stats.power.FTestPower.solve_power(effect_size=None, df_num=None, df_denom=None, nobs=None, alpha=None, power=None, ncc=1) æ–¹å·®åˆ†æï¼š statsmodels.stats.power.FTestAnovaPower.solve_power(effect_size=None, nobs=None, alpha=None, power=None, k_groups=2) ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:2:2","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"StatQuest Sample size Power Analysis å‚è€ƒï¼š https://en.wikipedia.org/wiki/Effect_size https://www.cnblogs.com/HuZihu/p/12009535.html ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:3:0","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"Quantile normalization is frequently used in microarray data analysis. It was introduced as quantile standardization and then renamed as quantile normalization. ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:0:0","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"Quantile, quartile, percentile ??? Quantiles are just the lines that divide data into equally sized groups. percentiles are just quantiles that divide the data into 100 equally sized groups Example: 0 quartile = 0 quantile = 0th percentile 1 quartile = 0.25 quantile = 25th percentile 2 quartile = .5 quantile = 50th percentile (median) 3 quartile = .75 quantile = 75th percentile 4 quartile = 1 quantile = 100th percentile ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:1:0","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"Quantile normalization Quantile normalization transform the statistical distributions across samples to be the same. ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:2:0","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"Assumptions The roughly same distribution of values across samples Most genes are not differentially expressed Assume global differences in the distribution are induced by only technical variation! ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:2:1","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"How Q-normalization work row: genes column: samples/Arrays Procedure: order values within each sample determine a rank from lowest to highest and record the order within each sample Average across rows and substitute value with average re-order averaged values in the original order recorded in 2. Tied rank entries ? Average the tied rank entriesâ€™ mean values and substitute. ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:2:2","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"When NOT to normalize Consider a dilution experiment. In which distributions are supposed to decrease (left plot), Q-normalization does the totally wrong thing (right plot). When you expect a real difference in distributions, Q-normalization will create weird artifacts. ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:2:3","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"Smooth quantile ormalizaiton ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:3:0","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"Assumptions the statistical distribution of each sample should be the same ( or have the same distributional shape) within biological groups or conditions, but allowing that they may differ between groups ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:3:1","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"How to At each quantile, a weight is computed comparing the variability between groups relative to the total variability between and within groups Let gene(g) denote the $g^{th}$ row after sorting each column in the data. For each row, gene(g), we compute the weight $w(g)$ âˆˆ [0,1], where a weight of 0 implies quantile normalization within groups is applied and a weight of 1 indicates quantile normalization is applied. The weight at each row depends on the between group sum of squares SSB(g) and total sum of squares SST(g), as follows: $$ w_{(g)} = \\operatorname{median} \\bigg\\lbrace 1- \\frac{SSB_{(i)}}{SST_{(i)}} \\bigg\\rbrace \\text{for } i = g -k, \\cdots, g, \\cdots, g+k $$ where $k$ = floor(Total number of genes * 0.05). The number 0.05 is a flexible parameter that can be altered to change the window of the number of genes considered. ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:3:2","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"StatQuest: Quantile Normalization ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:4:0","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"reference https://en.wikipedia.org/wiki/Quantile_normalization BIOMEDIN 245: Statistical and Machine Learning Methods for Genomics, Stanford Hicks SC, Okrah K, Paulson JN, Quackenbush J, Irizarry RA, Bravo HC. Smooth quantile normalization. Biostatistics. 2018;19(2):185â€198. doi:10.1093/biostatistics/kxx028 ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:5:0","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"Whatâ€™s Multilevel models, and how to deal with it ","date":"2020-06-13","objectID":"/2020-06-13-stats-hierachial-models/:0:0","tags":null,"title":"Multilevel (Hierachical) Models","uri":"/2020-06-13-stats-hierachial-models/"},{"categories":["Statistic"],"content":"What is multilevel model Multilevel model AKA: multilevel Models random-effects models hierarchical models variance-components models random-coefficient models mixed models Many kinds of data, including observational data collected in the human and biological sciences, have a hierarchical or clustered structure, or non-hierarchical structures.. ","date":"2020-06-13","objectID":"/2020-06-13-stats-hierachial-models/:1:0","tags":null,"title":"Multilevel (Hierachical) Models","uri":"/2020-06-13-stats-hierachial-models/"},{"categories":["Statistic"],"content":"A Simple Example Given a set of repeated measures data giving growth patterns for a sample of 26 boys in Oxford, England. The height of each boy is measured on nine different occasions. We could try modelling the growth pattern with a simple linear regression $$ H = \\beta_0 + \\beta_1 A + \\epsilon $$ where H: height A: age $\\epsilon$: the variation in height that cannot be explained by the linear relationship with age. However, If we try to use the model above for the complete set of data, the fit will be very poor (see figure above) To make model more realistic, we allow the intercept in the model above to vary from subject to subject. New multilevel model: $$ H_{ij} = \\beta_{0j} + \\beta_1 A_{ji} + \\epsilon_{ij} $$ Now, assume that the individual intercepts follow a normal distribution with variance $\\tau_{0}$, $$ \\beta_{0j} = \\beta_{0} + \\mu_{0j} $$ where, $\\mu_{0j} \\sim \\mathcal{N} (0, \\tau_0)$, $\\mu_{0j}$ accounts for the variation from one subject to another Fitting the multilevel model to the data, and obtain much better predictions ","date":"2020-06-13","objectID":"/2020-06-13-stats-hierachial-models/:2:0","tags":null,"title":"Multilevel (Hierachical) Models","uri":"/2020-06-13-stats-hierachial-models/"},{"categories":["Statistic"],"content":"How do multilevel models differ from regression models? $$ H_{ij} = (\\beta_0 + \\mu_{0j}) + \\beta_1 A_{ij} + \\epsilon_{ij} = \\beta_0 + \\beta_1 A_{ij} + \\mu_{0j} + \\epsilon_{ij} $$ The feature that distinguishes this model from an ordinary regression model is the presence of two random variables the measurement level random variable $\\epsilon_{ij}$ the subject level random variable $\\mu_{0j}$ Because multilevel models contain a mix of fixed effects and random effects, they are sometimes known as mixed-effects models. ","date":"2020-06-13","objectID":"/2020-06-13-stats-hierachial-models/:3:0","tags":null,"title":"Multilevel (Hierachical) Models","uri":"/2020-06-13-stats-hierachial-models/"},{"categories":["Statistic"],"content":"Benefits of multilevel modelling Generalize to a wider population Fewer parameters are needed Information can be shared between groups ","date":"2020-06-13","objectID":"/2020-06-13-stats-hierachial-models/:4:0","tags":null,"title":"Multilevel (Hierachical) Models","uri":"/2020-06-13-stats-hierachial-models/"},{"categories":["Statistic"],"content":"How to deal with hierachical structures Hierarchical structures : model all levels simultaneously Non- Hierarchical structures cross-classified structure multiple membership with weights â€¦ å‚è€ƒï¼š https://en.wikipedia.org/wiki/Multilevel_model http://www.statstutor.ac.uk/resources/uploaded/multilevelmodelling.pdf https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/hierarchical-models.pdf ","date":"2020-06-13","objectID":"/2020-06-13-stats-hierachial-models/:5:0","tags":null,"title":"Multilevel (Hierachical) Models","uri":"/2020-06-13-stats-hierachial-models/"},{"categories":["Machine Learning"],"content":"è’™ç‰¹å¡ç½—æ–¹æ³•ï¼Œåˆç§°ç»Ÿè®¡æ¨¡æ‹Ÿæ–¹æ³•(statistical simulation method), é€šè¿‡æ¦‚ç‡æ¨¡å‹çš„éšæœºæŠ½æ ·è¿›è¡Œè¿›è¡Œè¿‘ä¼¼æ•°å€¼è®¡ç®—çš„æ–¹æ³•ã€‚ é©¬å¯å¤«è’™ç‰¹å¡ç½—æ³•ï¼ˆMarkov Chain Monte Carlo, MCMCï¼‰åˆ™æ˜¯ä»¥é©¬å¯å¤«é“¾ä¸ºæ¦‚ç‡æ¨¡å‹çš„è’™ç‰¹å¡ç½—æ–¹æ³•ã€‚ Metropolis-Hastingsç®—æ³•æ˜¯æœ€åŸºæœ¬çš„MCMCã€‚ Gibbs samplingæ˜¯æ›´ç®€å•ã€ä½¿ç”¨æ›´å¹¿æ³›çš„MCMCã€‚ Markov Chain Monte Carlo (MCMC) ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:0:0","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"è’™ç‰¹å¡ç½—æ³•ï¼ˆMonte Carloï¼‰ è’™ç‰¹å¡ç½—æ³•è¦è§£å†³çš„é—®é¢˜æ˜¯ï¼Œå‡è®¾æ¦‚ç‡åˆ†å¸ƒçš„å®šä¹‰å·±çŸ¥ï¼Œé€šè¿‡æŠ½æ ·è·å¾—æ¦‚ç‡åˆ†å¸ƒçš„éšæœºæ ·æœ¬ï¼Œå¹¶é€šè¿‡å¾—åˆ°çš„éšæœºæ ·æœ¬å¯¹æ¦‚ç‡åˆ†å¸ƒçš„ç‰¹å¾è¿›è¡Œåˆ†æ ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:1:0","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"1. è’™ç‰¹å¡ç½—æ–¹æ³•çš„æ ¸å¿ƒ è’™ç‰¹å¡ç½—æ–¹æ³•çš„æ ¸å¿ƒæ˜¯éšæœºæŠ½æ ·(random sampling) ç›´æ¥æŠ½æ · æ¥å—-æ‹’ç»æŠ½æ ·ï¼š é€‚ç”¨äºæ¦‚ç‡å¯†åº¦å‡½æ•°å¤æ‚ï¼Œä¸èƒ½ç›´æ¥æŠ½æ ·çš„æƒ…å†µ é‡è¦æ€§æŠ½æ ·ï¼š é€‚ç”¨äºæ¦‚ç‡å¯†åº¦å‡½æ•°å¤æ‚ï¼Œä¸èƒ½ç›´æ¥æŠ½æ ·çš„æƒ…å†µ æ¥å—-æ‹’ç»æŠ½æ ·æ€æƒ³ï¼šæ‰¾ä¸€ä¸ªå¯ä»¥ç›´æ¥æŠ½æ ·çš„å»ºè®®åˆ†å¸ƒï¼ˆproposal distributionï¼‰ï¼Œå…¶æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸º$q(x)$, å¹¶ä¸”$q(x)$çš„$c$å€ä¸€å®šå¤§äº$p(x)$ï¼Œ å…¶ä¸­$c \u003e 0$,æŒ‰ç…§$q(x)$è¿›è¡ŒæŠ½æ ·ï¼Œå‡è®¾å¾—åˆ°ç»“æœ$x^\\ast$ï¼Œ å†æŒ‰ç…§$\\frac{p(x^\\ast)}{cq(x^\\ast )}$çš„æ¯”ä¾‹éšæœºå†³å®šæ˜¯å¦æ¥å—$x^\\ast$ã€‚è½åˆ°$p(x)$èŒƒå›´å†…çš„å°±æ¥å—ï¼Œè½åˆ°$p(x)$èŒƒå›´å¤–çš„å°±æ‹’ç»âŒã€‚ è¿™äº›æŠ½æ ·æ–¹æ³•çš„ç¼ºç‚¹ï¼š æŠ½æ ·æ•ˆç‡ä½ï¼Œ æ¯”å¦‚ $p(x)$ å  $cq(x)$ æ¶µç›–ä½“ç§¯æ¯”ä¾‹å¾ˆä½ å½“xä¸ºé«˜ç»´æ•°æ®æ—¶ï¼Œå¾ˆéš¾å¯»æ‰¾åˆé€‚çš„å»ºè®®åˆ†å¸ƒ ä¸€ä¸ªè§£å†³åŠæ³•å°±æ˜¯MCMC. ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:1:1","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"2. æ•°å­¦æœŸæœ›ä¼°è®¡(Estimation of mathematical expectation) æŒ‰ç…§æ¦‚ç‡åˆ†å¸ƒ $p(x)$ ç‹¬ç«‹æŠ½å–nä¸ªæ ·æœ¬åè®¡ç®—å‡½æ•°çš„æ ·æœ¬å‡å€¼ $$ \\hat f_{n}=\\frac{1}{n} \\sum_{i=1}^{n} f\\left(x_{i}\\right) $$ ä½œä¸ºæ•°å­¦æœŸæœ›çš„è¿‘ä¼¼å€¼ã€‚ æ ¹æ®å¤§æ•°å®šå¾‹å¯çŸ¥ï¼Œå½“æ ·æœ¬å®¹é‡å¢å¤§æ˜¯ï¼Œæ ·æœ¬å‡å€¼ä»¥æ¦‚ç‡1æ”¶æ•›æ€§äºæ•°å­¦æœŸæœ› $$ \\hat f_{n} \\rightarrow E_{p(x)}[f(x)], \\quad n \\rightarrow \\infty $$ äºæ˜¯ï¼Œå¾—åˆ°æ•°å­¦æœŸæœ›çš„è¿‘ä¼¼è®¡ç®—æ–¹æ³• $$ E_{p(x)}[f(x)] \\approx \\frac{1}{n} \\sum_{i=1}^{n} f\\left(x_{i}\\right) $$ ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:1:2","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"3. è’™ç‰¹å¡ç½—ç§¯åˆ†ï¼ˆMonte carlo intergrationï¼‰ è®¡ç®—å‡½æ•° $h(x)$ ç§¯åˆ† $$ \\int_{\\mathcal{X}} h(x) \\mathrm{d} x $$ å°† $h(x)$ åˆ†è§£æˆ $f(x)$ å’Œæ¦‚ç‡å¯†åº¦å‡½æ•° $p(x)$ çš„ä¹˜ç§¯ï¼Œå³å‡½æ•° $h(x)$ çš„ç§¯åˆ†å¯ä»¥è¡¨ç¤ºä¸ºå‡½æ•° $f(x)$ å…³äºæ¦‚ç‡å¯†åº¦å‡½æ•° $p(x)$ çš„æ•°å­¦æœŸæœ›ï¼š $$ \\int_{\\mathcal{X}} h(x) \\mathrm{d} x=\\int_{\\mathcal{X}} f(x) p(x) \\mathrm{d} x=E_{p(x)}[f(x)] $$ å› æ­¤ï¼Œå¯åˆ©ç”¨æ ·æœ¬å‡å€¼è®¡ç®—è¿‘ä¼¼ç§¯åˆ†ï¼š $$ \\int_{\\mathcal{X}} h(x) \\mathrm{d} x=E_{p(x)}[f(x)] \\approx \\frac{1}{n} \\sum_{i=1}^{n} f\\left(x_{i}\\right) $$ æ›´è¿›ä¸€æ­¥ $$ \\begin{aligned} E_{p(z)}[f(z)] \u0026= \\int f(z) p(z) dz \\cr \u0026= \\int \\underbrace{f(z) \\frac{p(z)}{q(z)}}_{new \\tilde{f} (z)} q(z) dz \\cr \u0026 \\approx \\frac{1}{N} \\sum_{n=1}^{N} f(z^{i}) \\frac{p(z^{i})}{q(z^{i})} \\end{aligned} $$ ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:1:3","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"Markov Chain ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:2:0","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"å®šä¹‰ é©¬å¯å¤«æ€§ï¼š éšæœºå˜é‡$X_t$åªä¾èµ–$X_{t-1}$ï¼Œè€Œä¸ä¾èµ–è¿‡å»çš„éšæœºå˜é‡ $\\lbrace X_{0}, X_{1}, \\cdots, X_{t-2} \\rbrace$ã€‚å³ $$ P\\left(X_{t} | X_{0}, X_{1}, \\cdots, X_{t-1}\\right)=P\\left(X_{t} | X_{t-1}\\right), \\quad t=1,2, \\cdots $$ é©¬å¯å¤«é“¾æˆ–é©¬å¯å¤«è¿‡ç¨‹ï¼ˆmarkov processï¼‰æŒ‡ï¼š å…·æœ‰é©¬å¯å¤«æ€§çš„éšæœºåºåˆ— $X=\\lbrace X_{0}, X_{1}, \\cdots, X_{t}, \\cdots \\rbrace$ã€‚ é©¬å¯å¤«é“¾çš„è½¬ç§»æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒä¸º $P(X_t | X_{t-1})$ ã€‚è½¬ç§»æ¦‚ç‡åˆ†å¸ƒå†³å®šé©¬å¯å¤«é“¾çš„ç‰¹æ€§ã€‚ æ—¶é—´é½æ¬¡é©¬å¯å¤«é“¾ï¼ˆtime homogenous Markov Chainï¼‰æ˜¯æŒ‡è½¬ç§»çŠ¶æ€åˆ†å¸ƒäºtæ— å…³çš„é©¬å¯å¤«é“¾ ç¦»æ•£çŠ¶æ€é©¬å¯å¤«é“¾ çŠ¶æ€è½¬ç§»çŸ©é˜µ å¹³ç¨³åˆ†å¸ƒï¼š é©¬å¯å¤«é“¾ $X$ï¼Œ å…¶çŠ¶æ€ç©ºé—´ä¸º$\\mathcal{S}$ï¼Œ è½¬ç§»çŸ©é˜µä¸º $P = (p_{ij})$ï¼Œ å¦‚æœå­˜åœ¨çŠ¶æ€ç©ºé—´ $\\mathcal{S}$ ä¸Šçš„ä¸€ä¸ªåˆ†å¸ƒ $$ \\pi = \\left[\\begin{array}{c} \\pi_1 \\cr \\pi_2 \\cr \\vdots \\end{array}\\right] $$ ä½¿å¾— $\\pi=P\\pi$, åˆ™ç§°$\\pi$ä¸ºé©¬å¯å¤«é“¾$X = {X_0, X_1, \\cdots, X_t, \\cdots }$ çš„å¹³ç¨³åˆ†å¸ƒ è¿ç»­çŠ¶æ€é©¬å¯å¤«é“¾ å®šä¹‰åœ¨è¿ç»­çŠ¶æ€ç©ºé—´ï¼Œè½¬ç§»æ¦‚ç‡åˆ†å¸ƒæœ‰æ¦‚ç‡è½¬ç§»æ ¸ï¼ˆtrainsition kernelï¼‰è¡¨ç¤º $$ P(x, A) = \\int_{A} p(x, y) dy $$ è½¬ç§»æ ¸$P(x, A)$è¡¨ç¤ºè½¬ç§»æ¦‚ç‡ $$ P (X_t = A | X_{t-1} = x) = P (x, A) $$ é©¬å¯å¤«é“¾çš„æ€§è´¨ ä¸å¯çº¦ (irreducible): æ—¶åˆ» 0 ä»çŠ¶æ€ $j$ å‡ºå‘ï¼Œæ—¶åˆ» $t$ åˆ°è¾¾çŠ¶æ€ $i$ çš„æ¦‚ç‡å¤§äº 0 ï¼Œåˆ™ç§°æ­¤é©¬å°”å¯å¤«é“¾ $X$ æ˜¯ä¸å¯çº¦çš„ $$ P(X_t = i | X_0 = j) \u003e 0 $$ éå‘¨æœŸï¼šä¸çº¯åœ¨ä¸€ä¸ªçŠ¶æ€ï¼Œä½¿å¾—å†è¿”å›åˆ°è¿™ä¸ªçŠ¶æ€æ‰€ç»å†çš„æ—¶é—´é•¿å‘ˆå‘¨æœŸæ€§ æ­£å¸¸è¿”(positive recurrent): ä»»æ„ä¸€ä¸ªçŠ¶æ€$i$ï¼Œä»å…¶ä»–ä»»æ„çŠ¶æ€ $j$ å‡ºå‘ï¼Œå½“æ—¶é—´è¶‹è¿‘æ— ç©·æ—¶ï¼Œé¦–æ¬¡è½¬ç§»åˆ°è¿™ä¸ªçŠ¶æ€$i$çš„æ¦‚ç‡ $p^t_{ij}$ ä¸ä¸º0 $$ \\lim_{t \\rightarrow \\infty} p^t_{ij} \u003e 0 $$ éå†å®šç†ï¼šæ»¡è¶³ç›¸åº”æ¡ä»¶çš„é©¬å°”å¯å¤«é“¾ï¼Œå½“æ—¶é—´è¶‹äºæ— ç©·æ—¶ï¼Œé©¬å°”å¯ å¤«é“¾çš„çŠ¶æ€åˆ†å¸ƒè¶‹è¿‘äºå¹³ç¨³åˆ†å¸ƒï¼Œéšæœºå˜é‡çš„å‡½æ•°çš„æ ·æœ¬å‡å€¼ä»¥æ¦‚ç‡ 1 æ”¶æ•›äºè¯¥å‡½æ•° çš„æ•°å­¦æœŸæœ› é©¬å¯å¤«é“¾ $X$ï¼Œ å…¶çŠ¶æ€ç©ºé—´ä¸º$\\mathcal{S}$ï¼Œ è‹¥é©¬å¯å¤«é“¾ $X$ ä¸å¯çº¦ã€éå‘¨æœŸä¸”æ­£å¸¸è¿”ï¼Œ åˆ™é©¬å¯å¤«é“¾æœ‰å”¯ä¸€çš„å¹³ç¨³åˆ†å¸ƒ $\\pi = (\\pi_1, \\pi_2, \\cdots)^T$ï¼Œ å¹¶ä¸”è½¬ç§»æ¦‚ç‡çš„æé™åˆ†å¸ƒæ˜¯é©¬å¯å¤«é“¾çš„å¹³ç¨³åˆ†å¸ƒ $$ \\lim_{t \\rightarrow \\infty} P(X_t = i | X_0 = j) = \\pi_i, i = 1,2, \\cdots ; j = 1,2,\\cdots $$ è‹¥ $f(X)$æ˜¯å®šä¹‰åœ¨çŠ¶æ€ç©ºé—´ä¸Šçš„å‡½æ•° $E_{pi}[ | f(X) | ] \u003c \\infty$, åˆ™ $$ P { \\hat{f_t} \\rightarrow E_{pi}[ f(X) ] } = 1 $$ è¿™é‡Œï¼Œ $$ \\hat{f_t} = \\frac{1}{t} \\sum^t_{s=1} f(x_s) $$ å…³äºå¹³ç¨³åˆ†å¸ƒ $\\pi = (\\pi_1, \\pi_2, \\cdots)^T$ çš„æ•°å­¦æœŸæœ› $E_{pi}[f(X)] = \\sum f(i)\\pi_i$, æœ‰ $$ \\hat{f_t} \\rightarrow E_{pi}[ f(X) ], t \\rightarrow \\infty $$ å¤„å¤„æˆç«‹æˆ–ä»¥æ¦‚ç‡1æˆç«‹ã€‚ ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:2:1","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"Markov Chain Monte Carlo é©¬å¯å¤«è’™ç‰¹å¡ç½—æ³•æ›´é€‚åˆéšæœºå˜é‡æ˜¯å¤šå…ƒçš„ã€å¯†åº¦å‡½æ•°æ˜¯éæ ‡å‡†å½¢å¼çš„ã€éšæœºå˜é‡å„åˆ†é‡ä¸ç‹¬ç«‹ç­‰æƒ…å†µ åŸºæœ¬æ€æƒ³ï¼š åœ¨éšæœºå˜é‡$x$çš„çŠ¶æ€ç©ºé—´$\\mathcal{S}$ä¸Šå®šä¸€ä¸ªæ»¡è¶³éå†å®šç†çš„é©¬å¯å¤«é“¾ï¼Œä½¿å…¶å¹³ç¨³åˆ†å¸ƒå°±æ˜¯æŠ½æ ·çš„ç›®æ ‡åˆ†å¸ƒ $p(x)$ï¼Œ ç„¶ååœ¨è¿™ä¸ªé©¬å¯å¤«é“¾ä¸Šè¿›è¡Œéšæœºæ¸¸èµ°ï¼Œæ¯ä¸ªæ—¶åˆ»å¾—åˆ°ä¸€ä¸ªæ ·æœ¬ã€‚æ ¹æ®éå†å®šç†ï¼Œå½“æ—¶é—´è¶‹äºæ— ç©·æ˜¯ï¼Œæ ·æœ¬çš„åˆ†å¸ƒè¶‹è¿‘å¹³ç¨³åˆ†å¸ƒï¼Œæ ·æœ¬å‡½æ•°å‡å€¼è¶‹è¿‘å‡½æ•°çš„æ•°å­¦æœŸæœ› $$ \\hat{E}f = \\frac{1}{n-m} \\sum^{n}_{i=m+1} f(x_i) $$ é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ³•çš„å…³é”®æ˜¯å¦‚ä½•æ„å»ºè½¬ç§»æ ¸å‡½æ•°æˆ–è½¬ç§»çŸ©é˜µï¼Œ åŒ…æ‹¬ï¼š Metropolis-Hastings å’Œ Gibbs samplingã€‚ é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ³•ä¸­å¾—åˆ°çš„æ ·æœ¬åºåˆ—ï¼Œç›¸é‚»çš„æ ·æœ¬ç‚¹æ˜¯ç›¸å…³çš„ï¼Œè€Œä¸æ˜¯ç‹¬ç«‹çš„ é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ³•çš„æ”¶æ•›æ€§çš„åˆ¤æ–­é€šå¸¸æ˜¯ç»éªŒæ€§çš„ åŸºæœ¬æ­¥éª¤ï¼š åœ¨éšæœºå˜é‡$x$çš„çŠ¶æ€ç©ºé—´$\\mathcal{S}$ä¸Šæ„å»ºä¸€ä¸ªæ»¡è¶³éå†å®šç†çš„é©¬å¯å¤«é“¾ï¼Œä½¿å…¶å¹³ç¨³åˆ†å¸ƒä¸ºç›®æ ‡åˆ†å¸ƒ $p(x)$ ä»çŠ¶æ€ç©ºé—´çš„æŸä¸€ç‚¹ $X_0$ å‡ºå‘ï¼Œç”¨æ„é€ çš„é©¬å¯å¤«é“¾è¿›è¡Œéšæœºæ¸¸èµ°ï¼Œäº§ç”Ÿæ ·æœ¬åºåˆ— ${x_0, x_1, \\cdots, x_t, \\cdots }$ã€‚ åº”ç”¨é©¬å¯å¤«é“¾çš„éå†å®šç†ï¼Œ ç¡®å®šæ­£æ•´æ•° $m$ å’Œ $n$ï¼Œ $m \u003c n$ï¼Œ å¾—åˆ°æ ·æœ¬é›†åˆ ${x_{m+1}, x_{m+2}ï¼Œ \\cdots, x_n }$ æ±‚å¾—å‡½æ•°fçš„ï¼ˆéå†ï¼‰å‡å€¼ $$ \\hat{E}f = \\frac{1}{n-m} \\sum^{n}_{i=m+1} f(x_i) $$ å‡ ä¸ªé‡è¦é—®é¢˜éœ€è¦æ³¨æ„ï¼š å¦‚ä½•å®šä¹‰é©¬å¯å¤«é“¾ï¼Œä¿è¯MCMCæˆç«‹ å¦‚ä½•ç¡®å®šæ”¶æ•›æ­¥æ•°mï¼Œä¿è¯æŠ½æ ·æ— åæ€§ å¦‚ä½•ç¡®å®šè¿­ä»£æ­¥æ•°nï¼Œ ä¿è¯éå†å‡å€¼çš„è®¡ç®—ç²¾åº¦ ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:3:0","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"Metropolis-Hastings Metropolis-Hastingæ˜¯é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ³•çš„ä»£è¡¨ç®—æ³• å¯ä»¥å¯¹å¤šå…ƒå˜é‡çš„æ¯ä¸€å˜é‡çš„æ¡ä»¶åˆ†å¸ƒä¾æ¬¡åˆ†åˆ«è¿›è¡ŒæŠ½æ ·ï¼Œ ä»è€Œå®ç°å¯¹æ•´ä¸ªå¤šå…ƒå˜é‡çš„ä¸€æ¬¡æŠ½æ ·ï¼Œè¿™å°±æ˜¯å•åˆ†é‡ Metropolis- Hastings (singlecomponent Metropolis- Hastings) ç®—æ³•ã€‚ ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:3:1","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"Gibbs Sampling å‰å¸ƒæ–¯æŠ½æ ·ï¼Œå¯ä»¥è®¤ä¸ºæ˜¯ Metropolis-Hastings ç®—æ³•çš„ç‰¹æ®Šæƒ…å†µï¼Œä½†æ˜¯æ›´å®¹æ˜“å®ç°ï¼Œå› è€Œè¢«å¹¿æ³›ä½¿ç”¨ã€‚ å‰å¸ƒæ–¯æŠ½æ ·ç”¨äºå¤šå…ƒå˜é‡è”åˆåˆ†å¸ƒçš„æŠ½æ ·å’Œä¼°è®¡ã€‚ å…¶åŸºæœ¬åšæ³•æ˜¯ï¼Œä»è”åˆæ¦‚ç‡åˆ†å¸ƒå®šä¹‰æ»¡æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼Œä¾æ¬¡å¯¹æ»¡æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒè¿›è¡ŒæŠ½æ ·ï¼Œå¾—åˆ°æ ·æœ¬çš„åºåˆ—ã€‚ å‰å¸ƒæ–¯æŠ½æ ·é€‚åˆäºæ»¡æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ å®¹æ˜“æŠ½æ · çš„æƒ…å†µï¼Œè€Œå•åˆ†é‡MetropolisHastings ç®—æ³•é€‚åˆäºæ»¡æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒä¸å®¹æ˜“æŠ½æ ·çš„æƒ…å†µï¼Œè¿™æ—¶ä½¿ç”¨å®¹æ˜“æŠ½æ ·çš„æ¡ä»¶åˆ†å¸ƒä½œå»ºè®®åˆ†å¸ƒã€‚ å‚è€ƒï¼š æèˆªã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹ ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:3:2","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"å¥‡å¼‚å€¼åˆ†è§£(SVD)æ˜¯ä¸€ç§çŸ©é˜µå› å­åˆ†è§£æ–¹æ³•ï¼Œåœ¨çº¿æ€§ä»£æ•°ä¸­ï¼Œè¢«å¹¿æ³›åº”ç”¨ã€‚ å¥‡å¼‚å€¼åˆ†è§£ä¹Ÿæ˜¯ä¸€ç§çŸ©é˜µè¿‘ä¼¼çš„æ–¹æ³•ï¼Œè¿™ä¸ªè¿‘ä¼¼æ˜¯åœ¨å¼—ç½—è´å°¼ä¹Œæ–¯èŒƒæ•°ï¼ˆFrobenius norm) æ„ä¹‰ä¸‹çš„è¿‘ä¼¼ã€‚ å¥‡å¼‚å€¼åˆ†è§£æ˜¯åœ¨å¹³æ–¹æŸå¤±(å¼—ç½—è´å°¼ä¹Œæ–¯èŒƒæ•°)æ„ä¹‰ä¸‹å¯¹çŸ©é˜µçš„æœ€ä¼˜è¿‘ä¼¼ï¼Œå³æ•°æ®å‹ç¼©ã€‚ ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:0:0","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"å®šä¹‰ ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:1:0","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"1. å¥‡å¼‚å€¼åˆ†è§£ å°†ä¸€ä¸ª $m \\times n$ çš„å®çŸ©é˜µ$A$ï¼Œ$A \\in \\mathbf{R}^{m \\times n}$ è¡¨ç¤ºä¸ºä»¥ä¸‹ä¸‰ä¸ªå®çŸ©é˜µä¹˜ç§¯å½¢å¼çš„è¿ç®—ï¼Œå³çŸ©é˜µå› å­åˆ†è§£ï¼š $$ A=U \\Sigma V^{\\mathrm{T}} $$ å…¶ä¸­ï¼Œ $U$æ˜¯$m$é˜¶æ­£äº¤çŸ©é˜µ: $UU^T = I$ $V$ä¸º$n$é˜¶æ­£äº¤çŸ©é˜µ: $VV^T = I$ $\\Sigma$æ˜¯ç”±é™åºæ’åˆ—çš„éè´Ÿçš„å¯¹è§’çº¿å…ƒç´ ç»„æˆçš„$m \\times n$çš„å¯¹è§’çŸ©é˜µ: $\\Sigma = diag(\\sigma_1, \\sigma_2, \\cdots, \\sigma_p)$ $\\sigma_{1} \\geqslant \\sigma_{2} \\geqslant \\cdots \\geqslant \\sigma_{p} \\geqslant 0$ $p = \\min(m, n)$ é‚£ä¹ˆï¼Œç§° $U \\Sigma V^{\\mathrm{T}}$ : çŸ©é˜µAçš„å¥‡å¼‚å€¼åˆ†è§£ $\\sigma_i$ ä¸ºAçš„å¥‡å¼‚å€¼(singluar value) $U$ çš„åˆ—å‘é‡ä¸ºå·¦å¥‡å¼‚å‘é‡(left singular vector) $V$ çš„åˆ—å‘é‡ä¸ºå³å¥‡å¼‚å‘é‡(right singular vector) âš ï¸æ³¨æ„ï¼ŒçŸ©é˜µçš„å¥‡å¼‚å€¼åˆ†è§£ä¸å”¯ä¸€ã€‚ å®é™…å¸¸ç”¨çš„æ˜¯ï¼š ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:1:1","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"2. ç´§å¥‡å¼‚å€¼åˆ†è§£(compact singular value decomposition)ï¼š æ— æŸå‹ç¼© è®¾$m \\times n$çš„å®çŸ©é˜µ$A$ï¼Œå…¶ç§©ä¸º$\\operatorname{rank}(A)=r$, $r \\leqslant \\min (m, n)$, åˆ™ç´§å¥‡å¼‚å€¼åˆ†è§£ä¸ºï¼š $$ A=U_r \\Sigma_r V^{\\mathrm{T}}_r $$ å…¶ä¸­ï¼Œ $U_r$ æ˜¯ $m \\times r$ çŸ©é˜µ $V_r$ æ˜¯ $n \\times r$ çŸ©é˜µ $\\Sigma_r$ æ˜¯ $r$ é˜¶å¯¹è§’é˜µ $r = \\operatorname{rank}(A)$ ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:1:2","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"3. æˆªæ–­å¥‡å¼‚å€¼åˆ†è§£(truncated singular value decomposition)ï¼š æœ‰æŸå‹ç¼© ä¸€èˆ¬è®²å¥‡å¼‚å€¼åˆ†è§£ï¼Œå®é™…ä¸Šå¤šæŒ‡æˆªæ–­å¥‡å¼‚å€¼åˆ†è§£ åœ¨å¥‡å¼‚å€¼åˆ†è§£ä¸­ï¼Œ åªå–æœ€å¤§çš„$k$ä¸ªå¥‡å¼‚å€¼($k \u003c r, r= \\operatorname{rank}(A)$ )å¯¹åº”çš„éƒ¨åˆ†ï¼Œå°±å¾—åˆ°æˆªæ–­å¥‡å¼‚å€¼åˆ†è§£ è®¾ $m \\times n$ çš„å®çŸ©é˜µ $A$ï¼Œå…¶ç§©ä¸º $\\operatorname{rank}(A)=r$, ä¸” $0 \u003c k \u003c r$, åˆ™æˆªæ–­å¥‡å¼‚å€¼åˆ†è§£ä¸ºï¼š $$ A \\approx U_{k} \\Sigma_{k} V_{k}^{\\mathrm{T}} $$ å…¶ä¸­ï¼Œ $U_k$ æ˜¯$m \\times k$ çŸ©é˜µ(å‰ $k$åˆ—) $V_k$ æ˜¯$n \\times k$ çŸ©é˜µ(å‰ $k$åˆ—) $\\Sigma_k$ æ˜¯ $k$ é˜¶å¯¹è§’é˜µ(å‰ $k$ä¸ª) $r = \\operatorname{rank}(A)$ ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:1:3","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"å‡ ä½•è§£é‡Š ä»çº¿æ€§å˜æ¢çš„è§’åº¦ç†è§£å¥‡å¼‚å€¼åˆ†è§£ï¼š å°† $m \\times n$ çš„å®çŸ©é˜µ $A$è¡¨ç¤ºä¸ºä» $n$ ç»´ç©ºé—´ $R_n$ åˆ° $m$ ç»´ç©ºé—´ $R_m$çš„ä¸€ä¸ªçº¿æ€§å˜æ¢ï¼š $$ T: x \\rightarrow A x $$ $x$, $Ax$ä¸ºå„è‡ªç©ºé—´çš„å‘é‡ã€‚ é‚£ä¹ˆçº¿æ€§å˜æ¢å¯ä»¥ç†è§£ä¸ºï¼š ä¸€ä¸ªåæ ‡ç³»çš„æ—‹è½¬æˆ–åå°„å˜æ¢ ä¸€ä¸ªåæ ‡è½´çš„ç¼©æ”¾å˜æ¢ å¦ä¸€ä¸ªåæ ‡ç³»çš„æ—‹è½¬æˆ–åå°„å˜æ¢ å¯¹Aè¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£ï¼ŒUå’ŒVéƒ½æ˜¯æ­£äº¤çŸ©é˜µ Vçš„åˆ—å‘é‡æ„æˆRnç©ºé—´çš„ä¸€ç»„æ ‡å‡†æ­£äº¤åŸºï¼Œè¡¨ç¤ºRnç©ºé—´ä¸­æ­£äº¤åæ ‡ç³»çš„æ—‹è½¬æˆ–åå°„å˜æ¢ Uçš„åˆ—å‘é‡éƒ½æˆRmç©ºé—´çš„ä¸€ç»„æ ‡å‡†æ­£äº¤åŸºï¼Œè¡¨ç¤ºRmç©ºé—´ä¸­æ­£äº¤åæ ‡ç³»çš„æ—‹è½¬æˆ–åå°„å˜æ¢ $\\Sigma$çš„å¯¹è§’å…ƒç´ æ˜¯ä¸€ç»„éè´Ÿå®æ•°ï¼Œè¡¨ç¤ºRnä¸­çš„åŸå§‹æ­£äº¤åæ ‡ç³»åæ ‡è½´çš„ç¼©æ”¾å˜æ¢ ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:2:0","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"å¥‡å¼‚å€¼è®¡ç®— çŸ©é˜µ$A$çš„å¥‡å¼‚å€¼åˆ†è§£å¯ä»¥é€šè¿‡æ±‚å¯¹é˜µçŸ©é˜µ$A^TA$çš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡å¾—åˆ°ã€‚ $A^TA$çš„ç‰¹å¾å‘é‡æ„æˆæ­£äº¤çŸ©é˜µ$V$çš„åˆ— $A^TA$çš„ç‰¹å¾å€¼$\\lambda_j$çš„å¹³æ–¹æ ¹ä¸ºå¥‡å¼‚å€¼$\\sigma_i$ï¼Œå³ $$ \\sigma_{j}=\\sqrt{\\lambda_{j}}, \\quad j=1,2, \\cdots, n $$ å¯¹$\\sigma_i$ä»å¤§åˆ°å°æ’åˆ—ï¼Œå¾—åˆ°å¯¹è§’çŸ©é˜µ $\\Sigma$ æ±‚æ­£å¥‡å¼‚å€¼å¯¹åº”çš„å·¦å¥‡å¼‚å‘é‡ï¼Œå†æ‰©å……çš„ $A^T$ çš„æ ‡å‡†æ­£äº¤åŸºï¼Œæ„æˆæ­£äº¤çŸ©é˜µ $U$ çš„åˆ— ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:3:0","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"æ±‚å€¼è¿‡ç¨‹ æ±‚å¯¹é˜µçŸ©é˜µ$A^TA$çš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ è®¡ç®—å¯¹ç§°çŸ©é˜µ $W= A^TA$ æ±‚è§£ç‰¹å¾æ–¹ç¨‹ $(W - \\lambda I)x = 0$ å¾—åˆ°ç‰¹å¾å€¼$\\lambda_i$ï¼Œå¹¶å°†ä¹‹é™åºæ’åˆ— $$ \\lambda_{1} \\geqslant \\lambda_{2} \\geqslant \\cdots \\geqslant \\lambda_{n} \\geqslant 0 $$ å°†ç‰¹å¾å€¼ $\\lambda_i$ ä»£å…¥ç‰¹å¾æ–¹ç¨‹æ±‚çš„å¯¹åº”ç‰¹å¾å‘é‡ æ±‚ $n$ é˜¶æ­£äº¤çŸ©é˜µ $V$ å°†ç‰¹å¾å‘é‡å•ä½åŒ–ï¼Œ å¾—åˆ°å•ä½ç‰¹å¾å‘é‡æ„æˆ $n$ é˜¶æ­£äº¤çŸ©é˜µV $$ V=\\left[\\begin{array}{llll} v_{1} \u0026 v_{2} \u0026 \\cdots \u0026 v_{n} \\end{array}\\right] $$ æ±‚ $m \\times n$ å¯¹è§’çŸ©é˜µ $\\Sigma$ è®¡ç®—Açš„å¥‡å¼‚å€¼ $$ \\sigma_{j}=\\sqrt{\\lambda_{j}}, \\quad j=1,2, \\cdots, n $$ æ„é€  $m \\times n$ çŸ©é˜µå¯¹è§’çŸ©é˜µ $\\Sigma$ï¼Œ ä¸»å¯¹è§’çº¿å…ƒç´ æ˜¯å¥‡å¼‚å€¼ï¼Œ å…¶ä½™å…ƒç´ ä¸º 0 $$ \\Sigma=\\operatorname{diag}\\left(\\sigma_{1}, \\sigma_{2}, \\cdots, \\sigma_{n}\\right) $$ æ±‚ $m$ é˜¶æ­£äº¤çŸ©é˜µ $U$ å¯¹ $A$ çš„å‰ $r$ä¸ªæ­£å¥‡å¼‚å€¼ï¼Œ ä»¤ $$ u_{j}=\\frac{1}{\\sigma_{j}} A v_{j}, \\quad j=1,2, \\cdots, r $$ å¾—åˆ° $$ U_{1}=\\left[\\begin{array}{llll} u_{1} \u0026 u_{2} \u0026 \\cdots \u0026 u_{r} \\end{array}\\right] $$ æ±‚ $A^T$ çš„é›¶ç©ºé—´çš„ä¸€ç»„æ ‡å‡†æ­£äº¤åŸº $$ \\lbrace u_{r+1}, u_{r+2}, \\cdots, u_{m} \\rbrace $$ ä»¤ $$ U_{2}=\\left[\\begin{array}{llll} u_{r+1} \u0026 u_{r+2} \u0026 \\cdots \u0026 u_{m} \\end{array}\\right] $$ ä¸”ä»¤ $$ U=\\left[\\begin{array}{ll} U_{1} \u0026 U_{2} \\end{array}\\right] $$ å¾—åˆ°å¥‡å¼‚å€¼åˆ†è§£ $$ A=U \\Sigma V^{\\mathrm{T}} $$ ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:3:1","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"ä¸¾ä¾‹ æ±‚: çŸ©é˜µ $A$ çš„å¥‡å¼‚å€¼åˆ†è§£ $$ A=\\left[\\begin{array}{ll} 1 \u0026 1 \\cr 2 \u0026 2 \\cr 0 \u0026 0 \\end{array}\\right] $$ è§£ï¼š æ±‚ $A^TA$ çš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ $$ A^{\\mathrm{T}} A=\\left[\\begin{array}{lll} 1 \u0026 2 \u0026 0 \\cr 1 \u0026 2 \u0026 0 \\end{array}\\right]\\left[\\begin{array}{ll} 1 \u0026 1 \\cr 2 \u0026 2 \\cr 0 \u0026 0 \\end{array}\\right]=\\left[\\begin{array}{ll} 5 \u0026 5 \\cr 5 \u0026 5 \\end{array}\\right] $$ æ»¡è¶³ç‰¹å¾æ–¹ç¨‹ $$ \\left(A^{\\mathrm{T}} A-\\lambda I\\right) x=0 $$ å¾—åˆ°é½æ¬¡çº¿æ€§æ–¹ç¨‹ç»„ $$ \\begin{cases} (5-\\lambda) x_{1} + \u0026 5 x_{2}=0 \\cr 5 x_{1} + \u0026 (5-\\lambda) x_{2}=0 \\end{cases} $$ è¯¥æ–¹ç¨‹ç»„æœ‰éé›¶è§£çš„å……è¦æ¡ä»¶æ˜¯ $$ \\left|\\begin{array}{cc} 5-\\lambda \u0026 5 \\cr 5 \u0026 5-\\lambda \\end{array}\\right|=0 $$ å³ $$ \\lambda^{2}-10 \\lambda=0 $$ å¾—åˆ° $\\lambda_1 = 10 $, $\\lambda_2 = 0$. ç‰¹å¾å€¼ä»£å…¥çº¿æ€§æ–¹ç¨‹ç»„ï¼Œåˆ†åˆ«å¾—åˆ°å¯¹åº”å•ä½ç‰¹å¾å‘é‡ $$ v_{1}=\\left[\\begin{array}{c} \\frac{1}{\\sqrt{2}} \\cr \\frac{1}{\\sqrt{2}} \\end{array}\\right], v_{2}=\\left[\\begin{array}{c} \\frac{1}{\\sqrt{2}} \\cr -\\frac{1}{\\sqrt{2}} \\end{array}\\right] $$ æ±‚æ­£äº¤çŸ©é˜µ $V$ æ„é€ æ­£äº¤çŸ©é˜µ $$ V=\\left[\\begin{array}{cc} \\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{2}} \\cr \\frac{1}{\\sqrt{2}} \u0026 -\\frac{1}{\\sqrt{2}} \\end{array}\\right] $$ æ±‚å¯¹è§’çŸ©é˜µ $\\Sigma$ å¥‡å¼‚å€¼ä¸º $\\sigma_1 = \\sqrt{\\lambda_1} = \\sqrt{10}$, $\\sigma_2 = 0$, é‚£ä¹ˆ $$ \\Sigma=\\left[\\begin{array}{cc} \\sqrt{10} \u0026 0 \\cr 0 \u0026 0 \\cr 0 \u0026 0 \\end{array}\\right] $$ âš ï¸æ³¨æ„ï¼š ä¸ºäº† $\\Sigma$ èƒ½ä¸$U$å’Œ$V$è¿›è¡ŒçŸ©é˜µä¹˜æ³•ï¼Œ $\\Sigma$è¦åŠ ä¸Šé›¶è¡Œå‘é‡ æ±‚æ­£äº¤çŸ©é˜µ $U$ åŸºäº $A$ çš„å¥‡å¼‚å€¼è®¡ç®—å¾—åˆ°åˆ—å‘é‡ $u_1$ $$ u_{1}=\\frac{1}{\\sigma_{1}} A v_{1}=\\frac{1}{\\sqrt{10}}\\left[\\begin{array}{cc} 1 \u0026 1 \\cr 2 \u0026 2 \\cr 0 \u0026 0 \\end{array}\\right]\\left[\\begin{array}{c} \\frac{1}{\\sqrt{2}} \\cr \\frac{1}{\\sqrt{2}} \\end{array}\\right]=\\left[\\begin{array}{c} \\frac{1}{\\sqrt{5}} \\cr \\frac{2}{\\sqrt{5}} \\cr 0 \\end{array}\\right] $$ åˆ—å‘é‡$u_2$ï¼Œ $u_3$æ˜¯ $A^T$çš„é›¶ç©ºé—´ $N(A^T)$ çš„ä¸€ç»„æ ‡å‡†æ­£äº¤åŸºï¼Œæ•…è€Œæ±‚è§£ä»¥ä¸‹çº¿æ€§æ–¹ç¨‹ç»„ $$ A^{\\mathrm{T}} x=\\left[\\begin{array}{lll} 1 \u0026 2 \u0026 0 \\cr 1 \u0026 2 \u0026 0 \\end{array}\\right]\\left[\\begin{array}{l} x_{1} \\cr x_{2} \\cr x_{3} \\end{array}\\right]=\\left[\\begin{array}{l} 0 \\cr 0 \\end{array}\\right] $$ å³ $$ \\begin{array}{c} x_{1}+2 x_{2}+0 x_{3}=0 \\cr x_{1}=-2 x_{2}+0 x_{3} \\end{array} $$ åˆ†åˆ«å– $x_2$ï¼Œ$x_3$ ä¸º $(1,0)$ å’Œ $(0,1)$ å¾—åˆ° $N(A^T)$çš„åŸº $$ (-2,1,0)^{\\mathrm{T}}, \\quad(0,0,1)^{\\mathrm{T}} $$ å¾— $N(A^T)$ çš„ä¸€ç»„æ ‡å‡†æ­£äº¤åŸºæ˜¯ $$ u_{2}=\\left(-\\frac{2}{\\sqrt{5}}, \\frac{1}{\\sqrt{5}}, 0\\right)^{\\mathrm{T}}, \\quad u_{3}=(0,0,1)^{\\mathrm{T}} $$ æœ€åï¼Œæ„é€ æ­£äº¤çŸ©é˜µ $U$ $$ U=\\left[\\begin{array}{ccc} \\frac{1}{\\sqrt{5}} \u0026 -\\frac{2}{\\sqrt{5}} \u0026 0 \\cr \\frac{2}{\\sqrt{5}} \u0026 \\frac{1}{\\sqrt{5}} \u0026 0 \\cr 0 \u0026 0 \u0026 1 \\end{array}\\right] $$ çŸ©é˜µ $A$ çš„å¥‡å¼‚å€¼åˆ†è§£ $$ A=U \\Sigma V^{\\mathrm{T}}=\\left[\\begin{array}{ccc} \\frac{1}{\\sqrt{5}} \u0026 -\\frac{2}{\\sqrt{5}} \u0026 0 \\cr \\frac{2}{\\sqrt{5}} \u0026 \\frac{1}{\\sqrt{5}} \u0026 0 \\cr 0 \u0026 0 \u0026 1 \\end{array}\\right]\\left[\\begin{array}{cc} \\sqrt{10} \u0026 0 \\cr 0 \u0026 0 \\cr 0 \u0026 0 \\end{array}\\right]\\left[\\begin{array}{cc} \\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{2}} \\cr \\frac{1}{\\sqrt{2}} \u0026 -\\frac{1}{\\sqrt{2}} \\end{array}\\right] $$ ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:3:2","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"çŸ©é˜µçš„å¤–ç§¯å±•å¼€å¼è¡¨ç¤º å°†Açš„å¥‡å¼‚å€¼åˆ†è§£çœ‹æˆçŸ©é˜µ $U\\Sigma$ å’Œ $V^T$ çš„ä¹˜ç§¯ï¼Œ å°† $U\\Sigma$ æŒ‰åˆ—å‘é‡åˆ†å—ï¼Œ $$ U \\Sigma=\\left[\\begin{array}{llll} \\sigma_{1} u_{1} \u0026 \\sigma_{2} u_{2} \u0026 \\cdots \u0026 \\sigma_{n} u_{n} \\end{array}\\right] $$ $V^T$ æŒ‰è¡Œå‘é‡åˆ†å— $$ V^{\\mathrm{T}}=\\left[\\begin{array}{c} v_{1}^{\\mathrm{T}} \\cr v_{2}^{\\mathrm{T}} \\cr \\vdots \\cr v_{n}^{\\mathrm{T}} \\end{array}\\right] $$ é‚£ä¹ˆå¤–ç§¯å±•å¼€å¼ä¸º $$ A=\\sigma_{1} u_{1} v_{1}^{\\mathrm{T}}+\\sigma_{2} u_{2} v_{2}^{\\mathrm{T}}+\\cdots+\\sigma_{n} u_{n} v_{n}^{\\mathrm{T}} $$ æˆ–è€… $$ A=\\sum_{k=1}^{n} A_{k}=\\sum_{k=1}^{n} \\sigma_{k} u_{k} v_{k}^{\\mathrm{T}} $$ å…¶ä¸­ $A_{k}=\\sigma_{k} u_{k} v_{k}^{\\mathrm{T}}$ æ˜¯ $m \\times n$ çŸ©é˜µ è€Œ $$ u_{i} v_{j}^{\\mathrm{T}}=\\left[\\begin{array}{c} u_{1 i} \\cr u_{2 i} \\cr \\vdots \\cr u_{m i} \\end{array}\\right]\\left[\\begin{array}{cccc} v_{1 j} \u0026 v_{2 j} \u0026 \\cdots \u0026 v_{n j} \\end{array}\\right] = \\left[\\begin{array}{cccc} u_{1 i} v_{1 j} \u0026 u_{1 i} v_{2 j} \u0026 \\cdots \u0026 u_{1 i} v_{n j} \\cr u_{2 i} v_{1 j} \u0026 u_{2 i} v_{2 j} \u0026 \\cdots \u0026 u_{2 i} v_{n j} \\cr \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\cr u_{m i} v_{1 j} \u0026 u_{m i} v_{2 j} \u0026 \\cdots \u0026 u_{m i} v_{n j} \\end{array}\\right] $$ ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:4:0","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"è¡¥å……ï¼š ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:5:0","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"Determinant è¡Œåˆ—å¼ Determinant: The determinant of a square matrix is a scalar that provides information about the matrix. e.g. Invertibility Geometrically, it can be viewed as the volume scaling factor of the linear transformation described by the matrix. ä¸€ä¸ªçŸ©é˜µçš„è¡Œåˆ—å¼å°±æ˜¯ä¸€ä¸ªè¶…å¹³è¡Œå¤šé¢ä½“çš„ï¼ˆæœ‰å‘çš„ï¼‰é¢ç§¯/ä½“ç§¯ï¼Œè¿™ä¸ªå¤šé¢ä½“çš„æ¯æ¡è¾¹å¯¹åº”ç€å¯¹åº”çŸ©é˜µçš„åˆ—ï¼› çŸ©é˜µ $A$ çš„è¡Œåˆ—å¼ $det(A)$ å°±æ˜¯çº¿æ€§å˜æ¢ $A$ ä¸‹çš„å›¾å½¢é¢ç§¯æˆ–ä½“ç§¯çš„ä¼¸ç¼©å› å­ã€‚ çŸ©é˜µçš„è¡Œåˆ—å¼çš„å‡ ä½•æ„ä¹‰æ˜¯çŸ©é˜µå¯¹åº”çš„çº¿æ€§å˜æ¢å‰åçš„é¢ç§¯æ¯” Property: $Det(I) = 1$ Exchanging rows only reverse the sign of det Determinant is â€œlinearâ€ for each row $det(A) \\neq 0$, $A$ is invertible Cramerâ€™s rule: $A^{-1} = \\frac{1}{det(A)}C^T$ $det(A)$: scalar $C$: cofactors of $A$ (C has the same size as $A$) $C^T$ is adjugate of $A$ (ä¼´éšçŸ©é˜µ) ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:5:1","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"Eigenvalue and Eigenvector Eigen (German word): â€œunique toâ€ or â€œbelonging toâ€ if $Av = \\lambda v$ ($v$ is a vector , $\\lambda$ is a scalar) $A$ must be square $v$ is an eigenvector of $A$, exluding zero vector $\\lambda$ is an eigenvalue of $A$ that correponds to $v$ $T$ is a linear operator if $T(v) = \\lambda v$ ( $v$ is a vector, $\\lambda$ is a scalar) $v$ is an eigenvector of $T$, exluding zero vector $\\lambda$ is an eigenvalue of $T$ that correponds to $v$ An eigenvector of A corresponds to a unique eigenvalue. An eigenvalue of A has infinitely many eigenvectors. =\u003e how to find eigenvalues t : $$ det(A - tI_n) = 0 $$ å‚è€ƒï¼š æèˆªã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹ ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:5:2","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Coding"],"content":"Get answers for Swift within 10s","date":"2020-05-30","objectID":"/2020-06-03-swift-unsafe-pointer/","tags":["Swift"],"title":"Pointer in UnSafe Swift","uri":"/2020-06-03-swift-unsafe-pointer/"},{"categories":["Coding"],"content":"Pointers Unsafe Swift pointers use a predictable naming scheme: Unsafe [Mutable][Raw][Buffer]Pointer[\u003cT\u003e] Explain: Pointers are just memory addresses. Direct memory access is Unsafe. Mutable means you can write to it. Raw means it points to a blob of bytes. Buffer means that is works like a collection. Generic [\u003cT\u003e] pointers are typed. ","date":"2020-05-30","objectID":"/2020-06-03-swift-unsafe-pointer/:1:0","tags":["Swift"],"title":"Pointer in UnSafe Swift","uri":"/2020-06-03-swift-unsafe-pointer/"},{"categories":["Coding"],"content":"Working with Pointers C Pointer Swift Type int * UnsaftMutablePointer const int * UnsafePointer NSDate ** AutoreleasingUnsafeMutablePointer struct UnknowType * OpaquePointer void * UnsafeMutableRawPointer const void * UnsafeRawPointer Explain: see here ","date":"2020-05-30","objectID":"/2020-06-03-swift-unsafe-pointer/:2:0","tags":["Swift"],"title":"Pointer in UnSafe Swift","uri":"/2020-06-03-swift-unsafe-pointer/"},{"categories":["Coding"],"content":"Usage see here and here ","date":"2020-05-30","objectID":"/2020-06-03-swift-unsafe-pointer/:3:0","tags":["Swift"],"title":"Pointer in UnSafe Swift","uri":"/2020-06-03-swift-unsafe-pointer/"},{"categories":["Coding"],"content":"Get answers for Swift within 10s","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Swift Cheat Sheet Stolen from iwasrobbed. I simplify it and add some more. Itâ€™s a high level and a quick reference to Swift. The purpose of this cheat sheet is to teach myself and get answers within 10s. ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:0:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Table of Contents Code Document Data Types Operators Operator Overloading Declaring Classes Declarations Lazy Property Property Observer Literals Functions Constants and Variables Naming Conventions Closures Generics Control Statements Extension Protocol Protocol Extension Error Handling Passing Information User Defaults Common Patterns Unicode Support File IO ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:0:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Code Documentation Two ways of commenting: // /* â€¦ */ Two ways of documenting with markdown (Reconigzed by xcode): /// /** â€¦ */ ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:1:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Markdown a few keywords that xcode can recognized automatically, with the format like - . The most common: Prameters, Throws, Returns /** - Prameters: - argument1: This is arg1 - argument2: This is arg2 - Returns: The results string. - Throws: `Error` if nil */ Other keywords /** - Precondition: - Postcondition: - Requires: All the information in the object should be sorted - Invariant: The object will maintained sorted - Complexity: O(n^2) - Important: - Warning: Very computation consuming - Attention: Same as Warning - Note: something to keep in mind - Remark: Same as note */ Metadata /** - Author: - Authors: - Copyright: - Date: - Since: - Version: */ Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:1:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"MARK Using MARK to organize your code: // MARK:- Use mark to logically organize your code // Declare some functions or variables here // MARK:- They also show up nicely in the properties/functions list in Xcode // Declare some more functions or variables here ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:1:2","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"FIXME Using FIXME to remember to fix your code: // Some broken code might be here // FIXME:Use fixme to create a reminder to fix broken code later FIXME works a lot like MARK because it makes organizing code easier, but itâ€™s used exclusively when you need to remember to fix something. ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:1:3","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"TODO Using TODO to remember to add, delete, or generally refactor your code: // Some incomplete code might be here // TODO:Use todo to create a reminder to finish things up later TODO is very similar to FIXME and MARK, but itâ€™s used exclusively when you need to remember to add, delete, or change your code later. Auto-generating method documentation: In a methodâ€™s preceding line, press âŒ¥ Option + âŒ˜ Command + / to automatically generate a documentation stub for your method. Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:1:4","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Data Types ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:2:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Size Permissible sizes of data types are determined by how many bytes of memory are allocated for that specific type and whether itâ€™s a 32-bit or 64-bit environment. In a 32-bit environment, long is given 4 bytes, which equates to a total range of 2^(4*8) (with 8 bits in a byte) or 4294967295. In a 64-bit environment, long is given 8 bytes, which equates to 2^(8*8) or 1.84467440737096e19. For a complete guide to 64-bit changes, please see the transition document. ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:2:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"C Primitives Unless you have a good reason to use C primitives, you should just use the Swift types to ensure compability going foward. In fact, Swift just aliases C types to a Swift equivalent: // C char is aliased as an Int8 and unsigned as UInt8 let aChar = CChar() let anUnsignedChar = CUnsignedChar() print(\"C char size: \\(MemoryLayout.size(ofValue: aChar))with min: \\(Int8.min)and max: \\(Int8.max)\") // C char size: 1 with min: -128 and max: 127 print(\"C unsigned char size: \\(MemoryLayout.size(ofValue: anUnsignedChar))with min: \\(UInt8.min)and max: \\(UInt8.max)\") // C unsigned char size: 1 with min: 0 and max: 255 // C short is aliased as an Int16 and unsigned as UInt16 let aShort = CShort() let unsignedShort = CUnsignedShort() print(\"C short size: \\(MemoryLayout.size(ofValue: aShort))with min: \\(Int16.min)and max: \\(Int16.max)\") // C short size: 2 with min: -32768 and max: 32767 print(\"C unsigned short size: \\(MemoryLayout.size(ofValue: unsignedShort))with min: \\(UInt16.min)and max: \\(UInt16.max)\") // C unsigned short size: 2 with min: 0 and max: 65535 // C int is aliased as an Int32 and unsigned as UInt32 let anInt = CInt() let unsignedInt = CUnsignedInt() print(\"C int size: \\(MemoryLayout.size(ofValue: anInt))with min: \\(Int32.min)and max: \\(Int32.max)\") // C int size: 4 with min: -2147483648 and max: 2147483647 print(\"C unsigned int size: \\(MemoryLayout.size(ofValue: unsignedInt))with min: \\(UInt32.min)and max: \\(UInt32.max)\") // C unsigned int size: 4 with min: 0 and max: 4294967295 // C long is aliased as an Int and unsigned as UInt let aLong = CLong() let unsignedLong = CUnsignedLong() print(\"C long size: \\(MemoryLayout.size(ofValue: aLong))with min: \\(Int.min)and max: \\(Int.max)\") // C long size: 8 with min: -9223372036854775808 and max: 9223372036854775807 print(\"C unsigned long size: \\(MemoryLayout.size(ofValue: unsignedLong))with min: \\(UInt.min)and max: \\(UInt.max)\") // C unsigned long size: 8 with min: 0 and max: 18446744073709551615 // C long long is aliased as an Int64 and unsigned as UInt64 let aLongLong = CLongLong() let unsignedLongLong = CUnsignedLongLong() print(\"C long long size: \\(MemoryLayout.size(ofValue: aLongLong))with min: \\(Int64.min)and max: \\(Int64.max)\") // C long long size: 8 with min: -9223372036854775808 and max: 9223372036854775807 print(\"C unsigned long long size: \\(MemoryLayout.size(ofValue: unsignedLongLong))with min: \\(UInt64.min)and max: \\(UInt64.max)\") // C unsigned long long size: 8 with min: 0 and max: 18446744073709551615 From the docs: C Type Swift Type bool CBool char, signed char CChar unsigned char CUnsignedChar short CShort unsigned short CUnsignedShort int CInt unsigned int CUnsignedInt long CLong unsigned long CUnsignedLong long long CLongLong unsigned long long CUnsignedLongLong wchar_t CWideChar char16_t CChar16 char32_t CChar32 float CFloat double CDouble Integers Integers can be signed or unsigned. When signed, they can be either positive or negative and when unsigned, they can only be positive. Apple states: Unless you need to work with a specific size of integer, always use Int for integer values in your code. This aids code consistency and interoperability. Even on 32-bit platforms, Int [â€¦] is large enough for many integer ranges. Fixed width integer types with their accompanying byte sizes as the variable names: // Exact integer types let aOneByteInt: Int8 = 127 let aOneByteUnsignedInt: UInt8 = 255 let aTwoByteInt: Int16 = 32767 let aTwoByteUnsignedInt: UInt16 = 65535 let aFourByteInt: Int32 = 2147483647 let aFourByteUnsignedInt: UInt32 = 4294967295 let anEightByteInt: Int64 = 9223372036854775807 let anEightByteUnsignedInt: UInt64 = 18446744073709551615 // Minimum integer types let aTinyInt: Int8 = 127 let aTinyUnsignedInt: UInt8 = 255 let aMediumInt: Int16 = 32767 let aMediumUnsignedInt: UInt16 = 65535 let aNormalInt: Int32 = 2147483647 let aNormalUnsignedInt: UInt32 = 4294967295 let aBigInt: Int64 = 9223372036854775807 let aBigUnsignedInt: UInt64 = 1844674407370955","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:2:2","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Enum \u0026 Bitmask Types Enumeration types can be defined as follows: // Specifying a typed enum with a name (recommended way) enum UITableViewCellStyle: Int { case default, valueOne, valueTwo, subtitle } // Accessing it: let cellStyle: UITableViewCellStyle = .default As of Swift 3, all enum options should be named in lowerCamelCased. Working with Bitmasks Newer Swift versions have a nice substitute for the old NS_OPTIONS macro for creating bitmasks to compare to. An example for posterity: struct Options: OptionSet { let rawValue: Int init(rawValue: Int) { self.rawValue = rawValue } init(number: Int) { self.init(rawValue: 1 \u003c\u003c number) } static let OptionOne = Options(number: 0) static let OptionTwo = Options(number: 1) static let OptionThree = Options(number: 2) } let options: Options = [.OptionOne, .OptionTwo] options.contains(.OptionOne) // true options.contains(.OptionThree) // false ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:2:3","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Type Casting Sometimes it is necessary to cast an object into a specific class or data type. Examples of this would be casting from a Float to an Int or from a UITableViewCell to a subclass such as RPTableViewCell. Checking Types Swift uses is and as both for checking object types as well as conformance to a given protocol. Operator: is Checking object type using is: if item is Movie { movieCount += 1 print(\"It is a movie.\") } else if item is Song { songCount += 1 print(\"It is a song.\") } The is operator returns true if an instance is of that object type, or conforms to the specified protocol, and returns false if it does not. Operators: as? and as! If you want to be able to easily access the data during one of these checks, you can use as? to optionally (or as! to force) unwrap the object when necessary: for item in library { if let movie = item as? Movie { print(\"Director: \\(movie.director)\") } else if let song = item as? Song { print(\"Artist: \\(song.artist)\") } } The as? version of the downcast operator returns an optional value of the object or protocolâ€™s type, and this value is nil if the downcast fails or this instance does not conform to the specified protocol. The as! version of the downcast operator forces the downcast to the specified object or protocol type and triggers a runtime error if the downcast does not succeed. Casting from Generic Types If youâ€™re working with AnyObject objects given from the Cocoa API, you can use: for movie in someObjects as! [Movie] { // do stuff } If given an array with Any objects, you can use a switch statement with the type defined for each case: var things = [Any]() for thing in things { switch thing { case 0 as Int: print(\"Zero as an Int\") case let someString as! String: print(\"S string value of \\\"\\(someString)\\\"\") case let (x, y) as! (Double, Double): print(\"An (x, y) point at \\(x), \\(y)\") case let movie as! Movie: print(\"A movie called '\\(movie.name)' by director \\(movie.director)\") default: print(\"Didn't match any of the cases specified\") } } Basic Casting Swift also offers some simple methods of casting between itâ€™s given data types. // Example 1: let aDifferentDataType: Float = 3.14 let anInt: Int = Int(aDifferentDataType) // Example 2: let aString: String = String(anInt) Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:2:4","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Operators Swift supports most standard C operators and improves several capabilities to eliminate common coding errors. The assignment operator = does not return a value, to prevent it from being mistakenly used when the equal to operator == is intended. Arithmetic operators (+, -, *, /, %) detect and disallow value overflow, to avoid unexpected results when working with numbers that become larger or smaller than the allowed value range of the type that stores them. Arithmetic Operators Operator Purpose + Addition - Subtraction * Multiplication / Division % Remainder Comparative Operators Operator Purpose == Equal to === Identical to != Not equal to !== Not identical to ~= Pattern match \u003e Greater than \u003c Less than \u003e= Greater than or equal to \u003c= Less than or equal to Assignment Operators Operator Purpose = Assign += Addition -= Subtraction *= Multiplication /= Division %= Remainder \u0026= Bitwise AND |= Bitwise Inclusive OR ^= Exclusive OR Â«= Shift Left Â»= Shift Right Logical Operators Operator Purpose ! NOT \u0026\u0026 Logical AND || Logical OR Range Operators Operator Purpose ..\u003c Half-open range â€¦ Closed range Bitwise Operators Operator Purpose \u0026 Bitwise AND | Bitwise Inclusive OR ^ Exclusive OR ~ Unary complement (bit inversion) Â« Shift Left Â» Shift Right Overflow and Underflow Operators Typically, assigning or incrementing an integer, float, or double past itâ€™s range would result in a runtime error. However, if youâ€™d instead prefer to safely truncate the number of available bits, you can opt-in to have the variable overflow or underflow using the following operators: Operator Purpose \u0026+ Addition \u0026- Subtraction \u0026* Multiplication Example for unsigned integers (works similarly for signed): var willOverflow = UInt8.max // willOverflow equals 255, which is the largest value a UInt8 can hold willOverflow = willOverflow \u0026+ 1 // willOverflow is now equal to 0 var willUnderflow = UInt8.min // willUnderflow equals 0, which is the smallest value a UInt8 can hold willUnderflow = willUnderflow \u0026- 1 // willUnderflow is now equal to 255 Other Operators Operator Purpose ?? Nil coalescing ?: Ternary conditional ! Force unwrap object value ? Safely unwrap object value Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:3:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Operator Overloading Swift allows you to overwrite existing operators or define new operators for existing or custom types. For example, this is why in Swift you can join strings using the + operator, even though it is typically used for math. Operator overloading is limited to the following symbols, / = - + * % \u003c \u003e ! \u0026 | ^ . ~, however you cannot overload the = operator by itself (it must be combined with another symbol). Operators can be specified as: prefix: goes before an object such as -negativeNumber infix: goes between two objects, such as a + b postfix: goes after an object, such as unwrapMe! ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:4:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Custom operators associativity: defines how operators of the same precedence are grouped together (left, right) precedence: gives some operators higher priority than others; these operators are applied first. Refer Operator Declarations to see full details about operator associativity and precedence. Example: DefaultPrecedence group // declare first and set rules with a precedence group infix operator ** // use DefaultPrecedence group Custom Precedence group // define a custom precedence group precedencegroup ExponentiationPrecedence { higherThan: MultiplicationPrecedence associativity: right // none, left, right //assignment: false } // now, replace original declaration of ** with infix operator **: ExponentiationPrecedence Thatâ€™s it. // impelment infix func ** (x: Double, p: Double) -\u003e Double { return pow(x, p) } 2**3 // 8 2**3**2 // 512 1+2**3**2 // 513 5*2**3**2 // 2560 see also docs Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:4:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Declaring Classes Classes are typically declared using separate .swift files, but multiple classes can also be created within the same file if youâ€™d like to organize it that way. Unlike Objective-C, thereâ€™s no need for an interface file (.h) in Swift. The implementation file should contain (in this order): Any needed import statements A class declaration which contains any constants or variables necessary for the class All public and private functions Example: MyClass.swift import UIKit class MyClass { // Declare any constants or variables at the top let kRPErrorDomain = \"com.myIncredibleApp.errors\" var x: Int, y: Int // MARK:- Class Methods, e.g. MyClass.functionName() class func alert() { print(\"This is a class function.\") } // MARK:- Instance Methods, e.g. myClass.functionName() init(x: Int, y: Int) { self.x = x self.y = y } // MARK:- Private Methods private func pointLocation() -\u003e String { return \"x: \\(x), y: \\(y)\" } } Instantiation When you want to create a new instance of a class, you use the syntax: let myClass = MyClass(x: 1, y: 2) where x and y are variables that are passed in at the time of instantiation. Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:5:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Declarations More info here in the docs. Preprocessor Swift doesnâ€™t come with a preprocessor so it only supports a limited number of statements for build time. Things like #define have been replaced with global constants defined outside of a class. Directive Purpose #if An if conditional statement #elif An else if conditional statement #else An else conditional statement #endif An end if conditional statement Imports Directive Purpose import Imports a framework Constants \u0026 Variables Directive Purpose let Declares local or global constant var Declares a local or global variable class Declares a class-level constant or variable static Declares a static type Classes, Structure, Functions and Protocols Directive Purpose typealias Introduces a named alias of an existing type enum Introduces a named enumeration struct Introduces a named structure class Begins the declaration of a class init Introduces an initializer for a class, struct or enum init? Produces an optional instance or an implicitly unwrapped optional instance; can return nil deinit Declares a function called automatically when there are no longer any references to a class object, just before the class object is deallocated func Begins the declaration of a function protocol Begins the declaration of a formal protocol static Defines as type-level within struct or enum convenience Delegate the init process to another initializer or to one of the classâ€™s designated initializers extension Extend the behavior of class, struct, or enum subscript Adds subscripting support for objects of a particular type, normally for providing a convenient syntax for accessing elements in a collective, list or sequence override Marks overriden initializers Operators Directive Purpose operator Introduces a new infix, prefix, or postfix operator Declaration Modifiers Directive Purpose dynamic Marks a member declaration so that access is always dynamically dispatched using the Objective-C runtime and never inlined or devirtualized by the compiler final Specifies that a class canâ€™t be subclassed, or that a property, function, or subscript of a class canâ€™t be overridden in any subclass lazy Indicates that the propertyâ€™s initial value is calculated and stored at most once, when the property is first accessed optional Specifies that a protocolâ€™s property, function, or subscript isnâ€™t required to be implemented by conforming members required Marks the initializer so that every subclass must implement it weak Indicates that the variable or property has a weak reference to the object stored as its value Access Control Directive Purpose open Can be subclassed outside of its own module and its methods overridden as well; truly open to modification by others and useful for framework builders public Can only be subclassed by its own module or have its methods overridden by others within the same module internal (Default) Indicates the entities are only available to the entire module that includes the definition, e.g. an app or framework target fileprivate Indicates the entities are available only from within the source file where they are defined private Indicates the entities are available only from within the declaring scope within the file where they are defined (e.g. within the { } brackets only) public class AccessLevelsShowCase { // Property accessible for other modules public var somePublicProperty = 0 // Property accessible from the module var someInternelProperty = 1 // Property accessible from its own defining source file fileprivate func someFilePrivateMethod() {} // Property accessible fro its enclosing declaration private func somePrivateMethod() {} } Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:6:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Literals Literals are compiler directives which provide a shorthand notation for creating common objects. Syntax What it does \"string\" Returns a String object 28 Returns an Int 3.14, 0xFp2, 1.25e2 Returns a Double object true, false Returns a Bool object [] Returns an Array object [keyName:value] Returns a Dictionary object 0b Returns a binary digit 0o Returns an octal digit 0x Returns a hexadecimal digit Strings Special characters can be included: Null Character: \\0 Backslash: \\\\ (can be used to escape a double quote) Horizontal Tab: \\t Line Feed: \\n Carriage Return: \\r Double Quote: \\\" Single Quote: \\' Unicode scalar: \\u{n} where n is between one and eight hexadecimal digits Multiline string literal let json = \"\"\" { \"username\": \"David\", \"loginCount\": 2} \"\"\" Array Access Syntax let example = [ \"hi\", \"there\", 23, true ] print(\"item at index 0: \\(example[0])\") Dictionary Access Syntax let example = [ \"hi\" : \"there\", \"iOS\" : \"people\" ] if let value = example[\"hi\"] { print(\"hi \\(value)\") } Mutability For mutable literals, declare it with var; immutable with let. Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:7:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Functions Declaration Syntax Functions without a return type use this format: // Does not return anything or take any arguments func doWork() { // Code } class precedes declarations of class functions: // Call on a class, e.g. MyClass.someClassFunction() class func someClassFunction() { // Code } static is similar to class functions where you donâ€™t need an instance of the class or struct in order to call a method on it: // Call on a class/struct, e.g. MyStruct.someStaticFunction() static func someStaticFunction() { // Code } Declare instance functions: // Called on an instance of a class, e.g. myClass.someInstanceFunction() func doMoreWork() { // Code } Function arguments are declared within the parentheses: // Draws a point func draw(point: CGPoint) Return types are declared as follows: // Returns a String object for the given String argument func sayHelloToMyLilFriend(lilFriendsName: String) -\u003e String { return \"Oh hello, \\(lilFriendsName). Cup of tea?\" } You can have multiple return values, referred to as a tuple: // Returns multiple objects func sayHelloToMyLilFriend(lilFriendsName: String) -\u003e (msg: String, nameLength: Int) { return (\"Oh hello, \\(lilFriendsName). Cup of tea?\", countElements(lilFriendsName)) } var hello = sayHelloToMyLilFriend(\"Rob\") print(hello.msg) // \"Oh hello, Rob. Cup of tea?\" print(hello.nameLength) // 3 And those multiple return values can be optional: func sayHelloToMyLilFriend(lilFriendsName: String) -\u003e (msg: String, nameLength: Int)? By default, external parameter names are given when you call the function, but you can specify that one or more are not shown in the method signature by putting a _ symbol in front of the parameter name: func sayHelloToMyLilFriend(_ lilFriendsName: String) { // Code } sayHelloToMyLilFriend(\"Rob\") or you can rename the variable once within the method scope: func sayHelloToMyLilFriend(friendsName lilFriendsName: String) { // Code } sayHelloToMyLilFriend(friendsName: \"Rob\") // and local variable is `lilFriendsName` You can also specify default values for the parameters: func sayHelloToMyLilFriend(_ lilFriendsName: String = \"Rob\") { // Code } sayHelloToMyLilFriend() // \"Oh hello, Rob. Cup of tea?\" sayHelloToMyLilFriend(\"Jimbob\") // \"Oh hello, Jimbob. Cup of tea?\" Swift also supports variadic parameters so you can have an open-ended number of parameters passed in: func sayHelloToMyLilFriends(_ lilFriendsName: String...) { // Code } sayHelloToMyLilFriends(\"Rob\", \"Jimbob\", \"Cletus\") // \"Oh hello, Rob, Jimbob and Cletus. Cup of tea?\" And lastly, you can also use a prefix to declare input parameters as inout. An in-out parameter has a value that is passed in to the function, is modified by the function, and is passed back out of the function to replace the original value. You may remember inout parameters from Objective-C where you had to sometimes pass in an \u0026error parameter to certain methods, where the \u0026 symbol specifies that youâ€™re actually passing in a pointer to the object instead of the object itself. The same applies to Swiftâ€™s inout parameters now as well. Calling Functions Functions are called using dot syntax: myClass.doWork() or self.sayHelloToMyLilFriend(\"Rob Phillips\") self is a reference to the functionâ€™s containing class. At times, it is necessary to call a function in the superclass using super.someMethod(). Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:8:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Constants and Variables Declaring a constant or variable allows you to maintain a reference to an object within a class or to pass objects between classes. Constants are defined with let and variables with var. By nature, constants are obviously immutable (i.e. cannot be changed once they are instantiated) and variables are mutable. class MyClass { let text = \"Hello\" // Constant var isComplete: Bool // Variable } There are many ways to declare properties in Swift, so here are a few examples: var myInt = 1 // inferred type var myExplicitInt: Int = 1 // explicit type var x = 1, y = 2, z = 3 // declare multiple variables let (a,b) = (1,2) // declare multiple constants Getters and Setters In Objective-C, variables were backed by getters, setters, and private instance variables created at build time. However, in Swift getters and setters are only used for computed properties and constants actually donâ€™t have a getter or setter at all. The getter is used to read the value, and the setter is used to write the value. The setter clause is optional, and when only a getter is needed, you can omit both clauses and simply return the requested value directly. However, if you provide a setter clause, you must also provide a getter clause. You can overrride the getter and setter of a property to create the illusion of the Objective-C property behavior, but youâ€™d need to store them as a private property with a different name (not recommended for most scenarios): private var _x: Int = 0 var x: Int { get { print(\"Accessing x...\") return _x } set { print(\"Setting x...\") _x = newValue } } Property Observer Swift also has callbacks for when a property will be or was set using willSet and didSet shown below: willset: before assignment didSet: after assignment class LightBulb { static let maxCurrent = 30 var current = 0 { willSet(newCurrent) { // do something before value assignment // newValue -\u003e newCurrent print(\"Current value changed, the change is \\(abs(current- newCurrent))\") } didSet { // do somthing afther value assignment if current == LightBulb.maxCurrent { print(\"current get to maximum point\") } // oldValue } } } let bulb = LightBulb() bulb.current = 20 bulb.current = 30 bulb.current = 40 Back to top Lazy Property lazy: only compute once and remember the value, wonâ€™t re-compute if called again. class ClosedRange { let start: Int let end: Int var width: Int { return end - start +1 } // note the = lazy var sum: Int = { var res = 0 print(\"run\") for i in self.start...self.end{ res += 1 } return }() // don't forget () init?(start: Int, end: Int){ if start \u003e end { return nil } self.start = start self.end = end } } // example if let range = ClosedRange(start: 0, end: 10_000) { range.width //1001 range.sum // will print out \"run\" range.sum // range.sum // } Back to top Accessing Local Variables Local variables and constants only exist within the scope of a function. func doWork() { let localStringVariable = \"Some local string variable.\" self.doSomething(string: localStringVariable) } Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:9:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Naming Conventions The general rule of thumb: Clarity and brevity are both important, but clarity should never be sacrificed for brevity. Functions and Properties These both use camelCase where the first letter of the first word is lowercase and the first letter of each additional word is capitalized. Class names and Protocols These both use CapitalCase where the first letter of every word is capitalized. ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:10:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Enums The options in an enum should be lowerCamelCased Functions These should use verbs if they perform some action (e.g. performInBackground). You should be able to infer what is happening, what arguments a function takes, or what is being returned just by reading a function signature. Example: // Correct func move(from start: Point, to end: Point) {} // Incorrect (likely too expressive, but arguable) func moveBetweenPoints(from start: Point, to end: Point) {} // Incorrect (not expressive enough and lacking argument clarity) func move(x: Point, y: Point) {} Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:10:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Closures Closures in Swift are similar to blocks in Objective-C and are essentially chunks of code, typically organized within a {} clause, that are passed between functions or to execute code as a callback within a function. Swiftâ€™s func functions are actually just a special case of a closure in use. Syntax { (params) -\u003e returnType in statements } Examples // Map just iterates over the array and performs whatever is in the closure on each item let people = [\"Rob\", \"Jimbob\", \"Cletus\"] people.map({ (person: String) -\u003e String in \"Oh hai, \\(person)...\" }) // Oh hai, Rob // Oh hai, Jimbob // Oh hai, Cletus // Closure for alphabetically reversing an array of names, where sorted is a Swift library function let names = [\"Francesca\", \"Joe\", \"Bill\", \"Sally\", ] var reversed = names.sorted { (s1: String, s2: String) -\u003e Bool in return s1 \u003e s2 } // Or on a single line: reversed = names.sorted{ (s1: String, s2: String) -\u003e Bool in return s1 \u003e s2 } // Or because Swift can infer the Bool type: reversed = names.sorted { s1, s2 in return s1 \u003e s2 } // Or because the return statement is implied: reversed = names.sorted { s1, s2 in s1 \u003e s2 } // Or even shorter using shorthand argument names, such as $0, $1, $2, etc.: reversed = names.sorted { $0 \u003e $1 } // Or just ridiculously short because Swift's String greater-than operator implementation exactly matches this function definition: reversed = names.sorted(by: \u003e) If the closure is the last parameter to the function, you can also use the trailing closure pattern. This is especially useful when the closure code is especially long and youâ€™d like some extra space to organize it: func someFunctionThatTakesAClosure(closure: () -\u003e ()) { // function body goes here } // Instead of calling like this: someFunctionThatTakesAClosure({ // closure's body goes here }) // You can use trailing closure like this: someFunctionThatTakesAClosure() { // trailing closure's body goes here } Capturing Values A closure can capture constants and variables from the surrounding context in which it is defined. The closure can then refer to and modify the values of those constants and variables from within its body, even if the original scope that defined the constants and variables no longer exists. In Swift, the simplest form of a closure that can capture values is a nested function, written within the body of another function. A nested function can capture any of its outer functionâ€™s arguments and can also capture any constants and variables defined within the outer function. func makeIncrementor(forIncrement amount: Int) -\u003e () -\u003e Int { var runningTotal = 0 func incrementor() -\u003e Int { runningTotal += amount return runningTotal } return incrementor } Swift determines what should be captured by reference and what should be copied by value. You donâ€™t need to annotate a variable to say that they can be used within the nested function. Swift also handles all memory management involved in disposing of variables when they are no longer needed by the function. Capturing Self If you create a closure that references self.* it will capture self and retain a strong reference to it. This is sometimes the intended behavior, but often could lead to retain cycles where both objects wonâ€™t get deallocated at the end of their lifecycles. The two best options are to use unowned or weak. This might look a bit messy, but saves a lot of headache. Use unowned when you know the closure will only be called if self still exists, but you donâ€™t want to create a strong (retain) reference. Use weak if there is a chance that self will not exist, or if the closure is not dependent upon self and will run without it. If you do use weak also remember that self will be an optional variable and should be checked for existence. typealias SomeClosureType = (_ value: String) -\u003e () class SomeClass { fileprivate var currentValue = \"\" init() { someMethod { (value) in // Retained self self.currentValue = value } someMethod { [unowned self] (value) in // Not retained, but ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:11:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Generics Coming soonâ€¦ Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:12:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Control Statements Swift uses all of the same control statements that other languages have: If-Else If-Else if someTestCondition { // Code to execute if the condition is true } else if someOtherTestCondition { // Code to execute if the other test condition is true } else { // Code to execute if the prior conditions are false } As you can see, parentheses are optional. Ternary Operators The shorthand notation for an if-else statement is a ternary operator of the form: someTestCondition ? doIfTrue : doIfFalse Example: func stringForTrueOrFalse(trueOrFalse: Bool) -\u003e String { return trueOrFalse ? \"True\" : \"False\" } Nil Coalescing Operators In Swift, we need to consider the use of optional values. One very basic way to handle nil cases is with an if-else statement: func stringForOptionalExistence(optionalValue: String?) -\u003e String { if optionalValue != nil { return optionalValue } else { return \"Empty\" } } In this particular case, we are returning optionalValue if it is not nil, and \"Empty\" if optionalValue is nil. The shorthand notation for this type of if(!=nil)-else statement is a nil coalescing operator of the form: optionalValue ?? nonOptionalValue Example: func stringForOptionalExistence(optionalValue: String?) -\u003e String { return optionalValue ?? \"Empty\" } For Loops Swift enables you to use ranges inside of for loops now: for index in 1...5 { print(\"\\(index)times 5 is \\(index * 5)\") } // Or if you don't need the value of the index let base = 3, power = 10 var answer = 1 for _ in 1...power { answer *= base } print(\"\\(base)to the power of \\(power)is \\(answer)\") // prints \"3 to the power of 10 is 59049\" Enumerating arrays \u0026 dictionaries // We explicitly cast to the Movie class from AnyObject class for movie in someObjects as [Movie] { // Code to execute each time } // Enumerating simple array let names = [\"Anna\", \"Alex\", \"Brian\", \"Jack\"] for name in names { print(\"Hello, \\(name)!\") } // Enumerating simple dictionary let numberOfLegs = [\"spider\": 8, \"ant\": 6, \"cat\": 4] for (animalName, legCount) in numberOfLegs { print(\"\\(animalName)s have \\(legCount)legs\") } If you need to cast to a certain object type, see the earlier discussion about the as! and as? keywords. While Loop while someTestCondition { // Code to execute while the condition is true } Repeat While Loop repeat { // Code to execute while the condition is true } while someTestCondition Switch Switch statements are often used in place of if statements if there is a need to test if a certain variable matches the value of another constant or variable. For example, you may want to test if an error code integer you received matches an existing constant value or if itâ€™s a new error code. switch errorStatusCode { case .network: // Code to execute if it matches case .wifi: // Code to execute if it matches default: // Code to execute if nothing else matched } Switch statements in Swift do not fall through the bottom of each case and into the next one by default. Instead, the entire switch statement finishes its execution as soon as the first matching switch case is completed, without requiring an explicit break statement. This makes the switch statement safer and easier to use than in C, and avoids executing more than one switch case by mistake. Exiting Loops Although break is not required in Swift, you can still use a break statement to match and ignore a particular case, or to break out of a matched case before that case has completed its execution. return : Stops execution and returns to the calling function. It can also be used to return a value from a function. break : Used to stop the execution of a loop. Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:13:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Extension Extensions add new functionality to an existing class, structure, enumeration or protocol type extension String { // Extending String type to calculate if a String instance is truthy of falsy var boolValue:Bool { if self == \"1\" return true } return false } let isTrue = \"0\".boolValue Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:14:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Protocol Define protocol Codable { // definitions var description: String var mustBeSettable: Int { get set } var doesNotNeedToBeSettable: Int { get } func dance () -\u003e Double static func someTypeMethod() mutating func toggle() // modify (or mutate) the instance it belongs to init(someParameter: Int) // require specific initializers } Usage import Foundation struct UserInfo: Codable { let username: String let loginCount: Int } extension UserInfo: CustomStringConvertible { var description: String { return \"\\(username)has tried to login \\(loginCount)time(s)\" } } ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:15:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Protocol Extension Protocols can be extended to provide method, initializer, subscript, and computed property implementations to conforming types. Very Important and Useful: Implementation to any method or computed property requirement of that protocol can only be in extension extension RandomNumberGenerator { func randomBool() -\u003e Bool { return random() \u003e 0.5 } } By creating an extension on the protocol, all conforming types automatically gain this method implementation without any additional modification. You can use protocol extensions to provide a default implementation to any method or computed property requirement of that protocol. extension PrettyTextRepresentable { var prettyTextualDescription: String { return textualDescription } } Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:15:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Error Handling Representing an Error enum BeverageMachineError: Error { case invalidSelection case insufficientFunds case outOfStock } func selectBeverage (_ selection: Int) throws -\u003e String { // do something return \"Waiting for beverage...\" } // us do...catch to handle error throwed by func let message:String do { message = try selectBeverage(20) } catch BeverageMachineError.invalidSelection { print(\"Invalid selection\") } catch BeverageMachineError.insufficientFunds { print(\"Insufficient Funds\") } catch BeverageMachineError.outOfStock { print(\"Out of Stock\") } catch { print (\"Generic error\") } // if throw error, return nil let message = try? selectBeverage(10) // if throw error, get a runtime error let message = try! selectBeverage(10) Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:16:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Passing Information Coming soonâ€¦ Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:17:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"User Defaults User defaults are basically a way of storing simple preference values which can be saved and restored across app launches. It is not meant to be used as a data storage layer, like Core Data or sqlite. ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:18:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Storing Values let userDefaults = UserDefaults.standard userDefaults.setValue(\"Some Value\", forKey: \"RPSomeUserPreference\") ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:18:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Retrieving Values let userDefaults = UserDefaults.standard let someValue = userDefaults.value(forKey: \"RPSomeUserPreference\") as AnyObject? There are also other convenience functions on UserDefaults instances such as bool(forKey:...), string(forKey:...), etc. Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:18:2","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Common Patterns For a comprehensive list of design patterns, as established by the Gang of Four, look here: Design Patterns in Swift ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:19:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Singletons Singletonâ€™s are a special kind of class where only one instance of the class exists for the current process. They are a convenient way to share data between different parts of an app without creating global variables or having to pass the data around manually, but they should be used sparingly since they often create tighter coupling between classes. To turn a class into a singleton, you use the following implementation where the function name is prefixed with shared plus another word which best describes your class. For example, if the class is a network or location manager, you would name the function sharedManager instead of sharedInstance. class MyClass { // MARK:- Instantiation // Naming convention: // sharedInstance, sharedManager, sharedController, etc. // depending on the class type static let sharedInstance = MyClass() // This prevents others from using the default '()' initializer for this class. private init() {} var isReady = true // More class code here } Explanation: The static constant sharedInstance is run as dispatch_once the first time that variable is accessed to make sure the initialization is atomic. This ensures it is thread safe, fast, lazy, and also bridged to ObjC for free. More from here. Usage: You would get a reference to that singleton class in another class with the following code: // Now you could do let myClass = MyClass.sharedInstance let answer = myClass.isReady ? \"Yep!\" : \"Nope!\" print(\"Are you ready to rock and roll? \\(answer)\") Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:19:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Unicode Support Although I donâ€™t recommend this, Swift will compile even if you use emojiâ€™s in your code since it offers Unicode support. More info from Apple here Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:20:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"FileIO ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:21:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"C style FileIO let fd = fopen(\"aFile.txt\", \"w\") fwrite(\"Hello Swift!\", 12, 1, fd) let res = fclose(file) if res != 0 { print(strerror(errno)) } let fd = fopen(\"aFile.txt\", \"r\") var array = [Int8](count: 13, repeatedValue: 0) fread(\u0026array, 12, 1, fd) fclose(fd) let str = String.fromCString(array) print(str) // Hello Swift! ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:21:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Swift Style FileIO let path = Bundle.main.path(forResource:\"test\", ofType: \"txt\") // read let lines = try? String(contentsOfFile: path!) .split{$0 == \"\\n\"} .map(String.init) // write do { let lines = self._outlines.joined(separator: \"\\n\") try lines.write(to: url, atomically: false, encoding: .utf8) } catch{} Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:21:2","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Machine Learning"],"content":"æå‡ï¼ˆBoostingï¼‰æ–¹æ³•ï¼š é€šè¿‡æ”¹å˜è®­ç»ƒæ ·æœ¬çš„æƒé‡ï¼ˆæ¦‚ç‡åˆ†å¸ƒï¼‰ï¼Œå­¦ä¹ nä¸ªåˆ†ç±»å™¨ï¼Œå¹¶å°†è¿™äº›åˆ†ç±»å™¨çº¿æ€§ç»„åˆï¼Œæé«˜åˆ†ç±»æ€§èƒ½ã€‚ ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:0:0","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"1. AdaBoost AdaBoosté€šè¿‡æé«˜è¢«å‰ä¸€è½®å¼±åˆ†ç±»å™¨é”™è¯¯åˆ†ç±»æ ·æœ¬çš„æƒå€¼ï¼Œä»è€Œé™ä½è¢«æ­£ç¡®åˆ†ç±»æ ·æœ¬çš„æƒå€¼ï¼Œå¹¶é‡‡å–åŠ æƒå¤šæ•°è¡¨å†³çš„æ–¹æ³•è¾¾åˆ°åˆ†ç±»ç›®çš„ã€‚ è¾“å…¥ï¼šè®­ç»ƒæ•°æ®é›†$T={(x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)}$, $\\mathcal{Y} = {-1,+1}$; è¾“å‡ºï¼šåˆ†ç±»å™¨$G(x)$ 1). åˆå§‹åŒ–è®­ç»ƒæ•°æ®æƒå€¼åˆ†å¸ƒ $$D_1 = (w_{11}, \\cdots, w_{1i}, \\cdots, w_{1N}), w_{1i} = \\frac{1}{N}, i = 1,2,\\cdots,N$$ 2). å¯¹ $m = 1ï¼Œ2ï¼Œ\\cdots, M$ a.å¯¹æƒå€¼åˆ†å¸ƒ$D_m$çš„è®­ç»ƒæ•°æ®é›†å­¦ä¹ ï¼Œå¾—åˆ°åŸºæœ¬åˆ†ç±»å™¨ $$ G_{m}(x): \\mathcal{X} \\rightarrow{-1,+1} $$ b.è®¡ç®—$G(x)$åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šçš„åˆ†ç±»è¯¯å·®ç‡ $$ e_{m}=\\sum_{i=1}^{N} P\\left(G_{m}\\left(x_{i}\\right) \\neq y_{i}\\right)=\\sum_{i=1}^{N} w_{m i} I\\left(G_{m}\\left(x_{i}\\right) \\neq y_{i}\\right) $$ c. è®¡ç®—$G(x)$çš„ç³»æ•° $$ \\alpha_{m}=\\frac{1}{2} \\log \\frac{1-e_{m}}{e_{m}} $$ d. æ›´æ–°è®­ç»ƒæ•°æ®çš„æƒå€¼åˆ†å¸ƒ $$ D_{m+1}=\\left(w_{m+1,1}, \\cdots, w_{m+1, i}, \\cdots, w_{m+1, N}\\right) $$ $$ w_{m+1, i} = \\frac{w_{m i}}{Z_{m}} \\exp \\left(-\\alpha_{m} y_{i} G_{m}\\left(x_{i}\\right)\\right), \\quad i=1,2, \\cdots, N $$ å…¶ä¸­ï¼Œ $$ Z_{m}=\\sum_{i=1}^{N} w_{m i} \\exp \\left(-\\alpha_{m} y_{i} G_{m}\\left(x_{i}\\right)\\right) $$ 3ï¼‰æ„å»ºåŸºæœ¬çº¿æ€§åˆ†ç±»å™¨ç»„åˆ $$ f(x)=\\sum_{m=1}^{M} \\alpha_{m} G_{m}(x) $$ å¾—åˆ°æœ€ç»ˆåˆ†ç±»å™¨ $$ \\begin{aligned} G(x) \u0026=\\operatorname{sign}(f(x)) \\cr \u0026=\\operatorname{sign}\\left(\\sum_{m=1}^{M} \\alpha_{m} G_{m}(x)\\right) \\end{aligned} $$ ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:1:0","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"1.1 AdaBoostç®—æ³•è¯¯å·® AdaBoostç®—æ³•æœ€ç»ˆåˆ†ç±»å™¨è®­ç»ƒè¯¯å·®ç•Œä¸º $$ \\frac{1}{N} \\sum_{i=1}^{N} I\\left(G\\left(x_{i}\\right) \\neq y_{i}\\right) \\leqslant \\frac{1}{N} \\sum_{i} \\exp \\left(-y_{i} f\\left(x_{i}\\right)\\right)=\\prod_{m} Z_{m} $$ è¿™ä¸€å®šç†è¯´æ˜ï¼Œæ¯ä¸€è½®é€‰å–é€‚å½“çš„$G_m$ä½¿$Z_m$æœ€å°ï¼Œä»è€Œä½¿è®­ç»ƒè¯¯å·®ä¸‹é™æœ€å¿«ã€‚ å¯¹äºäºŒåˆ†ç±»é—®é¢˜ï¼š $$ \\begin{aligned} \\prod_{m=1}^{M} Z_{m} \u0026=\\prod_{m=1}^{M}[2 \\sqrt{e_{m}\\left(1-e_{m}\\right)}] \\cr \u0026=\\prod_{m=1}^{M} \\sqrt{\\left(1-4 \\gamma_{m}^{2}\\right)} \\cr \u0026 \\leqslant \\exp \\left(-2 \\sum_{m=1}^{M} \\gamma_{m}^{2}\\right) \\end{aligned} $$ å…¶ä¸­ï¼Œ $\\gamma_{m}=\\frac{1}{2}-e_{m}$ ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:1:1","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"1.2 AdaBoostç®—æ³•è§£é‡Š AdaBooostå¯ä»¥è®¤ä¸ºï¼šæ¨¡å‹ä¸ºåŠ æ³•æ¨¡å‹ï¼ŒæŸå¤±å‡½æ•°ä¸ºæŒ‡æ•°å‡½æ•°ï¼Œå­¦ä¹ ç®—æ³•ä¸ºå‰å‘åˆ†å¸ƒç®—æ³•çš„äºŒåˆ†ç±»å­¦ä¹ æ–¹æ³• 1.2.1 å‰å‘åˆ†æ­¥ç®—æ³• è€ƒè™‘åŠ æ³•æ¨¡å‹ï¼ˆadditive modelï¼‰ $$ f(x)=\\sum_{m=1}^{M} \\beta_{m} b\\left(x ; \\gamma_{m}\\right) $$ å…¶ä¸­ï¼Œ$b(x; \\gamma_m)$ä¸ºåŸºå‡½æ•°ï¼Œ$gamma_m$ä¸ºå‚æ•°ï¼Œ $\\beta_m$ä¸ºç³»æ•°ã€‚ åœ¨ç»™å®šè®­ç»ƒé›†å’ŒæŸå¤±å‡½æ•°$L(y,f(x))$çš„æ¡ä»¶ä¸‹ï¼Œå­¦ä¹ åŠ æ³•æ¨¡å‹$f(x)$æˆä¸ºç»éªŒé£é™©æå°åŒ–ï¼ˆæŸå¤±å‡½æ•°æå°åŒ–ï¼‰é—®é¢˜ï¼š $$ \\min_{\\beta_{m}, \\gamma_{m}} \\sum_{i=1}^{N} L\\left(y_{i}, \\sum_{m=1}^{M} \\beta_{m} b\\left(x_{i} ; \\gamma_{m}\\right)\\right) $$ å‰å‘åˆ†å¸ƒç®—æ³•æ€æƒ³æ˜¯ï¼š ä»å‰å‘åï¼Œæ¯ä¸€æ­¥åªå­¦ä¸€ä¸ªåŸºå‡½æ•°åŠå…¶ç³»æ•°ï¼Œé€æ­¥é€¼è¿‘ä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼Œè¾¾åˆ°ä¼˜åŒ–æ­¥éª¤ç®€åŒ–çš„ç›®çš„ã€‚ å› æ­¤ï¼Œæ¯ä¸€æ­¥åªéœ€ä¼˜åŒ–å¦‚ä¸‹æŸå¤±å‡½æ•°ï¼š $$ \\min_{\\beta, \\gamma} \\sum_{i=1}^{N} L\\left(y_{i}, \\beta b\\left(x_{i} ; \\gamma\\right)\\right) $$ ç®—æ³•æ­¥éª¤ è¾“å…¥ï¼šè®­ç»ƒæ•°æ®é›†$T=\\lbrace (x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)\\rbrace$, æŸå¤±å‡½æ•°$L(y,f(x))$;åŸºå‡½æ•°é›†$\\lbrace b(x;\\gamma) \\rbrace$; è¾“å‡ºï¼šåŠ æ³•æ¨¡å‹$f(x)$ 1ï¼‰åˆå§‹åŒ–$f_0(x) = 0$ 2) å¯¹$m = 1,2,\\cdots, M$ a.æå°åŒ–æŸå¤±å‡½æ•° $$ \\left(\\beta_{m}, \\gamma_{m}\\right)=\\arg \\min _{\\beta, \\gamma} \\sum_{i=1}^{N} L\\left(y_{i}, f_{m-1}\\left(x_{i}\\right)+\\beta b\\left(x_{i} ; \\gamma\\right)\\right) $$ å¾—åˆ°å‚æ•°$\\beta_m$, $\\gamma_m$ã€‚ b.æ›´æ–° $$ f_{m}(x)=f_{m-1}(x)+\\beta_{m} b\\left(x ; \\gamma_{m}\\right) $$ 3ï¼‰å¾—åˆ°åŠ æ³•æ¨¡å‹ $$ f(x)=f_{M}(x)=\\sum_{m=1}^{M} \\beta_{m} b\\left(x ; \\gamma_{m}\\right) $$ ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:1:2","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"2. Boosting Tree æå‡æ ‘ğŸŒ²æ˜¯ä»¥å†³ç­–æ ‘ä¸ºåŸºæœ¬åˆ†ç±»å™¨çš„æå‡æ–¹æ³• ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:2:0","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"2.1 æå‡æ ‘æ¨¡å‹ é‡‡ç”¨åŠ æ³•æ¨¡å‹ï¼ˆåŸºå‡½æ•°çš„çº¿æ€§ç»„åˆï¼‰ä¸å‰å‘åˆ†å¸ƒç®—æ³•ï¼š $$ f_{M}(x)=\\sum_{m=1}^{M} T\\left(x ; \\Theta_{m}\\right) $$ å…¶ä¸­ $T\\left(x ; \\Theta_{m}\\right)$è¡¨ç¤ºå†³ç­–æ ‘ï¼Œ$\\Theta_{m}$å†³ç­–æ ‘å‚æ•°ï¼Œ $M$ä¸ºæ ‘çš„ä¸ªæ•° ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:2:1","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"2.2 æå‡æ ‘ç®—æ³• é‡‡ç”¨åŠ æ³•æ¨¡å‹å’Œå‰å‘åˆ†å¸ƒç®—æ³•å®ç°å­¦ä¹ ä¼˜åŒ–çš„è¿‡ç¨‹ã€‚ é¦–å…ˆç¡®å®šæå‡æ ‘$f_{0}(x)=0$ï¼Œ ç¬¬$m$æ­¥çš„æ¨¡å‹æ˜¯ $$ f_{m}(x)=f_{m-1}(x)+T\\left(x ; \\Theta_{m}\\right) $$ å…¶ä¸­ï¼Œ $f_{m-1}(x)$ä¸ºå½“å‰æ¨¡å‹ï¼Œé€šè¿‡ç»éªŒé£é™©æå°åŒ–ç¡®å®šä¸‹ä¸€åˆ»å†³ç­–æ ‘çš„å‚æ•°$\\Theta_{m}$ï¼š $$ \\hat \\Theta_m = \\arg \\min_{\\Theta_{m}} \\sum_{i=1}^{N} L(y_{i}, f_{m-1} (x_{i})+T (x_{i} ; \\Theta_{m} )) $$ 2.2.1 å›å½’é—®é¢˜æå‡æ ‘ è®­ç»ƒæ•°æ®é›†: $T=\\lbrace (x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)\\rbrace$, $x_{i} \\in \\mathcal{X} \\subseteq \\mathbf{R}^{n}$, $\\mathcal{X}$ä¸ºè¾“å…¥ç©ºé—´ï¼Œ $\\mathcal{Y} \\subseteq \\mathbf{R}$; å°†è¾“å…¥ç©ºé—´åˆ’åˆ†ä¸º$J$ä¸ªäº’ä¸ç›¸äº¤çš„åŒºåŸŸ$R1ï¼ŒR2, \\cdots, R_J$ï¼Œ å¹¶ä¸”æ¯ä¸ªåŒºåŸŸä¸Šç¡®å®šè¾“å‡ºçš„å¸¸é‡$c_j$ï¼Œé‚£ä¹ˆæ ‘å¯ä»¥è¡¨ç¤ºä¸ºï¼š $$ T(x ; \\Theta)=\\sum_{j=1}^{J} c_{j} I\\left(x \\in R_{j}\\right) $$ å…¶ä¸­ï¼Œ $$ \\Theta=\\lbrace \\left(R_{1}, c_{1}\\right),\\left(R_{2}, c_{2}\\right), \\cdots,\\left(R_{J}, c_{J}\\right)\\rbrace $$ è¡¨ç¤ºæ ‘çš„å´ä¸åˆ’åˆ†å’Œå„ä¸ªå–æ‚¦æ˜¯é‚£ä¸ªçš„å¸¸æ•°ã€‚ é‡‡ç”¨ä¸€ä¸‹å‰å‘åˆ†å¸ƒç®—æ³• $$ \\begin{aligned} \u0026f_{0}(x)=0\\cr \u0026\\begin{array}{l} f_{m}(x)=f_{m-1}(x)+T\\left(x ; \\Theta_{m}\\right), \\quad m=1,2, \\cdots, M \\cr f_{M}(x)=\\sum_{m=1}^{M} T\\left(x ; \\Theta_{m}\\right) \\end{array} \\end{aligned} $$ æ±‚è§£$\\hat \\Theta_{m}$ï¼Œ è‹¥ç”¨å¹³æ–¹è¯¯å·®æŸå¤±å‡½æ•°ï¼š $$ L(y, f(x))=(y-f(x))^{2} $$ åˆ™æŸå¤±å‡½æ•°ä¸ºï¼š $$ \\begin{aligned} L\\left(y, f_{m-1}(x)+T\\left(x ; \\Theta_{m}\\right)\\right) \u0026=\\left[y-f_{m-1}(x)-T\\left(x ; \\Theta_{m}\\right)\\right]^{2} \\cr \u0026=\\left[r-T\\left(x ; \\Theta_{m}\\right)\\right]^{2} \\end{aligned} $$ è¿™é‡Œï¼Œ $$ r=y-f_{m-1}(x) $$ æ˜¯å½“å‰æ¨¡å‹æ‹Ÿåˆæ•°æ®çš„æ®‹å·®ï¼ˆresidualï¼‰ã€‚å› æ­¤å¯¹äºå›å½’é—®é¢˜æå‡æ ‘ï¼Œåªéœ€æ‹Ÿåˆå½“å‰æ¨¡å‹æ®‹å·®ã€‚å¾—åˆ°$T\\left(x ; \\Theta_{m}\\right)$ï¼Œæ›´æ–°æ¨¡å‹ï¼Œå¾—åˆ°$f_m(x)$ã€‚ ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:2:2","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"3. æ¢¯åº¦æå‡ å½“æŸå¤±å‡½æ•°ä¸æ˜¯ç®€å•çš„å¹³æ–¹æŸå¤±ã€æŒ‡æ•°æŸå¤±æ—¶ï¼Œæå‡æ ‘çš„ä¼˜åŒ–å°±å¾ˆéš¾ã€‚æ¢¯åº¦æå‡ç®—æ³•åˆ©ç”¨æœ€é€Ÿä¸‹é™æ³•çš„è¿‘ä¼¼æ–¹æ³•ï¼Œè®¡ç®—æŸå¤±å‡½æ•°çš„è´Ÿæ¢¯åº¦åœ¨å½“å‰æ¨¡å‹çš„å€¼ $$ -\\left[\\frac{\\partial L\\left(y, f\\left(x_{i}\\right)\\right)}{\\partial f\\left(x_{i}\\right)}\\right]_{f(x)=f_{m-1}(x)} $$ å¹¶å°†å…¶ä½œä¸ºå›å½’é—®é¢˜æå‡æ ‘ç®—æ³•ä¸­çš„æ®‹å·®è¿‘ä¼¼å€¼ï¼Œæ‹Ÿåˆä¸€ä¸ªå›å½’æ ‘ã€‚ è¾“å…¥ï¼š è®­ç»ƒæ•°æ®é›†$T=\\lbrace (x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)\\rbrace$, $x_{i} \\in \\mathcal{X} \\subseteq \\mathbf{R}^{n}$,$\\mathcal{X}$ä¸ºè¾“å…¥ç©ºé—´ï¼Œ $\\mathcal{Y} \\subseteq \\mathbf{R}$; æŸå¤±å‡½æ•°$L(y,f(x))$ è¾“å‡ºï¼š å›å½’æ ‘$\\hat f(x)$ åˆå§‹åŒ– $$ f_{0}(x)=\\arg \\min _{c} \\sum_{i=1}^{N} L\\left(y_{i}, c\\right) $$ å¯¹ $m=1ï¼Œ2ï¼Œ\\cdots, M$ (1) å¯¹ $i=1ï¼Œ2ï¼Œ\\cdots, N$è®¡ç®— $$ r_{m i}=-\\left[\\frac{\\partial L\\left(y_{i}, f\\left(x_{i}\\right)\\right)}{\\partial f\\left(x_{i}\\right)}\\right]_{f(x)=f_{m-1}(x)} $$ (2) å¯¹$r_{mi}$æ‹Ÿåˆä¸€ä¸ªå›å½’æ ‘ï¼Œå¾—åˆ°ç¬¬$m$é¢—æ ‘çš„èŠ‚ç‚¹åŒºåŸŸ$R_{mj}$ (3) å¯¹$j=1,2,\\cdots, J$, è®¡ç®— $$ c_{m j}=\\arg \\min _{c} \\sum_{x_{i} \\in R_{m j}} L\\left(y_{i}, f_{m-1}\\left(x_{i}\\right)+c\\right) $$ (4)æ›´æ–° $$ f_{m}(x)=f_{m-1}(x)+\\sum_{j=1}^{J} c_{m j} I\\left(x \\in R_{m j}\\right) $$ å¾—åˆ°å›å½’æ ‘ $$ \\hat{f}(x)=f_{M}(x)=\\sum_{m=1}^{M} \\sum_{j=1}^{J} c_{m j} I\\left(x \\in R_{m j}\\right) $$ å‚è€ƒï¼š æèˆªã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹ ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:3:0","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"éšé©¬å¯å¤«æ¨¡å‹ï¼ˆHMMï¼‰æè¿°éšè—çš„é©¬å¯å¤«é“¾éšæœºç”Ÿæˆè§‚æµ‹åºåˆ—çš„è¿‡ç¨‹ï¼Œå±äºç”Ÿæˆæ¨¡å‹ã€‚ HMMåœ¨è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€ç”Ÿç‰©ä¿¡æ¯ã€æ¨¡å¼è¯†åˆ«ç­‰é¢†åŸŸç”±å¹¿æ³›åº”ç”¨ã€‚ ","date":"2020-05-03","objectID":"/2020-05-03-ml-hmm/:0:0","tags":["Hidden Markov Model","Expectation Maximization","Probabilistic Graphical Model","Statistical Learning"],"title":"Hidden Markov Model (HMM)","uri":"/2020-05-03-ml-hmm/"},{"categories":["Machine Learning"],"content":"1. HMMçš„å®šä¹‰ éšé©¬å¯å¤«æ¨¡å‹æ˜¯å…³äºæ—¶åºçš„æ¦‚ç‡æ¨¡å‹ï¼Œ æè¿°ç”±ä¸€ä¸ªéšè—çš„é©¬å¯å¤«é“¾éšæœºç”Ÿæˆä¸å¯è§‚æµ‹çš„çŠ¶æ€ï¼Œå†ç”±å„ä¸ªçŠ¶æ€ç”Ÿæˆä¸€ä¸ªè§‚æµ‹ï¼Œä»è€Œäº§ç”Ÿè§‚æµ‹éšæœºåºåˆ—çš„è¿‡ç¨‹ã€‚ ç®€è€Œè¨€ä¹‹ï¼Œéšé©¬å¯å¤«é“¾éšæœºæˆçŠ¶æ€åºåˆ—ï¼ˆstate sequenceï¼‰ï¼Œè€Œæ¯ä¸ªçŠ¶æ€ç”Ÿæˆè§‚æµ‹ï¼Œäº§ç”Ÿè§‚æµ‹åºåˆ—ï¼ˆobservation sequenceï¼‰ã€‚åºåˆ—çš„ä¸€ä¸ªä½ç½®å¯ä»¥çœ‹ä½œä¸€ä¸ªæ—¶åˆ»ã€‚ ä»¤$Q$ è¡¨ç¤ºæ‰€æœ‰å¯èƒ½çŠ¶æ€çš„é›†åˆï¼š$Q = { q_1, q_2, \\cdots, q_N }$; ä»¤$V$ è¡¨ç¤ºæ‰€æœ‰å¯èƒ½çš„è§‚æµ‹é›†åˆï¼š$V = {v_1, v_2, \\cdots, v_M }$; ä»¤$I$ è¡¨ç¤ºé•¿åº¦ä¸ºTçš„çŠ¶æ€åºåˆ—ï¼š $I = (i_1, i_2, \\cdots, i_T)$; ä»¤$O$ è¡¨ç¤ºå¯¹åº”çš„æ˜¯è§‚æµ‹åºåˆ—ï¼š $O = (o_1, o_2, \\cdots, o_T)$. ä»¤$A$æ˜¯è½¬ç§»æ¦‚ç‡çŸ©é˜µï¼š $$A = [a_{ij}]_{N \\times N}$$ å…¶ä¸­ï¼Œ $$a_{ij} = P(i_{t+1} = q_j | i_t = q_j), i=1,2, \\cdots, N; j = 1,2, \\cdots, N$$ æ˜¯åœ¨æ—¶åˆ»$t$å¤„äºçŠ¶æ€$q_i$çš„æ¡ä»¶ä¸‹ç”Ÿæˆè§‚æµ‹$t +1$è½¬ç§»åˆ°çŠ¶æ€$q_j$çš„æ¦‚ç‡ã€‚ ä»¤$B$æ˜¯è§‚æµ‹æ¦‚ç‡çŸ©é˜µï¼š $$B = [b_j(k)]_{N \\times M}$$ å…¶ä¸­ï¼Œ $$b_j(k) = P(o_t = v_k | i_t = q_j), k=1,2,\\cdots, M; j=1,2,\\cdots, N$$ æ˜¯åœ¨æ—¶åˆ»$t$å¤„äºçŠ¶æ€$q_j$çš„æ¡ä»¶ä¸‹ç”Ÿæˆè§‚æµ‹$v_k$ çš„æ¦‚ç‡ã€‚ ä»¤$\\pi$æ˜¯åˆå§‹çŠ¶æ€æ¦‚ç‡å‘é‡ï¼š $$\\pi = (\\pi_i)$$ å…¶ä¸­ï¼Œ $$\\pi_{i} = P(i_1 = q_i),i=1,2,\\cdots, N$$ æ˜¯æ—¶åˆ»t=1å¤„äºçŠ¶æ€$q_i$çš„æ¦‚ç‡. éšé©¬å¯å¤«æ¨¡å‹$\\lambda$ç”±$\\pi$ï¼Œ $A$ï¼Œ$B$å†³å®šã€‚ $$\\lambda = (A, B, \\pi)$$ å…¶ä¸­ï¼Œ$\\pi$å’Œ$A$å†³å®šçŠ¶æ€åºåˆ—ï¼Œ$B$å†³å®šè§‚æµ‹åºåˆ—ã€‚ éšé©¬å¯å¤«æ¨¡å‹çš„ä¸¤ä¸ªåŸºæœ¬å‡è®¾ é½æ¬¡é©¬å¯å¤«æ€§ éšé©¬å¯å¤«é“¾åœ¨ä»»æ„æ—¶åˆ»tçš„çŠ¶æ€å‰ä¸€æ—¶åˆ»çŠ¶æ€ï¼Œä¸å…¶ä»–æ—¶åˆ»çš„éšçŠ¶æ€å’Œè§‚æµ‹æ— å…³ï¼Œ ä¹Ÿä¸æ—¶åˆ»tæ— å…³ï¼š $$P(i_t | i_{t-1}, O_{t-1}, \\cdots, i_1, o_1) = P(i_t | i_{t-1}), t = 1,2,\\cdots,T$$ è§‚æµ‹ç‹¬ç«‹æ€§ ä»»æ„æ—¶åˆ»çš„è§‚æµ‹åªä¾èµ–æ”¹æ—¶åˆ»çš„é©¬å¯å¤«é“¾çŠ¶æ€ï¼Œä¸å…¶ä»–è§‚æµ‹å’ŒçŠ¶æ€æ— å…³: $$P(o_t | i_{T}, O_{T}, i_{T-1}, o_{T-1}\\cdots, i_{t+1}, O_{t+1}, i_{t-1}, O_{t-1}, i_1, o_1) = P(o_t | i_{t})$$ ","date":"2020-05-03","objectID":"/2020-05-03-ml-hmm/:1:0","tags":["Hidden Markov Model","Expectation Maximization","Probabilistic Graphical Model","Statistical Learning"],"title":"Hidden Markov Model (HMM)","uri":"/2020-05-03-ml-hmm/"},{"categories":["Machine Learning"],"content":"2. HMMçš„3ä¸ªåŸºæœ¬é—®é¢˜ æ¦‚ç‡è®¡ç®—ï¼šç»™å®šæ¨¡å‹$\\lambda = (A, B, \\pi)$å’Œè§‚æµ‹åºåˆ— $O = (o_1, o_2, \\cdots, o_T)$ï¼Œ æ±‚æ¦‚ç‡$P(O | \\lambda)$ å­¦ä¹ : å·²çŸ¥è§‚æµ‹åºåˆ—$O = (o_1, o_2, \\cdots, o_T)$ï¼Œä¼°è®¡æ¨¡å‹å‚æ•°$\\lambda = (A, B, \\pi)$ï¼Œ ä½¿æ¦‚ç‡$P(O \\vert \\lambda)$æœ€å¤§ï¼ˆç”¨æå¤§ä¼¼ç„¶ä¼°è®¡ï¼‰ã€‚ é¢„æµ‹ï¼šç»™å®šæ¨¡å‹$\\lambda = (A, B, \\pi)$å’Œè§‚æµ‹åºåˆ— $O = (o_1, o_2, \\cdots, o_T)$ï¼Œæ±‚æ¡ä»¶æ¦‚ç‡$P(I | O)$æœ€å¤§çš„çŠ¶æ€åºåˆ— $I = (i_1, i_2, \\cdots, i_T)$. ","date":"2020-05-03","objectID":"/2020-05-03-ml-hmm/:2:0","tags":["Hidden Markov Model","Expectation Maximization","Probabilistic Graphical Model","Statistical Learning"],"title":"Hidden Markov Model (HMM)","uri":"/2020-05-03-ml-hmm/"},{"categories":["Machine Learning"],"content":"2.1 æ¦‚ç‡è®¡ç®—å‰å‘ï¼ˆforwardï¼‰å’Œåå‘ï¼ˆbackwardï¼‰ç®—æ³• 2.1.1 å‰å‘ç®—æ³• ç»™å®šæ¨¡å‹$\\lambda$ï¼Œå½“æ—¶åˆ»$t$æ—¶ï¼ŒçŠ¶æ€ä¸º$q_i$ï¼Œéƒ¨åˆ†è§‚æµ‹åºåˆ—ä¸º$o_1, o_2, \\cdots, o_t$ï¼Œè®°ï¼š $$\\alpha_{t}(i) = P(o_1, o_2, \\cdots, o_t, i_t = q_i | \\lambda)$$ è¾“å…¥ï¼š éšé©¬å¯å¤«æ¨¡å‹ $\\lambda$ï¼Œ è§‚æµ‹åºåˆ—$O$; è¾“å‡ºï¼š è§‚æµ‹åºåˆ—æ¦‚ç‡$P(O | \\lambda)$ ï¼ˆ1ï¼‰åˆå€¼ $$ \\alpha_{1}(i)=\\pi_{i} b_{i}\\left(o_{1}\\right), \\quad i=1,2, \\cdots, N $$ ï¼ˆ2ï¼‰é€’æ¨ å¯¹ $t = 1,2, \\cdots, T-1,$ $$ \\alpha_{t+1}(i)=\\left[\\sum_{j=1}^{N} \\alpha_{t}(j) a_{j i}\\right] b_{i}\\left(o_{t+1}\\right), \\quad i=1,2, \\cdots, N $$ ï¼ˆ3ï¼‰ç»ˆæ­¢ $$ P(O | \\lambda)=\\sum_{i=1}^{N} \\alpha_{T}(i) $$ 2.1.2 åå‘ç®—æ³• ç»™å®šæ¨¡å‹$\\lambda$ï¼Œå½“æ—¶åˆ»$t$æ—¶ï¼ŒçŠ¶æ€ä¸º$q_i$ï¼Œéƒ¨åˆ†è§‚æµ‹åºåˆ—ä¸º$o_1, o_2, \\cdots, o_t$ï¼Œè®°ï¼š $$\\beta_{t}(i) = P(o_{t+1}, o_{t+2}, \\cdots, o_T | i_t = q_i, \\lambda)$$ è¾“å…¥ï¼š éšé©¬å¯å¤«æ¨¡å‹ $\\lambda$ï¼Œ è§‚æµ‹åºåˆ—$O$; è¾“å‡ºï¼š è§‚æµ‹åºåˆ—æ¦‚ç‡$P(O | \\lambda)$ ï¼ˆ1ï¼‰åˆå§‹ ä»¤æœ€ç»ˆæ—¶åˆ»æ‰€æœ‰çŠ¶æ€$q_i$ $$\\beta_T(i) = 1, i=1,2,\\cdots, N$$ ï¼ˆ2ï¼‰é€’æ¨ å¯¹$t=T-1, T-2, \\cdots, 1$ $$ \\beta_{t}(i)=\\sum_{j=1}^{N} a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j), \\quad i=1,2, \\cdots, N $$ ï¼ˆ3ï¼‰ç»ˆæ­¢ $$ P(O | \\lambda)=\\sum_{i=1}^{N} \\pi_{i} b_{i}\\left(o_{1}\\right) \\beta_{1}(i) $$ åˆ©ç”¨å‰åå‘æ¦‚ç‡å®šä¹‰ï¼Œå¯ä»¥å°†è§‚æµ‹åºåˆ—æ¦‚ç‡$P(O \\vert \\lambda)$ç»Ÿä¸€å†™æˆ $$ P(O | \\lambda)=\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{t}(i) a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j), \\quad t=1,2, \\cdots, T-1 $$ ","date":"2020-05-03","objectID":"/2020-05-03-ml-hmm/:2:1","tags":["Hidden Markov Model","Expectation Maximization","Probabilistic Graphical Model","Statistical Learning"],"title":"Hidden Markov Model (HMM)","uri":"/2020-05-03-ml-hmm/"},{"categories":["Machine Learning"],"content":"2.2 å­¦ä¹ é—®é¢˜çš„EMç®—æ³• å®è´¨ä¸Šæ±‚ä¸€ä¸ªéšå˜é‡çš„æ¦‚ç‡æ¨¡å‹çš„å‚æ•°ä¼°è®¡ï¼š $$ P(O | \\lambda)=\\sum_{I} P(O | I, \\lambda) P(I | \\lambda) $$ å‚æ•°ä¼°è®¡ç”±EMç®—æ³•å®ç°: (å¾…ç»­) è¾“å…¥ï¼šè§‚æµ‹æ•°æ®$O = (o_1, o_2, \\cdots, o_T)$; è¾“å‡ºï¼šéšé©¬å¯å¤«æ¨¡å‹å‚æ•° ï¼ˆ1ï¼‰åˆå§‹åŒ– å¯¹ n=0ï¼Œ é€‰å–$a_{ij}^{(0)}$, $b_{j}(k)^{(0)}$, $\\pi_{i}^{(0)}$, å¾—åˆ°æ¨¡å‹$\\lambda = (A^{(0)}, B^{(0)}, \\pi^{(0)})$. ï¼ˆ2ï¼‰é€’æ¨ å¯¹$n=1,2, \\cdots,$, æœ‰ $$ a_{i j}^{(n+1)}=\\frac{\\sum_{t=1}^{T-1} \\xi_{t}(i, j)}{\\sum_{t=1}^{T-1} \\gamma_{t}(i)} $$ å¦ï¼Œ $$ b_{j}(k)^{(n+1)}=\\frac{\\sum_{t=1, o_{t}=v_{k}}^{T} \\gamma_{t}(j)}{\\sum_{t=1}^{T} \\gamma_{t}(j)} $$ $$ \\pi_{i}^{(n+1)}=\\gamma_{1}(i) $$ å…¶ä¸­ï¼Œæ—¶åˆ»$t$å¤„äº$q_i$ï¼Œä¸”æ—¶åˆ»$t+1$å¤„äºçŠ¶æ€$q_j$çš„æ¦‚ç‡, è®° $$ \\xi_{t}(i, j)=P\\left(i_{t}=q_{i}, i_{t+1}=q_{j} | O, \\lambda\\right) $$ é‚£ä¹ˆ $$ \\xi_{t}(i, j)=\\frac{P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O | \\lambda\\right)}{P(O | \\lambda)}=\\frac{P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O | \\lambda\\right)}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O | \\lambda\\right)} $$ å’Œæ—¶åˆ»$t$å¤„äº$q_i$çš„æ¦‚ç‡ï¼Œ æœ‰ $$ \\gamma_{t}(i)=P\\left(i_{t}=q_{i} | O, \\lambda\\right)=\\frac{P\\left(i_{t}=q_{i}, O | \\lambda\\right)}{P(O | \\lambda)} $$ ï¼ˆ3ï¼‰ç»ˆæ­¢ å¾—åˆ°æ¨¡å‹å‚æ•°$\\lambda^{(n+1)} = (A^{(n+1)}, B^{(n+1)}, \\pi^{(n+1)})$ ","date":"2020-05-03","objectID":"/2020-05-03-ml-hmm/:2:2","tags":["Hidden Markov Model","Expectation Maximization","Probabilistic Graphical Model","Statistical Learning"],"title":"Hidden Markov Model (HMM)","uri":"/2020-05-03-ml-hmm/"},{"categories":["Machine Learning"],"content":"2.3 é¢„æµ‹ç®—æ³• åŒ…æ‹¬è¿‘ä¼¼ç®—æ³•å’Œç»´ç‰¹æ¯”ç®—æ³•ï¼ˆViterbi algorithmï¼‰ 2.3.1 è¿‘ä¼¼ç®—æ³• åœ¨æ¯ä¸ªæ—¶åˆ»$t$ï¼Œ é€‰æ‹©åœ¨è¯¥æ—¶åˆ»æœ€å¯èƒ½å‡ºç°çš„çŠ¶æ€ $i^*_t$ä»è€Œå¾—åˆ°ä¸€ä¸ªçŠ¶æ€åºåˆ— $I^* = (i^*_i, i^*_i, \\cdots, i^*_T)$ï¼Œå°†å®ƒæœ€ä¸ºé¢„æµ‹ç»“æœã€‚ ç»™å®šæ¨¡å‹$\\lambda$å’Œè§‚æµ‹åºåˆ—$O$ï¼Œ åœ¨æ—¶åˆ»$t$å¤„äºçŠ¶æ€$q_i$çš„æ¦‚ç‡$\\gamma_t(i)$æ˜¯ $$ \\gamma_{t}(i)=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{P(O | \\lambda)}=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{\\sum_{j=1}^{N} \\alpha_{t}(j) \\beta_{t}(j)} $$ è€Œæ¯ä¸€æ—¶åˆ»$t$æœ€æœ‰å¯èƒ½çš„çŠ¶æ€$i_{t}^{*}$æ˜¯ $$ i_{t}^{*}=\\arg \\max _{1 \\leqslant i \\leqslant N}\\left[\\gamma_{t}(i)\\right], \\quad t=1,2, \\cdots, T $$ ä»è€Œå¾—åˆ°çŠ¶æ€åºåˆ— $$I^* = (i^*_i, i^*_i, \\cdots, i^*_T)$$ ç¼ºç‚¹ï¼š ä¸èƒ½ä¿è¯é¢„æµ‹çŠ¶æ€åºåˆ—æ•´ä½“æ˜¯æœ€æœ‰å¯èƒ½çš„çŠ¶æ€åºåˆ—ï¼Œå› ä¸ºé¢„æµ‹çš„çŠ¶æ€åºåˆ—å®é™…å¯èƒ½ç”±ä¸å‘ç”Ÿçš„éƒ¨åˆ†ã€‚ 2.3.2 ç»´ç‰¹æ¯”ç®—æ³• å®è´¨æ˜¯è¿ç”¨åŠ¨æ€è§„åˆ’æ±‚æ¦‚ç‡æœ€å¤§è·¯å¾„ï¼Œä»è€Œè§£å†³HMMçš„é¢„æµ‹é—®é¢˜ ç»´ç‰¹æ¯”ç®—æ³•: åªéœ€ä»æ—¶åˆ»$t=1$å¼€å§‹ï¼Œé€’æ¨åœ°è®¡ç®—åœ¨æ—¶åˆ»$t$çŠ¶æ€ä¸º$q_i$çš„å„æ¡éƒ¨åˆ†è·¯å¾„çš„æœ€å¤§æ¦‚ç‡ï¼Œç›´è‡³å¾—åˆ°æ—¶åˆ»$t = T$çŠ¶æ€ä¸º$i$çš„å„æ¡è·¯å¾„çš„æœ€å¤§æ¦‚ç‡ã€‚æ—¶åˆ» $t = T$ çš„æœ€å¤§æ¦‚ç‡å³ä¸ºæœ€ä¼˜è·¯å¾„çš„æ¦‚ç‡ $P^\\ast$, æœ€ä¼˜è·¯å¾„çš„ç»ˆç»“ç‚¹$i^*_T$ ä¹ŸåŒæ—¶å¾—åˆ°ã€‚ä¹‹åï¼Œä¸ºäº†æ‰¾å‡ºæœ€ä¼˜è·¯å¾„çš„å„ä¸ªç»“ç‚¹ï¼Œä»ç»ˆç»“ç‚¹$i^*_T$å¼€å§‹ï¼Œç”±åå‘å‰é€æ­¥æ±‚å¾—ç»“ç‚¹ $i^*_{T-1}, \\cdots, i^*_1$ï¼Œå¾—åˆ°æœ€ä¼˜è·¯å¾„$I^* = (i^*_i, i^*_i, \\cdots, i^*_T)$ã€‚ å®šä¹‰åœ¨æ—¶åˆ»$t$çŠ¶æ€$i$çš„æ‰€æœ‰å•ä¸ªè·¯å¾„ä¸­æ¦‚ç‡æœ€å¤§å€¼ä¸º $$ \\delta_{t}(i)=\\max_{i_{1}, i_{2}, \\cdots, i_{t-1}} P\\left(i_{t}=i, i_{t-1}, \\cdots, i_{1}, o_{t}, \\cdots, o_{1} | \\lambda\\right), \\quad i=1,2, \\cdots, N $$ å› æ­¤ $$ \\begin{aligned} \\delta_{t+1}(i) \u0026=\\max _{i_{1}, i_{2}, \\cdots, i_{t}} P\\left(i_{t+1}=i, i_{t}, \\cdots, i_{1}, o_{t+1}, \\cdots, o_{1} | \\lambda\\right) \\cr \u0026= \\max _{1 \\leqslant j \\leqslant N}\\left[\\delta_{t}(j) a_{j i}\\right] b_{i}\\left(o_{t+1}\\right), \\quad i=1,2, \\cdots, N ; \\quad t=1,2, \\cdots, T-1 \\end{aligned} $$ å®šä¹‰åœ¨æ—¶åˆ»tçŠ¶æ€içš„æ‰€æœ‰å•ä¸ªè·¯å¾„ä¸­æ¦‚ç‡æœ€å¤§è·¯å¾„çš„ç¬¬$t-1$ä¸ªèŠ‚ç‚¹ä¸º $$ \\Psi_{t}(i)=\\arg \\max _{1 \\leqslant j \\leqslant N}\\left[\\delta_{t-1}(j) a_{j i}\\right], \\quad i=1,2, \\cdots, N $$ è¾“å…¥ï¼š éšé©¬å¯å¤«æ¨¡å‹ $\\lambda$ï¼Œ è§‚æµ‹åºåˆ—$O$; è¾“å‡ºï¼š æœ€ä¼˜è·¯å¾„$$I^* = (i^*_i, i^*_i, \\cdots, i^*_T)$$ (1) åˆå§‹åŒ–: $$ \\begin{array}{c} \\delta_{1}(i)=\\pi_{i} b_{i}\\left(o_{1}\\right), \\quad i=1,2, \\cdots, N \\cr \\Psi_{1}(i)=0, \\quad i=1,2, \\cdots, N \\end{array} $$ (2) é€’æ¨: $$ \\begin{array}{c} \\delta_{t}(i)=\\max _{1 \\leqslant j \\leqslant N}\\left[\\delta_{t-1}(j) a_{j i}\\right] b_{i}\\left(o_{t}\\right), \\quad i=1,2, \\cdots, N \\cr \\Psi_{t}(i)=\\arg \\max _{1 \\leqslant j \\leqslant N}\\left[\\delta_{t-1}(j) a_{j i}\\right], \\quad i=1,2, \\cdots, N \\end{array} $$ (3) ç»ˆæ­¢ $$ \\begin{array}{c} P^* = \\max _{1 \\leqslant i \\leqslant N} \\delta_T(i) \\cr i^*_T = \\arg \\max _{1 \\leqslant i \\leqslant N} [ \\delta_T(i)] \\end{array} $$ (4) æœ€ä¼˜è·¯å¾„å›æº¯ å¯¹$t=T-1, T-2, \\cdots, 1$, $$i^*_t = \\Psi_{t+1}(i^*_{t+1})$$ å‚è€ƒï¼š æèˆªã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹ ","date":"2020-05-03","objectID":"/2020-05-03-ml-hmm/:2:3","tags":["Hidden Markov Model","Expectation Maximization","Probabilistic Graphical Model","Statistical Learning"],"title":"Hidden Markov Model (HMM)","uri":"/2020-05-03-ml-hmm/"},{"categories":["Machine Learning"],"content":"æ½œåœ¨è¯­ä¹‰åˆ†æï¼ˆLSAï¼‰æ˜¯ä¸€ç§éç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºæ–‡æœ¬è¯é¢˜åˆ†æã€‚å…¶ç‰¹ç‚¹æ˜¯é€šè¿‡çŸ©é˜µåˆ†è§£å‘ç°æ–‡æœ¬äºå•è¯ä¹‹é—´çš„åŸºäºè¯é¢˜çš„è¯­ä¹‰å…³ç³»ã€‚ æ½œåœ¨è¯­ä¹‰åˆ†ææ˜¯ä¸€ç§éæ¦‚ç‡è¯é¢˜åˆ†ææ¨¡å‹ã€‚æ­¥éª¤ä¸ºï¼š å…ˆå°†æ–‡æœ¬é›†åˆè¡¨ç¤ºä¸ºå•è¯-æ–‡æœ¬çŸ©é˜µ å¯¹å•è¯-æ–‡æœ¬çŸ©é˜µè¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰æˆ–éè´ŸçŸ©é˜µåˆ†è§£ï¼ˆNMFï¼‰ï¼Œä»è€Œå¾—åˆ°è¯é¢˜å‘é‡ç©ºé—´ï¼Œä»¥åŠæ–‡æœ¬åœ¨è¯é¢˜å‘é‡ç©ºé—´çš„è¡¨ç¤º ","date":"2020-04-30","objectID":"/2020-04-30-ml-lsa/:0:0","tags":["TF-IDF","Latent Semantic Analysis","Statistical Learning"],"title":"Latent semantic analysis (LSA)","uri":"/2020-04-30-ml-lsa/"},{"categories":["Machine Learning"],"content":"1. å•è¯å‘é‡ç©ºé—´ ç»™å®šç”±nä¸ªæ–‡æœ¬çš„é›†åˆ $D = \\lbrace d_1, d_2, \\cdots, d_n \\rbrace$ ,ä»¥åŠæ‰€æœ‰æ–‡æœ¬ä¸­å•è¯é›† $W = \\lbrace w_1, w_2, \\cdots, w_m \\rbrace$, åˆ™å•è¯åœ¨æ–‡æœ¬ä¸­å‡ºç°çš„æ•°æ®ç”¨å•è¯-æ–‡æœ¬çŸ©é˜µ(word-document matrix)è¡¨ç¤º, è®°ä¸ºï¼š $$ X=\\left[\\begin{array}{cccc} x_{11} \u0026 x_{12} \u0026 \\cdots \u0026 x_{1 n} \\cr x_{21} \u0026 x_{22} \u0026 \\cdots \u0026 x_{2 n} \\cr \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\cr x_{m 1} \u0026 x_{m 2} \u0026 \\cdots \u0026 x_{m n} \\end{array}\\right] $$ å…¶ä¸­ï¼Œ $x_{ij}$è¡¨ç¤ºå•è¯$w_i$åœ¨æ–‡æœ¬$d_j$ä¸­å‡ºç°çš„é¢‘æ•°æˆ–æƒå€¼ã€‚ç”±äºå•è¯ç§ç±»å¤šï¼Œè€Œæ¯ä¸ªæ–‡æœ¬ä¸­å•è¯ç§ç±»é€šå¸¸å°‘ï¼Œæ‰€ä»¥ä¸ºå•è¯-æ–‡æœ¬çŸ©é˜µæ˜¯ç¨€ç–çŸ©é˜µã€‚ æƒå€¼å¸¸ç”¨å•è¯é¢‘ç‡-é€†æ–‡æœ¬é¢‘ç‡(term frequeny-inverse document frequency, TF-IDF)è¡¨ç¤º: $$ \\mathrm{TFIDF_{ij}}=\\frac{\\mathrm {tf_{ij}}}{\\mathrm {tf_{\\cdot j}}} \\log \\frac{\\mathrm{df}}{\\mathrm {df_i}}, \\quad i=1,2, \\cdots, m ; \\quad j=1,2, \\cdots, n $$ å…¶ä¸­ï¼Œ $\\mathrm{tf}_{ij}$ æ˜¯å•è¯ $w_i$ åœ¨æ–‡æœ¬ $d_j$ ä¸­çš„é¢‘æ•°; $\\mathrm{tf}_{\\cdot j}$ æ˜¯ $d_j$ ä¸­å‡ºç°çš„æ‰€æœ‰å•è¯é¢‘æ•°ä¹‹å’Œ; $\\mathrm{df}_{i}$ æ˜¯å«æœ‰å•è¯$w_i$çš„æ–‡æœ¬æ•°; $\\mathrm{df}$ æ˜¯æ–‡æœ¬é›†åˆ$D$çš„å…¨éƒ¨æ–‡æœ¬æ•°ã€‚ å•è¯-æ–‡æœ¬çŸ©é˜µçš„ç¬¬jåˆ—å‘é‡$x_j$è¡¨ç¤ºæ–‡æœ¬$d_j$ï¼š $$ x_{j}=\\left[\\begin{array}{c} x_{1 j} \\cr x_{2 j} \\cr \\vdots \\cr x_{m j} \\end{array}\\right], \\quad j=1,2, \\cdots, n $$ ä¸¤ä¸ªå•è¯å‘é‡å†…ç§¯æˆ–è€…æ ‡å‡†åŒ–å†…ç§¯è¡¨ç¤ºå¯¹åº”æ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œ å› æ­¤æ–‡æœ¬$d_i$ä¸$d_j$çš„ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼š $$ x_{i} \\cdot x_{j}, \\quad \\frac{x_{i} \\cdot x_{j}}{\\left|x_{i}\\right|\\left|x_{j}\\right|} $$ å•è¯å‘é‡ç©ºé—´æ¨¡å‹çš„ä¼˜ç¼ºç‚¹ ä¼˜ç‚¹ï¼š ç®€å•ï¼Œè®¡ç®—é«˜æ•ˆ ç¼ºç‚¹ï¼š ä¸€è¯å¤šä¹‰(polysemy)å’Œå¤šè¯ä¸€ä¹‰(synonymy)ä¸èƒ½å¾ˆå¥½å¤„ç†ï¼Œ å­˜åœ¨ç›¸ä¼¼åº¦è®¡ç®—ä¸å‡†ç¡®çš„é—®é¢˜ ","date":"2020-04-30","objectID":"/2020-04-30-ml-lsa/:0:1","tags":["TF-IDF","Latent Semantic Analysis","Statistical Learning"],"title":"Latent semantic analysis (LSA)","uri":"/2020-04-30-ml-lsa/"},{"categories":["Machine Learning"],"content":"2. è¯é¢˜ï¼ˆtopicï¼‰å‘é‡ç©ºé—´ è¯é¢˜æ²¡æœ‰ä¸¥æ ¼çš„å®šä¹‰ï¼Œæ˜¯æŒ‡æ–‡æœ¬è®¨è®ºçš„å†…å®¹æˆ–ä¸»é¢˜ã€‚è€ŒåŸºäºè¯é¢˜çš„æ¨¡å‹æ˜¯ä¸ºäº†è§£å†³å•è¯å‘é‡ä¸èƒ½å¾ˆå¥½å¤„ç†ä¸€è¯å¤šä¹‰(polysemy)å’Œå¤šè¯ä¸€ä¹‰(synonymy)çš„é—®é¢˜ã€‚ ç»™å®šç”±nä¸ªæ–‡æœ¬çš„é›†åˆ $D = \\lbrace d_1, d_2, \\cdots, d_n \\rbrace$, ä»¥åŠæ‰€æœ‰æ–‡æœ¬ä¸­å•è¯é›† $W = \\lbrace w_1, w_2, \\cdots, w_m \\rbrace$, åˆ™å•è¯åœ¨æ–‡æœ¬ä¸­å‡ºç°çš„æ•°æ®ç”¨å•è¯-æ–‡æœ¬çŸ©é˜µ(word-document matrix)è¡¨ç¤º, è®°ä¸ºï¼š $$ X=\\left[\\begin{array}{cccc} x_{11} \u0026 x_{12} \u0026 \\cdots \u0026 x_{1 n} \\cr x_{21} \u0026 x_{22} \u0026 \\cdots \u0026 x_{2 n} \\cr \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\cr x_{m 1} \u0026 x_{m 2} \u0026 \\cdots \u0026 x_{m n} \\end{array}\\right] $$ å‡è®¾æ‰€æœ‰æ–‡æœ¬å«æœ‰kä¸ªè¯é¢˜ï¼Œ æ¯ä¸ªè¯é¢˜lç”±ä¸€ä¸ªå®šä¹‰åœ¨å•è¯é›†åˆWå‚»å§‘å¨˜çš„mç»´å‘é‡ï¼š $$ t_{l}=\\left[\\begin{array}{c} t_{1 l} \\cr t_{2 l} \\cr \\vdots \\cr t_{m l} \\end{array}\\right], \\quad l=1,2, \\cdots, k $$ åˆ™æœ‰è¯é¢˜å‘é‡çŸ©é˜µTï¼š $$ T=\\left[\\begin{array}{cccc} t_{11} \u0026 t_{12} \u0026 \\cdots \u0026 t_{1 k} \\cr t_{21} \u0026 t_{22} \u0026 \\cdots \u0026 t_{2 k} \\cr \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\cr t_{m 1} \u0026 t_{m 2} \u0026 \\cdots \u0026 t_{m k} \\end{array}\\right] $$ ","date":"2020-04-30","objectID":"/2020-04-30-ml-lsa/:0:2","tags":["TF-IDF","Latent Semantic Analysis","Statistical Learning"],"title":"Latent semantic analysis (LSA)","uri":"/2020-04-30-ml-lsa/"},{"categories":["Machine Learning"],"content":"3. æ–‡æœ¬åœ¨è¯é¢˜å‘é‡ç©ºé—´çš„è¡¨ç¤º æ–‡æœ¬$d_j$åœ¨å•è¯å‘é‡ç©ºé—´ç”¨$x_j$è¡¨ç¤ºï¼Œå°†$x_j$æŠ•å½±åˆ°è¯é¢˜å‘é‡ç©ºé—´$T$ï¼Œå¾—åˆ°è¯é¢˜å‘é‡ç©ºé—´$y_j$ï¼Œ$y_j$æ˜¯ä¸ªkç»´å‘é‡: $$ y_{j}=\\left[\\begin{array}{c} y_{1 j} \\cr y_{2 j} \\cr \\vdots \\cr y_{k j} \\end{array}\\right], \\quad j=1,2, \\cdots, n $$ å› æ­¤ï¼Œè¯é¢˜-æ–‡æœ¬çŸ©é˜µ$Y$ï¼š $$ Y=\\left[\\begin{array}{cccc} y_{11} \u0026 y_{12} \u0026 \\cdots \u0026 y_{1 n} \\cr y_{21} \u0026 y_{22} \u0026 \\cdots \u0026 y_{2 n} \\cr \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\cr y_{k 1} \u0026 y_{k 2} \u0026 \\cdots \u0026 y_{k n} \\end{array}\\right] $$ ","date":"2020-04-30","objectID":"/2020-04-30-ml-lsa/:0:3","tags":["TF-IDF","Latent Semantic Analysis","Statistical Learning"],"title":"Latent semantic analysis (LSA)","uri":"/2020-04-30-ml-lsa/"},{"categories":["Machine Learning"],"content":"4. å•è¯å‘é‡ç©ºé—´åˆ°è¯é¢˜å‘é‡ç©ºé—´è½¬æ¢ å› æ­¤ï¼Œå•è¯å‘é‡ç©ºé—´çš„æ–‡æœ¬å‘é‡$x_j$ï¼Œå¯ä»¥ç”¨è¯é¢˜å‘é‡ç©ºé—´$y_j$è¿‘ä¼¼è¡¨ç¤ºï¼Œå³ä»¥kä¸ªè¯é¢˜å‘é‡ä»¥$y_j$ä¸ºç³»æ•°çš„çº¿æ€§ç»„åˆï¼š $$ x_{j} \\approx y_{1 j} t_{1}+y_{2 j} t_{2}+\\cdots+y_{k j} t_{k}, \\quad j=1,2, \\cdots, n $$ çŸ©é˜µå½¢å¼ï¼š $$ X \\approx TY $$ è¿™å°±æ˜¯æ½œåœ¨è¯­ä¹‰åˆ†æã€‚ å‚è€ƒï¼š æèˆªã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹ ","date":"2020-04-30","objectID":"/2020-04-30-ml-lsa/:0:4","tags":["TF-IDF","Latent Semantic Analysis","Statistical Learning"],"title":"Latent semantic analysis (LSA)","uri":"/2020-04-30-ml-lsa/"},{"categories":["Machine Learning"],"content":"CRFæ¡ä»¶éšæœºåœºï¼Œå¯åº”ç”¨äºæ ‡æ³¨é—®é¢˜ æ¦‚ç‡æ— å‘å›¾æ¨¡å‹Probabilistic undirected graphical model(Markov random field) æ˜¯ä¸€ä¸ªå¯ä»¥ç”±æ— å‘å›¾è¡¨ç¤ºçš„è”åˆæ¦‚ç‡åˆ†å¸ƒ ","date":"2020-04-29","objectID":"/2020-04-29-ml-crf/:0:0","tags":["Probabilistic Graphical Model","Statistical Learning"],"title":"Conditional random field (CRF)","uri":"/2020-04-29-ml-crf/"},{"categories":["Machine Learning"],"content":"0. æ¦‚ç‡å›¾æ¨¡å‹åˆ†ç±» æ¦‚ç‡å›¾æ¨¡å‹ï¼ˆprobabilistic graphical model, PGMï¼‰ï¼Œæ˜¯ä¸€ç§å­¦ä¹ ä»»åŠ¡çš„æ¡†æ¶æè¿°ï¼Œå®ƒå°†å­¦ä¹ ä»»åŠ¡å½’ç»“ä¸ºè®¡ç®—å˜é‡çš„æ¦‚ç‡åˆ†å¸ƒã€‚ æŒ‰ç…§æ¦‚ç‡å›¾ä¸­å˜é‡å…³ç³»çš„ä¸åŒï¼Œæ¦‚ç‡å›¾æ¨¡å‹å¯ä»¥å¤§è‡´åˆ†ä¸ºä¸¤ç±»ï¼š è´å¶æ–¯ç½‘ç»œï¼šæœ‰å‘å›¾æ¨¡å‹ï¼Œä½¿ç”¨æœ‰å‘æ— ç¯å›¾è¡¨è¾¾å…³ç³»ï¼ˆé€šå¸¸ï¼Œå˜é‡é—´å­˜åœ¨æ˜¾å¼çš„å› æœå…³ç³»ï¼‰ é©¬å°”ç§‘å¤«ç½‘ç»œï¼šæ— å‘å›¾æ¨¡å‹ï¼Œä½¿ç”¨æ— å›¾è¡¨è¾¾å…³ç³»ï¼ˆé€šå¸¸ï¼Œå˜é‡é—´å­˜æœ‰å…³ç³»ï¼Œä½†æ˜¯éš¾ä»¥æ˜¾å¼è¡¨è¾¾ï¼‰ åŒæ—¶å­˜æœ‰æœ‰å‘è¾¹å’Œæ— å‘è¾¹çš„æ¨¡å‹ï¼Œå¦‚æ¡ä»¶éšæœºåœºï¼ˆconditional random fieldï¼‰å’Œé“¾å›¾ï¼ˆchain graphï¼‰ï¼Œå•ç‹¬çœ‹åšä¸€ç±»å±€éƒ¨æœ‰å‘æ¨¡å‹ã€‚ è´å¶æ–¯ç½‘ç»œ å¯ä»¥åˆ†ä¸ºé™æ€è´å¶æ–¯ç½‘ç»œå’ŒåŠ¨æ€è´å¶æ–¯ç½‘ç»œã€‚ç›¸æ¯”äºé™æ€è´å¶æ–¯ç½‘ç»œï¼ŒåŠ¨æ€ï¼ˆdynamicï¼‰è´å¶æ–¯ç½‘ç»œä¸»è¦ç”¨äºæ—¶åºæ•°æ®å»ºæ¨¡ï¼ˆå¦‚è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è½¨è¿¹æ•°æ®æŒ–æ˜ç­‰ï¼‰ã€‚å…¶ä¸­ï¼Œä¸€ç§ç»“æ„æœ€ç®€å•çš„åŠ¨æ€è´å¶æ–¯ç½‘ç»œå°±æ˜¯éšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆhidden markov model, HMMï¼‰ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œè´å¶æ–¯ç½‘ç»œä¸­æ¯ä¸€ä¸ªç»“ç‚¹éƒ½å¯¹åº”äºä¸€ä¸ªå…ˆéªŒæ¦‚ç‡åˆ†å¸ƒæˆ–è€…æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼Œå› æ­¤æ•´ä½“çš„è”åˆåˆ†å¸ƒå¯ä»¥ç›´æ¥åˆ†è§£ä¸ºæ‰€æœ‰å•ä¸ªç»“ç‚¹æ‰€å¯¹åº”çš„åˆ†å¸ƒçš„ä¹˜ç§¯ã€‚ é©¬å°”å¯å¤«ç½‘ ç”±äºå˜é‡ä¹‹é—´æ²¡æœ‰æ˜ç¡®çš„å› æœå…³ç³»ï¼Œå®ƒçš„è”åˆæ¦‚ç‡åˆ†å¸ƒé€šå¸¸ä¼šè¡¨è¾¾ä¸ºä¸€ç³»åˆ—åŠ¿å‡½æ•°ï¼ˆpotential functionï¼‰çš„ä¹˜ç§¯ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œè¿™äº›ä¹˜ç§¯çš„ç§¯åˆ†å¹¶ä¸ç­‰äº1ï¼Œå› æ­¤ï¼Œè¿˜è¦å¯¹å…¶è¿›è¡Œå½’ä¸€åŒ–æ‰èƒ½å½¢æˆä¸€ä¸ªæœ‰æ•ˆçš„æ¦‚ç‡åˆ†å¸ƒâ€”â€”è¿™ä¸€ç‚¹å¾€å¾€åœ¨å®é™…åº”ç”¨ä¸­ç»™å‚æ•°ä¼°è®¡é€ æˆéå¸¸å¤§çš„å›°éš¾ã€‚ æŒ‰ç…§è¡¨ç¤ºçš„æŠ½è±¡çº§åˆ«ä¸åŒï¼Œæ¦‚ç‡å›¾æ¨¡å‹å¯ä»¥åˆ†ä¸ºï¼š åŸºäºéšæœºå˜é‡çš„æ¦‚ç‡å›¾æ¨¡å‹ï¼Œå¦‚è´å¶æ–¯ç½‘ã€é©¬å°”å¯å¤«ç½‘ã€æ¡ä»¶éšæœºåœºå’Œé“¾å›¾ç­‰ åŸºäºæ¨¡æ¿çš„æ¦‚ç‡å›¾æ¨¡å‹ï¼è¿™ç±»æ¨¡å‹æ ¹æ®åº”ç”¨åœºæ™¯ä¸åŒåˆå¯åˆ†ä¸ºä¸¤ç§ï¼š æš‚æ€æ¨¡å‹ï¼ŒåŒ…æ‹¬åŠ¨æ€è´å¶æ–¯ç½‘ï¼ˆDynamic Bayesian Network, DBNï¼‰å’ŒçŠ¶æ€è§‚æµ‹æ¨¡å‹ï¼Œå…¶ä¸­çŠ¶æ€è§‚æµ‹æ¨¡å‹åˆåŒ…æ‹¬çº¿æ€§åŠ¨æ€ç³»ç»Ÿï¼ˆLinear Dynamic System, LDSï¼‰å¦‚å¡å°”æ›¼æ»¤æ³¢å™¨ï¼Œè¿˜æœ‰éšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHidden Markov Model, HMMï¼‰ï¼› å¯¹è±¡å…³ç³»é¢†åŸŸçš„æ¦‚ç‡å›¾æ¨¡å‹ï¼ŒåŒ…æ‹¬ç›˜æ¨¡å‹ï¼ˆPlate Modelï¼ŒPMï¼‰ã€æ¦‚ç‡å…³ç³»æ¨¡å‹ï¼ˆProbabilistic Relational Model, PRMï¼‰å’Œå…³ç³»é©¬å°”å¯å¤«ç½‘ï¼ˆRelational Markov Network, RMNï¼‰ã€‚ å‚è€ƒï¼šæ¦‚ç‡å›¾æ¨¡å‹æ€»è§ˆ ","date":"2020-04-29","objectID":"/2020-04-29-ml-crf/:0:1","tags":["Probabilistic Graphical Model","Statistical Learning"],"title":"Conditional random field (CRF)","uri":"/2020-04-29-ml-crf/"},{"categories":["Machine Learning"],"content":"1. æ¨¡å‹å®šä¹‰ æ¦‚ç‡å›¾æ¨¡å‹ï¼šç”±å›¾ï¼ˆGraphï¼‰è¡¨ç¤ºçš„æ¦‚ç‡åˆ†å¸ƒã€‚ ä»¤æ— å‘å›¾ G = (V, E) è¡¨ç¤ºè”åˆæ¦‚ç‡åˆ†å¸ƒP(Y)ï¼Œå³Gä¸­ï¼Œ èŠ‚ç‚¹$v \\in V$ è¡¨ç¤ºéšæœºå˜é‡$Y_{v}, Y=\\left(Y_{v}\\right)_{v \\in V}$ï¼› è¾¹$e \\in E$è¡¨ç¤ºéšæœºå˜é‡ä¹‹é—´çš„æ¦‚ç‡ä¾èµ–å…³ç³» æ— å‘å›¾è¡¨ç¤ºçš„éšæœºå˜é‡å­˜åœ¨: æˆå¯¹é©¬å¯å¤«æ€§ pariwise Markov property æŒ‡ä»»æ„ä¸¤ä¸ªæ²¡æœ‰è¾¹è¿æ¥çš„èŠ‚ç‚¹ï¼Œåœ¨ç»™å®šéšæœºå˜é‡ç»„ï¼ˆå…¶ä»–æ‰€æœ‰èŠ‚ç‚¹ï¼‰æ¡ä»¶ä¸‹ï¼Œè¯¥ä¸¤èŠ‚ç‚¹æ˜¯æ¡ä»¶ç‹¬ç«‹ å±€éƒ¨é©¬å¯å¤«æ€§ local Markov property å…¨å±€é©¬å¯å¤«æ€§ global Markov property æ¦‚ç‡æ— å‘å›¾æ¨¡å‹ï¼šæ— å‘å›¾ $G = (V, E)$ è¡¨ç¤ºè”åˆæ¦‚ç‡åˆ†å¸ƒ $P(Y)$ï¼Œå¦‚æœè”åˆæ¦‚ç‡åˆ†å¸ƒ $P(Y)$ æ»¡è¶³æˆå¯¹ã€å±€éƒ¨æˆ–å…¨å±€é©¬å¯å¤«æ€§ï¼Œå°±ç§°æ­¤è”åˆæ¦‚ç‡åˆ†å¸ƒ $P(Y)$ ä¸ºæ¦‚ç‡æ— å‘å›¾æ¨¡å‹ï¼Œæˆ–é©¬å¯å¤«éšæœºåœº å›¢ï¼ˆcliqueï¼‰ï¼šå›¾Gä¸­ä»»ä½•ä¸¤ä¸ªèŠ‚ç‚¹å‡æœ‰è¾¹è¿æ¥çš„èŠ‚ç‚¹å­é›† æœ€å¤§å›¢ï¼ˆmaximal cliqueï¼‰ï¼šå›¢Cä¸­ä¸èƒ½å†åŠ ä»»ä½•ä¸€ä¸ªèŠ‚ç‚¹ä½¿å®ƒæˆä¸ºæ›´å¤§çš„å›¢ï¼Œåˆ™ç§°æœ€å¤§å›¢ ","date":"2020-04-29","objectID":"/2020-04-29-ml-crf/:0:2","tags":["Probabilistic Graphical Model","Statistical Learning"],"title":"Conditional random field (CRF)","uri":"/2020-04-29-ml-crf/"},{"categories":["Machine Learning"],"content":"2. æ¡ä»¶éšæœºåœº æ¡ä»¶éšæœºåœºæŒ‡ç»™å®šéšæœºå˜é‡Xæ¡ä»¶ä¸‹ï¼Œ éšæœºå˜é‡Yçš„é©¬å¯å¤«éšæœºåœºã€‚ 2.1 æ¡ä»¶éšæœºåœºï¼š è‹¥éšæœºå˜é‡$Y$æ„æˆä¸€ä¸ªç”±æ— å‘å›¾$G = (V, E)$è¡¨ç¤ºçš„é©¬å¯å¤«éšæœºåœºï¼Œå³ $$ P\\left(Y_{v} | X, Y_{w}, w \\neq v\\right)=P\\left(Y_{v} | X, Y_{w}, w \\sim v\\right) $$ å¯¹äºä»»æ„èŠ‚ç‚¹$v$æˆç«‹ï¼Œ åˆ™ç§°æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(Y\\vert X)$ä¸ºæ¡ä»¶éšæœºåœºã€‚å…¶ä¸­$w \\sim v$è¡¨ç¤ºåœ¨å›¾$G = (V, E)$ä¸­ä¸èŠ‚ç‚¹$v$æœ‰è¾¹è¿æ¥çš„æ‰€æœ‰èŠ‚ç‚¹$w$ï¼Œ $w \\neq v$è¡¨ç¤ºèŠ‚ç‚¹vä»¥å¤–çš„æ‰€æœ‰èŠ‚ç‚¹ã€‚ 2.2 çº¿æ€§é“¾æ¡ä»¶éšæœºåœºï¼ˆ linear chain conditional random fieldï¼‰ çº¿æ€§é“¾æ¡ä»¶éšæœºåœºä¹Ÿæ˜¯å¯¹æ•°çº¿æ€§æ¨¡å‹(log linear model)ï¼Œå®šä¹‰ä¸ºï¼š $$ P\\left(Y_{i} | X, Y_{1}, \\cdots, Y_{i-1}, Y_{i+1}, \\cdots, Y_{n}\\right)=P\\left(Y_{i} | X, Y_{i-1}, Y_{i+1}\\right) $$ åœ¨æ¡ä»¶æ¦‚ç‡æ¨¡å‹$P(Y | X)$ä¸­ï¼Œ $Y$æ˜¯è¾“å‡ºå˜é‡ï¼Œè¡¨ç¤ºæ ‡è®°åºåˆ—ï¼ˆçŠ¶æ€åºåˆ—ï¼Œå‚è§HMMï¼‰ï¼›$X$ä½¿è¾“å…¥å˜é‡ï¼Œè¡¨ç¤ºéœ€è¦æ ‡æ³¨çš„è§‚æµ‹åºåˆ—ã€‚åˆ©ç”¨è®­ç»ƒé›†ï¼Œé€šè¿‡æå¤§ä¼¼ç„¶ä¼°è®¡æˆ–æ­£åˆ™åŒ–çš„æå¤§ä¼¼ç„¶ä¼°è®¡å¾—åˆ°æ¡ä»¶æ¦‚ç‡æ¨¡å‹$\\hat{P}(Y | X)$;é¢„æµ‹æ—¶ï¼Œå¯¹äºç»™å®šè¾“å…¥åºåˆ—$x$ï¼Œæ±‚æ¡ä»¶æ¦‚ç‡$\\hat{P}(Y | X)$æœ€å¤§çš„è¾“å‡ºåºåˆ—$\\hat{y}$ã€‚ 2.3 æ¡ä»¶éšæœºåœºçš„å‚æ•°åŒ–å½¢å¼ è®¾$P(Y\\vert X)$ä¸ºçº¿æ€§é“¾æ¡ä»¶éšæœºåœºï¼ŒXå–å€¼ä¸ºxï¼Œ Yå–å€¼ä¸ºyçš„æ¡ä»¶æ¦‚ç‡å…·æœ‰å¦‚ä¸‹å½¢å¼ï¼š $$ P(y | x)=\\frac{1}{Z(x)} \\exp \\left(\\sum_{i, k} \\lambda_{k} t_{k}\\left(y_{i-1}, y_{i}, x, i\\right)+\\sum_{i, l} \\mu_{l} s_{l}\\left(y_{i}, x, i\\right)\\right) $$ å…¶ä¸­ï¼Œ $$ Z(x)=\\sum_{y} \\exp \\left(\\sum_{i, k} \\lambda_{k} t_{k}\\left(y_{i-1}, y_{i}, x, i\\right)+\\sum_{i, l} \\mu_{l} s_{l}\\left(y_{i}, x, i\\right)\\right) $$ å¼ä¸­ï¼Œ$t_{k}$å’Œ$s_{l}$æ˜¯ç‰¹å¾å‡½æ•°, $\\lambda_{k}$å’Œ$\\mu_{l}$æ˜¯å¯¹åº”çš„æƒå€¼ã€‚ $Z(x)$æ˜¯è§„èŒƒåŒ–å› å­ã€‚åœ¨æ‰€æœ‰å¯èƒ½è¾“å‡ºçš„åºåˆ—ä¸Šè¿›è¡Œæ±‚å’Œæ“ä½œã€‚ å…³äºç‰¹å¾å‡½æ•°ï¼š ä»¤$t_{k}$æ˜¯å®šä¹‰åœ¨è¾¹ä¸Šçš„ç‰¹å¾å‡½æ•°ï¼Œç§°ä¸ºè½¬ç§»ç‰¹å¾ï¼Œä¾èµ–å½“å‰å’Œå‰ä¸€ä¸ªä½ç½® ä»¤$s_{l}$æ˜¯å®šä¹‰åœ¨èŠ‚ç‚¹ä¸Šçš„ç‰¹å¾å‡½æ•°ï¼Œç§°ä¸ºçŠ¶æ€ç‰¹å¾ï¼Œä¾èµ–å½“å‰ä½ç½® ç‰¹å¾å‡½æ•°$t_{k}$å’Œ$s_{l}$å–å€¼0æˆ–1ï¼›æ»¡è¶³æ¡ä»¶å–1ï¼Œåä¹‹0 æ¡ä»¶éšæœºé•¿å®Œå…¨ç”±ç‰¹å¾å‡½æ•°$t_{k}$å’Œ$s_{l}$ï¼Œ å’Œå¯¹åº”çš„æƒå€¼$\\lambda_{k}$å’Œ$\\mu_{l}$ç¡®å®šã€‚ 2.4 æ¡ä»¶éšæœºåœºçš„çŸ©é˜µå½¢å¼ å¯¹äºè§‚æµ‹åºåˆ—xçš„æ¯ä¸ªä½ç½®ï¼Œyåœ¨mä¸ªæ ‡è®°ä¸­å–å€¼ï¼Œå¯ä»¥å®šä¹‰ä¸€ä¸ªmé˜¶çš„çŸ©é˜µéšæœºå˜é‡ï¼š $$ M_{i}(x) = [ M_{i}(y_{i-1}, y_{i}|x) ] $$ çŸ©é˜µéšæœºå˜é‡å…ƒç´ ä¸º $$ \\begin{aligned} \u0026M_{i}\\left(y_{i-1}, y_{i} | x\\right)=\\exp \\left(W_{i}\\left(y_{i-1}, y_{i} | x\\right)\\right)\\cr \u0026W_{i}\\left(y_{i-1}, y_{i} | x\\right)=\\sum_{k=1}^{K} w_{k} f_{k}\\left(y_{i-1}, y_{i}, x, i\\right) \\end{aligned} $$ è¿™é‡Œ$w_k$ä¸º $$ w_{k}=\\begin{cases} \\lambda_{k}, \u0026 k=1,2, \\cdots, K_{1} \\cr \\mu_{l}, \u0026 k=K_{1}+l ; l=1,2, \\cdots, K_{2} \\end{cases} $$ å’Œ$f_k$ä¸º $$ f_{k}\\left(y_{i-1}, y_{i}, x, i\\right)=\\begin{cases} t_{k}\\left(y_{i-1}, y_{i}, x, i\\right), \u0026 k=1,2, \\cdots, K_{1} \\cr s_{l}\\left(y_{i}, x, i\\right), \u0026 k=K_{1}+l ; l=1,2, \\cdots, K_{2} \\end{cases} $$ äºæ˜¯ï¼Œæ¡ä»¶æ¦‚ç‡$P_{w}(y \\vert x)$: $$ P_{w}(y | x)=\\frac{1}{Z_{w}(x)} \\prod_{i=1}^{n+1} M_{i}\\left(y_{i-1}, y_{i} | x\\right) $$ å…¶ä¸­ï¼Œ $$ Z_{w}(x)=\\left[M_{1}(x) M_{2}(x) \\cdots M_{n+1}(x)\\right]_{\\mathrm{start}, \\mathrm{stop}} $$ æ³¨ï¼Œ $y_{0} = \\mathrm{start}$ï¼Œè¡¨ç¤ºå¼€å§‹çŠ¶æ€ï¼› $y_{n+1} = \\mathrm{stop}$ï¼Œ è¡¨ç¤ºç»ˆæ­¢çŠ¶æ€ å‚è€ƒï¼š æèˆªã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹ ","date":"2020-04-29","objectID":"/2020-04-29-ml-crf/:0:3","tags":["Probabilistic Graphical Model","Statistical Learning"],"title":"Conditional random field (CRF)","uri":"/2020-04-29-ml-crf/"},{"categories":["Statistic"],"content":"Probability, P-value, Likelihood ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:0:0","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"Probability and likelihood likehood \u0026 maximum likehood åœ¨éæ­£å¼åœºåˆä¼¼ç„¶ï¼ˆlikelihoodï¼‰å’Œæ¦‚ç‡ï¼ˆProbabilityï¼‰å‡ ä¹æ˜¯ä¸€å¯¹åŒä¹‰è¯ï¼Œä½†æ˜¯åœ¨ç»Ÿè®¡å­¦ä¸­ä¼¼ç„¶å’Œæ¦‚ç‡å´æ˜¯ä¸¤ä¸ªä¸åŒçš„æ¦‚å¿µã€‚ æ¦‚ç‡: åœ¨ç‰¹å®šç¯å¢ƒä¸‹æŸä»¶äº‹æƒ…å‘ç”Ÿçš„å¯èƒ½æ€§ï¼Œä¹Ÿå°±æ˜¯ç»“æœæ²¡æœ‰äº§ç”Ÿä¹‹å‰ä¾æ®ç¯å¢ƒæ‰€å¯¹åº”çš„å‚æ•°æ¥é¢„æµ‹æŸä»¶äº‹æƒ…å‘ç”Ÿçš„å¯èƒ½æ€§ã€‚ æ¯”å¦‚æŠ›ç¡¬å¸ï¼ŒæŠ›ä¹‹å‰æˆ‘ä»¬ä¸çŸ¥é“æœ€åæ˜¯å“ªä¸€é¢æœä¸Šï¼Œä½†æ˜¯æ ¹æ®ç¡¬å¸çš„æ€§è´¨æˆ‘ä»¬å¯ä»¥æ¨æµ‹ä»»ä½•ä¸€é¢æœä¸Šçš„å¯èƒ½æ€§å‡ä¸º50%ï¼Œè¿™ä¸ªæ¦‚ç‡åªæœ‰åœ¨æŠ›ç¡¬å¸ä¹‹å‰æ‰æ˜¯æœ‰æ„ä¹‰çš„ï¼ŒæŠ›å®Œç¡¬å¸åçš„ç»“æœä¾¿æ˜¯ç¡®å®šçš„ï¼› ä¼¼ç„¶: åˆšå¥½ç›¸åï¼Œæ˜¯åœ¨ç¡®å®šçš„ç»“æœä¸‹å»æ¨æµ‹äº§ç”Ÿè¿™ä¸ªç»“æœçš„å¯èƒ½ç¯å¢ƒï¼ˆå‚æ•°ï¼‰ã€‚ å‡è®¾éšæœºæŠ›æ·ä¸€æšç¡¬å¸1,000æ¬¡ï¼Œç»“æœ500æ¬¡äººå¤´æœä¸Šï¼Œ500æ¬¡æ•°å­—æœä¸Šï¼Œé‚£ä¹ˆä¸¤é¢æœä¸Šçš„æ¦‚ç‡å‡ä¸º50%ã€‚è¿ç”¨å‡ºç°çš„ç»“æœæ¥åˆ¤æ–­è¿™ä¸ªäº‹æƒ…æœ¬èº«çš„æ€§è´¨ï¼ˆå‚æ•°ï¼‰ï¼Œä¹Ÿå°±æ˜¯ä¼¼ç„¶ã€‚ å½“ç»“æœå’Œå‚æ•°ç›¸äº’å¯¹åº”ï¼Œä¼¼ç„¶å’Œæ¦‚ç‡åœ¨æ•°å€¼ä¸Šç›¸ç­‰ã€‚ ç”¨ Î¸ è¡¨ç¤ºç¯å¢ƒå¯¹åº”çš„å‚æ•°ï¼Œx è¡¨ç¤ºç»“æœï¼Œé‚£ä¹ˆæ¦‚ç‡å¯ä»¥è¡¨ç¤ºä¸ºï¼š $$P(x | \\theta )$$ $p(x \\vert Î¸)$ æ˜¯æ¡ä»¶æ¦‚ç‡çš„è¡¨ç¤ºæ–¹æ³•ã€‚Î¸ æ˜¯å‰ç½®æ¡ä»¶ï¼Œç†è§£ä¸ºåœ¨ Î¸ çš„å‰æä¸‹ï¼Œäº‹ä»¶ x å‘ç”Ÿçš„æ¦‚ç‡ï¼Œç›¸å¯¹åº”çš„ä¼¼ç„¶å¯ä»¥è¡¨ç¤ºä¸º: $$\\mathcal{L}(\\theta | x)$$ å¯ä»¥ç†è§£ä¸ºå·²çŸ¥ç»“æœä¸º x ï¼Œå‚æ•°ä¸º Î¸ (ä¼¼ç„¶å‡½æ•°é‡Œ Î¸ æ˜¯å˜é‡ï¼Œè¿™é‡Œè¯´çš„å‚æ•°å’Œå˜é‡æ˜¯ç›¸å¯¹ä¸æ¦‚ç‡è€Œè¨€çš„)å¯¹åº”çš„æ¦‚ç‡ï¼Œå³ï¼š $$\\mathcal{L}(\\theta | x)=P(x | \\theta)$$ ä¸¤è€…åœ¨æ•°å€¼ä¸Šç›¸ç­‰ï¼Œä½†æ˜¯æ„ä¹‰å¹¶ä¸ç›¸åŒ, $\\mathcal{L}$ æ˜¯å…³äº Î¸ çš„å‡½æ•°ï¼Œè€Œ P åˆ™æ˜¯å…³äº x çš„å‡½æ•°ã€‚ ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:1:0","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"Probability and P-value A p-value is the probability that random chance generated the data, or something else that is equal or rarer. A p-value is composed of three parts: The probability random chance would result in the observation. The probability of observing something else that is equally rare. The probability of observing something rarer or more extreme. But probability $$ \\text{Probalibility} = \\frac{ \\text{Number of outcomes of interest}} { \\text{The total number of outcomes}}$$ In hypothesis testing, p-values are numbers, between 0 and 1, that, how small does a p-value have to be before we are confident that interested A is different from B. Statquest: P Values, clearly explained ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:2:0","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"PDF (probability density function) PDFï¼šæ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆprobability density functionï¼‰, è¿ç»­å‹éšæœºå˜é‡çš„æ¦‚ç‡å¯†åº¦å‡½æ•°æ˜¯ä¸€ä¸ªæè¿°æŸä¸ªç¡®å®šçš„å–å€¼ç‚¹é™„è¿‘çš„å¯èƒ½æ€§çš„å‡½æ•°ã€‚ æ•°å­¦è¡¨ç¤ºï¼šç”¨PDFåœ¨æŸä¸€åŒºé—´ä¸Šçš„ç§¯åˆ†æ¥åˆ»ç”»éšæœºå˜é‡è½åœ¨è¿™ä¸ªåŒºé—´ä¸­çš„æ¦‚ç‡ $$ \\operatorname{Pr}(a \\leq X \\leq b)=\\int_{a}^{b} f_{X}(x) d x $$ ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:2:1","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"PMF (probability mass function) PMF : æ¦‚ç‡è´¨é‡å‡½æ•°ï¼ˆprobability mass function), åœ¨æ¦‚ç‡è®ºä¸­ï¼Œæ¦‚ç‡è´¨é‡å‡½æ•°æ˜¯ç¦»æ•£éšæœºå˜é‡åœ¨å„ç‰¹å®šå–å€¼ä¸Šçš„æ¦‚ç‡ã€‚ æ•°å­¦è¡¨ç¤ºï¼š PMFå…¶å®å°±æ˜¯é«˜ä¸­æ‰€å­¦çš„ç¦»æ•£å‹éšæœºå˜é‡çš„åˆ†å¸ƒå¾‹ã€‚ $$ f_{X}(x)=\\operatorname{Pr}(X=x) $$ ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:2:2","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"CDF (cumulative distribution function) CDF : ç´¯ç§¯åˆ†å¸ƒå‡½æ•° (cumulative distribution function)ï¼Œæ˜¯æ¦‚ç‡å¯†åº¦å‡½æ•°çš„ç§¯åˆ†ï¼Œèƒ½å®Œæ•´æè¿°ä¸€ä¸ªå®éšæœºå˜é‡Xçš„æ¦‚ç‡åˆ†å¸ƒã€‚ CDFæ˜¯PDFçš„ï¼ˆä»è´Ÿæ— ç©·$-\\infty$åˆ°å½“å‰å€¼çš„ï¼‰ç§¯åˆ†ï¼ŒPDFæ˜¯CDFçš„å¯¼æ•°ï¼ï¼ˆä¸ºäº†ä¾¿äºæ¦‚ç‡çš„è®¡ç®—ï¼Œå¼•å…¥CDFçš„æ¦‚å¿µï¼‰ CDFç›¸å½“äºå…¶å·¦ä¾§çš„é¢ç§¯ï¼Œä¹Ÿç›¸å½“äºå°äºè¯¥å€¼çš„æ¦‚ç‡ï¼Œè´Ÿæ— ç©·çš„CDFå€¼ä¸ºï¼ï¼Œæ­£æ— ç©·çš„CDFå€¼æ€»ä¸ºï¼‘ï¼ å¯¹äºè¿ç»­å˜é‡ï¼Œæœ‰ $$ F_{X}(x)=\\operatorname{Pr}(X \\leq x)=\\int_{-\\infty}^{x} f_{X}(t) dt $$ å¯¹äºç¦»æ•£å‹å˜é‡ï¼Œæœ‰å¦‚ $$ F_{X}(x)=\\operatorname{Pr}(X \\leq x)= \\begin{cases} 0 \\text { if } x\u003c0 \\cr \\frac{1}{2} \\text { if } 0 \\leq x\u003c1 \\cr 1 \\text { if } x \\geq 1 \\end{cases} $$ ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:2:3","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"Central Limit Theorem ä¸­å¿ƒæé™å®šç†ï¼ˆCentral Limit Theoremï¼‰ ç»™å®šä¸€ä¸ªä»»æ„åˆ†å¸ƒçš„æ€»ä½“ï¼Œ æ¯æ¬¡ä»è¿™äº›æ€»ä½“ä¸­éšæœºæŠ½å– n ä¸ªæŠ½æ ·ï¼Œä¸€å…±æŠ½ m æ¬¡ï¼Œ ç„¶åæŠŠè¿™ m ç»„æŠ½æ ·åˆ†åˆ«æ±‚å‡ºå¹³å‡å€¼ï¼Œ å½“mè¶³å¤Ÿå¤§æ—¶ï¼Œè¿™mæ¬¡çš„å¹³å‡å€¼çš„åˆ†å¸ƒï¼ˆç§°ä¸ºæŠ½æ ·åˆ†å¸ƒï¼‰æ¥è¿‘æ­£æ€åˆ†å¸ƒã€‚ ç‹¬ç«‹åŒåˆ†å¸ƒçš„ä¸­å¿ƒæé™å®šç† $$ \\lim_{n \\rightarrow \\infty} F_{\\mathcal{X}}(x) = \\lim_{n \\rightarrow \\infty} P \\Bigg\\lbrace \\frac{\\sum_{k=1}^{n}X_k - n\\mu}{\\sqrt{n}\\sigma} \\leq x \\Bigg\\rbrace = \\int_{-\\infty}^{x} \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^2}{2}}dt $$ å¾·è«ä½›ï¼æ‹‰æ™®æ‹‰æ–¯å®šç†: è®¾éšæœºå˜é‡åºåˆ—$\\lbrace \\eta_1, \\eta_2,\\cdots, \\eta_n \\rbrace$ æœä»å‚æ•°ä¸º$n, p (0 \u003c p \u003c 1)$ çš„äºŒé¡¹åˆ†å¸ƒ $$ \\lim_{n \\rightarrow +\\infty} P \\Bigg\\lbrace \\frac{\\eta_n - np }{\\sqrt{np(1-p)}} \\leq x \\Bigg\\rbrace = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^2}{2}}dt $$ ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:2:4","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"Law of Large Numbers å½“æ ·æœ¬æ•°æ®æ— é™å¤§æ—¶ï¼Œæ ·æœ¬å‡å€¼è¶‹äºæ€»ä½“å‡å€¼ $$ \\bar{X} = \\frac{1}{n} \\sum_{k=1}^n X_k \\xrightarrow{p} \\mu $$ å¤§æ•°å®šå¾‹å‘Šè¯‰æˆ‘ä»¬èƒ½ç”¨é¢‘ç‡è¿‘ä¼¼ä»£æ›¿æ¦‚ç‡ï¼›èƒ½ç”¨æ ·æœ¬å‡å€¼è¿‘ä¼¼ä»£æ›¿æ€»ä½“å‡å€¼ã€‚ è¾›é’¦å¤§æ•°å®šå¾‹: è®¾$X_1, X_2, \\cdots, X_n$æ˜¯ç›¸äº’ç‹¬ç«‹ä¸”æœä»åŒåˆ†å¸ƒçš„éšæœºå˜é‡åºåˆ—ï¼Œ å…·æœ‰æ•°å­¦æœŸæœ›$E(X_k) = \\mu$, $k=1,2,3,\\cdots$ã€‚å¯¹äºä»»æ„ $\\epsilon \u003e 0$, æœ‰ $$ \\lim_{n \\rightarrow +\\infty} P \\bigg\\lbrace | \\frac{1}{n} \\sum_{k=1}^n X_k - \\mu | \u003c \\epsilon \\bigg\\rbrace = 1 $$ åˆ‡æ¯”é›ªå¤«å¤§æ•°å®šå¾‹: éšæœºå˜é‡åºåˆ—$X$å…·æœ‰ç›¸åŒæœŸæœ›å’Œæ–¹å·®, æ ·æœ¬å‡å€¼ä¾æ¦‚ç‡$p$æ”¶æ•›äº $\\mu$ $$ \\frac{1}{n} \\sum_{k=1}^n X_k \\xrightarrow{p} \\mu $$ ä¼¯åŠªåˆ©å¤§æ•°å®šå¾‹: $n_A$æ˜¯næ¬¡ç‹¬ç«‹é‡å¤è¯•éªŒä¸­äº‹ä»¶Aå‘ç”Ÿçš„æ¬¡æ•°ï¼Œ pæ˜¯äº‹ä»¶Aåœ¨æ¯æ¬¡è¯•éªŒä¸­å‘ç”Ÿçš„æ¦‚ç‡ï¼Œä»»æ„$\\epsilon \u003e 0$ $$ \\lim_{n \\rightarrow +\\infty} P \\bigg\\lbrace | \\frac{n_A}{n} - p | \u003c \\epsilon \\bigg\\rbrace = 1 $$ æ¯”è¾ƒ å®šå¾‹ åˆ†å¸ƒ æœŸæœ› æ–¹å·® ç»“è®º è¾›é’¦å¤§æ•°å®šå¾‹ ç›¸äº’ç‹¬ç«‹ä¸”åŒåˆ†å¸ƒ å­˜åœ¨ ä¼°ç®—æœŸæœ› åˆ‡æ¯”é›ªå¤«å¤§æ•°å®šå¾‹ ç›¸äº’ç‹¬ç«‹ ç›¸åŒ ç›¸åŒ ä¼°ç®—æœŸæœ› ä¼¯åŠªåˆ©å¤§æ•°å®šå¾‹ äºŒé¡¹åˆ†å¸ƒ ç›¸åŒ ç›¸åŒ é¢‘ç‡=æ¦‚ç‡ ç›¸åŒç‚¹ï¼š$n \\rightarrow +\\infty$, ä¾æ¦‚ç‡è¶‹è¿‘ æ¡ä»¶ç»„ä»¶å˜å¾—ä¸¥æ ¼ ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:2:5","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"Confidence interval ç½®ä¿¡åŒºé—´ï¼ˆconfidence intervalï¼‰ ç½®ä¿¡åŒºé—´æ˜¯æŒ‡ç”±æ ·æœ¬ç»Ÿè®¡é‡æ‰€æ„é€ çš„æ€»ä½“å‚æ•°çš„ä¼°è®¡åŒºé—´ã€‚ ç½®ä¿¡åŒºé—´å±•ç°çš„æ˜¯è¿™ä¸ªå‚æ•°çš„çœŸå®å€¼è½åœ¨æµ‹é‡å€¼ï¼ˆæ¨æµ‹å€¼ï¼‰çš„å‘¨å›´çš„å¯ä¿¡ç¨‹åº¦ã€‚ ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:2:6","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"Additional: StatQuest How to calculate P-value ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:3:0","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Make bioinfo uncool again"],"content":"The correct way to convert seurat Robj to Scanpy h5ad","date":"2020-04-28","objectID":"/2020-04-28-seurat2scanpy/","tags":["scRNA-seq","Scanpy","Seurat"],"title":"Convert Seurat Robj to Scanpy h5ad","uri":"/2020-04-28-seurat2scanpy/"},{"categories":["Make bioinfo uncool again"],"content":"It costed me a lot of time to convert seurat objects to scanpy. Itâ€™s not a pleasant experience. Finally, I solved it. ","date":"2020-04-28","objectID":"/2020-04-28-seurat2scanpy/:0:0","tags":["scRNA-seq","Scanpy","Seurat"],"title":"Convert Seurat Robj to Scanpy h5ad","uri":"/2020-04-28-seurat2scanpy/"},{"categories":["Make bioinfo uncool again"],"content":"1. Install Seurat v3.0.2, or python kernel will always died!!! Donâ€™t know why latest seurat not work. ","date":"2020-04-28","objectID":"/2020-04-28-seurat2scanpy/:0:1","tags":["scRNA-seq","Scanpy","Seurat"],"title":"Convert Seurat Robj to Scanpy h5ad","uri":"/2020-04-28-seurat2scanpy/"},{"categories":["Make bioinfo uncool again"],"content":"2. Set the R version for rpy2 # user defined R installation import os # path to your libR.so, only Seurat v3.0.2 works! # create a conda R env for seurat 3.0.2 first os.environ['R_HOME'] = '/home/fangzq/miniconda/envs/seurat/lib/R' # path depends on where you installed Python. os.environ['R_USER'] = '/home/fangzq/miniconda/lib/python3.7/site-packages/rpy2' ","date":"2020-04-28","objectID":"/2020-04-28-seurat2scanpy/:0:2","tags":["scRNA-seq","Scanpy","Seurat"],"title":"Convert Seurat Robj to Scanpy h5ad","uri":"/2020-04-28-seurat2scanpy/"},{"categories":["Make bioinfo uncool again"],"content":"3. Now, youâ€™er good to go import scanpy as sc import glob Install anndata2ri first import anndata2ri from rpy2.robjects import r from rpy2.robjects.conversion import localconverter # activate rpy2 env anndata2ri.activate() robjs = glob.glob(\"data/*Robj\") Convert to h5ad r('library(Seurat)') for robj in robjs: r(f'x\u003c-load(\"{robj}\")') r('y=get(x)') r('rm(x)') r('DefaultAssay(y) \u003c- \"RNA\"') # get raw count matrix to save # seurat2 object # adata = r('as.SingleCellExperiment(UpdateSeuratObject(y))') adata = r('as.SingleCellExperiment(y)') adata.write_h5ad(filename=robj.replace(\"Robj\",\"h5ad\")) ","date":"2020-04-28","objectID":"/2020-04-28-seurat2scanpy/:0:3","tags":["scRNA-seq","Scanpy","Seurat"],"title":"Convert Seurat Robj to Scanpy h5ad","uri":"/2020-04-28-seurat2scanpy/"},{"categories":["Make bioinfo uncool again"],"content":"4. other way seurat -\u003e loom -\u003e scanpy Itâ€™s much easier, but I did not test. save to loom format fist. pbmc.loom \u003c- as.loom(pbmc.seurat, filename = \"../output/pbmc3k.loom\", verbose = FALSE) pbmc.loom read into scanpy pbmc3k = sc.read_loom(\"../output/pbmc3k.loom\") use sceasy to save h5ad. ","date":"2020-04-28","objectID":"/2020-04-28-seurat2scanpy/:0:4","tags":["scRNA-seq","Scanpy","Seurat"],"title":"Convert Seurat Robj to Scanpy h5ad","uri":"/2020-04-28-seurat2scanpy/"},{"categories":["Make bioinfo uncool again"],"content":"GATK is design for human genetics, it works not so good on homogeneous inbred mouse. But we could use bcftools instead. One of my colleague who studies mouse genetics, said, I tried the haplotype caller from GATK. But it seems that the haplotype caller is designed for heterogeneous genome like human than for mice. Therefore, the result coming out of HC is worse than samtools, as I manually inspected a few regions that HC calls didnâ€™t make sense. In addition, in one of their mouse genomic paper that we reviewed, they even skipped the second recalibration step. We asked them why and they said it was because of the same reason: good for human but not that good for the homogeneous inbred mouse. But we still could collect the resource bundle for mouse. I found a workflow here. However, the script is out of date. Also, see discussion here For GATK4, we have ","date":"2020-03-10","objectID":"/2020-03-10-gatk4-mm10-bundle/:0:0","tags":["GATK","Genetics"],"title":"GATK for inbred mouse is not good","uri":"/2020-03-10-gatk4-mm10-bundle/"},{"categories":["Make bioinfo uncool again"],"content":"1. Genome Download from NCBI (mm10) or Sanger Mouse Genetics Programme # NCBI wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/635/GCF_000001635.26_GRCm38.p6/GCF_000001635.26_GRCm38.p6_genomic.fna.gz -O GRCm38_68.fa.gz # or Sanger MGP wget ftp://ftp-mouse.sanger.ac.uk/ref/GRCm38_68.fa -O GRCm38_68.fa ","date":"2020-03-10","objectID":"/2020-03-10-gatk4-mm10-bundle/:0:1","tags":["GATK","Genetics"],"title":"GATK for inbred mouse is not good","uri":"/2020-03-10-gatk4-mm10-bundle/"},{"categories":["Make bioinfo uncool again"],"content":"2. dbSNP Depends on your study design. Download All in one vcf file from NCBI wget ftp://ftp.ncbi.nih.gov/snp/organisms/archive/mouse_10090/VCF/00-All.vcf.gz \\ -O mouse.dbsnp.vcf.gz Download from the Sanger Mouse Genetics Programme (Sanger MGP) wget ftp://ftp-mouse.sanger.ac.uk/REL-1505-SNPs_Indels/mgp.v5.merged.snps_all.dbSNP142.vcf.gz ","date":"2020-03-10","objectID":"/2020-03-10-gatk4-mm10-bundle/:0:2","tags":["GATK","Genetics"],"title":"GATK for inbred mouse is not good","uri":"/2020-03-10-gatk4-mm10-bundle/"},{"categories":["Make bioinfo uncool again"],"content":"3. Known Indels For mouse indels, the Sanger Mouse Genetics Programme (Sanger MGP) is probably the best resource. Download all MGP indels (5/2015 release): wget ftp://ftp-mouse.sanger.ac.uk/REL-1505-SNPs_Indels/mgp.v5.merged.indels.dbSNP142.normed.vcf.gz \\ -O mgp.v5.indels.vcf.gz Filter for passing variants # take header first zcat mgp.v5.indels.vcf.gz | head -1000 | grep \"^#\" | cut -f 1-8 \\ \u003e mgp.v5.indels.pass.chr.vcf # keep only passing and append zcat mgp.v5.indels.vcf.gz | grep -v \"^#\" | cut -f 1-8 \\ | grep -w \"PASS\" \u003e\u003e mgp.v5.indels.pass.chr.vcf Sort VCF (automatically generated index has to be deleted due to a known bug -\u003e No anymore): gatk SortVcf -SD GRCm38_68.dict -I mgp.v5.indels.pass.chr.vcf -O mgp.v5.indels.pass.chr.sort.vcf # rm .idx # rm mgp.v5.indels.pass.chr.sort.vcf.idx ","date":"2020-03-10","objectID":"/2020-03-10-gatk4-mm10-bundle/:0:3","tags":["GATK","Genetics"],"title":"GATK for inbred mouse is not good","uri":"/2020-03-10-gatk4-mm10-bundle/"},{"categories":["Coding"],"content":"Get answers for C/C++ within ? s","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"Just some advanced C/C++ code snippets to keep in mind. ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:0:0","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"Header naming Never used some header file name with std. Sometimes, compiler could not find the std headers.!!! ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:1:0","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"FileIO The simplest example #include \u003ciostream\u003e#include \u003cfstream\u003e // output file std::ofstream output; output.open(\"test.compact.txt\"); // read input file string line; std::ifstream input(\"test.chrX.vcf\"); if (input.is_open()) { while (getline(input, line)) output \u003c\u003c line \u003c\u003c'\\n'; } input.close(); output.close(); ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:2:0","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"Strings: split and strip split string by delimiter std::vector\u003cstd::string\u003e split(const std::string\u0026 s, char delimiter) { std::vector\u003cstd::string\u003e tokens; std::string token; std::istringstream tokenStream(s); while (std::getline(tokenStream, token, delimiter)) { tokens.push_back(token); } return tokens; } strip strings std::string trim(const std::string\u0026 str, const std::string delimiter = \" \\n\\r\\t\") { // std::string s; // s.erase(s.find_last_not_of(\" \\n\\r\\t\")+1); size_t first = str.find_first_not_of(delimiter); if (std::string::npos == first) { return str; } size_t last = str.find_last_not_of(delimiter); return str.substr(first, (last - first + 1)); } ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:3:0","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"Containor ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:4:0","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"remove duplicated elements: O(nlogn) sort(nums.begin(),nums.end()); // inplace // unique do not change vector size, only put dup elements to end of containor // and return a iter which points to the first non-uniqdup element vector\u003cint\u003e::iterator iter = unique(nums.begin(), nums.end()); nums.erase(iter, nums.end()); //remove duplciates inplace // nums.resize( std::distance(nums.begin(),iter) ); ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:4:1","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"Remove elements Vector or Deque: algorithm remove() followed by erase() vector\u003cint\u003e vec = {1,1,2,3,4,4,6}; auto itr = remove(vec.begin(), vec.end(), 4); vec.erase(iter, vec.end()); vec.shrink_to_fit(); // reduce capacity List: member function .remove() Associative Container or Unordered Container: .erase() ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:4:2","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"Lambda Syntax auto basicLambda = [] { cout \u003c\u003c \"Hello, world!\" \u003c\u003c endl; }; basicLambda(); // Hello, world! // return type auto add = [](int a, int b) -\u003e int { return a + b; }; // inference return type auto multiply = [](int a, int b) { return a * b; }; Capture parameters int x = 10; auto add_x = [x](int a) { return a + x; }; // copy capture x auto multiply_x = [\u0026x](int a) { return a * x; }; // ref capture x []ï¼šé»˜è®¤ä¸æ•è·ä»»ä½•å˜é‡ï¼› [=]ï¼šé»˜è®¤ä»¥å€¼æ•è·æ‰€æœ‰å˜é‡ï¼› [\u0026]ï¼šé»˜è®¤ä»¥å¼•ç”¨æ•è·æ‰€æœ‰å˜é‡ï¼› [ x ]ï¼šä»…ä»¥å€¼æ•è·xï¼Œå…¶å®ƒå˜é‡ä¸æ•è·ï¼› [\u0026x]ï¼šä»…ä»¥å¼•ç”¨æ•è·xï¼Œå…¶å®ƒå˜é‡ä¸æ•è·ï¼› [=, \u0026x]ï¼šé»˜è®¤ä»¥å€¼æ•è·æ‰€æœ‰å˜é‡ï¼Œä½†æ˜¯xæ˜¯ä¾‹å¤–ï¼Œé€šè¿‡å¼•ç”¨æ•è·ï¼› [\u0026, x]ï¼šé»˜è®¤ä»¥å¼•ç”¨æ•è·æ‰€æœ‰å˜é‡ï¼Œä½†æ˜¯xæ˜¯ä¾‹å¤–ï¼Œé€šè¿‡å€¼æ•è·ï¼› [this]ï¼šé€šè¿‡å¼•ç”¨æ•è·å½“å‰å¯¹è±¡ï¼ˆå…¶å®æ˜¯å¤åˆ¶æŒ‡é’ˆï¼‰ï¼› [*this]ï¼šé€šè¿‡ä¼ å€¼æ–¹å¼æ•è·å½“å‰å¯¹è±¡ï¼› capture expression // capture by expression int x = 4; auto y = [\u0026r = x, x = x + 1] { r += 2; return x * x; }(); // x = 6ï¼Œy = 25 // initialize directly auto z = [str = \"string\"]{ return str; }(); // z: const char * generic: auto auto add = [](auto x, auto y) { return x + y; }; int x = add(2, 3); // 5 double y = add(2.5, 3.5); // 6.0 ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:5:0","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"Design Pattern ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:6:0","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"Singleton Define class Singleton { private: /* Here will be the instance stored. */ static Singleton* instance; /* Private constructor to prevent instancing. */ Singleton() {}; public: /* Static access method. */ static Singleton* getInstance() { if (instance == 0) instance = new Singleton(); return instance; } }; /* NULL, because instance will be initialized on demand. */ Singleton* Singleton::instance = 0; Usage #include \u003ciostream\u003eint main() { //new Singleton(); // Won't work Singleton* s = Singleton::getInstance(); // Ok Singleton* r = Singleton::getInstance(); /* The addresses will be the same. */ std::cout \u003c\u003c s \u003c\u003c std::endl; std::cout \u003c\u003c r \u003c\u003c std::endl; } ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:6:1","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"Delegate Donâ€™t confuse with delegate constructor!!! A delegate is a class that wraps a pointer or reference to an object instance, a member method of that objectâ€™s class to be called on that object instance, and provides a method to trigger that call. Example 1 #include \u003ciostream\u003eusing namespace std; class RealPrinter { public: void print() { std::cout \u003c\u003c \"real-printer\" \u003c\u003c std::endl; } }; class Printer { public: Printer() : p(RealPrinter()) {} void print() { p.print(); } private: RealPrinter p; }; int main() { Printer* printer = new Printer(); printer-\u003eprint(); } Example 2: #include \u003ciostream\u003eclass I //interface { public: virtual void f() = 0; virtual void g() = 0; }; class A : public I { public: void f(){std::cout \u003c\u003c \"A::f()\" \u003c\u003c std::endl;} void g(){std::cout \u003c\u003c \"A::g()\" \u003c\u003c std::endl;} }; class B : public I { public: void f(){std::cout \u003c\u003c \"B::f()\" \u003c\u003c std::endl;} void g(){std::cout \u003c\u003c \"B::g()\" \u003c\u003c std::endl;} }; class C : public I { public: C() { m_i = new A();/*delegation*/ } void f(){ m_i-\u003ef(); } void g(){ m_i-\u003eg(); } // normal attributes void toA(){ m_i = new A(); } void toB(){ m_i = new B(); } private: I* m_i; } int main() { C cc = C(); cc.f(); // output: A::f() cc.g(); // output: A::g() cc.toB(); cc.f(); // output: B::f() cc.g(); // output: B::g() } ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:6:2","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"Composite Composite is a structural design pattern that allows composing objects into a tree-like structure and work with the it as if it was a singular object. ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:6:3","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"Get answers for C/C++ within ? s","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"C++ Notes Just some C/C++ code snippets to keep in mind. C/C++ is tremendous complicated, but itâ€™s still the most powerful programming language. ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:0:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Table of Contents char to Int Pointer Pointer and Smart Pointer Array as Argument Operator that canâ€™t be overloaded Object Instantization Object Relationship Virtual Function and Ploymorphism Friend Const Constexpr Extern and static ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:1:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Char to Int ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:2:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"1. Char sotred as ASCII C store Char as ASCII (Int) by default. So, Char is equal to ASCII code. char a = 'A'; // 65 int c = a; //c = 65 ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:2:1","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"2. Char to Int, to String, and vice versa char and int char c='5' int res = c -'0' ; // 5 int i=5; char res = I + '0'; // '5' char*,int and string // char * to string const char * str_c = \"hello\"; std::string str = str_c; // string to char* str.c_str(); // return const char* // int to string std::to_string() // string to int std::stoi() ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:2:2","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"3. Char as subscript of array: legal! This is useful when create hashmap. e.g. counting chars int test[200] = {0}; test['A'] = 1; // legal test['b'] = 2; // legal ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:2:3","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Operator Could not be Overloaded operator memo . Member access or dot operator .* Pointer-to-member Operator :: Scope Resolution operator ? : conditional operator, ternary operator sizeof object size operator, built-in operations typeid object type operator, built-in operations OK, Whatâ€™s .* ? //we have a class struct X { void f() {} void g() {} }; typedef void (X::*pointer)(); //ok, let's take a pointer and assign f to it. pointer somePointer = \u0026X::f; //now I want to call somePointer. But for that, I need an object X x; //now I call the member function on x like this (x.*somePointer)(); //will call x.f() //now, suppose x is not an object but a pointer to object X* px = new X; //I want to call the memfun pointer on px. I use -\u003e* (px -\u003e* somePointer)(); //will call px-\u003ef(); Now, you canâ€™t use x.somePointer(), or px-\u003esomePointer() because there is no such member in class X. see here ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:3:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Pointer Pointer syntax Rule: read from right to left int a; // an int int *a; // a pointer point to int int **a; // secondary int pointer, point to another int pointer int a[10]; // int array int *a[10]; // a poiter array, point to int int (*a)[10]; // a int pointer point to an int array int (*a)(int); // a pointer point to a function, will return an int int (*a[10])(int); // a poiter array, point to a functionï¼Œwill return an int Declare two pointers int* a, b; // equal to int* a; int b; int *a, *b; // correct way ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:4:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Pointer and Smart Pointer #include \u003cmemory\u003e // smart pointer header // [pomter to smart pointer struct Base {}; struct Derived: Base {}; // pointer to smart Base *p1 = new Derived(); // upcast std::shared_ptr\u003cBase\u003e sp(p1); // a polymorphic type Base *p = new Derived(); // upcast, dynamic_cast is unnecessary Derived* dp = dynamic_cast\u003cDerived*\u003e (p); // downcast // smart pointer convert to pointer std::shared_ptr\u003cBase\u003e smart = std::make_shared\u003cDerived\u003e(); Base* p2 = smart.get(); // .get() // smart pointer cast // downcast std::shared_ptr\u003cDerived\u003e dsmart = std::dynamic_pointer_cast\u003cDerived\u003e(smart); // upcast // case 1 std::shared_ptr\u003cBase\u003e foo(new Derived()); // case 2 std::shared_ptr\u003cDerived\u003e bar = std::make_shared\u003cBase\u003e(); std::shared_ptr\u003cBase\u003e foo = std::dynamic_pointer_cast\u003cA\u003e(bar); ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:5:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Array ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:6:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Array as formal arguments An Array could not copy to anther Array (Copy pointer is not allowed!), so call-by-value is not allowed. So, use array pointer: //these are same void print(const int*); void print(const int[]); void print(const int[5]); multi-dimension array void print(const int(*p)[3], int rowsize); void print(const int p[][5], int rowsize); When use pointer to an Array, the dimension is unknown. So, need an extra argument to specify it explicitly. Example: void print1(int (*p)[3]) { cout\u003c\u003cp[1][1]\u003c\u003cendl; } void print2(int p[][3]) { cout\u003c\u003cp[0][0]\u003c\u003cendl; } int a[2][3]={ {1,2},{3,4} }; print1(a); // 4 print2(a); // 1 int b[2][4]={ {1,2,5,6},{3,4,7,8} }; print1(b); // error ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:6:1","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Object Instantization ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:7:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"1. without new stack ClassName object(param); // A a(1);Â ClassName object2 = ClassName(param); // A b = A(1); ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:7:1","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"2. with new heap ClassName *object = newÂ ClassName(param);//A *a = new A(); delete object; ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:7:2","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"3. copy constructor // ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:7:3","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"4. Smart Pointer std::unique_ptr\u003cClassName\u003e object (new ClassName(param)); // recommend this way of instantization std::unique_ptr\u003cClassName\u003e object = std::make_unique\u003cClassName\u003e(param); ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:7:4","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Friend The friend declaration appears in a class body and grants a function or another class access to private and protected members of the class where the friend declaration appears. ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:8:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"1. friend function Declare anywhere inside a class, but define outside // function friend \u003ctype\u003e \u003cName\u003e(\u003carguments\u003e); Example: class A { public: A(int _a):a(_a){}; // non-member function friend int getA_a(A \u0026_classA); private: int a; }; // without the friend keyword int getA_a(A \u0026_classA) { //access member by formal arguments return _classA.a; } A _classA(3); std::cout\u003c\u003cgetA_a(_classA); // 3 ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:8:1","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"2. friend class Delare inside class, define outside // class friend class \u003cName\u003e; Note: friend class X {}; is an error Example: class B { public: B(int _b):b(_b){}; friend class C; // friend class private: int b; }; class C { public: int getB_b(B _classB){ //access member by formal arguments return _classB.b; }; }; B _classB(3); C _classC; // an instance of a friend class _classC.getB_b(_classB); ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:8:2","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"3. Others: friend ostream, friend template â€¦ class Y { int data; // the non-member function operator\u003c\u003c will have access to Y's private members friend std::ostream\u0026 operator\u003c\u003c(std::ostream\u0026 out, const Y\u0026 o); friend char* X::foo(int); // members of other classes can be friends too friend X::X(char), X::~X(); // constructors and destructors can be friends }; // this operator\u003c\u003c still needs to be defined, as a non-member std::ostream\u0026 operator\u003c\u003c(std::ostream\u0026 out, const Y\u0026 y) { // can access private member Y::data return out \u003c\u003c y.data; } Back to top ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:8:3","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Object Relationships relation types â€œis-aâ€ â€œhas-aâ€ â€œuses-aâ€ â€œdepends-onâ€ Property Composition Aggregation Association Relationship type Whole/part Whole/part Otherwise unrelated Members can belong to multiple classes No Yes Yes Members existence managed by class Yes No No Directionality Unidirectional Unidirectional Unidirectional or bidirectional Relationship verb Part-of Has-a Uses-a ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:9:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Composition: has a data member Building complex objects from simpler ones is called object composition . object composition models a â€œhas-aâ€ relationship between two objects. In C++, It means structs and classes can have data members of various types. class Aï¼› class B { public: B(){} ~B(){} private: A a; int b; }ï¼› Summary: Typically use normal member variables Can use pointer members if the class handles object allocation/deallocation itself Responsible for creation/destruction of parts ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:9:1","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Aggregation: â€œhas aâ€ Unlike a composition, parts can belong to more than one object at a time, and the whole object is not responsible for the existence and lifespan of the parts. Summary: Typically use pointer or reference members that point to or reference objects that live outside the scope of the aggregate class Not responsible for creating/destroying parts ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:9:2","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Association: â€œuses aâ€ Association models as â€œuses-aâ€ relationship. The doctor â€œusesâ€ the patient (to earn income). The patient uses the doctor (for whatever health purposes they need). ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:9:3","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Delegate: â€œhas aâ€ or called pImpl(Pointer to IMPLementation) Delegate: Composition by reference has a pointer of another object class Aï¼› class B { public: B(){} ~B(){} private: A *a; int b; }ï¼› ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:9:4","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Inheritance: â€œis aâ€ public, protected, private class A { public: A(){} virtual ~A(){} } class B : public A { }; ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:9:5","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Virtual Functions and Runtime Polymorphism Declare: vitrual keyword class TestA { public: virtual void func() { cout \u003c\u003c \"virtual function\" \u003c\u003c endl; } }; class Test : public TestA { public: virtual void func() { // virtual could be omited cout \u003c\u003c \"Test virtual function\" \u003c\u003c endl; } ~Test() { } }; TestA* t = new Test; // parent pointer point to child (Ploymorphism) t-\u003efunc(); // Resultsï¼šTest virtual function delete t; Member Could not be virtual inline function constructor non-member function static function: only one copy of all objects. friend function: itâ€™s non-member function member function template ! Pure virtual function declare virtual void fun() = 0; class with pure virtual functoin could not be instantized! a derived class of virtual class has to define pure virtual function. then the derived class could be instantized. abstract class: class with pure virtual function virtual deconstrutor A parent pointer point to itâ€™s child. When delete the parent pointer, only parent constuctor is called. if declared a virtual deconstuctor, childâ€™s deconstuctor is called first, then the parent deconsturctor. virtural keyword could be omited if a parent deconstructor is declared. delete a pointer will only called objectâ€™s deconstructor where the pointer point to. see also Pointer and smart pointer cast ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:10:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Const ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:11:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"1. const before or behind type/class, the syntax semantic are same // they are same const int x; // (int x) is const/inmutable int const x; // (const x) has type int ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:11:1","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"2. Pointer with const: constÂ int* p,Â intÂ const* p and intÂ *const p Dirty trick: use * as a separator, const restrict the type according to the side where it belong to point to const: These two expression are same // -\u003e (const int) | p; p : a mutable pointer points to a const/immutable int const int * p; // -\u003e (int const) | p; p2: a mutable pointer points a const which has type int int const * p2; const pointer: But these two not the same // -\u003e int | (const p); p3: a const pointer, point to an mutable int int * const p3; // -\u003e (const int) | const p; p4: a const pointer, pointing to an immutable/const int const int * const p4; ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:11:2","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"3. class member func with const: () const a. const object could not change class variable could not call non-const function class Number { public: void set(int num) { number = num; } int get() { return number; } int get2() const {return number;} int number = 0; }; // Example const Number n; n.number = 1; // Error, n is const n.set(1); // Error, n is const, non-const `set()` n.get(); // Error, non-const `get()` n.get2(); // OK b. () const could not change class variable, except static could get variable class Number { private: int a; static int b; const int c = 20; public: void set() { a = 10; // error when `this` argument has type 'const' void set2() const { b = 20; // OK } int get() const { // OK return a; // did not change a } }; const Number n; n.set(); // Error n.set2(); // OK n.get(); // OK Easy to understand, when pointer this is const void Number::set(const Number *const this, int num) { number = num; } // illegal -\u003e const this c. () const overloading ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:11:3","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"constexpr The constexpr specifier declares that it is possible to evaluate the value of the function or variable at compile time. ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:12:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"default and delete in class special class member: default constructor deconstructor copy constructor operater = when use default and delete default class X { public: X()=default; // with this, you could declare like this: X x; X(int){}; }; X x; // works delete: prohibit func call marked by delete class X { public: X(); X(const X\u0026) = delete; X\u0026 operator = (const X \u0026) = delete; }; // example X x1; X x2=x1; // Error, copy constructor is prohibited ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:13:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"extern, static global variable defined outside all functions and available to all functions. unaffected by scopes and are always available (exists until the program ends) extern declare a global variable (exists on the whole project): variable could be used in multi- .cpp files extern \"C\" {/* c code */}: compile c code. static declare a local global variable (file scope): only be accessed in its translation unit or .o file, thatâ€™s, in the file where it is created. declare a static class member (class scope): initialization should be outside class body static data member static function: no this pointer: only access to other static member/function could declare as private ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:14:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Get answers for C/C++ within ? s","date":"2020-02-10","objectID":"/2020-02-10-cpp-pointer/","tags":["C++"],"title":"C++ Notes: Return Pointer or Reference","uri":"/2020-02-10-cpp-pointer/"},{"categories":["Coding"],"content":"Explaination A pointer or reference could not be return if they point/refer to a local variable stored in stack inside a function (local variable stored in stack will be destoried automatically when return, and the pointer become wild) Situations when a function could return pointer or reference variable defined outside a function scope global variable local static variable local variable stored in heap ( new opterator, malloc()) Other process could not access the memory of variable stored in heap until it is released. Thatâ€™s why. ","date":"2020-02-10","objectID":"/2020-02-10-cpp-pointer/:1:0","tags":["C++"],"title":"C++ Notes: Return Pointer or Reference","uri":"/2020-02-10-cpp-pointer/"},{"categories":["Coding"],"content":"Example #include \u003ciostream\u003e#include \u003cstring.h\u003e#include \u003cstdlib.h\u003e using namespace std; string\u0026 f1(const string \u0026s) { static string result = s; return result; } string \u0026f2(const string \u0026s) { string *p = new string; *p = s; return *p; } int *f3() { int *a = (int *)malloc(sizeof(int) * 10); *a = 10; *(a + 1) = 11; return a; } int \u0026f4() { int *a = (int *)malloc(sizeof(int) * 10); *a = 10; *(a + 1) = 11; return *a; } int main() { int *a = \u0026f7(); cout\u003c\u003c(*(a + 1))\u003c\u003cendl; free(a); // free the memory when done. return 0; } Return *this, or alrealy exist objets // ref A\u0026 A::operator++() { count++; return *this; // already existed object, created outside } ","date":"2020-02-10","objectID":"/2020-02-10-cpp-pointer/:2:0","tags":["C++"],"title":"C++ Notes: Return Pointer or Reference","uri":"/2020-02-10-cpp-pointer/"},{"categories":["Coding"],"content":"Whatâ€™s the defference between new and malloc() malloc() is a function that takes a number (of bytes) as its argument; it __returns a void*__ pointing to unitialized storage. new is an operator that **takes a type** and (optionally) a set of initializers for that type as its arguments; it **returns a pointer to an** (optionally) initialized **object of its type**. The difference is most obvious when you want to allocate an object of a user-defined type with non-trivial initialization semantics ","date":"2020-02-10","objectID":"/2020-02-10-cpp-pointer/:3:0","tags":["C++"],"title":"C++ Notes: Return Pointer or Reference","uri":"/2020-02-10-cpp-pointer/"},{"categories":["Machine Learning"],"content":"A recipe for interactive computing using custom Jupyter kernels on Stanford's Sherlock.","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"A recipe for interactive computing using custom Jupyter kernels on Stanfordâ€™s Sherlock. ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:0:0","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"Setting up custom conda environment on Sherlockâ€™s login node ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:0","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"1. Download and install Miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh # install bash Miniconda3-latest-Linux-x86_64.sh conda config --set always_yes yes ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:1","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"2. Install jupyter notebook/lab and secure your notebooks with a password # install the default py3 kernel for jupyter notebook conda install ipython jupyter notebook jupyterlab # add password jupyter notebook password ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:2","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"3. (Optional) Add custom conda environment. i.e. fastai conda create -n fastai ipython ipykernel # add the custom to Jupyter notebook conda activate fastai python -m ipykernel install --user --name fastai --display-name FastAI you could also add R, Julia etc kernel. ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:3","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"4. Install pytorch/tensorflow You should select the existed cuda version which installed in Sherlock conda install -c pytorch pytorch torchvision cudatoolkit=10.1 tensorflow conda install tensorflow-gpu cudatoolkit=10.1 ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:4","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"5. Load gpu modules. Select the corresponding cuda version youâ€™ve just installed # this is my version module load cuda/10.1.168 module load cudnn/7.6.4 module load nccl ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:5","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"6. now, open ipython, run import torch print(torch.cuda.is_avilable()) if print out is True, then youâ€™er OK to use GPUs. Follow these steps on your local machine see details here. ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:6","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"7. Download the forward repo git clone https://github.com/vsoch/forward cd forward ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:7","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"8. Generate your parameters bash setup.sh Select Sherlock partition: gpu ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:8","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"9. SSH Credentials bash hosts/sherlock_ssh.sh \u003e\u003e ~/.ssh/config ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:9","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"10. create a sbatch script in forward/sbatches/sherlock and save as jupyter-gpu.sbatch #!/bin/bash PORT=$1 NOTEBOOK_DIR=$2 if [ -z \"$NOTEBOOK_DIR\" ]; then cd $SCRATCH else cd $NOTEBOOK_DIR fi ## to compile libtorch C++ code, load these modules # module load gcc/7.3.0 # module load gdb # module load cmake # export CC=$(which gcc) # export CXX=$(which g++) # select cuda version you need module load cuda/10.1.168 module load cudnn/7.6.4 module load nccl # activate fastai env source activate fastai jupyter lab --no-browser --port=$PORT ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:10","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"11. Start a session The default working directory is $SCRATCH bash start.sh jupyter-gpu change the working directory bash start.sh jupyter /path/to/dir ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:11","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"12. open your browser in local machine and type if your port is 51888, then http://localhost:51888/ here is my jupyter lab computing environment. Have fun! fastai kernel Test GPUs ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:12","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"13. Resume a session bash resume.sh jupyter-gpu # or bash resume.sh jupyter-gpu /path/to/dir ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:13","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"14. Stop a session bash end.sh jupyter-gpu # or bash end.sh jupyter-gpu /path/to/dir ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:14","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Algorithm and data structure"],"content":"Graphs ","date":"2020-02-09","objectID":"/2020-02-09-cs-graph/:0:0","tags":["C++"],"title":"Graph","uri":"/2020-02-09-cs-graph/"},{"categories":["Algorithm and data structure"],"content":"Some data structures to keep in my mind. BinaryHeap: Complete binary tree MaxHeap: Parent \u003e Both Children IndexMaxHeap MinHeap: Parent \u003c Both Children IndexMinHeap Priority queue (MaxHeap) BinarySearchTree Not always complete binary tree Value: leftChild \u003c Parent \u003c rightChild DenseGraph SparseGraph Code snippets take from Play with Algorithm ","date":"2020-02-09","objectID":"/2020-02-09-cs-graph/:0:1","tags":["C++"],"title":"Graph","uri":"/2020-02-09-cs-graph/"},{"categories":["Algorithm and data structure"],"content":"1. Dense Graph #include \u003ciostream\u003e#include \u003cvector\u003e#include \u003ccassert\u003e using namespace std; // ç¨ å¯†å›¾ - é‚»æ¥çŸ©é˜µ class DenseGraph{ private: int n, m; bool directed; vector\u003cvector\u003cbool\u003e\u003e g; public: DenseGraph( int n , bool directed ){ this-\u003en = n; this-\u003em = 0; this-\u003edirected = directed; for( int i = 0 ; i \u003c n ; i ++ ) g.push_back( vector\u003cbool\u003e(n, false) ); } ~DenseGraph(){ } int V(){ return n;} int E(){ return m;} void addEdge( int v , int w ){ assert( v \u003e= 0 \u0026\u0026 v \u003c n ); assert( w \u003e= 0 \u0026\u0026 w \u003c n ); if( hasEdge( v , w ) ) return; g[v][w] = true; if( !directed ) g[w][v] = true; m ++; } bool hasEdge( int v , int w ){ assert( v \u003e= 0 \u0026\u0026 v \u003c n ); assert( w \u003e= 0 \u0026\u0026 w \u003c n ); return g[v][w]; } void show(){ for( int i = 0 ; i \u003c n ; i ++ ){ for( int j = 0 ; j \u003c n ; j ++ ) cout\u003c\u003cg[i][j]\u003c\u003c\"\\t\"; cout\u003c\u003cendl; } } class adjIterator{ private: DenseGraph \u0026G; int v; int index; public: adjIterator(DenseGraph \u0026graph, int v): G(graph){ this-\u003ev = v; this-\u003eindex = -1; } int begin(){ index = -1; return next(); } int next(){ for( index += 1 ; index \u003c G.V() ; index ++ ) if( G.g[v][index] ) return index; return -1; } bool end(){ return index \u003e= G.V(); } }; }; ","date":"2020-02-09","objectID":"/2020-02-09-cs-graph/:0:2","tags":["C++"],"title":"Graph","uri":"/2020-02-09-cs-graph/"},{"categories":["Algorithm and data structure"],"content":"2. Sparse Graph #include \u003ciostream\u003e#include \u003cvector\u003e#include \u003ccassert\u003e using namespace std; // ç¨€ç–å›¾ - é‚»æ¥è¡¨ class SparseGraph{ private: int n, m; bool directed; vector\u003cvector\u003cint\u003e\u003e g; public: SparseGraph( int n , bool directed ){ this-\u003en = n; this-\u003em = 0; this-\u003edirected = directed; for( int i = 0 ; i \u003c n ; i ++ ) g.push_back( vector\u003cint\u003e() ); } ~SparseGraph(){ } int V(){ return n;} int E(){ return m;} void addEdge( int v, int w ){ assert( v \u003e= 0 \u0026\u0026 v \u003c n ); assert( w \u003e= 0 \u0026\u0026 w \u003c n ); g[v].push_back(w); if( v != w \u0026\u0026 !directed ) g[w].push_back(v); m ++; } bool hasEdge( int v , int w ){ assert( v \u003e= 0 \u0026\u0026 v \u003c n ); assert( w \u003e= 0 \u0026\u0026 w \u003c n ); for( int i = 0 ; i \u003c g[v].size() ; i ++ ) if( g[v][i] == w ) return true; return false; } void show(){ for( int i = 0 ; i \u003c n ; i ++ ){ cout\u003c\u003c\"vertex \"\u003c\u003ci\u003c\u003c\":\\t\"; for( int j = 0 ; j \u003c g[i].size() ; j ++ ) cout\u003c\u003cg[i][j]\u003c\u003c\"\\t\"; cout\u003c\u003cendl; } } class adjIterator{ private: SparseGraph \u0026G; int v; int index; public: adjIterator(SparseGraph \u0026graph, int v): G(graph){ this-\u003ev = v; this-\u003eindex = 0; } int begin(){ index = 0; if( G.g[v].size() ) return G.g[v][index]; return -1; } int next(){ index ++; if( index \u003c G.g[v].size() ) return G.g[v][index]; return -1; } bool end(){ return index \u003e= G.g[v].size(); } }; }; ","date":"2020-02-09","objectID":"/2020-02-09-cs-graph/:0:3","tags":["C++"],"title":"Graph","uri":"/2020-02-09-cs-graph/"},{"categories":["Algorithm and data structure"],"content":"Binary trees Difference ","date":"2020-02-08","objectID":"/2020-02-08-cs-binarytree/:0:0","tags":["C++"],"title":"Binary tree","uri":"/2020-02-08-cs-binarytree/"},{"categories":["Algorithm and data structure"],"content":"Some data structures to keep in my mind. BinaryHeap: Complete binary tree MaxHeap: Parent \u003e Both Children IndexMaxHeap MinHeap: Parent \u003c Both Children IndexMinHeap Priority queue (MaxHeap) BinarySearchTree Not always complete binary tree Value: leftChild \u003c Parent \u003c rightChild DenseGraph SparseGraph Code snippets take from Play with Algorithm ","date":"2020-02-08","objectID":"/2020-02-08-cs-binarytree/:0:1","tags":["C++"],"title":"Binary tree","uri":"/2020-02-08-cs-binarytree/"},{"categories":["Algorithm and data structure"],"content":"1. MaxHeap #include \u003calgorithm\u003e#include \u003ccassert\u003e using namespace std; template\u003ctypename Item\u003e class MaxHeap{ private: Item *data; int count; int capacity; void shiftUp(int k){ while( k \u003e 1 \u0026\u0026 data[k/2] \u003c data[k] ){ swap( data[k/2], data[k] ); k /= 2; } } void shiftDown(int k){ while( 2*k \u003c= count ){ int j = 2*k; // which child is larger, then swap if( j+1 \u003c= count \u0026\u0026 data[j+1] \u003e data[j] ) j ++; if( data[k] \u003e= data[j] ) break; swap( data[k] , data[j] ); k = j; } } public: MaxHeap(int capacity){ data = new Item[capacity+1]; count = 0; this-\u003ecapacity = capacity; } MaxHeap(Item arr[], int n){ data = new Item[n+1]; capacity = n; for( int i = 0 ; i \u003c n ; i ++ ) data[i+1] = arr[i]; count = n; for( int i = count/2 ; i \u003e= 1 ; i -- ) shiftDown(i); } ~MaxHeap(){ delete[] data; } int size(){ return count; } bool isEmpty(){ return count == 0; } void insert(Item item){ assert( count + 1 \u003c= capacity ); data[count+1] = item; shiftUp(count+1); count ++; } Item extractMax(){ assert( count \u003e 0 ); Item ret = data[1]; swap( data[1] , data[count] ); count --; shiftDown(1); return ret; } Item getMax(){ assert( count \u003e 0 ); return data[1]; } }; ","date":"2020-02-08","objectID":"/2020-02-08-cs-binarytree/:0:2","tags":["C++"],"title":"Binary tree","uri":"/2020-02-08-cs-binarytree/"},{"categories":["Algorithm and data structure"],"content":"2. IndexMaxHeap Need 3 vector: data, indexes, reverse Code #include \u003calgorithm\u003e#include \u003ccassert\u003e using namespace std; template\u003ctypename Item\u003e class IndexMaxHeap{ private: Item *data; int *indexes; int *reverse; int count; int capacity; void shiftUp( int k ){ while( k \u003e 1 \u0026\u0026 data[indexes[k/2]] \u003c data[indexes[k]] ){ swap( indexes[k/2] , indexes[k] ); reverse[indexes[k/2]] = k/2; reverse[indexes[k]] = k; k /= 2; } } void shiftDown( int k ){ while( 2*k \u003c= count ){ int j = 2*k; if( j + 1 \u003c= count \u0026\u0026 data[indexes[j+1]] \u003e data[indexes[j]] ) j += 1; if( data[indexes[k]] \u003e= data[indexes[j]] ) break; swap( indexes[k] , indexes[j] ); reverse[indexes[k]] = k; reverse[indexes[j]] = j; k = j; } } public: IndexMaxHeap(int capacity){ data = new Item[capacity+1]; indexes = new int[capacity+1]; reverse = new int[capacity+1]; for( int i = 0 ; i \u003c= capacity ; i ++ ) reverse[i] = 0; count = 0; this-\u003ecapacity = capacity; } ~IndexMaxHeap(){ delete[] data; delete[] indexes; delete[] reverse; } int size(){ return count; } bool isEmpty(){ return count == 0; } // ä¼ å…¥çš„iå¯¹ç”¨æˆ·è€Œè¨€,æ˜¯ä»0ç´¢å¼•çš„ void insert(int i, Item item){ assert( count + 1 \u003c= capacity ); assert( i + 1 \u003e= 1 \u0026\u0026 i + 1 \u003c= capacity ); i += 1; data[i] = item; indexes[count+1] = i; reverse[i] = count+1; count++; shiftUp(count); } Item extractMax(){ assert( count \u003e 0 ); Item ret = data[indexes[1]]; swap( indexes[1] , indexes[count] ); reverse[indexes[count]] = 0; reverse[indexes[1]] = 1; count--; shiftDown(1); return ret; } int extractMaxIndex(){ assert( count \u003e 0 ); int ret = indexes[1] - 1; swap( indexes[1] , indexes[count] ); reverse[indexes[count]] = 0; reverse[indexes[1]] = 1; count--; shiftDown(1); return ret; } Item getMax(){ assert( count \u003e 0 ); return data[indexes[1]]; } int getMaxIndex(){ assert( count \u003e 0 ); return indexes[1]-1; } bool contain( int i ){ assert( i + 1 \u003e= 1 \u0026\u0026 i + 1 \u003c= capacity ); return reverse[i+1] != 0; } Item getItem( int i ){ assert( contain(i) ); return data[i+1]; } void change( int i , Item newItem ){ assert( contain(i) ); i += 1; data[i] = newItem; // æ‰¾åˆ°indexes[j] = i, jè¡¨ç¤ºdata[i]åœ¨å †ä¸­çš„ä½ç½® // ä¹‹åshiftUp(j), å†shiftDown(j) // for( int j = 1 ; j \u003c= count ; j ++ ) // if( indexes[j] == i ){ // shiftUp(j); // shiftDown(j); // return; // } int j = reverse[i]; shiftUp( j ); shiftDown( j ); } // test reverse index bool testReverseIndex(){ int *copyIndexes = new int[count+1]; int *copyReverseIndexes = new int[count+1]; for( int i = 0 ; i \u003c= count ; i ++ ){ copyIndexes[i] = indexes[i]; copyReverseIndexes[i] = reverse[i]; } copyIndexes[0] = copyReverseIndexes[0] = 0; std::sort(copyIndexes, copyIndexes + count + 1); std::sort(copyReverseIndexes, copyReverseIndexes + count + 1); bool res = true; for( int i = 1 ; i \u003c= count ; i ++ ) if( copyIndexes[i-1] + 1 != copyIndexes[i] || copyReverseIndexes[i-1] + 1 != copyReverseIndexes[i] ) res = res || false; delete[] copyIndexes; delete[] copyReverseIndexes; if( !res ){ cout\u003c\u003c\"Error 1\"\u003c\u003cendl; return res; } for( int i = 1 ; i \u003c= count ; i ++ ) if( reverse[ indexes[i] ] != i ){ cout\u003c\u003c\"Error 2\"\u003c\u003cendl; return false; } return true; } }; ","date":"2020-02-08","objectID":"/2020-02-08-cs-binarytree/:0:3","tags":["C++"],"title":"Binary tree","uri":"/2020-02-08-cs-binarytree/"},{"categories":["Algorithm and data structure"],"content":"3. BinarySearchTree #include \u003ciostream\u003e#include \u003cqueue\u003e#include \u003ccassert\u003e using namespace std; template \u003ctypename Key, typename Value\u003e class BST{ private: struct Node{ Key key; Value value; Node *left; Node *right; Node(Key key, Value value){ this-\u003ekey = key; this-\u003evalue = value; this-\u003eleft = this-\u003eright = NULL; } Node(Node *node){ this-\u003ekey = node-\u003ekey; this-\u003evalue = node-\u003evalue; this-\u003eleft = node-\u003eleft; this-\u003eright = node-\u003eright; } }; Node *root; int count; public: BST(){ root = NULL; count = 0; } ~BST(){ destroy( root ); } int size(){ return count; } bool isEmpty(){ return count == 0; } void insert(Key key, Value value){ root = insert(root, key, value); } bool contain(Key key){ return contain(root, key); } Value* search(Key key){ return search( root , key ); } // å‰åºéå† void preOrder(){ preOrder(root); } // ä¸­åºéå† void inOrder(){ inOrder(root); } // ååºéå† void postOrder(){ postOrder(root); } // å±‚åºéå† void levelOrder(){ queue\u003cNode*\u003e q; q.push(root); while( !q.empty() ){ Node *node = q.front(); q.pop(); cout\u003c\u003cnode-\u003ekey\u003c\u003cendl; if( node-\u003eleft ) q.push( node-\u003eleft ); if( node-\u003eright ) q.push( node-\u003eright ); } } // å¯»æ‰¾æœ€å°çš„é”®å€¼ Key minimum(){ assert( count != 0 ); Node* minNode = minimum( root ); return minNode-\u003ekey; } // å¯»æ‰¾æœ€å¤§çš„é”®å€¼ Key maximum(){ assert( count != 0 ); Node* maxNode = maximum(root); return maxNode-\u003ekey; } // ä»äºŒå‰æ ‘ä¸­åˆ é™¤æœ€å°å€¼æ‰€åœ¨èŠ‚ç‚¹ void removeMin(){ if( root ) root = removeMin( root ); } // ä»äºŒå‰æ ‘ä¸­åˆ é™¤æœ€å¤§å€¼æ‰€åœ¨èŠ‚ç‚¹ void removeMax(){ if( root ) root = removeMax( root ); } // ä»äºŒå‰æ ‘ä¸­åˆ é™¤é”®å€¼ä¸ºkeyçš„èŠ‚ç‚¹ void remove(Key key){ root = remove(root, key); } private: // å‘ä»¥nodeä¸ºæ ¹çš„äºŒå‰æœç´¢æ ‘ä¸­,æ’å…¥èŠ‚ç‚¹(key, value) // è¿”å›æ’å…¥æ–°èŠ‚ç‚¹åçš„äºŒå‰æœç´¢æ ‘çš„æ ¹ Node* insert(Node *node, Key key, Value value){ if( node == NULL ){ count ++; return new Node(key, value); } if( key == node-\u003ekey ) node-\u003evalue = value; else if( key \u003c node-\u003ekey ) node-\u003eleft = insert( node-\u003eleft , key, value); else // key \u003e node-\u003ekey node-\u003eright = insert( node-\u003eright, key, value); return node; } // æŸ¥çœ‹ä»¥nodeä¸ºæ ¹çš„äºŒå‰æœç´¢æ ‘ä¸­æ˜¯å¦åŒ…å«é”®å€¼ä¸ºkeyçš„èŠ‚ç‚¹ bool contain(Node* node, Key key){ if( node == NULL ) return false; if( key == node-\u003ekey ) return true; else if( key \u003c node-\u003ekey ) return contain( node-\u003eleft , key ); else // key \u003e node-\u003ekey return contain( node-\u003eright , key ); } // åœ¨ä»¥nodeä¸ºæ ¹çš„äºŒå‰æœç´¢æ ‘ä¸­æŸ¥æ‰¾keyæ‰€å¯¹åº”çš„value Value* search(Node* node, Key key){ if( node == NULL ) return NULL; if( key == node-\u003ekey ) return \u0026(node-\u003evalue); else if( key \u003c node-\u003ekey ) return search( node-\u003eleft , key ); else // key \u003e node-\u003ekey return search( node-\u003eright, key ); } // å¯¹ä»¥nodeä¸ºæ ¹çš„äºŒå‰æœç´¢æ ‘è¿›è¡Œå‰åºéå† void preOrder(Node* node){ if( node != NULL ){ cout\u003c\u003cnode-\u003ekey\u003c\u003cendl; preOrder(node-\u003eleft); preOrder(node-\u003eright); } } // å¯¹ä»¥nodeä¸ºæ ¹çš„äºŒå‰æœç´¢æ ‘è¿›è¡Œä¸­åºéå† void inOrder(Node* node){ if( node != NULL ){ inOrder(node-\u003eleft); cout\u003c\u003cnode-\u003ekey\u003c\u003cendl; inOrder(node-\u003eright); } } // å¯¹ä»¥nodeä¸ºæ ¹çš„äºŒå‰æœç´¢æ ‘è¿›è¡Œååºéå† void postOrder(Node* node){ if( node != NULL ){ postOrder(node-\u003eleft); postOrder(node-\u003eright); cout\u003c\u003cnode-\u003ekey\u003c\u003cendl; } } void destroy(Node* node){ if( node != NULL ){ destroy( node-\u003eleft ); destroy( node-\u003eright ); delete node; count --; } } // åœ¨ä»¥nodeä¸ºæ ¹çš„äºŒå‰æœç´¢æ ‘ä¸­,è¿”å›æœ€å°é”®å€¼çš„èŠ‚ç‚¹ Node* minimum(Node* node){ if( node-\u003eleft == NULL ) return node; return minimum(node-\u003eleft); } // åœ¨ä»¥nodeä¸ºæ ¹çš„äºŒå‰æœç´¢æ ‘ä¸­,è¿”å›æœ€å¤§é”®å€¼çš„èŠ‚ç‚¹ Node* maximum(Node* node){ if( node-\u003eright == NULL ) return node; return maximum(node-\u003eright); } // åˆ é™¤æ‰ä»¥nodeä¸ºæ ¹çš„äºŒåˆ†æœç´¢æ ‘ä¸­çš„æœ€å°èŠ‚ç‚¹ // è¿”å›åˆ é™¤èŠ‚ç‚¹åæ–°çš„äºŒåˆ†æœç´¢æ ‘çš„æ ¹ Node* removeMin(Node* node){ if( node-\u003eleft == NULL ){ Node* rightNode = node-\u003eright; delete node; count --; return rightNode; } node-\u003eleft = removeMin(node-\u003eleft); return node; } // åˆ é™¤æ‰ä»¥nodeä¸ºæ ¹çš„äºŒåˆ†æœç´¢æ ‘ä¸­çš„æœ€å¤§èŠ‚ç‚¹ // è¿”å›åˆ é™¤èŠ‚ç‚¹åæ–°çš„äºŒåˆ†æœç´¢æ ‘çš„æ ¹ Node* removeMax(Node* node){ if( node-\u003eright == NULL ){ Node* leftNode = node-\u003eleft; delete node; count --; return leftNode; } node-\u003eright = removeMax(node-\u003eright); return node; } // åˆ é™¤æ‰ä»¥nodeä¸ºæ ¹çš„äºŒåˆ†æœç´¢æ ‘ä¸­é”®å€¼ä¸ºkeyçš„èŠ‚ç‚¹ // è¿”å›åˆ é™¤èŠ‚ç‚¹åæ–°çš„äºŒåˆ†æœç´¢æ ‘çš„æ ¹ Node* remove(Node* node, Key key){ if( node == NULL ) return NULL; if( key \u003c node-\u003ekey ){ node-\u003eleft = remove( node-\u003eleft , key ); return node; } else if( key \u003e node-\u003ekey ){ node-\u003eright = remove( node-\u003eright, key ); return node; } else{ // ","date":"2020-02-08","objectID":"/2020-02-08-cs-binarytree/:0:4","tags":["C++"],"title":"Binary tree","uri":"/2020-02-08-cs-binarytree/"},{"categories":["Algorithm and data structure"],"content":"Sort algorithms Code snippets are taken from Play with Algorithm ","date":"2020-02-08","objectID":"/2020-02-08-cs-sort/:0:0","tags":["C++"],"title":"Sort","uri":"/2020-02-08-cs-sort/"},{"categories":["Algorithm and data structure"],"content":"Some algorithm to keep in my mind. selectionSort insertionSort mergeSort quickSort two way three way heapSort ","date":"2020-02-08","objectID":"/2020-02-08-cs-sort/:0:1","tags":["C++"],"title":"Sort","uri":"/2020-02-08-cs-sort/"},{"categories":["Algorithm and data structure"],"content":"1. insertionSort #include \u003ciostream\u003e#include \u003calgorithm\u003e using namespace std; template\u003ctypename T\u003e void insertionSort(T arr[], int n){ for( int i = 1 ; i \u003c n ; i ++ ) { T e = arr[i]; int j; for (j = i; j \u003e 0 \u0026\u0026 arr[j-1] \u003e e; j--) arr[j] = arr[j-1]; arr[j] = e; } return; } // å¯¹arr[l...r]èŒƒå›´çš„æ•°ç»„è¿›è¡Œæ’å…¥æ’åº template\u003ctypename T\u003e void insertionSort(T arr[], int l, int r){ for( int i = l+1 ; i \u003c= r ; i ++ ) { T e = arr[i]; int j; for (j = i; j \u003e l \u0026\u0026 arr[j-1] \u003e e; j--) arr[j] = arr[j-1]; arr[j] = e; } return; } ","date":"2020-02-08","objectID":"/2020-02-08-cs-sort/:0:2","tags":["C++"],"title":"Sort","uri":"/2020-02-08-cs-sort/"},{"categories":["Algorithm and data structure"],"content":"2. mergeSort #include \u003ciostream\u003e#include \u003calgorithm\u003e#include \"InsertionSort.h\" using namespace std; template\u003ctypename T\u003e void __merge(T arr[], int l, int mid, int r){ T aux[r-l+1]; for( int i = l ; i \u003c= r; i ++ ) aux[i-l] = arr[i]; int i = l, j = mid+1; for( int k = l ; k \u003c= r; k ++ ){ if( i \u003e mid ) { arr[k] = aux[j-l]; j ++;} else if( j \u003e r ){ arr[k] = aux[i-l]; i ++;} else if( aux[i-l] \u003c aux[j-l] ){ arr[k] = aux[i-l]; i ++;} else { arr[k] = aux[j-l]; j ++;} } } template\u003ctypename T\u003e void __mergeSort(T arr[], int l, int r){ if( r - l \u003c= 15 ){ insertionSort(arr, l, r); return; } int mid = (l+r)/2; __mergeSort(arr, l, mid); __mergeSort(arr, mid+1, r); if( arr[mid] \u003e arr[mid+1] ) __merge(arr, l, mid, r); } template\u003ctypename T\u003e void mergeSort(T arr[], int n){ __mergeSort( arr , 0 , n-1 ); } template \u003ctypename T\u003e void mergeSortBU(T arr[], int n){ for( int i = 0 ; i \u003c n ; i += 16 ) insertionSort(arr,i,min(i+15,n-1)); for( int sz = 16; sz \u003c= n ; sz += sz ) for( int i = 0 ; i \u003c n - sz ; i += sz+sz ) if( arr[i+sz-1] \u003e arr[i+sz] ) __merge(arr, i, i+sz-1, min(i+sz+sz-1,n-1) ); } ","date":"2020-02-08","objectID":"/2020-02-08-cs-sort/:0:3","tags":["C++"],"title":"Sort","uri":"/2020-02-08-cs-sort/"},{"categories":["Algorithm and data structure"],"content":"3. qucikSort #include \u003ciostream\u003e#include \u003cctime\u003e#include \u003calgorithm\u003e#include \"InsertionSort.h\" using namespace std; template \u003ctypename T\u003e int _partition(T arr[], int l, int r){ // select a rand index in arr, and swap swap( arr[l] , arr[rand()%(r-l+1)+l] ); T v = arr[l]; int i = l+1, j = r; while( true ){ while( i \u003c= r \u0026\u0026 arr[i] \u003c v ) i ++; while( j \u003e= l+1 \u0026\u0026 arr[j] \u003e v ) j --; if( i \u003e j ) break; swap( arr[i] , arr[j] ); i ++; j --; } swap( arr[l] , arr[j]); return j; } template \u003ctypename T\u003e void _quickSort(T arr[], int l, int r){ if( r - l \u003c= 15 ){ insertionSort(arr,l,r); return; } int p = _partition(arr, l, r); _quickSort(arr, l, p-1 ); _quickSort(arr, p+1, r); } template \u003ctypename T\u003e void quickSort(T arr[], int n){ srand(time(NULL)); _quickSort(arr, 0, n-1); } template \u003ctypename T\u003e void __quickSort3Ways(T arr[], int l, int r){ if( r - l \u003c= 15 ){ insertionSort(arr,l,r); return; } swap( arr[l], arr[rand()%(r-l+1)+l ] ); T v = arr[l]; int lt = l; // arr[l+1...lt] \u003c v int gt = r + 1; // arr[gt...r] \u003e v int i = l+1; // arr[lt+1...i) == v while( i \u003c gt ){ if( arr[i] \u003c v ){ swap( arr[i], arr[lt+1]); i ++; lt ++; } else if( arr[i] \u003e v ){ swap( arr[i], arr[gt-1]); gt --; } else{ // arr[i] == v i ++; } } swap( arr[l] , arr[lt] ); __quickSort3Ways(arr, l, lt-1); __quickSort3Ways(arr, gt, r); } template \u003ctypename T\u003e void quickSort3Ways(T arr[], int n){ srand(time(NULL)); __quickSort3Ways( arr, 0, n-1); } ","date":"2020-02-08","objectID":"/2020-02-08-cs-sort/:0:4","tags":["C++"],"title":"Sort","uri":"/2020-02-08-cs-sort/"},{"categories":["Algorithm and data structure"],"content":"4. heapSort #include \"Heap.h\" using namespace std; template\u003ctypename T\u003e void heapSort2(T arr[], int n){ MaxHeap\u003cT\u003e maxheap = MaxHeap\u003cT\u003e(arr,n); for( int i = n-1 ; i \u003e= 0 ; i-- ) arr[i] = maxheap.extractMax(); } template\u003ctypename T\u003e void heapSort1(T arr[], int n){ MaxHeap\u003cT\u003e maxheap = MaxHeap\u003cT\u003e(n); for( int i = 0 ; i \u003c n ; i ++ ) maxheap.insert(arr[i]); for( int i = n-1 ; i \u003e= 0 ; i-- ) arr[i] = maxheap.extractMax(); } ","date":"2020-02-08","objectID":"/2020-02-08-cs-sort/:0:5","tags":["C++"],"title":"Sort","uri":"/2020-02-08-cs-sort/"},{"categories":["Algorithm and data structure"],"content":"What on earth is Big O? Time complexity and space complexity ","date":"2020-02-01","objectID":"/2020-02-01-cs-algo/:0:1","tags":["C++"],"title":"What is Big O","uri":"/2020-02-01-cs-algo/"},{"categories":["Algorithm and data structure"],"content":"Time complexity O(f(n)): number of commands need to execute. proportional to f(n). è¡¨ç¤ºè¿è¡Œç®—æ³•æ‰€éœ€è¦æ‰§è¡Œçš„æŒ‡ä»¤æ•°ï¼Œå’Œf(n)æˆæ­£ã€‚ ä¸¥æ ¼æ¥è®²ï¼ŒO(f(n))è¡¨ç¤ºç®—æ³•æ‰§è¡Œçš„ä¸Šç•Œã€‚ä¸šç•Œé»˜è®¤ä¸ºç®—æ³•æ‰§è¡Œçš„æœ€ä½ä¸Šç•Œ(æœ€åæƒ…å†µï¼‰ã€‚ n represents the data scale æ•°æ®è§„æ¨¡ when n is a large number, the constant is usually ignored. algorithm n of cmd Binary reserach $O(logn)$ $a*logn$ Max/min in an array $O(n)$ b*n merge sort $O(nlogn)$ $c*nlogn$ select sort $O(n^2)$ $d*n^2$ quick sort $O(nlogn)$ e*nlogn adjacent graph $O(V+E)$ Lazy Prim $O(ElogE)$ Prim $O(ElogV)$ Kruskal $O(ElogE)$ Dijkstra $O(ElogV)$ Bellman-Ford $O(EV)$ minimum span tree Shortest path tree (Single source shortest path) ","date":"2020-02-01","objectID":"/2020-02-01-cs-algo/:0:2","tags":["C++"],"title":"What is Big O","uri":"/2020-02-01-cs-algo/"},{"categories":["Algorithm and data structure"],"content":"Space complexity cmd complexity new an array $O(n)$ new 2d array $O(n^2)$ new an constant space $O(1)$ recursive function: the depth (n) of a recursive function, the extra space need $O(n)$. ","date":"2020-02-01","objectID":"/2020-02-01-cs-algo/:0:3","tags":["C++"],"title":"What is Big O","uri":"/2020-02-01-cs-algo/"},{"categories":["Algorithm and data structure"],"content":"Make sense of n If you want to solve the problem in 1 second, then an algorithm of complexity cmds n $O(n^2)$ could exec cmd n = $10^4$ $O(n)$ could exec cmd n = $10^8$ $O(nlogn)$ could exec cmd n = $10^7$ ","date":"2020-02-01","objectID":"/2020-02-01-cs-algo/:0:4","tags":["C++"],"title":"What is Big O","uri":"/2020-02-01-cs-algo/"},{"categories":["Algorithm and data structure"],"content":"Example binarySearch find from n element find from $n/2$ element find from $n/4$ element ... find from 1 Thatâ€™s, need how many steps of search when n = 1? $log_{2}n = O(logn)$. int2string. Set num \u003e 0 string int2string(int num) { string s=\"\"; while(num) { s += '0' + num%10; num /= 10; } reverse(s); // O(n) return s; } That is, how many â€œ/10â€ steps when num = 0? $log_{10}n = O(logn)$. Case: two nested for loop, not always $O(n^2)$ void hello(int n){ for (int sz =1; sz \u003c n; sz ++ sz) // logn here for( int i=1; i \u003c n;; i++) //n cout\u003c\u003c\"hello, complex\" \u003c\u003cendl; } So, should be $O(nlogn)$ isPrime: $O(\\sqrt{n})$ // set n \u003e 1 bool isPrime(int n){ for( int x =2; x*x \u003c= n; x++){ if( n%x == 0) return false; return true; } } recursive function single recursive function call int binarySearch(int arr[], int l, int r, int target) { if (l\u003er) return -1; int mid = l +(r-l)/2; if (arr[mid] == target) return mid; else if (arr[mid] \u003e target) return binarySearch(arr, ;, mid-1, target); else return binarySearch(arr, mid+1, r, target); } each step need O(1), so overall complexity depend on recursive exec depth. That is, if each function call needs time T, then time complexity: O(T*depth) -\u003e O(n). Another example: recursion depth logn, them time complexity O(logn). double pow( double x, int n){ assert(n \u003e=0); if (n==0) return 1.0; double t = pow(x, n/2); if( n%2) return x*t*t; return t*t; } multi recursive exec how many exec step? int f(int n) { assert(n \u003e=0); if(n == 0) return 1; return f(n-1) + f(n); } thatâ€™s, count how many nodes on a full binary tree. $2^{n+1} -1 = O(2^n)$ how to think about this? void mergeSort(int arr[]. int l. int r){ if (l \u003e=r) return; int mid = (l+r) /2; mergeSort(arr, l, mid); mergeSort(arr, mid+1, r); merge(arr, l, mid, r); } For binary tree, complexity for each level O(n), while tree depth O(logn). Overall, O(nlogn) Amortized time i.e. dynamic vector/stack/deque template\u003cT\u003e class MyVector{ private: T* data; int capacity; int size; //O(n) void resize(int newCapacity){ assert(newCapacity \u003e= size); T* newData = new T[newCapacity]; for(int i = 0; i \u003c size; i++ ){ newData[i] = data[i]; } delete[] data; data = newData; capacity = newCapacity; } public: MyVector() { data = new T[10]; capacity = 10; size = 0; } ~MyVector() { delete[] data; } // Average: O(1) void push_back(T e){ //assert(size \u003c capacity) if (size == capacity) resize (2 *capacity); data[size++] = e; } // Average O(1) T pop_back(){ assert (size \u003e0); T ret = data[size-1]; size --; // note the denominator here. To advoid ossilation of space complexity if(size == capacity / 4) resize(capacity /2); return ret; } }; ","date":"2020-02-01","objectID":"/2020-02-01-cs-algo/:0:5","tags":["C++"],"title":"What is Big O","uri":"/2020-02-01-cs-algo/"},{"categories":["Machine Learning"],"content":"A breif review over the foundations of statistical inference ","date":"2020-01-30","objectID":"/2020-01-30-ml-bayes/:0:0","tags":["Naive Bayes","Bayesian Inference"],"title":"Statistical Modeling and Inference","uri":"/2020-01-30-ml-bayes/"},{"categories":["Machine Learning"],"content":"Statistical Models and Inference statistical inference: a formal approach to characterizing a random phenomenon using observations, either by providing a description of a past phenomenon or by giving some predictions about future phenomenon of similar nature. ","date":"2020-01-30","objectID":"/2020-01-30-ml-bayes/:1:0","tags":["Naive Bayes","Bayesian Inference"],"title":"Statistical Modeling and Inference","uri":"/2020-01-30-ml-bayes/"},{"categories":["Machine Learning"],"content":"1. Statistical Models The first step in statistical inference is to specify a statistical model, under some simplifying assumptions (i.e. independence assumptions). Hierarchical models: the probability distribution of one parameter is dependent on the values of other hierachical paramters (i.e. conditional independent). Steps: Set assumptions (i.e. independent), parameter and model. Make explicit assumptions on the probability distributions. focus on parametric modeling, because of limited data nonparametric not consider here, used for hypothesis testing or when sample size is very large. Once the model is specified, then choose a method of inference, as well as an algorithm to obtain estimates. Most commonly use: Maximum likelihood inference Bayesian inference ","date":"2020-01-30","objectID":"/2020-01-30-ml-bayes/:1:1","tags":["Naive Bayes","Bayesian Inference"],"title":"Statistical Modeling and Inference","uri":"/2020-01-30-ml-bayes/"},{"categories":["Machine Learning"],"content":"2. Maximum likelihood inference Quantifying confidence: the Fisher Information Matrix Newtonâ€™s algorithm Approximate Techniques Monte Carlo Sampling for intractable likelihoods Composite likelihood ","date":"2020-01-30","objectID":"/2020-01-30-ml-bayes/:1:2","tags":["Naive Bayes","Bayesian Inference"],"title":"Statistical Modeling and Inference","uri":"/2020-01-30-ml-bayes/"},{"categories":["Machine Learning"],"content":"3. Bayesian Inference A statistical model describes the uncertainty about how the data was produced. The ultimate aim for statistical inference is to obtain information about the unknown parameter $\\theta$ given the data $\\mathcal{D}$. Frequentist: $\\theta$ is fixed but unknown quantity. Bayesian: use a fully probabilistic model and treat $\\theta$ as a random quantity. To do so, chose an appropriate prior distribution $\\mathbb{P}(\\theta)$, which reflects the knowledge (i.e. uncertainty) about $\\theta$ prior to the experiment the goal is to update the knowledge given the information contained in the data $\\mathcal{D}$. the updated knowledge (i.e. reduced uncertainty) is encapsulated in the posterior distribution $\\mathbb{P}(\\theta \\vert \\mathcal{D})$, which is calculated via Bayesâ€™theorem. $$ \\mathbb{P}(\\boldsymbol{\\theta} | \\mathcal{D})=\\frac{\\mathbb{P}(\\mathcal{D} | \\boldsymbol{\\theta}) \\mathbb{P}(\\boldsymbol{\\theta})}{\\mathbb{P}(\\mathcal{D})} $$ The bayesian paradigm boils down to the slogan: posterior $\\propto$ likelihood $\\times$ prior 3.1 Choice of prior distributions Conjugate priors the prior and the posterior lie in the same class of distributions. often chosen, because it leads to a well-known form of the posterior, which simplifies the calculations choose a prior that contains as little information about the parameter as possible at first choice would, of course, be a locally uniform prior. Under a uniform prior we have $\\mathbb{P}(\\boldsymbol{\\theta} \\vert \\boldsymbol{D}) \\propto \\mathcal{L}(\\boldsymbol{\\theta})$. Jeffreyâ€™s prior, but often hard to come by 3.2 Bayesian point estimates and confidence intervals Bayesian point estimates: the posterior mean, mode and median $$ \\hat{\\theta}=\\mathbb{E}[\\theta | D]=\\int \\theta \\mathbb{P}(\\theta | \\mathcal{D}) \\mathrm{d} \\theta $$ confidence: highest posterior density (HPD) region for a threshold value $\\pi$, the region $\\mathcal{C}_{\\alpha}={\\theta: \\mathbb{P}(\\theta \\vert \\mathcal{D})\u003e\\pi}$, we get $$ \\int_{C_{\\alpha}} \\mathbb{P}(\\theta | \\mathcal{D}) \\mathrm{d} \\theta=1-\\alpha $$ This region $\\mathcal{C}_{\\alpha}$ is the HPD region. 3.3 Markov Chain Monte Carlo A common challenge in Bayesian inference is that the integral $$ \\mathbb{P}(D)=\\int \\mathbb{P}(D | \\theta) \\mathbb{P}(\\theta) d_{\\theta} $$ canâ€™t be solved analytically. to be continuedâ€¦ 3.4 Empirical Bayes for Latent Variable Problems The first step is to infer point estimates for the parameters at higher levels by integrating out those at lower levels, and the infer posterior distributions for lower level parameters while setting those at a higher level to their point estimate. 3.5 Approximate Bayesian Computation Approximate Bayesian computation (ABC) is a class of simulation-based techniques to conduct Bayesian inference under models with intractable likelihoods 3.6 Model selection how to compare the several candidate models explaining the data $\\mathcal{D}$? the most commonly used methods: likelihood ratio statistic model posterior probabilities Bayes factors others: cross-validation, Akaikeâ€™s information criterion (AIC), Bayesian information criterion (BIC) ","date":"2020-01-30","objectID":"/2020-01-30-ml-bayes/:1:3","tags":["Naive Bayes","Bayesian Inference"],"title":"Statistical Modeling and Inference","uri":"/2020-01-30-ml-bayes/"},{"categories":["Machine Learning"],"content":"4. Naive Bayes and Bayesian estimation Naive Bayes and Bayesian estimation are two different concepts! Naive Bayes is a statistical learning method. For a give training set, learn the join probability distribution of $P(X,Y)$. Based on this model, for a given input $x$, output a $y$ with maximal posterior probability (Bayes theorem). Set prior prob distribution: $$ P\\left(Y=c_{k}\\right), \\quad k=1,2, \\cdots, K $$ Conditional prob distribution: $$ P\\left(X=x | Y=c_{k}\\right)=P\\left(X^{(1)}=x^{(1)}, \\cdots, X^{(n)}=x^{(n)} | Y=c_{k}\\right), \\quad k=1,2, \\cdots, K $$ Naive Bayes make a strong assumption that conditional prob distribution are all conditional independent, which is: $$ \\begin{aligned} P\\left(X=x | Y=c_{k}\\right) \u0026=P\\left(X^{(1)}=x^{(1)}, \\cdots, X^{(n)}=x^{(n)} | Y=c_{k}\\right) \\cr \u0026=\\prod_{j=1}^{n} P\\left(X^{(j)}=x^{(j)} | Y=c_{k}\\right) \\end{aligned} $$ then, posterior prob is: $$ P\\left(Y=c_{k} | X=x\\right)=\\frac{P\\left(X=x | Y=c_{k}\\right) P\\left(Y=c_{k}\\right)}{\\sum_{k} P\\left(X=x | Y=c_{k}\\right) P\\left(Y=c_{k}\\right)} $$ If conditional probability of each input variable is not independent, then model become Baysian Network! Naive Bayes Classifier is: $$ y=f(x)=\\arg \\max_{c_{k}} \\frac{P\\left(Y=c_{k}\\right) \\prod_{j} P\\left(X^{(j)}=x^{(j)} | Y=c_{k}\\right) }{\\sum_{k} P\\left(Y=c_{k}\\right) \\prod_{j} P\\left(X^{(j)}=x^{(j)} | Y=c_{k}\\right)} $$ and itâ€™s short form: $$ y=f(x)= \\arg \\max_{c_{k}} \\overbrace{P\\left(Y=c_{k}\\right)}^{\\text{prior}} \\overbrace{ \\prod_{j} P\\left(X^{(j)}=x^{(j)} | Y=c_{k}\\right)}^{\\text{likelihood}} $$ In sentiment analysis of NLP, the naive bayes classifiter make two assumptions. bag of words assumption: position doesnâ€™t matter. Each feature only encode word identity not position. naive bayes assumption: conditional independence. ","date":"2020-01-30","objectID":"/2020-01-30-ml-bayes/:1:4","tags":["Naive Bayes","Bayesian Inference"],"title":"Statistical Modeling and Inference","uri":"/2020-01-30-ml-bayes/"},{"categories":["Machine Learning"],"content":"Multi-label classification, tasks commonly be seen on health record data (multi symptoms). Loss function design: Multi binary cross-entropy each class has a binary output Label smoothing, another regularization technique Itâ€™s designed to make the model a little bit less certain of itâ€™s decision by changing a little bit its target: instead of wanting to predict 1 for the correct class and 0 for all the others, we ask it to predict 1-Îµ for the correct class and Îµ for all the others, with Îµ a (small) positive number and N the number of classes. This can be written as: $$ \\text {loss}=(1-\\varepsilon) c e(i)+\\varepsilon \\sum c e(j) / N $$ where ce(x) is cross-entropy of x (i.e. âˆ’log(px)), and i is the correct class. finally, for multi-label loss function: $$ (1-\\epsilon) \\sum_{i}\\left(-\\frac{\\log p_{i}}{n}\\right)+\\frac{\\epsilon}{N} \\sum\\left(-\\log p_{i}\\right) $$ See the fastai implementation here: LabelSmoothingCrossEntropy about line 285: class LabelSmoothingCrossEntropy(Module): y_int = True def __init__(self, eps:float=0.1, reduction='mean'): self.eps,self.reduction = eps,reduction def forward(self, output, target): c = output.size()[-1] log_preds = F.log_softmax(output, dim=-1) if self.reduction=='sum': loss = -log_preds.sum() else: loss = -log_preds.sum(dim=-1) if self.reduction=='mean': loss = loss.mean() return loss*self.eps/c + (1-self.eps) * F.nll_loss(log_preds, target.long(), reduction=self.reduction) def activation(self, out): return F.softmax(out, dim=-1) def decodes(self, out): return out.argmax(dim=-1) ","date":"2020-01-29","objectID":"/2020-01-29-ml-lossfunc/:0:0","tags":["Pytorch"],"title":"Loss function for multi-label classification","uri":"/2020-01-29-ml-lossfunc/"},{"categories":["Machine Learning"],"content":"Maximum Likelihood Estimation Gaussian Mixture Model Expectation Maximization ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:0:0","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"1. Probability and likelihood likehood \u0026 maximum likehood åœ¨éæ­£å¼åœºåˆä¼¼ç„¶ï¼ˆlikelihoodï¼‰å’Œæ¦‚ç‡ï¼ˆProbabilityï¼‰å‡ ä¹æ˜¯ä¸€å¯¹åŒä¹‰è¯ï¼Œä½†æ˜¯åœ¨ç»Ÿè®¡å­¦ä¸­ä¼¼ç„¶å’Œæ¦‚ç‡å´æ˜¯ä¸¤ä¸ªä¸åŒçš„æ¦‚å¿µã€‚ æ¦‚ç‡: åœ¨ç‰¹å®šç¯å¢ƒä¸‹æŸä»¶äº‹æƒ…å‘ç”Ÿçš„å¯èƒ½æ€§ï¼Œä¹Ÿå°±æ˜¯ç»“æœæ²¡æœ‰äº§ç”Ÿä¹‹å‰ä¾æ®ç¯å¢ƒæ‰€å¯¹åº”çš„å‚æ•°æ¥é¢„æµ‹æŸä»¶äº‹æƒ…å‘ç”Ÿçš„å¯èƒ½æ€§ã€‚ æ¯”å¦‚æŠ›ç¡¬å¸ï¼ŒæŠ›ä¹‹å‰æˆ‘ä»¬ä¸çŸ¥é“æœ€åæ˜¯å“ªä¸€é¢æœä¸Šï¼Œä½†æ˜¯æ ¹æ®ç¡¬å¸çš„æ€§è´¨æˆ‘ä»¬å¯ä»¥æ¨æµ‹ä»»ä½•ä¸€é¢æœä¸Šçš„å¯èƒ½æ€§å‡ä¸º50%ï¼Œè¿™ä¸ªæ¦‚ç‡åªæœ‰åœ¨æŠ›ç¡¬å¸ä¹‹å‰æ‰æ˜¯æœ‰æ„ä¹‰çš„ï¼ŒæŠ›å®Œç¡¬å¸åçš„ç»“æœä¾¿æ˜¯ç¡®å®šçš„ï¼› ä¼¼ç„¶: åˆšå¥½ç›¸åï¼Œæ˜¯åœ¨ç¡®å®šçš„ç»“æœä¸‹å»æ¨æµ‹äº§ç”Ÿè¿™ä¸ªç»“æœçš„å¯èƒ½ç¯å¢ƒï¼ˆå‚æ•°ï¼‰ã€‚ å‡è®¾éšæœºæŠ›æ·ä¸€æšç¡¬å¸1,000æ¬¡ï¼Œç»“æœ500æ¬¡äººå¤´æœä¸Šï¼Œ500æ¬¡æ•°å­—æœä¸Šï¼Œé‚£ä¹ˆä¸¤é¢æœä¸Šçš„æ¦‚ç‡å‡ä¸º50%ã€‚è¿ç”¨å‡ºç°çš„ç»“æœæ¥åˆ¤æ–­è¿™ä¸ªäº‹æƒ…æœ¬èº«çš„æ€§è´¨ï¼ˆå‚æ•°ï¼‰ï¼Œä¹Ÿå°±æ˜¯ä¼¼ç„¶ã€‚ å½“ç»“æœå’Œå‚æ•°ç›¸äº’å¯¹åº”ï¼Œä¼¼ç„¶å’Œæ¦‚ç‡åœ¨æ•°å€¼ä¸Šç›¸ç­‰ã€‚ ç”¨ Î¸ è¡¨ç¤ºç¯å¢ƒå¯¹åº”çš„å‚æ•°ï¼Œx è¡¨ç¤ºç»“æœï¼Œé‚£ä¹ˆæ¦‚ç‡å¯ä»¥è¡¨ç¤ºä¸ºï¼š $$P(x | \\theta )$$ $p(x \\vert Î¸)$ æ˜¯æ¡ä»¶æ¦‚ç‡çš„è¡¨ç¤ºæ–¹æ³•ã€‚Î¸ æ˜¯å‰ç½®æ¡ä»¶ï¼Œç†è§£ä¸ºåœ¨ Î¸ çš„å‰æä¸‹ï¼Œäº‹ä»¶ x å‘ç”Ÿçš„æ¦‚ç‡ï¼Œç›¸å¯¹åº”çš„ä¼¼ç„¶å¯ä»¥è¡¨ç¤ºä¸º: $$\\mathcal{L}(\\theta | x)$$ å¯ä»¥ç†è§£ä¸ºå·²çŸ¥ç»“æœä¸º x ï¼Œå‚æ•°ä¸º Î¸ (ä¼¼ç„¶å‡½æ•°é‡Œ Î¸ æ˜¯å˜é‡ï¼Œè¿™é‡Œè¯´çš„å‚æ•°å’Œå˜é‡æ˜¯ç›¸å¯¹ä¸æ¦‚ç‡è€Œè¨€çš„)å¯¹åº”çš„æ¦‚ç‡ï¼Œå³ï¼š $$\\mathcal{L}(\\theta | x)=P(x | \\theta)$$ ä¸¤è€…åœ¨æ•°å€¼ä¸Šç›¸ç­‰ï¼Œä½†æ˜¯æ„ä¹‰å¹¶ä¸ç›¸åŒ, $\\mathcal{L}$ æ˜¯å…³äº Î¸ çš„å‡½æ•°ï¼Œè€Œ P åˆ™æ˜¯å…³äº x çš„å‡½æ•°ã€‚ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:1:0","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"2. Maximum Likelihood Estimation å•é«˜æ–¯æ¨¡å‹ $x \\sim \\mathcal{N}(\\mu, \\Sigma)$, $x_{i} \\in \\mathcal{D}$, é‚£ä¹ˆå¯¹å‚æ•° $\\mu$å’Œ $\\Sigma$ è¿›è¡Œä¼°è®¡ï¼Œåªéœ€è¦æœ€å¤§åŒ–log-likelihoodå‡½æ•°ï¼š $$ \\begin{aligned} \\log p(X) \u0026=\\sum_{i=1}^{N} \\log \\mathcal{N}\\left(x_{i} | \\mu, \\Sigma\\right) \\cr \u0026=\\sum_{i=1}^{N} \\log \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{\\left(x_{i}-\\mu\\right)^{2}}{2 \\sigma^{2}}} \\cr \u0026=\\sum_{i=1}^{N} \\log \\frac{1}{\\sqrt{2 \\pi} \\sigma}+\\sum_{i=1}^{N}-\\frac{\\left(x_{i}-\\mu\\right)^{2}}{2 \\sigma^{2}} \\cr \u0026=-\\frac{N}{2} \\log 2 \\pi-\\frac{N}{2} \\log \\sigma^{2}-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{N}\\left(x_{i}-\\mu\\right)^{2} \\end{aligned} $$ æ±‚åå¯¼æ•°ï¼Œå¾—åˆ°å‚æ•°ä¼°è®¡ï¼š $$ \\begin{aligned} \\frac{\\partial \\log p(X)}{\\partial \\mu} \u0026=\\frac{1}{\\sigma^{2}} \\sum_{i=1}^{N}\\left(x_{i}-\\mu\\right)=0 \\cr \u0026 \\Rightarrow \\mu=\\frac{1}{N} \\sum_{i=1}^{N} x_{i} \\cr \\frac{\\partial \\log p(X)}{\\partial \\sigma^{2}} \u0026=-\\frac{N}{2 \\sigma^{2}}+\\frac{1}{2 \\sigma^{4}} \\sum_{i=1}^{N}\\left(x_{i}-\\mu\\right)^{2}=0 \\cr \u0026 \\Rightarrow \\sigma^{2}=\\frac{1}{N} \\sum_{i=1}^{N}\\left(x_{i}-\\mu\\right)^{2} \\end{aligned} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:2:0","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"3. Gaussian Mixture Model å¦‚æœæœ‰Kä¸ªé«˜æ–¯çº¿æ€§å åŠ : $$ \\begin{aligned} p(x)=\u0026 \\sum_{k=1}^{K} \\pi_{k} \\mathcal{N}\\left(x | \\mu_{k}, \\Sigma_{k}\\right) \\cr \u0026 \\text { s.t. } \\sum_{k=1}^{K} \\pi_{k}=1 \\cr \u0026 0 \\leq \\pi_{k} \\leq 1 \\end{aligned} $$ é‚£ä¹ˆå¯¹æ•°ä¼¼ç„¶å‡½æ•°ä¸º $$ \\log p(X)=\\sum_{i=1}^{N} \\log \\lbrace \\sum_{k=1}^{K} \\pi_{k} \\mathcal{N} (x_{i} | \\mu_{k}, \\Sigma_{k}) \\rbrace $$ å› ä¸ºå¯¹æ•°é‡Œæœ‰æ±‚å’Œï¼Œå› æ­¤æ— æ³•æ— æ³•ç›´æ¥é€šè¿‡æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ–¹æ³•è¿›è¡Œå‚æ•°ä¼°è®¡ã€‚ å…¶ä¸­ï¼Œå¦‚æœ$\\pi_{k}$æ˜¯æ¯ä¸ªé«˜æ–¯å‡ºç°çš„æ¦‚ç‡$p(k)$ï¼Œåˆ™é«˜æ–¯æ··åˆæ¨¡å‹åˆ†è§£ä¸ºä»¥$p(k)$è·å¾—ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œç„¶ååœ¨åˆ†å¸ƒä¸­è·å¾—$x$ï¼Œå› æ­¤$x$è¾¹ç¼˜æ¦‚ç‡åˆ†å¸ƒä¸ºï¼š $$ p(x)=\\sum_{k=1}^{K} p(k) p(x | k) $$ åéªŒæ¦‚ç‡$p(k\\vert x)$è¡¨ç¤º$x$å±äºæ¯ä¸ªé«˜æ–¯çš„æ¦‚ç‡ï¼ˆç¦»æ•£å€¼ï¼‰: $$ \\begin{aligned} p(k | x) \u0026=\\frac{p(x | k) p(k)}{\\sum_{l} p(x | l) p(l)} \\cr \u0026=\\frac{\\pi_{k} \\mathcal{N}\\left(x | \\mu_{k}, \\Sigma_{k}\\right)}{\\sum_{l} \\pi_{l} \\mathcal{N}\\left(x | \\mu_{l}, \\Sigma_{l}\\right)} \\end{aligned} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:3:0","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"4. Expectation Maximization æ€æƒ³ï¼š é€šè¿‡å¼•å…¥éšå˜é‡ï¼Œè¿ç”¨è¿­ä»£æ–¹æ³•ï¼Œæ±‚è§£æ··åˆé«˜æ–¯æ¨¡å‹ $$ \\theta^{(t+1)}=\\underset{\\theta}{\\arg \\max } \\mathcal{L}(\\theta ; X) $$ å¼•å…¥éšå˜é‡Zi(çŠ¶æ€iï¼‰ï¼Œ zæœä»å¤šé¡¹åˆ†å¸ƒï¼Œé€‰æ‹©ziçš„æ¦‚ç‡ä¸ºp(zi),åˆ™é«˜æ–¯æ··åˆæ¨¡å‹ä¸ºï¼š $$ \\begin{aligned} z_{i} \u0026 \\sim \\operatorname{Multinoimal}\\left(\\pi_{1}, \\cdots, \\pi_{k}\\right) \\cr x_{i} | z_{i} \u0026 \\sim \\mathcal{N}\\left(\\mu_{z_{i}}, \\Sigma_{z_{i}}\\right) \\end{aligned} $$ æ­¥éª¤ï¼š E-Step: åœ¨ç°æœ‰$\\theta^{(t)}$ä¸‹æœ€å¤§åŒ–ä¼¼ç„¶ä¸‹ç•Œ, è®¡ç®—éšå˜é‡$z$çš„æœŸæœ›$Q\\left(z_{i}\\right)=p\\left(z_{i} \\vert x_{i}, \\theta\\right)$ ä½œä¸ºå…¶ä¸‹ç•Œ M-Step: åœ¨ä¸Šé¢$Q(z_{i})$ä¸‹è®¡ç®—å‚æ•°åˆ—è¡¨$\\theta$æ¥æœ€å¤§åŒ–ä¼¼ç„¶ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:4:0","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"(0) ç†è§£EMçš„å‰æ å‡¹å‡¸å‡½æ•°: $\\forall_{x \\in \\mathbb{R}}, f^{\\prime \\prime}(x) \\geq 0$,åˆ™$f$ä¸ºå‡¸å‡½æ•°ã€‚ å½“$x$ä¸ºå‘é‡ï¼Œå¦‚æœå…¶hessiançŸ©é˜µ $H$ æ˜¯åŠæ­£å®šçš„($H \\geq 0$),åˆ™$f$ä¸ºå‡¸å‡½æ•° å¦‚æœ$f^{\\prime \\prime}(x)\u003e0$æˆ–è€…$H\u003e0$, $f$æ˜¯ä¸¥æ ¼å‡¸å‡½æ•°ã€‚ å¦‚æœ$f^{\\prime \\prime}(x)\u003c0$æˆ–è€…$H\u003e0$, $f$æ˜¯å‡¹å‡½æ•°ã€‚ Jensen ä¸ç­‰å¼: å¦‚æœ$f$ä¸ºå‡¸å‡½æ•°, åˆ™$E[f(X)] \\geq f(E [ X ])$ã€‚å½“ä¸”ä»…å½“$x$æ˜¯å¸¸æ•°æ—¶ï¼Œ$E[f(x)]=f(E[ x ])$ã€‚ å¦‚æœ$f$æ˜¯å‡¹å‡½æ•°, åˆ™$E[f(X)] \\leq f(E[ X ])$ã€‚ å¼•å…¥éšå˜é‡åï¼Œå˜æ¢å¯¹æ•°ä¼¼ç„¶å‡½æ•°: $$ \\begin{aligned} \\mathcal{L}(\\theta ; X) \u0026=\\sum_{i=1}^{N} \\log p\\left(x_{i} | \\theta\\right) \\cr \u0026=\\sum_{i=1}^{N} \\log \\sum_{z_{i}} p\\left(x_{i}, z_{i} | \\theta\\right) \\cr \u0026=\\sum_{i=1}^{N} \\log \\sum_{z_{i}} Q\\left(z_{i}\\right) \\frac{p\\left(x_{i}, z_{i} | \\theta\\right)}{Q\\left(z_{i}\\right)} \\cr \u0026 \\geq \\sum_{i=1}^{N} \\sum_{z_{i}} Q\\left(z_{i}\\right) \\log \\frac{p\\left(x_{i}, z_{i} | \\theta\\right)}{Q\\left(z_{i}\\right)} \\end{aligned} $$ æ¨å¯¼: æŠŠå¼ä¸­çš„logå‡½æ•°ä½“çœ‹æˆæ˜¯ä¸€ä¸ªæ•´ä½“ï¼Œç”±äº$\\log (x)$çš„äºŒé˜¶å¯¼æ•°ä¸º$-\\frac{1}{x^2}$, å°äº0ï¼Œä¸ºå‡¹å‡½æ•°ã€‚æ‰€ä»¥ä½¿ç”¨Jensenä¸ç­‰å¼æ—¶ï¼Œåº”ç”¨ç¬¬äºŒæ¡å‡†åˆ™ï¼š$f(E [ X ] ) \\geq E[f(x)]$ã€‚ $$ f\\left(E_{z_{i} \\sim Q}\\left[\\frac{p\\left(x_{i}, z_{i} | \\theta\\right)}{Q\\left(z_{i}\\right)}\\right]\\right) \\geq E_{z_{i} \\sim Q}\\left[f\\left(\\frac{p\\left(x_{i}, z_{i} | \\theta\\right)}{Q\\left(z_{i}\\right)}\\right)\\right] $$ è¿™é‡Œï¼Œ$Q\\left(z_{i}\\right)$æ˜¯$z_{i}$çš„å‡½æ•°ï¼Œ ä¸”$\\sum_{z_{i}} Q\\left(z_{i}\\right)=1$ã€‚ ç”±æ•°å­¦æœŸæœ›$E_{x \\sim p}[g(X)]=\\sum_{x} g(x) p(x)$ï¼Œä¸Šå¼å¯ä»¥ç†è§£ä¸º: $p(x)$å¯¹åº”$Q\\left(z_{i}\\right)$, g(x)å¯¹åº”$\\log \\frac{p\\left(x_{i}, z_{i} \\vert \\theta\\right)}{Q\\left(z_{i}\\right)}$è¡¨ç¤º$z_{i}$çš„å‡½æ•°ã€‚ ä¼¼ç„¶å‡½æ•°: $\\mathcal{L}(\\theta) \\geq \\mathcal{J}(z,Q)$ï¼ˆ$z$ä¸ºéšå«å˜é‡ï¼‰ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸æ–­çš„æœ€å¤§åŒ–$\\mathcal{J}$çš„ä¸‹ç•Œï¼Œæ¥ä½¿å¾—$\\mathcal{L}(\\theta)$ä¸æ–­æé«˜ï¼Œæœ€ç»ˆè¾¾åˆ°å®ƒçš„æœ€å¤§å€¼ã€‚ æœ€å¤§åŒ–$\\mathcal{L}(\\theta)$å‡½æ•°çš„ä¸‹ç•Œï¼Œå³è®©$g(x)$ä¸ºå¸¸æ•°c: $$ \\frac{p\\left(x_{i}, z_{i} | \\theta\\right)}{Q\\left(z_{i}\\right)}=c $$ Jensenä¸ç­‰å¼ä¸­è¯´åˆ°ï¼Œå½“è‡ªå˜é‡$X=E(X)$æ—¶ï¼Œå³ä¸ºå¸¸æ•°çš„æ—¶å€™ï¼Œç­‰å¼æˆç«‹! å˜æ¢å…¬å¼, å¯¹æ‰€æœ‰$z$æ±‚å’Œå¾—: $$ \\begin{aligned} p\\left(x_{i}, z_{i} | \\theta\\right) \u0026=c \\cdot Q\\left(z_{i}\\right) \\cr \\sum_{z_{i}} p\\left(x_{i}, z_{i} | \\theta\\right) \u0026=c \\cdot \\sum_{z_{i}} Q\\left(z_{i}\\right) \\cr c \u0026=\\sum_{z_{i}} p\\left(x_{i}, z_{i} | \\theta\\right) \\end{aligned} $$ å…¶ä¸­ï¼Œ$\\sum_{z_{i}} Q\\left(z_{i}\\right) = 1$, ä¹Ÿå¾—ï¼š $$ \\begin{aligned} Q\\left(z_{i}\\right) \u0026=\\frac{p\\left(x_{i}, z_{i} | \\theta\\right)}{\\sum_{z_{i}} p\\left(x_{i}, z_{i} | \\theta\\right)} \\cr \u0026=p\\left(z_{i} | x_{i}, \\theta\\right) \\end{aligned} $$ è‡³æ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†åœ¨å›ºå®šå‚æ•°Î¸åï¼Œä½¿ä¸‹ç•Œæ‹‰å‡çš„$Q(z)$çš„è®¡ç®—å…¬å¼å°±æ˜¯åéªŒæ¦‚ç‡ï¼ˆæ¡ä»¶æ¦‚ç‡ï¼‰ï¼Œä¸€å¹¶è§£å†³äº†$Q(z)$å¦‚ä½•é€‰æ‹©çš„é—®é¢˜ã€‚æ­¤æ­¥å°±æ˜¯EMç®—æ³•çš„E-stepã€‚ æ‰§è¡ŒE-Stepåä¸ä¸‹ç•Œé‡åˆï¼Œæ­¤æ—¶ä¼¼ç„¶å˜ä¸ºï¼š $$ \\mathcal{L}\\left(\\theta^{(t)} ; X\\right)=\\sum_{i=1}^{N} \\sum_{z_{i}} Q^{(t)}\\left(z_{i}\\right) \\log \\frac{p\\left(x_{i}, z_{i} | \\theta^{(t)}\\right)}{Q^{(t)}\\left(z_{i}\\right)} $$ è¿™æ—¶ï¼Œå¯¹å…¬å¼æ±‚å¯¼ $$ \\theta^{(t+1)}=\\underset{\\theta}{\\arg \\max } \\mathcal{L}(\\theta ; X) $$ å¾—åˆ° $t+1$ æ­¥çš„ä¼¼ç„¶å‡½æ•° $\\mathcal{L}\\left(\\theta^{(t+1)} ; X\\right)$ã€‚ é€šè¿‡ä¸æ–­çš„è¿­ä»£ï¼Œå¯ä»¥å¾—åˆ°ä½¿ä¼¼ç„¶å‡½æ•°$\\mathcal{L}(\\theta)$æœ€å¤§åŒ–çš„å‚æ•° $\\theta$ï¼Œç›´è‡³å‡½æ•°æ”¶æ•›ã€‚ åªéœ€è¦è¯æ˜$\\mathcal{L}\\left(\\theta^{(t+1)} ; X\\right) \\geq \\mathcal{L}\\left(\\theta^{(t)} ; X\\right)$, åˆ™å¯è¯æ˜EMçš„æ”¶æ•›æ€§: $$ \\begin{aligned} \\mathcal{L}\\left(\\theta^{(t+1)} ; X\\right) \u0026=\\sum_{i=1}^{N} \\log \\sum_{z_{i}} Q^{(t)}\\left(z_{i}\\right) \\frac{p\\left(x_{i}, z_{i} | \\theta^{(t+1)}\\right)}{Q^{(t)}\\left(z_{i}\\right)} \\cr \u0026 \\geq \\sum_{i=1}^{N} \\sum_{z_{i}} Q^{(t)}\\left(z_{i}\\right) \\log \\frac{p\\left(x_{i}, z_{i} | \\theta^{(t+1)}\\right)}{Q^{(t)}\\left(z_{i}\\right)} \\cr \u0026 \\geq \\sum_{i=1}^{N} \\sum_{z_{i}} Q^{(t)}\\left(z_{i}\\right) \\log \\frac{p\\left(x_{i}, z_{i} | \\theta^{(t)}\\right)}{Q^{(t)}\\left(z_{i}\\right)} \\cr \u0026=\\mathcal{L}\\left(\\theta^{(t)} ; X\\right) \\end{aligned} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:4:1","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"5. æ±‚è§£GMM ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:5:0","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"(1) GMM E-Step: å·²çŸ¥$\\theta^{(t)}$, æ±‚$Q^{(t+1)}\\left(z_{i}\\right)$: $$ \\begin{aligned} Q^{(t+1)}\\left(z_{i}\\right) \u0026=\\frac{p\\left(x_{i}, z_{i} | \\theta^{(t)}\\right)}{p\\left(x_{i} | \\theta^{(t)}\\right)} \\cr \u0026=\\frac{p\\left(x_{i}, z_{i} | \\theta^{(t)}\\right)}{\\sum_{l \\in z_{i}} p\\left(x_{i}, l | \\theta^{(t)}\\right)} \\cr \u0026=\\frac{p\\left(x_{i} | z_{i}, \\theta^{(t)}\\right) p\\left(z_{i} | \\theta^{(t)}\\right)}{\\sum_{l \\in z_{i}} p\\left(x_{i} | l, \\theta^{(t)}\\right) p\\left(l | \\theta^{(t)}\\right)} \\cr \u0026=\\frac{\\mathcal{N}\\left(\\mu_{z_{i}}, \\Sigma_{z_{i}}\\right) \\pi_{z_{i}}}{\\sum_{l \\in z_{i}} \\mathcal{N}\\left(\\mu_{l}, \\Sigma_{l}\\right) \\pi_{l}} \\end{aligned} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:5:1","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"(2) GMM M-Step: å·²çŸ¥$Q^{(t+1)}\\left(z_{i}\\right)$, æ±‚ $\\theta^{(t+1)}$: $$ \\begin{aligned} \\mathcal{L}(\\theta ; X) \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log \\frac{p\\left(x_{i}, l | \\theta\\right)}{Q_{i}(l)} \\cr \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log p\\left(x_{i}, l | \\theta\\right)-\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log Q_{i}(l) \\cr \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log p\\left(x_{i}, l | \\theta\\right)-\\text {Constant } \\cr \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log \\pi_{l} \\mathcal{N}\\left(\\mu_{l}, \\Sigma_{l}\\right)-\\text {Constant } \\cr \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log \\pi_{l}+\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log \\mathcal{N}\\left(\\mu_{l}, \\Sigma_{l}\\right)-\\text {Constant} \\end{aligned} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:5:2","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"(3) æ±‚ $\\pi$: ä»¤ $\\forall_{l \\in{1, \\cdots, K}}$ $$ \\begin{aligned} \\frac{\\partial \\mathcal{L}(\\theta ; X)}{\\partial \\pi_{l}} \u0026=0 \\cr \\text { s.t. } \\sum_{l}^{K} \\pi_{l} \u0026= 1 \\end{aligned} $$ æ‹‰æ ¼æœ—æ—¥ä¹˜æ³•çº¦æŸ $$ \\begin{cases}\\begin{aligned} L_{\\pi_{l}} \u0026=\\frac{\\partial \\mathcal{L}(\\theta ; X)}{\\partial \\pi_{l}}+\\lambda(\\sum_{l}^{K} \\pi_{l}-1)=0 \\cr L_{\\lambda} \u0026=\\sum_{l}^{K} \\pi_{l}-1=0 \\end{aligned}\\end{cases} $$ æ±‚å¯¼ï¼š $$ \\begin{cases}\\begin{array}{c} \\frac{1}{\\pi_{1}} \\sum_{i}^{N} Q_{i}(1)-\\lambda=0 \\cr \\vdots \\cr \\frac{1}{\\pi_{l}} \\sum_{i}^{N} Q_{i}(l)-\\lambda=0 \\end{array}\\end{cases} $$ ç›¸åŠ å¾—ï¼š $$ \\sum_{l}^{K} \\sum_{i}^{N} Q_{i}(l)=\\lambda \\sum_{l}^{K} \\pi_{l}=\\lambda $$ ç”± $Q_{i}(l)=p\\left(l \\vert x_{i}, \\theta\\right)$, å¾— $$ \\begin{aligned} \\sum_{l}^{K} \\sum_{i}^{N} Q_{i}(l) \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\cr \u0026=\\sum_{i}^{N} \\sum_{l}^{K} p\\left(l | x_{i}, \\theta\\right) \\cr \u0026=\\sum_{i}^{N} 1 \\cr \u0026=N \\end{aligned} $$ åˆ™ $$ \\begin{aligned} \\pi_{l} \u0026=\\frac{1}{\\lambda} \\sum_{i}^{N} Q_{i}(l) \\cr \u0026=\\frac{1}{N} \\sum_{i}^{N} Q_{i}(l) \\cr \u0026=\\frac{1}{N} \\sum_{i}^{N} p\\left(l | x_{i}, \\theta\\right) \\end{aligned} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:5:3","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"(4) è®¡ç®—$\\mu$ $$ \\begin{aligned} \u0026\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log \\mathcal{N}\\left(\\mu_{l}, \\Sigma_{l}\\right)\\cr \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log \\frac{1}{\\sqrt{2 \\pi} \\sigma_{l}} e^{-\\frac{\\left(x_{i}-\\mu_{l}\\right)^{2}}{2 \\sigma_{l}^{2}}}\\cr \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\lbrace -\\frac{1}{2} \\log 2 \\pi-\\frac{1}{2} \\log \\sigma_{l}^{2}-\\frac{\\left(x_{i}-\\mu_{l}\\right)^{2}}{2 \\sigma_{l}^{2}}\\rbrace \\end{aligned} $$ æ±‚åå¯¼ï¼š $$ \\begin{aligned} \\frac{\\partial \\mathcal{L}(\\theta ; X)}{\\partial \\mu_{l}} \u0026=\\sum_{i}^{N} Q_{i}(l) \\frac{x_{i}-\\mu_{l}}{\\sigma^{2}} \\cr \u0026=0 \\end{aligned} $$ å¾—$\\mu$ï¼š $$ \\mu_{l}=\\frac{\\sum_{i}^{N} Q_{i}(l) x_{i}}{\\sum_{i}^{N} Q_{i}(l)} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:5:4","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"(5) è®¡ç®—$\\sigma$ $$ \\begin{aligned} \\frac{\\partial \\mathcal{L}(\\theta ; X)}{\\partial \\sigma_{l}^{2}} \u0026=\\sum_{i}^{N} Q_{i}(l) \\bigg\\lbrace -\\frac{1}{2 \\sigma_{l}^{2}}+\\frac{\\left(x_{i}-\\mu_{l}\\right)^{2}}{2 \\sigma_{l}^{4}} \\bigg\\rbrace \\cr \u0026=0 \\end{aligned} $$ å¾—åˆ° $$ \\sigma_{l}=\\frac{\\sum_{i}^{N} Q_{i}(l)\\left(x_{i}-\\mu_{l}\\right)^{2}}{\\sum_{i}^{N} Q_{i}(l)} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:5:5","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"6 ä»KLæ•£åº¦è§’åº¦è§£é‡ŠEM $$ \\begin{aligned} K L(q | p) \u0026=\\sum_{z} q(z) \\log \\frac{q(z)}{p(z | x, \\theta)} \\cr \u0026=\\sum_{z} q(z) \\log \\frac{q(z) p(x | \\theta)}{p(z, x | \\theta)} \\cr \u0026=-\\sum_{z} q(z) \\log \\frac{p(z, x | \\theta)}{q(z)}+\\sum_{z} q(z) \\log p(x | \\theta) \\cr \u0026=-\\sum_{z} q(z) \\log \\frac{p(z, x | \\theta)}{q(z)}+\\log p(x | \\theta) \\sum_{z} q(z) \\cr \u0026=-\\sum_{z} q(z) \\log \\frac{p(z, x | \\theta)}{q(z)}+\\log p(x | \\theta) \\cr \\log p(x | \\theta) \u0026=K L(q | p)+\\sum_{z} q(z) \\log \\frac{p(z, x | \\theta)}{q(z)} \\cr \u0026=K L(q | p)+\\mathcal{L}(q, \\theta) \\end{aligned} $$ å‚è€ƒï¼š å¾äº¦è¾¾-æœºå™¨å­¦ä¹ -EM ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:6:0","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Make bioinfo uncool again"],"content":"Usefull tools","date":"2020-01-20","objectID":"/2020-01-20-begin/","tags":["Bioinformatics","Linux"],"title":"Cheatsheet for command line","uri":"/2020-01-20-begin/"},{"categories":["Make bioinfo uncool again"],"content":"usefull tools for linux command line ","date":"2020-01-20","objectID":"/2020-01-20-begin/:0:0","tags":["Bioinformatics","Linux"],"title":"Cheatsheet for command line","uri":"/2020-01-20-begin/"},{"categories":["Make bioinfo uncool again"],"content":"a. Make terminal cool, install OhMyZsh # install zsh sudo apt-get install zsh # ubuntu # change default shell to zsh chsh -s /usr/bin/zsh # install ohmyzsh sh -c \"$(wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)\" source ~/.zshrc ","date":"2020-01-20","objectID":"/2020-01-20-begin/:0:1","tags":["Bioinformatics","Linux"],"title":"Cheatsheet for command line","uri":"/2020-01-20-begin/"},{"categories":["Make bioinfo uncool again"],"content":"b. Terminal keyboard short cuts Jump to head: Ctrl + a Jump to end: Ctrl + e Delete strings ahead: Ctrl + u Delete strings follow: Ctrl + k ","date":"2020-01-20","objectID":"/2020-01-20-begin/:0:2","tags":["Bioinformatics","Linux"],"title":"Cheatsheet for command line","uri":"/2020-01-20-begin/"},{"categories":["Make bioinfo uncool again"],"content":"c. Program keeps running in the background 1. Run cmd using nohup nohup command [options] \u0026 2. Run cmd using Tmux Outside Tmux: Typically these are run outside, but you can also run them inside an existing session a. Start New Session tmux new -s myname b. Attach To Existing Session tmux attach -t myname #by name tmux attach 4 #by number (in this case 4) c. List Sessions tmux ls d. Kill Session tmux kill-session -t myname Inside Tmux Session: Start each command with CTRL + b, release, then press one of the following: Panes % vertical split \" horizontal split d detach from session (it keeps running in the background) x kill pane Up/Down/Left/Right move between panes PageUP/PageDown CTRL+c to exit the PageUp/Down mode Fn+Up/Down PageUp/Down: Mac keyboard : + resize-pane -D Resizes the current pane down : + resize-pane -U Resizes the current pane upward : + resize-pane -L Resizes the current pane left : + resize-pane -R Resizes the current pane right : + resize-pane -D 20 Resizes the current pane down by 20 cells ","date":"2020-01-20","objectID":"/2020-01-20-begin/:0:3","tags":["Bioinformatics","Linux"],"title":"Cheatsheet for command line","uri":"/2020-01-20-begin/"},{"categories":["Make bioinfo uncool again"],"content":"d. File compression and decompression Decompression File type Cmd e.g. *.tar tar -xvf *.tar.gz or *.tgz tar -xvzf *bz2 bzip2 -d or bunzip2 *.tar.bz2 tar -xjf *.Z uncompress *.tar.Z tar -xZf *.rar unrar e or rar x unrar e file.rar *.zip unzip *.gz gunzip Compression File type Cmd e.g. *.tar tar -cvf *.tar.gz or *.tgz tar -cvzf *bz2 bzip2 -z *.tar.bz2 tar -cjf *.Z compress *.tar.Z tar -cZf *.rar rar a rar a -ep1 newname /home/user/cpp *.zip zip *.gz gzip For rar installation sudo apt-get install rar ","date":"2020-01-20","objectID":"/2020-01-20-begin/:0:4","tags":["Bioinformatics","Linux"],"title":"Cheatsheet for command line","uri":"/2020-01-20-begin/"},{"categories":["Make bioinfo uncool again"],"content":"e. Handy tricks for handling filepath very useful to strip file sufix, path et.al. # e.g. var=./home/fastq/filename_R1.fq.gz # extract filename ${var#*/} # -\u003e home/fastq/filename_R1.fq.gz var1=${var##*/} # -\u003e filename_R1.fq.gz # remove file suffix ${var1%.*} # -\u003e filename_R1.fq ${var1%%.*} # -\u003e filename_R1 # get basebame var2=$(basename \"${var}\" .fq.gz) #-\u003e filename_R1 ","date":"2020-01-20","objectID":"/2020-01-20-begin/:0:5","tags":["Bioinformatics","Linux"],"title":"Cheatsheet for command line","uri":"/2020-01-20-begin/"},{"categories":["Make bioinfo uncool again"],"content":"hisat2-htseq-deseq2","date":"2020-01-20","objectID":"/2020-01-20-rnaseq/","tags":["RNA-seq"],"title":"Shortcut to bulk RNA-seq analysis","uri":"/2020-01-20-rnaseq/"},{"categories":["Make bioinfo uncool again"],"content":"hisat2-htseq-deseq2 ","date":"2020-01-20","objectID":"/2020-01-20-rnaseq/:0:0","tags":["RNA-seq"],"title":"Shortcut to bulk RNA-seq analysis","uri":"/2020-01-20-rnaseq/"},{"categories":["Make bioinfo uncool again"],"content":"3.1 transcriptom mapping step 0: install tools conda install htseq hisat2 stringtie step 1: build index and extract splice sites build index hisat2-build -p {threads} genome/hg38.fa hisat2_index/hg38 extract known splice sites for alignmnet hisat2_extract_splice_sites.py gencode.gtf \u003e hisat2_index/splicesites.txt hisat2_extract_exons.py gencode.gtf \u003e histat2_index/exon.txt step2: mapping hisat2 --dta --threads ${threads} \\ -x hisat2_index/hg38 \\ --known-splicesite-infile hisat2_index/splicesites.txt \\ -1 R1.fq.gz \\ -2 R2.fq.gz \\ -S output.sam step 3: sam to bam samtools view -Sbh -q 25 \\ -@ ${threads} \\ -o ouput.bam \\ input.sam step 4: bam sort and index samtools sort -@ ${threads} input.bam \u003e output.sorted.bam samtools index input.sorted.bam #generate input.sorted.bam.bai step 5: bam to bigwig bamCoverage -p ${threads} \\ --normalizeUsing RPKM \\ # note: other normalization options -b input.sorted.bam \\ -o output.bw ","date":"2020-01-20","objectID":"/2020-01-20-rnaseq/:0:1","tags":["RNA-seq"],"title":"Shortcut to bulk RNA-seq analysis","uri":"/2020-01-20-rnaseq/"},{"categories":["Make bioinfo uncool again"],"content":"3.2 Differentially expressed genes analysis step 1: count reads htseq-count -r pos -s no \\ --additional-attr gene_name \\ --additional-attr gene_type \\ -f bam input.sorted.bam gencode.gtf \u003e output.count step2: differentially expressed genes analysis (1) construct read count table option 1: HTSeq count file input library(\"DESeq2\") directory \u003c- \"/path/to/your/readCountFiles/\" sampleFiles \u003c- grep(\"count\", list.files(directory), value=TRUE) condition \u003c- factor(c(\"KO\",\"KO\", \"WT\",\"WT\"), levels = c(\"WT\", \"KO\")) # phenotable sampleTable \u003c- data.frame(sampleName = sampleFiles, fileName = sampleFiles, condition = condition) # construct read count table ddsHTSeq \u003c- DESeqDataSetFromHTSeqCount(sampleTable = sampleTable, directory = directory, design= ~ condition) option 2: combined read count file into a single table first, then run library(DESeq2) # read count table database \u003c- read.table(file = \"raw.counts.csv\", sep = \",\", header = TRUE, row.names = 1) database \u003c- round(as.matrix(database)) # set level condition \u003c- factor(c(\"KO\",\"KO\", \"WT\",\"WT\"), levels = c(\"WT\", \"KO\")) # build DESeq object coldata \u003c- data.frame(row.names = colnames(database), condition) dds \u003c- DESeqDataSetFromMatrix(countData=database, colData=coldata, design=~condition + treatmement) (2) run DESeq2 and get output library(DESeq2) dds \u003c- dds[ rowSums(counts(dds)) \u003e 1, ] # run statistical test dds \u003c- DESeq(dds) # get results res \u003c- results(dds) # summary(res) count_r \u003c- counts(dds, normalized=T) #normalized count matrix # export results res \u003c- res[order(res$padj),] diff_gene \u003c- subset(res, padj \u003c 0.05 \u0026 (log2FoldChange \u003e 1 | log2FoldChange \u003c -1)) diff_gene \u003c- row.names(diff_gene) resdata \u003c- merge(as.data.frame(res), as.data.frame(counts(dds, normalized=TRUE)), by=\"row.names\", sort=FALSE) write.csv(resdata, file = \"DEGs.csv\", row.names = FALSE) 3.3 Gene set enrichrment analysis GO clusterprofiler Enrichr (GSEApy) GSEA ","date":"2020-01-20","objectID":"/2020-01-20-rnaseq/:0:2","tags":["RNA-seq"],"title":"Shortcut to bulk RNA-seq analysis","uri":"/2020-01-20-rnaseq/"},{"categories":["Make bioinfo uncool again"],"content":"3.4 Alternative splicing analysis rMATS ","date":"2020-01-20","objectID":"/2020-01-20-rnaseq/:0:3","tags":["RNA-seq"],"title":"Shortcut to bulk RNA-seq analysis","uri":"/2020-01-20-rnaseq/"},{"categories":["Make bioinfo uncool again"],"content":"bowtie2-macs2-deeptools","date":"2020-01-20","objectID":"/2020-01-20-chip/","tags":["ChIP-seq"],"title":"Shortcut to ChIP-seq analysis","uri":"/2020-01-20-chip/"},{"categories":["Make bioinfo uncool again"],"content":"bowtie2-macs2-deeptools ","date":"2020-01-20","objectID":"/2020-01-20-chip/:0:0","tags":["ChIP-seq"],"title":"Shortcut to ChIP-seq analysis","uri":"/2020-01-20-chip/"},{"categories":["Make bioinfo uncool again"],"content":"2.1 Genome mapping Step 0: install software # install miniconda, then call conda conda install -c bioconda bowtie2 hisat2 samtools deeptools step 1: build index bowtie2-build hg38.fa bowtie2_index/hg38 step 2: mapping Unpaired data bowtie2 -p ${threads} -x index/hg38 \\ -U input.fastq.gz \\ -S ouput.sam Paired data bowtie2 -p 4 -x index/hg38 \\ -1 input_R1.fastq.gz \\ -2 input_R2.fastq.gz \\ -S ouput.sam step 3: sam to bam samtools view -Sbh -q 25 \\ -@ ${threads} \\ -o ouput.bam \\ input.sam step 4: bam sort and index samtools sort -@ ${threads} input.bam \u003e output.sorted.bam samtools index input.sorted.bam #generate input.sorted.bam.bai step 5: bam to bigwig bamCoverage -p ${threads} \\ --normalizeUsing RPKM \\ # note: other normalization options --centerReads \\ -e 200 \\ -b input.sorted.bam \\ -o output.bw ","date":"2020-01-20","objectID":"/2020-01-20-chip/:0:1","tags":["ChIP-seq"],"title":"Shortcut to ChIP-seq analysis","uri":"/2020-01-20-chip/"},{"categories":["Make bioinfo uncool again"],"content":"2.2 Peaks analysis note:: macs2 (\u003ev2.2.x) supports python 3. step 0: install tools conda install macs2 bedtools pygenometracks step 1: callpeaks (1) narrow peaks, e.g. TFs, h3k4m3 # bam file input macs2 callpeak -t ChIP.elute.sorted.bam \\ -c ChIP.input.sorted.bam \\ -f BAM \\ -g hs # organism \\ -B -q 0.05 \\ -n ${outFileName}\\ --outdir macs_out (2) Broad peaks, e.g. h3k27me3 # sam file also works fine macs2 callpeak -t ./bowtie_out/WTme2ChIP.sam \\ -c ./bowtie_out/ESCInput.sam \\ -f SAM \\ -g mm \\ -B --SPMR \\ --nomodel --extsize 147 \\ --broad -n WTme2ChIP --outdir macs_out step 2: advanced analysis tools: bedtools, deeptools, pyGenomeTracks, igv genome algebra overlap with other peaks: bedtools visualization heatmap: deeptools signal tracks: pyGenomeTracks, igv ","date":"2020-01-20","objectID":"/2020-01-20-chip/:0:2","tags":["ChIP-seq"],"title":"Shortcut to ChIP-seq analysis","uri":"/2020-01-20-chip/"},{"categories":["Make bioinfo uncool again"],"content":"salmon-tximport-deseq2","date":"2020-01-20","objectID":"/2020-01-23-rnaseqturbo/","tags":["RNA-seq"],"title":"SuperFast RNA-seq","uri":"/2020-01-23-rnaseqturbo/"},{"categories":["Make bioinfo uncool again"],"content":"salmon-tximport-deseq2 ","date":"2020-01-20","objectID":"/2020-01-23-rnaseqturbo/:0:0","tags":["RNA-seq"],"title":"SuperFast RNA-seq","uri":"/2020-01-23-rnaseqturbo/"},{"categories":["Make bioinfo uncool again"],"content":"Step 0: install salmon and download transcriptome cdna from gencode conda install salmon wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_32/gencode.v32.transcripts.fa.gz ","date":"2020-01-20","objectID":"/2020-01-23-rnaseqturbo/:0:1","tags":["RNA-seq"],"title":"SuperFast RNA-seq","uri":"/2020-01-23-rnaseqturbo/"},{"categories":["Make bioinfo uncool again"],"content":"Step 1. build salmon index salmon index -p 8 --gencode -t gencode.v32.transcripts.fa.gz -i salmonIndex_hg38 ","date":"2020-01-20","objectID":"/2020-01-23-rnaseqturbo/:0:2","tags":["RNA-seq"],"title":"SuperFast RNA-seq","uri":"/2020-01-23-rnaseqturbo/"},{"categories":["Make bioinfo uncool again"],"content":"Step 2: quantification salmon quant -i salmonIndex_hg38 -l A \\ -1 ${fn}/${samp}_1.fastq.gz \\ -2 ${fn}/${samp}_2.fastq.gz \\ -p 8 --validateMappings -o quants/${samp}_quant ","date":"2020-01-20","objectID":"/2020-01-23-rnaseqturbo/:0:3","tags":["RNA-seq"],"title":"SuperFast RNA-seq","uri":"/2020-01-23-rnaseqturbo/"},{"categories":["Make bioinfo uncool again"],"content":"Step 3: merge quantification outputs use tximport in R # R code library(tximport) library(readr) suppressMessages(library('EnsDb.Hsapiens.v86')) txdb \u003c- EnsDb.Hsapiens.v86 k \u003c- keys(txdb, keytype = \"GENEID\") df \u003c- select(txdb, keys = k, keytype = \"GENEID\", columns = c(\"TXID\",\"GENEID\")) tx2gene \u003c- df[, 2:1] # tx ID, then gene ID #tx2gene \u003c- read.table(tx2gene, header= T, sep=\"\\t\", stringsAsFactors = F) samples \u003c- unlist(strsplit(sample_ids,\",\")) salmon.files \u003c- file.path('salmon',samples, \"quant.sf\") names(salmon.files) \u003c- samples all(file.exists(salmon.files)) # get transcript level results txi.transcripts \u003c- tximport(salmon.files, type = \"salmon\", txOut = TRUE, tx2gene = tx2gene,) # ignoreTxVersion = TRUE) # get gene level results txi.salmon \u003c- summarizeToGene(txi.transcripts, tx2gene) #save raw counts salmon.counts\u003c- txi.salmon$counts salmon.counts\u003c- as.data.frame(salmon.counts) write.table(salmon.counts, out_counts, sep=\"\\t\", quote=F) #save gene tpms salmon.TPM\u003c- txi.salmon$abundance salmon.TPM\u003c- as.data.frame(salmon.TPM) write.table(salmon.TPM, out_tpm, sep=\"\\t\", quote=F) #save transcripts tpms salmon.trans.TPM\u003c- txi.transcripts$abundance salmon.trans.TPM\u003c- as.data.frame(salmon.trans.TPM) write.table(salmon.trans.TPM, outTrans_tpm, sep=\"\\t\", quote=F) save(txi.salmon, file=\"txi.salmon.RData\") ","date":"2020-01-20","objectID":"/2020-01-23-rnaseqturbo/:0:4","tags":["RNA-seq"],"title":"SuperFast RNA-seq","uri":"/2020-01-23-rnaseqturbo/"},{"categories":["Make bioinfo uncool again"],"content":"Step 4: Differentially expressed gene analysis DESeq2 pipeline demo load(\"txi.salmon.RData\") dds \u003c- DESeqDataSetFromTximport(txi.salmon, sampleTable, ~condition) dds$condition \u003c- relevel(dds$condition, ref=ctrl) dds \u003c- DESeq(dds, parallel=TRUE) res \u003c- results(dds, contrast=c(\"condition\", treat, ctrl)) resOrdered \u003c- res[order(res$padj),] resOrdered = as.data.frame(resOrdered) write.table(resOrdered, file=\"degs.txt\", quote=F, sep=\"\\t\") ","date":"2020-01-20","objectID":"/2020-01-23-rnaseqturbo/:0:5","tags":["RNA-seq"],"title":"SuperFast RNA-seq","uri":"/2020-01-23-rnaseqturbo/"},{"categories":null,"content":"My PhD thesis","date":"2019-12-10","objectID":"/publication/2019-12-10-sox21/","tags":null,"title":"SOX21 Ensures Rostral Forebrain Identity by Suppression of WNT8B during Neural Regionalization of Human Embryonic Stem Cells","uri":"/publication/2019-12-10-sox21/"},{"categories":null,"content":"My PhD thesis. Published in Stem Cell Reports, 2019 Highlights The transcriptomic analysis of rostrocaudal patterning of hESC-derived NPCs SOX21 KO leads to caudalized regional identity in rostral forebrain progenitors SOX21 represses Wnt signaling to ensure the rostral forebrain identity WNT8B is a major downstream target of SOX21 Download here ","date":"2019-12-10","objectID":"/publication/2019-12-10-sox21/:0:0","tags":null,"title":"SOX21 Ensures Rostral Forebrain Identity by Suppression of WNT8B during Neural Regionalization of Human Embryonic Stem Cells","uri":"/publication/2019-12-10-sox21/"},{"categories":null,"content":"Ding J, Fang Z, Liu X, Zhu Z, Wen C, Wang H, et al. CDK11 safeguards the identity of human embryonic stem cells via fine-tuning signaling pathways. J Cell Physiol. 2019 Download here ","date":"2019-10-15","objectID":"/publication/2019-10-15/:0:0","tags":null,"title":"CDK11 safeguards the identity of human embryonic stem cells via fineâ€tuning signaling pathways","uri":"/publication/2019-10-15/"},{"categories":null,"content":"Hu J, Li S, Sun X, Fang Z, Wang L, Xiao F, et al. Stk40 deletion elevates c-JUN protein level and impairs mesoderm differentiation. J Biol Chem. 2019;294(25):9959-72. Download here ","date":"2019-06-25","objectID":"/publication/2019-06-21/:0:0","tags":null,"title":"Stk40 deletion elevates c-JUN protein level and impairs mesoderm differentiation","uri":"/publication/2019-06-21/"},{"categories":null,"content":"Xu Y, Luo X, Fang Z, Zheng X, Zeng Y, Zhu C, et al. Transcription coactivator Cited1 acts as an inducer of trophoblast-like state from mouse embryonic stem cells through the activation of BMP signaling. Cell Death Dis. 2018;9(9):924. Download here ","date":"2018-09-11","objectID":"/publication/2018-09-11/:0:0","tags":null,"title":"Transcription coactivator Cited1 acts as an inducer of trophoblast-like state from mouse embryonic stem cells through the activation of BMP signaling","uri":"/publication/2018-09-11/"},{"categories":null,"content":"Iâ€™m honored to be one of the contributors. It is definitely worthy. Thanks for the Bioconda team, so we could install bioinformatic tools so easy! The open source Bioconda project is a milestone in computational biology. It saves lots of time when trying to install bioinfo tools. Download here ","date":"2018-07-02","objectID":"/publication/2018-07-02/:0:0","tags":null,"title":"Bioconda: sustainable and comprehensive software distribution for the life sciences","uri":"/publication/2018-07-02/"},{"categories":null,"content":"Single cell analysis of mouse E5.5, E6.5 embryos. In this work, I dissected the embryos and prepared the single cell cDNA libraries.","date":"2017-06-09","objectID":"/publication/2016-06-09/","tags":null,"title":"Single-cell analysis reveals lineage segregation in early post-implantation mouse embryos","uri":"/publication/2016-06-09/"},{"categories":null,"content":"Wen J, Zeng Y, Fang Z, Gu J, Ge L, et al. Single-cell analysis reveals lineage segregation in early post-implantation mouse embryos. The Journal of biological chemistry 2017;292:9840-54 Download here ","date":"2017-06-09","objectID":"/publication/2016-06-09/:0:0","tags":null,"title":"Single-cell analysis reveals lineage segregation in early post-implantation mouse embryos","uri":"/publication/2016-06-09/"},{"categories":null,"content":"Wang L, Yu H, Cheng H, He K, Fang Z, et al. Deletion of Stk40 impairs definitive erythropoiesis in the mouse fetal liver. Cell Death Dis. 2017;8:e2722 Download here ","date":"2017-03-30","objectID":"/publication/2017-03-30/:0:0","tags":null,"title":"Deletion of Stk40 impairs definitive erythropoiesis in the mouse fetal liver","uri":"/publication/2017-03-30/"},{"categories":null,"content":"Liao B, Zhong XM, Xu HM, Xiao F, Fang ZQ, et al. Itch, an E3 ligase of Oct4, is required for embryonic stem cell self-renewal and pluripotency induction. Journal of Cellular Physiology 2013;228:1443-51 Download here ","date":"2012-12-18","objectID":"/publication/2012-12-18/:0:0","tags":null,"title":"Itch, an E3 ligase of Oct4, is required for embryonic stem cell self-renewal and pluripotency induction","uri":"/publication/2012-12-18/"},{"categories":null,"content":"I would like to use pandas to explore my data, but I did not find a convenient tool to do gene set enrichment analysis in python. I want something like this: Ability to run inside python console without switching to R. User friendly for both wet and dry lab users. Produce or reproduce publishable figures. Perform batch jobs easy. Command line support (Win, Mac, Linux). Thatâ€™s why GSEApy comes to play. See code here ","date":"0001-01-01","objectID":"/portfolio/portfolio-1/:0:0","tags":null,"title":"GSEApy","uri":"/portfolio/portfolio-1/"},{"categories":null,"content":"My bioinformatic workflows using snakemake. I would update it if only I have more time. Why snakemake? Good: Community Easy to use and update Reusable workflows Scalable Support conda env Support slurm, qsub â€¦ python syntax Drawback: Hard to debug workflows. TODO: Integrate snakemake-wrappers to current workflows to make it more reproducible. See code here ","date":"0001-01-01","objectID":"/portfolio/portfolio-3/:0:0","tags":null,"title":"Snakeflow","uri":"/portfolio/portfolio-3/"},{"categories":null,"content":"My journey to object detection began with YOLOv3. I think itâ€™s really a good starting point for someone like me without computer vison background to understand whatâ€™s going on behind the scence. While learning object detection, I made a simple modified C++ version (with LibTorch) based on othersâ€™ work 1 2 3. See code here ","date":"0001-01-01","objectID":"/portfolio/portfolio-2/:0:0","tags":null,"title":"YOLOv3 Inference Framework in C++","uri":"/portfolio/portfolio-2/"},{"categories":null,"content":"Performance test code: yolov3 models/yolov3.cfg models/yolov3.weights images Results: tested with CPU: Core i9 Windows MAC: average time (682 ms/image). tested with GPU: Tesla V100 Linux: average time (22 ms/image). ","date":"0001-01-01","objectID":"/portfolio/portfolio-2/:1:0","tags":null,"title":"YOLOv3 Inference Framework in C++","uri":"/portfolio/portfolio-2/"},{"categories":null,"content":"Features Supports NMS Soft NMS Weighted NMS ","date":"0001-01-01","objectID":"/portfolio/portfolio-2/:2:0","tags":null,"title":"YOLOv3 Inference Framework in C++","uri":"/portfolio/portfolio-2/"},{"categories":null,"content":"TODO Support training â€¦ ","date":"0001-01-01","objectID":"/portfolio/portfolio-2/:3:0","tags":null,"title":"YOLOv3 Inference Framework in C++","uri":"/portfolio/portfolio-2/"},{"categories":null,"content":"Credits YOLOv3_CPP repo is created based on the implementations below: weixu000. â†©ï¸ PyTorch-YOLOv3. â†©ï¸ YOLO_v3_tutorial_from_scratch. â†©ï¸ ","date":"0001-01-01","objectID":"/portfolio/portfolio-2/:4:0","tags":null,"title":"YOLOv3 Inference Framework in C++","uri":"/portfolio/portfolio-2/"}]