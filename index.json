[{"categories":["Machine Learning"],"content":"More about Graph Neural Network ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:0:0","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"Algebra presentation of Graphs ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:1:0","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"1. Adjacency matrix $$ A_{i j}= \\begin{cases} 1 \u0026 \\text { if }\\lbrace v_{i}, v_{j}\\rbrace \\in E \\text { and } i \\neq j \\cr 0 \u0026 \\text { otherwise } \\end{cases} $$ ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:1:1","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"2. Degree matrix: D is a diagonal matrix, where $$ D_{ii} = d(v_i) $$ ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:1:2","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"3. Laplacian matrix What and why Laplacian matrix if we consider all edges in graph $G$ to be undirected, then Laplacian matrix $L$ could be defined as $$ L = D-A $$ Thus, we have the elements: $$ L_{i j}=\\begin{cases} d\\left(v_{i}\\right) \u0026 \\text { if } i=j \\cr -1 \u0026 \\text { if }\\lbrace v_{i}, v_{j}\\rbrace \\in E \\text { and } i \\neq j \\cr 0 \u0026 \\text { otherwise. } \\end{cases}. $$ ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:1:3","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"4. Symmetric normalized Laplacian the symmetric normalized Laplacian is define as: $$ \\begin{aligned} L^{sym} \u0026=D^{-\\frac{1}{2}} L D^{-\\frac{1}{2}} \\cr \u0026=I-D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}} \\end{aligned} $$ The elements are given by: $$ L_{i j}^{s y m}=\\begin{cases} 1 \u0026 \\text { if } i=j \\text { and } d\\left(v_{i}\\right) \\neq 0 \\cr -\\frac{1}{\\sqrt{d\\left(v_{i}\\right) d\\left(v_{j}\\right)}} \u0026 \\text { if } \\lbrace v_{i}, v_{j} \\rbrace \\in E \\text { and } i \\neq j \\cr 0 \u0026 \\text { otherwise. } \\end{cases} $$ ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:1:4","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"5. Random walk nmormalized Laplacian $$ L^{rw} = D^{-1}L = I - D^{-1}A $$ The elements can be computed by: $$ L_{i j}^{r w}= \\begin{cases} 1 \u0026 \\text { if } i=j \\text { and } d\\left(v_{i}\\right) \\neq 0 \\cr -\\frac{1}{d\\left(v_{i}\\right)} \u0026 \\text { if }\\lbrace v_{i}, v_{j}\\rbrace \\in E \\text { and } i \\neq j \\cr 0 \u0026 \\text { otherwise } \\end{cases} $$ ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:1:5","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"6. Incidence Matrix $$ M_{i j}= \\begin{cases} 1 \u0026 \\text { if } \\exists k \\text { s.t } e_{j}=\\lbrace v_{i}, v_{k} \\rbrace \\cr -1 \u0026 \\text { if } \\exists k \\text { s.t } e_{j}=\\lbrace v_{k}, v_{i} \\rbrace \\cr 0 \u0026 \\text { otherwise. } \\end{cases} $$ for undrected graph, the corresponding incidence matrix statisfies that $$ M_{i j}=\\begin{cases} 1 \u0026 \\text { if } \\exists k \\text { s.t } e_{j}=\\lbrace v_{i}, v_{k}\\rbrace \\cr 0 \u0026 \\text { otherwise. } \\end{cases}. $$ ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:1:6","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"Vanilla Graph Neural Networks A node is defined by its features and related nodes in the graph. The aim of GNN is to lean a state embedding $h_v \\in R^s$, which encodes the information of the neighborhood, for each node. The state embedding $h_v$ is used to produce an output $O_v$, such as the distribution of the predicted node lable. ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:2:0","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"Model $$ h_v = f(X_v, X_{co[v]}, h_{ne[v]}, X_{ne[v]}) $$ $$ o_v = g(h_v, X_v) $$ $f$: local transition function, shared amoing all nodes $g$: local output function $X$: the input feature $h$: hidden state $co[v]$: the set of edges connected to node v $ne[v]$: the set of neighbors of node $v$ Now, we have a compact form as $$ H = F(H,X) \\\\ O = G(H, X_N) $$ $F$: the global transition function $G$: the global output funciton GNN use the classic iterative scheme to compute the state $$ H^{t+1} = F(H^t, X) $$ Next question is how to learn parameters of $f$ and $g$. The loss can be written as $$ loss = \\sum_{i = 1}^p(t_i - o_i) $$ where $p$: the number of supervised nodes the state $h_{v}^{t}$ are iteratively updated until a time step $T$. ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:2:1","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"Graph Convolutional Networks Spectral approaches and spatial approaches. Four calssic models (Spectral Network, ChebNet, GCN, and AGCN) ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:3:0","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"Spectral approaches Spectral Network the convolution operation is defined in the Fourier domain by computing the eigendecomposition of the graph Laplacian. The operation can be defined as multiplication of a signal x (a scalar for each node) with a filter $g_{\\theta} = \\text{diag}(\\theta)$: $$ \\mathbf{g_{\\theta}} \\star \\mathbf{x}=\\mathbf{U g}_{\\theta}(\\Lambda) \\mathbf{U}^{T} \\mathbf{x} $$ $\\mathbf{U}$: the matrix of eigenvectors of the normalized graph Laplacian $L = I_N - D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} = \\mathbf{U}\\Lambda\\mathbf{U}^T$ This operation results in potentially intense computations and non-spatially localized filters CHEBNET the operation is $$ \\mathbf{g_\\theta} \\star \\mathbf{x} \\approx \\sum_{k=0}^{K} \\boldsymbol{\\theta}_{k} \\mathbf{T}_{k}(\\tilde{\\mathbf{L}}) \\mathbf{x} $$ $\\tilde{\\mathbf{L}} = \\frac{2}{\\lambda_{max}}L - I_N$ $\\lambda_{max}$: the largest eigenvalue of $L$. $\\theta$: a vector of Chebyshev coefficients $T_{k}(x)$: the Chebyshev ploynomials $T_k(x) = 2xT_{k-1}(x) -T_{k-2}(x)$. $T_0(x) = 1$, $T_1(x) = x$. GCN $$ \\mathbf{g_\\theta^{\\prime}} \\star \\mathbf{x} \\approx \\theta_{0}^{\\prime} \\mathbf{x}+\\theta_{1}^{\\prime}\\left(\\mathbf{L}-\\mathbf{I}_{N}\\right) \\mathbf{x}=\\theta_{0}^{\\prime} \\mathbf{x}-\\theta_{1}^{\\prime} \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{A} \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{x} $$ then, constraining the number of parameters with $\\theta = \\theta_{0}^{\\prime} = - \\theta_{1}^{\\prime}$, get $$ \\mathbf{g_\\theta} \\star \\mathbf{x} \\approx \\theta\\left(\\mathbf{I}_{N}+\\mathbf{D}^{-\\frac{1}{2}} \\mathbf{A} \\mathbf{D}^{-\\frac{1}{2}}\\right) \\mathbf{x} $$ this operator could lead to numberical instabilities and exploding/vanishing gradients. Finally, introudce the renormalization trick: $I_N + \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{A} \\mathbf{D}^{-\\frac{1}{2}} \\rightarrow \\tilde{\\mathbf{D}}^{-\\frac{1}{2}} \\tilde{\\mathbf{A}} \\tilde{\\mathbf{D}}^{-\\frac{1}{2}}$, with $\\tilde{A} = A + I_N$, and $\\tilde{D} = \\sum_j\\tilde{A}_{ij}$. Finally $$ \\mathbf{Z}=\\tilde{\\mathbf{D}}^{-\\frac{1}{2}} \\tilde{\\mathbf{A}} \\tilde{\\mathbf{D}}^{-\\frac{1}{2}} \\mathbf{X} \\Theta $$ $\\Theta \\in \\mathbb{R}^{C \\times F}$: a matrix of filter paramters $\\mathbf{Z} \\in \\mathbb{R}^{N \\times F}$: the convolved signal matrix AGCN Adaptive Graph Convolution Network (AGCN) is proposed to learn the underlying relations. AGCN learns a ‚Äúresidual‚Äù graph Laplacian $\\mathbf{L}_{res}$ and add it to the original Lapalcian matrix $$ \\widehat{\\mathbf{L}}=\\mathbf{L}+\\alpha \\mathbf{L}_{r e s} $$ $\\mathbf{L}_{res}$ is computed by learned graph adjacency matrix $\\widehat{\\mathbf{A}}$ $$ \\begin{aligned} \\mathbf{L_{res}} \u0026=\\mathbf{I}-\\widehat{\\mathbf{D}}^{-\\frac{1}{2}} \\widehat{\\mathbf{A}} \\widehat{\\mathbf{D}}^{-\\frac{1}{2}} \\cr \\widehat{\\mathbf{D}} \u0026=\\operatorname{degree}(\\widehat{\\mathbf{A}}) \\end{aligned} $$ $\\widehat{\\mathbf{A}}$ is computed via a learned metric The idea behind the adaptive metric is that Euclidean distance is not suitatble for graph structured data and the metric should be adaptive the the task and input features. ACGN use the generalized Mahalanobis distance $$ D(\\mathbf x_{i}, \\mathbf x_{j})= \\sqrt{ (\\mathbf x_{i}-\\mathbf x_{j})^{T} \\mathbf{M}(\\mathbf x_{i}-\\mathbf x_{j})} $$ where M is a learned prameter taht statisfies $M = W_d W_{d}^T$. ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:3:1","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"Spatial Methods spatial approches defined convolutions directly on the graph, operating on spatially close neighbors. The major challenge of spatial approches is defining the convolution operation with differently sized neighborhoods and maintaining the local invariance of CNNs. Nerual FPS Use different weight matris for nodes with different degrees $$ \\begin{aligned} \\mathbf{x} \u0026=\\mathbf{h}{v}^{t-1}+\\sum{i=1}^{\\left|N_{v}\\right|} \\mathbf{h}_{i}^{t-1} \\ \\mathbf{h}_{v}^{t} \u0026=\\sigma\\left(\\mathbf{x} \\mathbf{W}_{t}^{\\left|N_{v}\\right|}\\right) \\end{aligned} $$ Patchy-SAN first selects and normalized exactly k neigbors for each node, then normalized neighhorhos servers as the receptive filed and the convolutional operation is applied. Node sequence selection Neighorhood assembly Graph normalization Convolution architecture. DCNN the diffusion-convolution neural network (DCNN) for node classification $$ H = \\sigma(W^c \\odot P^*X) $$ P*: N x K x N tensor, contains the power series $\\lbrace P, P^2, \\cdots, P^K \\rbrace$ X: N x F tensor of input features for graph classification $$ H = \\sigma(W^c \\odot 1^T_N P^* X /N) $$ $1_N$: N x 1 vector of ones. DGCN dual grpah convolutional network, which jointly consider the local consistency and global consitency on graphs. It use 2 convolutional networks to capture the local/global consistency and adopts an unsupervised loss to ensemble them. first network: $$ \\mathbf{Z}=\\tilde{\\mathbf{D}}^{-\\frac{1}{2}} \\tilde{\\mathbf{A}} \\tilde{\\mathbf{D}}^{-\\frac{1}{2}} \\mathbf{X} \\Theta $$ second network replace the adjacency matrix with positive pointwise mutual information (PPMI) matrix $$ \\mathbf{H}^{\\prime}=\\sigma\\left(\\mathbf{D}{P}^{-\\frac{1}{2}} \\mathbf{X}{P} \\mathbf{D}_{P}^{-\\frac{1}{2}} \\mathbf{H} \\Theta\\right) $$ Assemble these two convolutions via the final loss function $$ L=L_{0}\\left(\\operatorname{Conv}_{A}\\right)+\\lambda(t) L_{r e g}\\left(\\operatorname{Conv}_{A}, \\operatorname{Conv}_{P}\\right) $$ $\\lambda(t)$ is the dynamic weight to blance the importance of these two loss functions. $$ L_{0}\\left(\\operatorname{Conv}_{A}\\right)=-\\frac{1}{\\left|y_{L}\\right|} \\sum_{l \\in y_{L}} \\sum_{i=1}^{c} Y_{l, i} \\ln \\left(\\widehat{Z}_{l, i}^{A}\\right) $$ $$ L_{r e g}\\left(\\operatorname{Conv}_{A}, \\operatorname{Conv}_{P}\\right)=\\frac{1}{n} \\sum_{i=1}^{n}\\left|\\widehat{Z}_{i,:}^{P}-\\widehat{Z}_{i,:}^{A}\\right|^{2} $$ LGCN learnable graph conovlutional networks MONET A propose spatial-domain model (MoNet) on non-Euclidean domains which could generalized serveal previous network. GCNN, ACNN, GCN, DCNN GRAPHSAGE a general inductive framework. $$ \\begin{aligned} \\mathbf{h}{N{v}}^{t} \u0026=\\text { AGGREGATE }{t}\\left(\\left{\\mathbf{h}{u}^{t-1}, \\forall u \\in N_{v}\\right}\\right) \\ \\mathbf{h}_{v}^{t} \u0026=\\sigma\\left(\\mathbf{W}^{t} \\cdot\\left[\\mathbf{h}_{v}^{t-1} | \\mathbf{h}_{N_{v}}^{t}\\right]\\right) \\end{aligned} $$ Mean aggregator $$ \\mathbf{h}{v}^{t}=\\sigma\\left(\\mathbf{W} \\cdot \\operatorname{MEAN}\\left(\\left{\\mathbf{h}{v}^{t-1}\\right} \\cup\\left{\\mathbf{h}{u}^{t-1}, \\forall u \\in N{v}\\right}\\right)\\right. $$ LSTM aggregator Pooling aggregator $$ \\mathbf{h}{N{v}}^{t}=\\max \\left(\\left{\\sigma\\left(\\mathbf{W}{\\mathrm{pool}} \\mathbf{h}{u}^{t-1}+\\mathbf{b}\\right), \\forall u \\in N_{v}\\right}\\right) $$ an unsupervised loss function which encouage nearby nodes to have similar representations while distant nodes have different representations: $$ J_{G}\\left(\\mathbf{z}_{u}\\right)=-\\log \\left(\\sigma\\left(\\mathbf{z}_{u}^{T} \\mathbf{z}_{v}\\right)\\right)-Q \\cdot E_{v_{n} \\sim P_{n}(v)} \\log \\left(\\sigma\\left(-\\mathbf{z}_{u}^{T} \\mathbf{z}_{v_{n}}\\right)\\right) $$ ","date":"2020-07-26","objectID":"/2020-07-26-dl-gnn/:3:2","tags":["Deep Learning"],"title":"Graph Neural Network","uri":"/2020-07-26-dl-gnn/"},{"categories":["Machine Learning"],"content":"Introduction of Graph Neural Networks ","date":"2020-07-25","objectID":"/2020-07-25-dl-geometric/:0:0","tags":["Deep Learning"],"title":"Geometric Deep Learning","uri":"/2020-07-25-dl-geometric/"},{"categories":["Machine Learning"],"content":"Data Eculidean Structure Data: image, video, voice ‚Ä¶ easy to find adjacent neighbors easy to define distance Non-Eculidean data: Graph, Manifold hard to define adjacent neighbors or the numbers of adjacent nodes varies. means hard to define distance, convolution ‚Ä¶ ","date":"2020-07-25","objectID":"/2020-07-25-dl-geometric/:1:0","tags":["Deep Learning"],"title":"Geometric Deep Learning","uri":"/2020-07-25-dl-geometric/"},{"categories":["Machine Learning"],"content":"Embed (project) Non-Eculidean Data into Eculidean Space using geometric deep learning ","date":"2020-07-25","objectID":"/2020-07-25-dl-geometric/:2:0","tags":["Deep Learning"],"title":"Geometric Deep Learning","uri":"/2020-07-25-dl-geometric/"},{"categories":["Machine Learning"],"content":"Graph Neural Network ","date":"2020-07-25","objectID":"/2020-07-25-dl-geometric/:3:0","tags":["Deep Learning"],"title":"Geometric Deep Learning","uri":"/2020-07-25-dl-geometric/"},{"categories":["Machine Learning"],"content":"Common tasks graph classification: classcify graphs according to its topology. each graph has a label. most common definition: graph $G = (A, F)$ Adjacency matrix (of G): $A \\in \\lbrace 0,1 \\rbrace ^{n \\times n}$ Feature matrix (of nodes): $F \\in R^{n \\times d}$, with n nodes, d features Given $\\mathcal{D} = \\lbrace (G_1, y_1), \\cdots, (G_n, y_n) \\rbrace$, learn $$ f: \\mathcal{G} \\rightarrow \\mathcal{Y} $$ node classification: each node has a label generative graph (models): e.g. virtual drug screen ","date":"2020-07-25","objectID":"/2020-07-25-dl-geometric/:3:1","tags":["Deep Learning"],"title":"Geometric Deep Learning","uri":"/2020-07-25-dl-geometric/"},{"categories":["Machine Learning"],"content":"Algebra Presentation of Graphs Adjacency matrix $$ A_{i j}= \\begin{cases} 1 \u0026 \\text { if }\\lbrace v_{i}, v_{j}\\rbrace \\in E \\text { and } i \\neq j \\cr 0 \u0026 \\text { otherwise } \\end{cases} $$ Degree matrix: D is a diagonal matrix, where $$ D_{ii} = d(v_i) $$ Laplacian matrix: if we consider all edges in graph $G$ to be undirected, then Laplacian matrix $L$ could be defined as $$ L = D-A $$ Thus, we have the elements: $$ L_{i j}=\\begin{cases} d\\left(v_{i}\\right) \u0026 \\text { if } i=j \\cr -1 \u0026 \\text { if }\\lbrace v_{i}, v_{j}\\rbrace \\in E \\text { and } i \\neq j \\cr 0 \u0026 \\text { otherwise. } \\end{cases}. $$ What and why Laplacian matrix Symmetric normalized Laplacian the symmetric normalized Laplacian is define as: $$ \\begin{aligned} L^{sym} \u0026=D^{-\\frac{1}{2}} L D^{-\\frac{1}{2}} \\cr \u0026=I-D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}} \\end{aligned} $$ The elements are given by: $$ L_{i j}^{s y m}=\\begin{cases} 1 \u0026 \\text { if } i=j \\text { and } d\\left(v_{i}\\right) \\neq 0 \\cr -\\frac{1}{\\sqrt{d\\left(v_{i}\\right) d\\left(v_{j}\\right)}} \u0026 \\text { if } \\lbrace v_{i}, v_{j} \\rbrace \\in E \\text { and } i \\neq j \\cr 0 \u0026 \\text { otherwise. } \\end{cases} $$ Random walk nmormalized Laplacian $$ L^{rw} = D^{-1}L = I - D^{-1}A $$ The elements can be computed by: $$ L_{i j}^{r w}= \\begin{cases} 1 \u0026 \\text { if } i=j \\text { and } d\\left(v_{i}\\right) \\neq 0 \\cr -\\frac{1}{d\\left(v_{i}\\right)} \u0026 \\text { if }\\lbrace v_{i}, v_{j}\\rbrace \\in E \\text { and } i \\neq j \\cr 0 \u0026 \\text { otherwise } \\end{cases} $$ Incidence Matrix $$ M_{i j}= \\begin{cases} 1 \u0026 \\text { if } \\exists k \\text { s.t } e_{j}=\\lbrace v_{i}, v_{k} \\rbrace \\cr -1 \u0026 \\text { if } \\exists k \\text { s.t } e_{j}=\\lbrace v_{k}, v_{i} \\rbrace \\cr 0 \u0026 \\text { otherwise. } \\end{cases} $$ for undrected graph, the corresponding incidence matrix statisfies that $$ M_{i j}=\\begin{cases} 1 \u0026 \\text { if } \\exists k \\text { s.t } e_{j}=\\lbrace v_{i}, v_{k}\\rbrace \\cr 0 \u0026 \\text { otherwise. } \\end{cases}. $$ ","date":"2020-07-25","objectID":"/2020-07-25-dl-geometric/:3:2","tags":["Deep Learning"],"title":"Geometric Deep Learning","uri":"/2020-07-25-dl-geometric/"},{"categories":["Machine Learning"],"content":"Convolution on Spectral The key to understand graph convolution: Laplacian matrix and Newton Cool‚Äôs Law Now, the convolution operation is defined in the Fourier domain by computing the eigendecomposition of the Laplacian Matrix $$ L = U \\Lambda U^{-1} = U \\Lambda U^T $$ Note: $U$ is an orthognal matrix, $U^{-1} = U^T$ Then, given $x \\in R^n$, the fourier transform: $\\hat{x} = U^{T} x$ reverse fourier transform: $x = U \\hat{x}$ Finally, given signal $x$ and kernel $y$, the graph fourier transform ($*_{\\mathcal{g}}$) is $$ x *_{\\mathcal{G}} y=U\\left(\\left(U^{T} x\\right) \\odot\\left(U^{T} y\\right)\\right) $$ $\\odot$: element-wise multiplication As we have a kernel $g_{\\theta}(\\sdot)$, $$ y=g_{\\theta}(L)(x)=g_{\\theta}\\left(U \\Lambda U^{T}\\right) x=U g_{\\theta}(\\Lambda) U^{T} x $$ where $$ g_{\\theta}(\\Lambda)=\\operatorname{diag}(\\theta)=\\left[\\begin{array}{ccc} \\theta_{1} \u0026 \\cdots \u0026 0 \\cr \\vdots \u0026 \\ddots \u0026 \\vdots \\cr 0 \u0026 \\cdots \u0026 \\theta_{n-1} \\end{array}\\right] $$ The learned parameters are in $\\operatorname{diag}(\\theta)$ The problems: lost local connectivity on space (e.g. CNN on images preserve locality) computational complexity $O(n)$, not well generalized on large scale Graphs Need more knowledge of the Chebyshev ploynomials to get deeper. see my next post about GNN. ","date":"2020-07-25","objectID":"/2020-07-25-dl-geometric/:4:0","tags":["Deep Learning"],"title":"Geometric Deep Learning","uri":"/2020-07-25-dl-geometric/"},{"categories":["Machine Learning"],"content":"Convolution on Spatial Another way to understand graph convolution: Message Passing. ","date":"2020-07-25","objectID":"/2020-07-25-dl-geometric/:5:0","tags":["Deep Learning"],"title":"Geometric Deep Learning","uri":"/2020-07-25-dl-geometric/"},{"categories":["Machine Learning"],"content":"Message passing Message passing: node $\\mathcal{S}_1$ and its neigbor $\\mathcal{N}$ (B1, B2, B3), aggregate $\\mathcal{N}$'s message to $\\mathcal{S}_1$. for example, aggreate (sum) each nodes‚Äôs features $H^{(l)} \\in R^d$, $$ \\sum_{u \\in \\mathcal{N}(v)} H^{(l)}(u) \\in \\mathbb{R}^{d_{i}} $$ node $v$'s neigbors: $\\mathcal{N(v)}$, layer: $l$ Generally, we add a linear transform matrix $W^{(l)} \\in R^{d_i \\times d_o}$ to change the feature dimension. $$ \\left(\\sum_{u \\in \\mathcal{N}(v)} H^{(l)}(u)\\right) W^{(l)} \\in \\mathbb{R}^{d_{o}} $$ After add activate function, get a more compact equation $$ f(H^{(l)}, A) = \\sigma ( A H^{(l)}W^{(l)}) $$ $A$: Adjacency Matrix ","date":"2020-07-25","objectID":"/2020-07-25-dl-geometric/:5:1","tags":["Deep Learning"],"title":"Geometric Deep Learning","uri":"/2020-07-25-dl-geometric/"},{"categories":["Machine Learning"],"content":"Example Given graph input feature (10 dim): $f_{in} \\in R^{10}$ output feature (20 dim): $f_{out} \\in R^{20}$ each node‚Äôs feature: $H^{(l)} \\in R^{6 \\times 10}$ weight: $W^{(l)} \\in R^{10 \\times 20}$ adjcency matrix: $A \\in R^{6 \\times 6}$ Message passing step: feature dimension change: $HW \\in R^{6 \\times 20}$ select the neigborhood nodes: $AHW$ The problems: Each node have different degree, make the scale of output feature map will completely change (see each row of adjcency matrix). So, we have to normalize laplacian matrix. Each node did not include information from itself. So need to make a self connection. ","date":"2020-07-25","objectID":"/2020-07-25-dl-geometric/:5:2","tags":["Deep Learning"],"title":"Geometric Deep Learning","uri":"/2020-07-25-dl-geometric/"},{"categories":["Machine Learning"],"content":"Definition Adjcency Matrix: $$ \\tilde{A} = A + I_n $$ Degree Matrix: $$ \\tilde D_{ii} = \\sum_{j} \\tilde{A}_{ij} $$ Random Walk Normalization of A: make row sum equal to 1 $$ \\tilde{A} = D^{-1}A $$ Symmetric Normalization: used more in practice, more dynamic. $$ A = D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} $$ Laplacian matrix normliaztion: $$ \\begin{aligned} L^{sym} \u0026= D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}} \\cr \u0026= D^{-\\frac{1}{2}}(D-A)D^{-\\frac{1}{2}} \\cr \u0026= I_n - D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} \\end{aligned} $$ Finally, we have $$ H^{(l+1)} = \\sigma ( \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)}) $$ $\\tilde{A} = A+ I_n$ $\\tilde D_ii = \\sum_j \\tilde A_{ij}$ ","date":"2020-07-25","objectID":"/2020-07-25-dl-geometric/:5:3","tags":["Deep Learning"],"title":"Geometric Deep Learning","uri":"/2020-07-25-dl-geometric/"},{"categories":["Coding"],"content":"Get answers for C/C++ within ? s","date":"2020-07-11","objectID":"/2020-07-28-cpp-cuda/","tags":["C++","CUDA"],"title":"C++ Notes: CUDA","uri":"/2020-07-28-cpp-cuda/"},{"categories":["Coding"],"content":"A Cuda/C++ starter cheatsheet ","date":"2020-07-11","objectID":"/2020-07-28-cpp-cuda/:0:0","tags":["C++","CUDA"],"title":"C++ Notes: CUDA","uri":"/2020-07-28-cpp-cuda/"},{"categories":["Coding"],"content":"Harward and software Thread block and grid are logical threads, make programming easy. In hardware, each GPU made of lots of streaming multiprocessor(hardware), which have lots of threads. ","date":"2020-07-11","objectID":"/2020-07-28-cpp-cuda/:1:0","tags":["C++","CUDA"],"title":"C++ Notes: CUDA","uri":"/2020-07-28-cpp-cuda/"},{"categories":["Coding"],"content":"Concepts kernel: the code (function) run on GPU one kernel, only have one grid, grid have blocks, block has threads. thread, block,grid threadIdx: each thread have a unique id, threadIdx.x, .y,.z blockIdx: each block have a unique id, blockIdx.x, .y,.z dimemnsion size: blockDim.x,gridDim.x, .y,.z ","date":"2020-07-11","objectID":"/2020-07-28-cpp-cuda/:2:0","tags":["C++","CUDA"],"title":"C++ Notes: CUDA","uri":"/2020-07-28-cpp-cuda/"},{"categories":["Coding"],"content":"thread management Stream Each GPU made of lots of Streams(hardware). When a kernel grid activate, multi block will assign blocks to avaibable stream to run. Warp For SM(hardware), CUDA run as warp(Á∫øÁ®ãÊùü), SM don‚Äôt know where the block, who they are. In hardware, the thread resource are limited, not all logical threads run at the same time. The minimun physical threads run at the same time are called warp. for example: if one block assigned 128 threads, when running on Stream, this block divied into warp0: thread 0,........thread31 warp1: thread 32,........thread63 warp2: thread 64,........thread95 warp3: thread 96,........thread127 ","date":"2020-07-11","objectID":"/2020-07-28-cpp-cuda/:3:0","tags":["C++","CUDA"],"title":"C++ Notes: CUDA","uri":"/2020-07-28-cpp-cuda/"},{"categories":["Coding"],"content":"special keywords device: GPU host: CPU compile with nvcc, not gcc keyword execution called by other __global__ device device or host must return void __device__ device device __host__ host host defaut, could omit kernel launch: \u003c\u003c\u003cDg,Db,Ns,S\u003e\u003e\u003e Dg (dim3): specifies the dimension and size of the grid. Db (dim3): specifies the dimension and size of each block Ns (size_t): specifies the number of bytes in shared memory that is dynamically allocated per block for this call in addition to the statically allocated memory. S (cudaStream_t): specifies the associated stream, is an optional parameter which defaults to 0. from docs ","date":"2020-07-11","objectID":"/2020-07-28-cpp-cuda/:4:0","tags":["C++","CUDA"],"title":"C++ Notes: CUDA","uri":"/2020-07-28-cpp-cuda/"},{"categories":["Coding"],"content":"Vertors ","date":"2020-07-11","objectID":"/2020-07-28-cpp-cuda/:5:0","tags":["C++","CUDA"],"title":"C++ Notes: CUDA","uri":"/2020-07-28-cpp-cuda/"},{"categories":["Coding"],"content":"init 1 dim vector #include \u003cstdio.h\u003e __global__ void initWith(float num, float *a, int N) { int index = threadIdx.x + blockIdx.x * blockDim.x; int stride = blockDim.x * gridDim.x; for(int i = index; i \u003c N; i += stride) { a[i] = num; } } ","date":"2020-07-11","objectID":"/2020-07-28-cpp-cuda/:5:1","tags":["C++","CUDA"],"title":"C++ Notes: CUDA","uri":"/2020-07-28-cpp-cuda/"},{"categories":["Coding"],"content":"3 dimension index tid=threadIdx.x+threadIdx.y*blockDim.x+threadIdx.z*blockDim.x*blockDim.y ","date":"2020-07-11","objectID":"/2020-07-28-cpp-cuda/:5:2","tags":["C++","CUDA"],"title":"C++ Notes: CUDA","uri":"/2020-07-28-cpp-cuda/"},{"categories":["Coding"],"content":"vectorized addition __global__ void addVectorsInto(float *result, float *a, float *b, int N) { int index = threadIdx.x + blockIdx.x * blockDim.x; int stride = blockDim.x * gridDim.x; for(int i = index; i \u003c N; i += stride) { result[i] = a[i] + b[i]; } } ","date":"2020-07-11","objectID":"/2020-07-28-cpp-cuda/:5:3","tags":["C++","CUDA"],"title":"C++ Notes: CUDA","uri":"/2020-07-28-cpp-cuda/"},{"categories":["Coding"],"content":"Common Program Flow ","date":"2020-07-11","objectID":"/2020-07-28-cpp-cuda/:6:0","tags":["C++","CUDA"],"title":"C++ Notes: CUDA","uri":"/2020-07-28-cpp-cuda/"},{"categories":["Coding"],"content":"1. Get device int deviceId; int numberOfSMs; // get device cudaGetDevice(\u0026deviceId); cudaDeviceGetAttribute(\u0026numberOfSMs, cudaDevAttrMultiProcessorCount, deviceId); ","date":"2020-07-11","objectID":"/2020-07-28-cpp-cuda/:6:1","tags":["C++","CUDA"],"title":"C++ Notes: CUDA","uri":"/2020-07-28-cpp-cuda/"},{"categories":["Coding"],"content":"3. GPU memory allocation const int N = 2\u003c\u003c24; size_t size = N * sizeof(float); float *a; float *b; float *c; // GPU memory allocation cudaMallocManaged(\u0026a, size); cudaMallocManaged(\u0026b, size); cudaMallocManaged(\u0026c, size); // send to GPU cudaMemPrefetchAsync(a, size, deviceId); cudaMemPrefetchAsync(b, size, deviceId); cudaMemPrefetchAsync(c, size, deviceId); size_t threadsPerBlock; size_t numberOfBlocks; // why 32, beacause warp is usually 32 on the hardware design side. // device run more effeciently threadsPerBlock = 256; numberOfBlocks = 32 * numberOfSMs; cudaError_t addVectorsErr; // error handling cudaError_t asyncErr; ","date":"2020-07-11","objectID":"/2020-07-28-cpp-cuda/:6:2","tags":["C++","CUDA"],"title":"C++ Notes: CUDA","uri":"/2020-07-28-cpp-cuda/"},{"categories":["Coding"],"content":"4. Create streams /* * Create 3 streams to run initialize the 3 data vectors in parallel. */ cudaStream_t stream1, stream2, stream3; cudaStreamCreate(\u0026stream1); cudaStreamCreate(\u0026stream2); cudaStreamCreate(\u0026stream3); ","date":"2020-07-11","objectID":"/2020-07-28-cpp-cuda/:6:3","tags":["C++","CUDA"],"title":"C++ Notes: CUDA","uri":"/2020-07-28-cpp-cuda/"},{"categories":["Coding"],"content":"5. Kernel launch /* * Give each `initWith` launch its own non-standard stream. * note the \u003c\u003c\u003c \u003e\u003e\u003e: also called a ‚Äúkernel launch‚Äù */ initWith\u003c\u003c\u003cnumberOfBlocks, threadsPerBlock, 0, stream1\u003e\u003e\u003e(3, a, N); initWith\u003c\u003c\u003cnumberOfBlocks, threadsPerBlock, 0, stream2\u003e\u003e\u003e(4, b, N); initWith\u003c\u003c\u003cnumberOfBlocks, threadsPerBlock, 0, stream3\u003e\u003e\u003e(0, c, N); // run addVectorsInto\u003c\u003c\u003cnumberOfBlocks, threadsPerBlock\u003e\u003e\u003e(c, a, b, N); addVectorsErr = cudaGetLastError(); if(addVectorsErr != cudaSuccess) printf(\"Error: %s\\n\", cudaGetErrorString(addVectorsErr)); // critical !!! asyncErr = cudaDeviceSynchronize(); if(asyncErr != cudaSuccess) printf(\"Error: %s\\n\", cudaGetErrorString(asyncErr)); ","date":"2020-07-11","objectID":"/2020-07-28-cpp-cuda/:6:4","tags":["C++","CUDA"],"title":"C++ Notes: CUDA","uri":"/2020-07-28-cpp-cuda/"},{"categories":["Coding"],"content":"6. fetch data and run on CPU // fetch data to CPU memory cudaMemPrefetchAsync(c, size, cudaCpuDeviceId); // run a func in CPU checkElementsAre(7, c, N); ","date":"2020-07-11","objectID":"/2020-07-28-cpp-cuda/:6:5","tags":["C++","CUDA"],"title":"C++ Notes: CUDA","uri":"/2020-07-28-cpp-cuda/"},{"categories":["Coding"],"content":"7. free memory /* * Destroy streams when they are no longer needed. */ cudaStreamDestroy(stream1); cudaStreamDestroy(stream2); cudaStreamDestroy(stream3); // free GPU memory cudaFree(a); cudaFree(b); cudaFree(c); Here is a function run on CPU void checkElementsAre(float target, float *vector, int N) { for(int i = 0; i \u003c N; i++) { if(vector[i] != target) { printf(\"FAIL: vector[%d] - %0.0f does not equal %0.0f\\n\", i, vector[i], target); exit(1); } } printf(\"Success! All values calculated correctly.\\n\"); } ","date":"2020-07-11","objectID":"/2020-07-28-cpp-cuda/:6:6","tags":["C++","CUDA"],"title":"C++ Notes: CUDA","uri":"/2020-07-28-cpp-cuda/"},{"categories":["Coding"],"content":"Get answers for C/C++ within ? s","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"Learn C++11 thread library. Code snippets from Concurrent Programming with C++11 ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:0:0","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"Process vs. Threads ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:1:0","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"Usage Summary A short summary of thread library in STL thread and async /* thread */ std::thread t1(factorial, 6); // create a new thread std::this_thread::sleep_for(chrono::milliseconds(3)); chrono::steady_clock::time_point tp = chrono::steady_clock::now() + chrono::microseconds(4); std::this_thread::sleep_until(tp); /* async() */ std::future\u003cint\u003e fu = async(factorial, 6); // create a new thread mutex /* Mutex */ std::mutex mu; std::lock_guard\u003cmutex\u003e locker(mu); std::unique_lock\u003cmutex\u003e ulocker(mu); ulocker.try_lock(); ulocker.try_lock_for(chrono::nanoseconds(500)); ulocker.try_lock_until(tp); condition variable /* Condition Variable */ std:condition_variable cond; cond.wait_for(ulocker, chrono::microseconds(2)); cond.wait_until(ulocker, tp); future and promise /* Future and Promise */ std::promise\u003cint\u003e p; std::future\u003cint\u003e f = p.get_future(); f.get(); f.wait(); f.wait_for(chrono::milliseconds(2)); f.wait_until(tp); Packaged task /* Packaged Task */ std::packaged_task\u003cint(int)\u003e t(factorial); std::future\u003cint\u003e fu2 = t.get_future(); t(6); ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:2:0","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"Cases ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:3:0","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"thread two way to create new thread std::thread t1(func); std::async(std::launch::async, func); exmample of thread #incldue \u003cthread\u003e void function1() { std::cout \u003c\u003c\"hello\"\u003c\u003cstd::endl; } std::tread t1(function1); // t1 start running // t1.join(); // main thread wait for t1 to finish t1.detach(); // t1 will freely on its own -- deamon process`` /// once detach, forever detach. if (t1.joinable()) t1.join(); // if detached, this line crashed. ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:3:1","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"thread managment class Fctor { public: void operator()(std::string \u0026 s) { std::cout\u003c\u003c\"this is thread\"\u003c\u003cstd::endl; } }; std::string = \"string int\" std::thread t1((Fctor()), s); // alway pass by value std::thread t2((Fctor()), std::ref(s)); // pass by ref std::thread t3((Fctor()), std::move(s)); // move s from main to thread std::thread t4 = std::move(t3); // thread could not be copy, only move try { std::cout\u003c\u003c\"this is main\"\u003c\u003cstd::endl; } catch (...) { t1.join(); t2.jion(); throw; } t1.join(); t2.join(); t4.join(); if oversubscription, limit threads with maximum cpu cores std:üßµ:hardware_concurrency(); // indication, number of cpu cores ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:3:2","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"race condition and mutex #include \u003cthread\u003e#include \u003cmutex\u003e std::mutex mu; void shared_print(std::string msg, int id) { std::lock_guard\u003cstd::mutex\u003e guard(mu); // RAII //mu.lock(); // +\u003e safeguard no two threads using cout at the same time, std::cout\u003c\u003cmsg \u003c\u003c id \u003c\u003c std::endl; //mu.unlock(); // if error thrown between .lock() and .unlock(), generate zombie process } void function_1() { for(int i=0; i \u003c 100; i++) shared_print(\"from functino t1\", i); } int main() { std::thread t1(function_1); for (int i=0; i \u003c 100; i++) shared_print(\"from main\", i); t1.join() } more practical example #include \u003cthread\u003e#include \u003cmutex\u003e class LogFile { std::mutex mu; std::ofstream f; public: LogFile() { f.open(\"log.txt\"); } void shared_print(std::string msg, int id) { std::lock_guard\u003cstd::mutex\u003e guard(mu); // RAII f \u003c\u003c msg \u003c\u003c id \u003c\u003c std::endl; } // never return f to outside wworld // never pass f as an argument for user }; void function_1(LogFile\u0026 log) { for(int i=0; i \u003c 100; i++) log.shared_print(\"from functino t1\", i); } int main() { LogFile log; std::thread t1(function_1, std::ref(log)); for (int i=0; i \u003c 100; i++) log.shared_print(\"from main\", i); t1.join() } ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:3:3","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"Advoid Deadlock prefer locking single mutex Advoid locking a mutex and then calling a user provded function use std::lock() to lock more than one mutex lock the mutexs in same order. ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:3:4","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"unique_lock and lazy initialization class LogFile { std::mutex mu; std::ofstream f; std::once_flag flag; public: LogFile() { f.open(\"log.txt\"); } void shared_print(std::string msg, int id) { // if you need to check whether a file is open in each call, use once_flag to rescue // std::call_once(flag, [\u0026](){ f.open(\"log.txt\");}) // file only open once std::unique_lock\u003cstd::mutex\u003e locker(mu, std::defer_lock); // note here // do something else locker.lock(); f \u003c\u003c msg \u003c\u003c id \u003c\u003c std::endl; locker.unlock(); // call again locker.lock(); // do something ... locker.unlock(); std::unique_lock\u003cstd::mutex\u003e locker2 = std::move(locker); // could change ownership } }; ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:3:5","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"condition variable Condition variable is to synchronize the execution order of threads std::condition_variable cond; // usage 1 std::unique_lock\u003cstd::mutex\u003e locker(mu); cond.wait(locker); // spurious wake cond.wait(locker, [](){return !q.empty();}) // cond.notify_one(); // notify one waiting thread cond.notify_all(); // ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:3:6","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"future and promise Future and promise provide a convenience way to communicate between threads. e.g. return value to main thread int factorial(int n) { int res = 1; return res+ n; } int x; std::future\u003cint\u003e fu = std::async(function, 4); // future, get something in future x = fu.get(); // fu.get(); //crash std::future\u003cint\u003e fu2 = std::async(std::launch::deferred, factorial, 4); // means not excuate unitl call .get() x = fu2.get(); // only excuate fu2 when called get std::future\u003cint\u003e fu3 = std::async(std::launch::async | std::launch::deferred , factorial, 4); // create new thread by calling async or not x = fu3.get(); // only excuate fu2 when called get usage of promise int factorial(std::future\u003cint\u003e \u0026f) { int res = 1; int N = f.get(); // note here return res + N; } int x; std::promise\u003cint\u003e p; std::future\u003cint\u003e f = p.get_future(); std::future\u003cint\u003e fu4 = std::async(std::launch::async, factorial, std::ref(f)); // do something else ... //// if p not set, throw error // p.set_exception(std::make_exception_ptr)(std::runtime_error(\"To err is human\")); // set p p.set_value(4); // get from child x = fu4.get(); shared_future for multi-threads int factorial(std::shared_future\u003cint\u003e \u0026f) { int res = 1; int N = f.get(); // note here return res + N; } int x; std::promise\u003cint\u003e p; std::future\u003cint\u003e f = p.get_future(); std::shared_future\u003cint\u003e sf = f.shared(); std::future\u003cint\u003e fu5 = std::async(std::launch::async, factorial, sf); std::future\u003cint\u003e fu6 = std::async(std::launch::async, factorial, sf); std::future\u003cint\u003e fu7 = std::async(std::launch::async, factorial, sf); p.set_value(4); // get from child x = fu4.get(); ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:3:7","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"using callable object class A { public: void f(int, char) {} long g(double x) {return 0;} int operator()(int n) {return 0;} }; A a; std::thread t1(a, 6); // copy of a() in a different thread std::thread t2(std::ref(a), 6) // a() in a different thread std::thread t3(A(), 6); // temp A std::thread t4([](int x){return x*x;}, 6); std::thread t5(\u0026A::f, a, 6, 'w'); // copy of a.f(6,'w') in a different thread // these feature could be used in // std::bind, std::async ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:3:8","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Coding"],"content":"packagee tasks packaged_task provides a way to implement a task pool. It can conveniently convey the returned value from a task to a different thread std::thread t1(factorial, 6); // could pass args std::packaged_task\u003cint(int)\u003e t(factorial); // could not pass additional args std::packaged_task\u003cint()\u003e t2(std::bind(factorial, 6)); // now we could pass args using std::bind // ... t(6); // in a different context, t alwaly return void, so int x = t.get_future().get(); // get value // call t2 by t2(); int factorial(int N) { int res =1; for (int i=N; i \u003e 1; i --) res *= i; return res; } std::deque\u003cstd::packaged_task\u003cint()\u003e\u003e task_q; std::mutex mu; std::condition_variable cond; void thread_1() { std::packaged_task\u003cint()\u003e t; { //std::lock_guard\u003cstd::mutex\u003e locker(mu); // advoid data race std::unique_lock\u003cstd::mutex\u003e locker(mu); cond.wait(locker, [](){return !taks_q.empty();}) t = std::move(task_q.front()); task_q.pop_front(); } t(); } int main() { std::thread t1(thread_1); // so, task_q run in t1; std::packaged_task\u003cint()\u003e t(std::bind(factorical, 6)); std::future\u003cint\u003e fu = t.get_future(); // get returned value to main thread { std::lock_guard\u003cstd::mutex\u003e locker(mu); task_q.push_bask(std::move(t)); } std::cout\u003c\u003cfu.get(); t1.join(); return 0; } Summary: 3 method to get a future promise::get_future() packaged_task::get_future() async() returns a future ","date":"2020-07-10","objectID":"/2020-07-10-cpp-threading/:3:9","tags":["C++"],"title":"C++ Notes: Concurrency","uri":"/2020-07-10-cpp-threading/"},{"categories":["Statistic"],"content":"one-way ANOVA from scratch Calculate the Sum of Squares Total (SST): $$ SS_{total} = \\sum_{j=1}^k \\sum_{i=1}^l (X_{ij} - \\bar{X})^2 $$ Calculate the Sum of Squares Within Groups (SSW): $$ SS_{within} = \\sum_{j=1}^k \\sum_{i=1}^l (X_{ij} - \\bar{X_j})^2 $$ Calculate the Sum of Squares Between Groups (SSB): $$ SS_{between} = \\sum_{j=1}^k n_j ( \\bar X_{j} - \\bar{X}) ^2 $$ $n_j$: numbers of individual point in group j. Verify that $$ SS_{total} = SS_{between} + SS_{within} $$ Calculate the Degrees of Freedom (df) Calculate the Degrees of Freedom Total (DFT) $$ df_{total} = n -1 $$ Calculate the Degrees Between k Groups (DFB) $$ df_{bewteen} = k -1 $$ Calculate the Degrees of Freedom Within Groups (DFW) $$ df_{within} = n - k $$ Verify that $$ df_t = df_w + df_b $$ Calculate the Mean Squares Calculate the Mean Squares Between (MSB) $$ MS_{between} = \\frac{SS_{between}}{df_{between}} $$ Calculate the Mean Squares Within (MSW) $$ MS_{within} = \\frac{SS_{within}}{df_{within}} $$ Calculate the F Statistic $$ F = \\frac{ MS_{between}}{MS_{within}} $$ get pvalue import scipy.stats as stat pvalue = stat.f.sf(F, dfb, dfw) # sf: pvalue = 1 - stat.f.cdf() ","date":"2020-06-30","objectID":"/2020-06-30-stats-anova/:1:0","tags":null,"title":"ANOVA","uri":"/2020-06-30-stats-anova/"},{"categories":["Statistic"],"content":"ANOVA Effect size Omega squared (œâ2) is a measure of effect size, or the degree of association for a population. It is an estimate of how much variance in the response variables are accounted for by the explanatory variables. Omega squared is widely viewed as a lesser biased alternative to eta-squared, especially when sample sizes are small. MSerror: mean square error SSE/df(error) Formula $$ \\omega^2 = \\frac {SS_{Effect} - df_{Effect} MS_{error}}{SS_{total} + MS_{error}} $$ for multi-factor, completely randomized design, Formula $$ \\omega^2 = \\frac {SS_{Effect} - df_{Effect} MS_{errors}} {SS_{Effect} + (N-df_{Effect}) MS_{error}} $$ Interpreting Results œâ2 can have values between ¬± 1. Zero indicates no effect. If the observed F is less than one, œâ2 will be negative. ","date":"2020-06-30","objectID":"/2020-06-30-stats-anova/:2:0","tags":null,"title":"ANOVA","uri":"/2020-06-30-stats-anova/"},{"categories":["Statistic"],"content":"ANOVA Post-hoc comparison ANOVA does not tell which group are significantly different from each other. To know the pairs of significant different groups, we could perform multiple pairwise comparison (Post-hoc comparison) analysis using Tukey HSD test. from statsmodels.stats.multicomp import pairwise_tukeyhsd # perform multiple pairwise comparison (Tukey HSD) m_comp = pairwise_tukeyhsd(endog=d_melt['value'], groups=d_melt['treatments'], alpha=0.05) ","date":"2020-06-30","objectID":"/2020-06-30-stats-anova/:3:0","tags":null,"title":"ANOVA","uri":"/2020-06-30-stats-anova/"},{"categories":["Statistic"],"content":"Two-way (two factor) ANOVA example # load packages import statsmodels.api as sm from statsmodels.formula.api import ols # Ordinary Least Squares (OLS) model # C(): as categorical # C(Genotype):C(years) represent interaction term model = ols('value ~ C(Genotype) + C(years) + C(Genotype):C(years)', data=d_melt).fit() anova_table = sm.stats.anova_lm(model, typ=2) anova_table see more about ANOVA in python here ","date":"2020-06-30","objectID":"/2020-06-30-stats-anova/:4:0","tags":null,"title":"ANOVA","uri":"/2020-06-30-stats-anova/"},{"categories":["Statistic"],"content":"Advanced ","date":"2020-06-30","objectID":"/2020-06-30-stats-anova/:5:0","tags":null,"title":"ANOVA","uri":"/2020-06-30-stats-anova/"},{"categories":["Statistic"],"content":"One way Anova is a multiple regression model $$ y = \\beta_{0} + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\cdots, H_0 : y = \\beta_0 $$ $x_i$Ôøº are indicators ( $x = \\lbraceÔøº 0,1\\rbrace$), where at most one $x_i = 1$ while all other $x_i = 0$. The Kruskal-wallis test (non-parametric test) is simply a one-way ANOVA on the rank-transformed y (value). $$ rank(y) = \\beta_{0} + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\cdots $$ ","date":"2020-06-30","objectID":"/2020-06-30-stats-anova/:5:1","tags":null,"title":"ANOVA","uri":"/2020-06-30-stats-anova/"},{"categories":["Statistic"],"content":"ANCOVA This is simply ANOVA with a continuous regressor added so that it now contains continuous and (dummy-coded) categorical predictors. $$ y = \\beta_{0} + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_n age $$ $\\beta_0$ is now the mean for the first group at age=0. ","date":"2020-06-30","objectID":"/2020-06-30-stats-anova/:5:2","tags":null,"title":"ANOVA","uri":"/2020-06-30-stats-anova/"},{"categories":["Statistic"],"content":"Reference See: Common statistical tests are linear models (or: how to teach stats) R version by Jonas Kristoffer Lindel√∏v Python port by George Ho ","date":"2020-06-30","objectID":"/2020-06-30-stats-anova/:5:3","tags":null,"title":"ANOVA","uri":"/2020-06-30-stats-anova/"},{"categories":["Statistic"],"content":"Censoring Censoring Surivial without Censoring Surivial with Censoring ","date":"2020-06-22","objectID":"/2020-06-22-stats-survival-analysis/:1:0","tags":null,"title":"Survival Analysis","uri":"/2020-06-22-stats-survival-analysis/"},{"categories":["Statistic"],"content":"Kaplan Meier Curve More individual in each group, better sepration of the group, better p-value Takes censoring into account Estimates probabilitu of ‚Äúsurvival‚Äù on a given day Conditional probability of surviving on a given day: $$ \\frac {N_{ \\text{‚Äúalive‚Äù day before}} - N_{ \\text{‚Äúdying‚Äù nextday}}} { \\text{‚Äúalive‚Äù day before}} $$ Kaplan-Meier survival curve Survival times $t_1 \\leq t_2 \\leq \\cdots \\leq t_n$ The proportion of subjects, $S(t)$, surviving beyoind any follow up time $t$ is estimated by (conditional probability): $$ S(t) = \\frac {r_1 - d_1}{r_1} \\times \\frac {r_2 - d_2}{r_2} \\times \\cdots \\times \\frac{r_p - d_p}{r_p} $$ where $t_p$ is the largest survival time less han or equal to $t$ $r_i$ is the number of subjects alive just before time $t_i$ $d_i$ = numebr who died at time $t_i$ for censored obeservations $d_i = 0$ ","date":"2020-06-22","objectID":"/2020-06-22-stats-survival-analysis/:2:0","tags":null,"title":"Survival Analysis","uri":"/2020-06-22-stats-survival-analysis/"},{"categories":["Statistic"],"content":"Statistic ","date":"2020-06-22","objectID":"/2020-06-22-stats-survival-analysis/:3:0","tags":null,"title":"Survival Analysis","uri":"/2020-06-22-stats-survival-analysis/"},{"categories":["Statistic"],"content":"Log Rank Test Compares survival times of two independent groups. Assumes that the relative risk of event (e.g. death) between the two groups is constant (proportional hazards) Ranks the survial times combined and compared observed and expected rates Null hypothesis: the rates of events (death) in the two groups are equal under $H_0$, $$ X^2 = \\frac { (O_A - E_A)^2}{E_A} + \\frac { (O_B - E_B)^2}{E_B} \\sim \\chi^2 $$ $O_A$: observed events in group A $E_A$: expected events in gorup A under null hypohesis expect = (proportion in risk set) * (# of failures over both groups) $$ e_{1j} = ( \\frac{ n_{1j}}{ n_{1j} + n_{2j}}) \\times ( m_{1j} + m_{2j}) $$ $$ e_{2j} = ( \\frac{ n_{2j}}{ n_{1j} + n_{2j}}) \\times ( m_{1j} + m_{2j}) $$ ","date":"2020-06-22","objectID":"/2020-06-22-stats-survival-analysis/:3:1","tags":null,"title":"Survival Analysis","uri":"/2020-06-22-stats-survival-analysis/"},{"categories":["Statistic"],"content":"Cox Regression Extends comparison of survial times to allow different predictors (estimate k variable together) Models the hazard: probability of dying at a point in time, given survival to that point in time $$ H(t) = H_0(t) \\times \\exp(b_1X_1 + b_2X_2 + \\cdots + b_kX_k ) $$ Model links to a baseline hazard, $H_0(t)$ Can accomodate many variables, both discrete and continuous measures of event times Proportional hazards assumption: the hazard for any individual is a fixed proportion of the hazard for any other individual Hazard ratio Exp(B) give the hazard ratio (or relative hazard/risk) ","date":"2020-06-22","objectID":"/2020-06-22-stats-survival-analysis/:3:2","tags":null,"title":"Survival Analysis","uri":"/2020-06-22-stats-survival-analysis/"},{"categories":["Nature Language Processing"],"content":"Basic concepts in NLP ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:0:0","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"NLP ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:0","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Languwage model MOdels that assigns probabilities to sequences of words are called languwage models. ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:1","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Count-based Representation one-hot representation BoW: Bag of words Bow describes the occurrence of words within a document. including A Vocabulary of known words A measure of the presence of known words, e.g. count TF or TF-IDF representation: Term Frequency Inverse Document Frequency TF: the sum of the one-hot representation of a phrase, sentence or document‚Äôs constituent words $$ TF (w) = \\frac { \\text{ Number of the term w appears in the document }} { \\text{Number of terms in the document}} $$ IDF: penalizes common tokens and rewards rare tokens $$ IDF(w) = \\log \\frac{\\text{Number of documents}}{\\text{Number of documents with term w}} $$ TF-IDF: $TF(w) \\times IDF(w)$ ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:2","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Corpora, Tokens, and Types corpus (plural: corpora): a text dataset tokens (English): words and numeric sequences separated by white-spaces characters or punctuation instance or data point: the text along with its metatdata dataset: a collection of instances types: unique tokens present in a corpus. vocabulary or lexicon: the set of all types in a corpus ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:3","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Unigram, Bigrams, Trigrams, ‚Ä¶ , N-grams N-grams are fixed-length consecutive token sequence occurring in the text ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:4","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Lemmas and Stems Lemmas are the root forms of words. e.g. the root form of the word fly, can be inflected into other words ‚Äì flow, flew, flies, flown, flowing ‚Ä¶ Stemming: use handcrafted rules to strip endings of words to reduce them to a common form called stems ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:5","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Word Senses and Semantics Senses: the different meanings of a word ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:6","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Categorizing Words: POS Tagging part-of-speech (POS) tagging: labeling individual words or tokens ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:7","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Categorizing Spans: Chunking and Named Entity Recognition a span of text: a contiguous multi-token boundary. chunking or shallow parsing: identify the noun phrases (NP) and verb pharses (VP) in a span of text. A named entity is a string mention of a real-world concept like a person, location, organization, drug name, et. al. ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:8","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Coreference The task of deciding whether two strings refer to same entity ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:9","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Minimumn Edit distance A way to quantify string similarity. ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:10","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Perplexity The perplexity (sometimes called PP) of a language model on a test set is the inverse probability of the test set, normalised by the number of words. $$\\begin{aligned} \\mathrm{PP}(W) \u0026=P(w_{1} w_{2} \\ldots w_{N})^{-\\frac{1}{N}} \\cr \u0026=\\sqrt[N]{\\frac{1}{P(w_{1} w_{2} \\ldots w_{N})}} \\cr \u0026= \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_{i} \\mid w_{1} \\ldots w_{i-1})}} \\end{aligned} $$ Another way to hink about perplexity: as the weighted average branching factor of a language. ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:11","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Entropy Entropy is a measure of information. The entropy of the random variable X is: $$ H(X)=-\\sum_{x \\in \\chi} p(x) \\log _{2} p(x) $$ the log can be computed in any base. If we use log base 2, the resulting value of entropy will mesured in bits. One intuitive way to think about entorpy is as a lower bound on the number of bits it would take to encode a certain desision or piece of information in the optimal coding scheme. ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:12","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Cross-entropy The cross-entropy is useful when we don‚Äôt know the actual probability distribution p that generated some data. The cross-entropy of m (a model of p) on p is defined by $$ H(p,m) = \\lim_{n \\rightarrow \\infty} - \\frac{1}{n}\\sum_{W \\in L} p (w_1,\\cdots,w_n) \\log m (w_1, cdots, w_n) $$ the cross-entropy $H(p,m)$ is an upper bound on the entropy $H(p)$. For any model m: $$ H(p) \\leq H(p,m) $$ The more accurate m is, the closer the cross-entropy $H(p,m)$ will be to the true entropy $H(p)$. ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:13","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"The relation of perplexity and cross-entropy The approximation to the cross-entropy of a model $M = P(w_i | W_{i-N+1} \\cdots W_{i-1})$ on a sequence of words W is $$ H(W) = - \\frac{1}{N} \\log P(w_1 w_2 \\cdots w_N) $$ The perplexity of a model P on a seqence of words W is defined as exp of this cross-entropy $$ \\begin{aligned} \\operatorname{Perplexity}(W) \u0026=2^{H(W)} \\cr \u0026=P\\left(w_{1} w_{2} \\ldots w_{N}\\right)^{-\\frac{1}{N}} \\cr \u0026=\\sqrt[N]{\\frac{1}{P\\left(w_{1} w_{2} \\ldots w_{N}\\right)}} \\cr \u0026=\\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P\\left(w_{i} \\mid w_{1} \\ldots w_{i-1}\\right)}} \\end{aligned} $$ ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:14","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Structure of Sentences: Parse trees Shallow parsing identifies phrasal units, the task of identifying the relationship between them is called parsing. Parse trees indicate how different grammatical units in a sentence are related hierachically. (aslo refer to constituent parse, chart-based ) dependency parsing: directed graph (graph-based) node -\u003e word edge -\u003e relation all the words have one incoming edge, except ROOT there is a unique path from each word to ROOT maximum spanning tree SyntaxNet: ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:1:15","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"statistical testing The approach to computing p-values(x) in NLP is to use non-parametric tests. e.g. bootstrap test approximate randomization bootstrapping refers to repeated drawing large numbers of smaller samples with replacement from an orignial larger sample. the intuition of the bootstrap test is that we can create many virtual test sets from an observed test set by repeated sampling from it. the method only maks the assumption that sample is representative of the population ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:2:0","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Nature Language Processing"],"content":"Reference Hung-yi Lee: Deep Learning for human language processing ","date":"2020-06-20","objectID":"/2020-06-20-nlp-basics/:3:0","tags":["NLP"],"title":"NLP concepts","uri":"/2020-06-20-nlp-basics/"},{"categories":["Statistic"],"content":"Ê†∑Êú¨Èáè„ÄÅÊïàÂ∫îÈáè„ÄÅÊòæËëóÊ∞¥Âπ≥ÂíåÁªüËÆ°ÂäüÊïàÁöÑÁªüËÆ°ÂéüÁêÜÂíåËÆ°ÁÆó ÊïàÂ∫îÈáèÈÄöÂ∏∏Áî®‰∏âÁßçÊñπÂºèÊù•Ë°°ÈáèÔºöÊ†áÂáÜÂùáÂ∑ÆÔºàstandardized mean differenceÔºâÔºåÂá†ÁéáÔºàodd ratioÔºâÔºå(3) Áõ∏ÂÖ≥Á≥ªÊï∞Ôºàcorrelation coefficientÔºâ„ÄÇ ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:0:0","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"ÁªüËÆ°ÂéüÁêÜ ÂèØËßÜÂåñÊ†∑Êú¨Èáè„ÄÅÊïàÂ∫îÈáè„ÄÅŒ±ÂíåÁªüËÆ°ÂäüÊïàÁöÑÂÖ≥Á≥ª Significance The probability of a type I error is usually denoted by $\\alpha$ and is commonly referred to as the signiÔ¨Åcance level of a test. \\The probability of a type II error is usually denoted by $\\beta$. ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:1:0","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"ÊòæËëóÊ∞¥Âπ≥ Œ± Type Error ÂÆö‰πâ Ë°®Á§∫ ‰∏æ‰æã ‚Ö† ÂûãÈîôËØØ ÊãíÁªùÂÆûÈôÖ‰∏äÊàêÁ´ãÁöÑ$H_0$ ‚Ö† ÂûãÈîôËØØÁöÑÊ¶ÇÁéáÁî®ÊòæËëóÊ∞¥Âπ≥ Œ± Ë°®Á§∫, ÂÅáÈò≥ÊÄß„ÄÅËØØËØä II ÂûãÈîôËØØ ÊãíÁªùÔºà‚ÄúÊé•Âèó‚ÄùÔºâÂÆûÈôÖ‰∏ä‰∏çÊàêÁ´ãÁöÑ$H_0$ II ÂûãÈîôËØØÊ¶ÇÁéáÁî® Œ≤ Ë°®Á§∫ ÂÅáÈò¥ÊÄß„ÄÅÊºèËØä Œ± Â∏∏ÂèñÂÄº0.05„ÄÅ0.01ÔºåŒ±ÂèØ‰ª•ÂèñÂçïÂ∞æ„ÄÅÂèåÂ∞æ„ÄÇÈúÄÂÅáËÆæÊ£ÄÈ™åÂâçÈ¢ÑÂÖàËÆæÂÆö„ÄÇ Œ≤ Âè™ÂèñÂçïÂ∞æ ÂΩ¢Ë±°ÂåñÁêÜËß£ ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:1:1","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"ÂäüÊïàÔºàpowerÔºâ ÂäüÊïàÔºàpowerÔºâÔºöÊ≠£Á°ÆÊãíÁªùÂéüÂÅáËÆæÁöÑÊ¶ÇÁéáÔºåËÆ∞‰Ωú1-Œ≤, Âç≥ $$ 1 - \\beta = \\operatorname{Pr} ( \\text{rejecting } H_0 | H_1 true) $$ ÂÅáËÆæÊ£ÄÈ™åÁöÑÂäüÊïàÂèó‰ª•‰∏ã‰∏â‰∏™Âõ†Á¥†ÂΩ±ÂìçÔºö Ê†∑Êú¨Èáè (n)ÔºöÂÖ∂‰ªñÊù°‰ª∂‰øùÊåÅ‰∏çÂèòÔºåÊ†∑Êú¨ÈáèË∂äÂ§ßÔºåÂäüÊïàÂ∞±Ë∂äÂ§ß„ÄÇ ÊòæËëóÊÄßÊ∞¥Âπ≥ (Œ±)Ôºö ÂÖ∂‰ªñÊù°‰ª∂‰øùÊåÅ‰∏çÂèòÔºåÊòæËëóÊÄßÊ∞¥Âπ≥Ë∂ä‰ΩéÔºåÂäüÊïàÂ∞±Ë∂äÂ∞è„ÄÇ ‰∏§ÊÄª‰Ωì‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºöÂÖ∂‰ªñÊù°‰ª∂‰øùÊåÅ‰∏çÂèòÔºåÊÄª‰ΩìÂèÇÊï∞ÁöÑÁúüÂÆûÂÄºÂíå‰º∞ËÆ°ÂÄº‰πãÈó¥ÁöÑÂ∑ÆÂºÇË∂äÂ§ßÔºåÂäüÊïàÂ∞±Ë∂äÂ§ß„ÄÇ‰πüÂèØ‰ª•ËØ¥ÔºåÊïàÂ∫îÈáèÔºàeffect sizeÔºâË∂äÂ§ßÔºåÂäüÊïàÂ∞±Ë∂äÂ§ß„ÄÇ ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:1:2","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"ÊïàÂ∫îÈáèÔºàeffect sizeÔºâ ÊïàÂ∫îÈáèÔºö Ê†∑Êú¨Èó¥Â∑ÆÂºÇÊàñÁõ∏ÂÖ≥Á®ãÂ∫¶ÁöÑÈáèÂåñÊåáÊ†á„ÄÇÊïàÂ∫îÈáèË∂äÂ§ßÔºåË°®Á§∫‰∏§‰∏™ÊÄª‰ΩìÈáçÂè†ÁöÑÁ®ãÂ∫¶Ë∂äÂ∞èÔºåÊïàÂ∫îË∂äÊòéÊòæ„ÄÇ ÊïàÂ∫îÈáèÈÄöÂ∏∏Áî®‰∏âÁßçÊñπÂºèÊù•Ë°°ÈáèÔºö(1) Ê†áÂáÜÂùáÂ∑ÆÔºàstandardized mean differenceÔºâÔºå(2) Âá†ÁéáÔºàodd ratioÔºâÔºå(3) Áõ∏ÂÖ≥Á≥ªÊï∞Ôºàcorrelation coefficientÔºâ„ÄÇ Difference family: Effect sizes based on differences between means Ê†áÂáÜÂùáÂ∑ÆÔºàstandardized mean differenceÔºâ Standardized mean difference: Âü∫‰∫éÊÄª‰ΩìÂùáÂÄºÂíåÊñπÂ∑ÆÔºå ÊïàÂ∫îÈáè‰∏∫ $$ \\theta = \\frac{\\mu_1 - \\mu_2}{\\sigma} $$ Cohen‚Äôs d : ‰∏§ÊÄª‰ΩìÂùáÂÄº‰πãÈó¥ÁöÑÊ†áÂáÜÂ∑ÆÂºÇ„ÄÇÈÄÇÁî®‰∫é‰∏§ÁªÑÊ†∑Êú¨ÁöÑÊ†∑Êú¨ÈáèÂíåÊñπÂ∑ÆÁõ∏‰ººÁöÑÊÉÖÂÜµ„ÄÇ $$ d = \\frac{ \\bar{x}_1 - \\bar{x}_2}{s} = \\frac{\\mu_1 - \\mu_2}{\\sigma} $$ sÊòØÊ†∑Êú¨ÊñπÂ∑Æ $$ s = \\sqrt{\\frac{(n_1 - 1)s^2_1 + (n_2 - 1)s^2_2}{n_1 + n_2}} $$ $$ s_1 = \\frac{1}{n_1 -1} \\sum_{i=1}^{n_1} (x_{1,i} - \\bar{x}_1)^2 $$ d = 0.01 to 2.0 Effect size d Reference Very small 0.01 Sawilowsky, 2009 Small 0.20 Cohen, 1988 Medium 0.50 Cohen, 1988 Large 0.80 Cohen, 1988 Very large 1.20 Sawilowsky, 2009 Huge 2.0 Sawilowsky, 2009 Hedges‚Äô g: ÊòØcohenÁöÑÊñπÊ≥ïÁöÑÊîπËøõÔºåÈÄÇÁî®‰∫é‰∏§ÁªÑÊ†∑Êú¨ÁöÑÊ†∑Êú¨Èáè‰∏çÂêåÁöÑÊÉÖÂÜµ„ÄÇ $$ g = \\frac{ \\bar{x}_1 - \\bar{x}_2}{s^*} $$ ËÄå$s^{*}$ÊòØ $$ s^* = \\sqrt{\\frac{(n_1 - 1)s^2_1 + (n_2 - 1)s^2_2}{n_1 + n_2 -2 }} $$ Glass‚Äôs Œî ÔºàdeltaÔºâ: ÂíåcohenÁöÑÊñπÊ≥ïÁ±ª‰ººÔºå‰ΩÜÊòØÂè™Èô§‰ª•ÊéßÂà∂ÁªÑ(control)ÁöÑÊ†áÂáÜÂ∑Æ„ÄÇÈÄÇÁî®‰∫é‰∏§ÁªÑÊ†∑Êú¨ÁöÑÊñπÂ∑Æ‰∏çÂêåÁöÑÊÉÖÂÜµ„ÄÇ $$ \\Delta = \\frac{ \\bar{x}_1 - \\bar{x}_2}{s_2} $$ Categorical family: Effect sizes for associations among categorical variables Odd ratio (OR) The odds ratio is the odds of success in the treatment group relative to the odds of success in the control group. ÈÄÇÁî®‰∫ébinaryÊï∞ÊçÆ„ÄÇ Relative risk (RR) or risk ratio the risk (probability) of an event relative to some independent variable. Risk difference or absolute risk reduction the difference in risk (probability) of an event between two groups Cramer‚Äôs œÜ (Phi) or Cramer‚Äôs V: Áî®‰∫éÊµãÁÆóÁ±ªÂà´ÂûãÊï∞ÊçÆ (nominal data) ÁöÑÊïàÂ∫îÈáè„ÄÇÂΩìÁ±ªÂà´ÂûãÂèòÈáèÂåÖÂê´2‰∏™Á±ªÂà´Êó∂Ôºå‰ΩøÁî®Cramer‚Äôs phiÔºåÂ¶ÇÊûúË∂ÖËøá2‰∏™Á±ªÂà´ÔºåÈÇ£‰πà‰ΩøÁî®Cramer‚Äôs V„ÄÇ Cohen‚Äôs w ‚Ä¶ Correlation family: Effect sizes based on ‚Äúvariance explained‚Äù Pearson r correlation Effect size r small ~ 0.1 medium ~ 0.3 large r \u003e 0.5 Cohen‚Äôs $f^2$: Áî®‰∫éÊµãÁÆóÊñπÂ∑ÆÂàÜÊûêANOVAÔºåÂ§öÂÖÉÂõûÂΩí‰πãÁ±ªÁöÑÊïàÂ∫îÈáè„ÄÇ Â§öÂÖÉÂõûÂΩíÁöÑÊïàÂ∫îÈáè $$ f^2 = \\frac{R^2}{1-R^2} $$ where $R^2$ is the squared multiple correlation (Coefficient of determination) ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:1:3","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"ÂäüÊïà„ÄÅÊïàÂ∫îÈáèÂíåÊ†∑Êú¨ÈáèËÆ°ÁÆó ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:2:0","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"ËÆ°ÁÆóÊ†∑Êú¨Èáè determining-sample-size ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:2:1","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"ËÆ°ÁÆóÊïàÂ∫îÈáè Áî®statsmodelsÂ∫ìËÆ°ÁÆóÂäüÊïàÔºåÊïàÂ∫îÈáèÂíåÊ†∑Êú¨ÈáèÁöÑÂáΩÊï∞ÈÉΩÊòØÂêå‰∏Ä‰∏™ÔºåÂè™Ë¶ÅÊääÈúÄË¶ÅËÆ°ÁÆóÁöÑÈÇ£‰∏™ÂÄº‰ªçÁÑ∂ËÆæ‰∏∫NoneÔºåÊääÂÖ∂‰ªñÊÉ≥Ë¶ÅËææÂà∞ÁöÑÊï∞ÂÄºÂ°´‰∏äÂç≥ÂèØ ÂçïÊ†∑Êú¨tÊ£ÄÈ™åÔºö statsmodels.stats.power.tt_solve_power(effect_size=None, nobs=None, alpha=None, power=None, alternative='two-sided') Áã¨Á´ãÊ†∑Êú¨tÊ£ÄÈ™åÔºö statsmodels.stats.power.tt_ind_solve_power(effect_size=None, nobs1=None, alpha=None, power=None, ratio=1.0, alternative='two-sided') Âç°ÊñπÊãüÂêà‰ºòÂ∫¶Ê£ÄÈ™åÔºö statsmodels.stats.power.GofChisquarePower.solve_power(effect_size=None, nobs=None, alpha=None, power=None, n_bins=2) FÊñπÂ∑ÆÈΩêÊÄßÊ£ÄÈ™åÔºö statsmodels.stats.power.FTestPower.solve_power(effect_size=None, df_num=None, df_denom=None, nobs=None, alpha=None, power=None, ncc=1) ÊñπÂ∑ÆÂàÜÊûêÔºö statsmodels.stats.power.FTestAnovaPower.solve_power(effect_size=None, nobs=None, alpha=None, power=None, k_groups=2) ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:2:2","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"StatQuest Sample size Power Analysis ÂèÇËÄÉÔºö https://en.wikipedia.org/wiki/Effect_size https://www.cnblogs.com/HuZihu/p/12009535.html ","date":"2020-06-14","objectID":"/2020-06-14-stats-effectsize/:3:0","tags":null,"title":"Power, Effect size, Sample size","uri":"/2020-06-14-stats-effectsize/"},{"categories":["Statistic"],"content":"Quantile normalization is frequently used in microarray data analysis. It was introduced as quantile standardization and then renamed as quantile normalization. ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:0:0","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"Quantile, quartile, percentile ??? Quantiles are just the lines that divide data into equally sized groups. percentiles are just quantiles that divide the data into 100 equally sized groups Example: 0 quartile = 0 quantile = 0th percentile 1 quartile = 0.25 quantile = 25th percentile 2 quartile = .5 quantile = 50th percentile (median) 3 quartile = .75 quantile = 75th percentile 4 quartile = 1 quantile = 100th percentile ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:1:0","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"Quantile normalization Quantile normalization transform the statistical distributions across samples to be the same. ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:2:0","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"Assumptions The roughly same distribution of values across samples Most genes are not differentially expressed Assume global differences in the distribution are induced by only technical variation! ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:2:1","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"How Q-normalization work row: genes column: samples/Arrays Procedure: order values within each sample determine a rank from lowest to highest and record the order within each sample Average across rows and substitute value with average re-order averaged values in the original order recorded in 2. Tied rank entries ? Average the tied rank entries‚Äô mean values and substitute. ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:2:2","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"When NOT to normalize Consider a dilution experiment. In which distributions are supposed to decrease (left plot), Q-normalization does the totally wrong thing (right plot). When you expect a real difference in distributions, Q-normalization will create weird artifacts. ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:2:3","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"Smooth quantile ormalizaiton ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:3:0","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"Assumptions the statistical distribution of each sample should be the same ( or have the same distributional shape) within biological groups or conditions, but allowing that they may differ between groups ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:3:1","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"How to At each quantile, a weight is computed comparing the variability between groups relative to the total variability between and within groups Let gene(g) denote the $g^{th}$ row after sorting each column in the data. For each row, gene(g), we compute the weight $w(g)$ ‚àà [0,1], where a weight of 0 implies quantile normalization within groups is applied and a weight of 1 indicates quantile normalization is applied. The weight at each row depends on the between group sum of squares SSB(g) and total sum of squares SST(g), as follows: $$ w_{(g)} = \\operatorname{median} \\bigg\\lbrace 1- \\frac{SSB_{(i)}}{SST_{(i)}} \\bigg\\rbrace \\text{for } i = g -k, \\cdots, g, \\cdots, g+k $$ where $k$ = floor(Total number of genes * 0.05). The number 0.05 is a flexible parameter that can be altered to change the window of the number of genes considered. ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:3:2","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"StatQuest: Quantile Normalization ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:4:0","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"reference https://en.wikipedia.org/wiki/Quantile_normalization BIOMEDIN 245: Statistical and Machine Learning Methods for Genomics, Stanford Hicks SC, Okrah K, Paulson JN, Quackenbush J, Irizarry RA, Bravo HC. Smooth quantile normalization. Biostatistics. 2018;19(2):185‚Äê198. doi:10.1093/biostatistics/kxx028 ","date":"2020-06-14","objectID":"/2020-06-14-stats-quantile-normalization/:5:0","tags":null,"title":"Quantile normalization","uri":"/2020-06-14-stats-quantile-normalization/"},{"categories":["Statistic"],"content":"What‚Äôs Multilevel models, and how to deal with it ","date":"2020-06-13","objectID":"/2020-06-13-stats-hierachial-models/:0:0","tags":null,"title":"Multilevel (Hierachical) Models","uri":"/2020-06-13-stats-hierachial-models/"},{"categories":["Statistic"],"content":"What is multilevel model Multilevel model AKA: multilevel Models random-effects models hierarchical models variance-components models random-coefficient models mixed models Many kinds of data, including observational data collected in the human and biological sciences, have a hierarchical or clustered structure, or non-hierarchical structures.. ","date":"2020-06-13","objectID":"/2020-06-13-stats-hierachial-models/:1:0","tags":null,"title":"Multilevel (Hierachical) Models","uri":"/2020-06-13-stats-hierachial-models/"},{"categories":["Statistic"],"content":"A Simple Example Given a set of repeated measures data giving growth patterns for a sample of 26 boys in Oxford, England. The height of each boy is measured on nine different occasions. We could try modelling the growth pattern with a simple linear regression $$ H = \\beta_0 + \\beta_1 A + \\epsilon $$ where H: height A: age $\\epsilon$: the variation in height that cannot be explained by the linear relationship with age. However, If we try to use the model above for the complete set of data, the fit will be very poor (see figure above) To make model more realistic, we allow the intercept in the model above to vary from subject to subject. New multilevel model: $$ H_{ij} = \\beta_{0j} + \\beta_1 A_{ji} + \\epsilon_{ij} $$ Now, assume that the individual intercepts follow a normal distribution with variance $\\tau_{0}$, $$ \\beta_{0j} = \\beta_{0} + \\mu_{0j} $$ where, $\\mu_{0j} \\sim \\mathcal{N} (0, \\tau_0)$, $\\mu_{0j}$ accounts for the variation from one subject to another Fitting the multilevel model to the data, and obtain much better predictions ","date":"2020-06-13","objectID":"/2020-06-13-stats-hierachial-models/:2:0","tags":null,"title":"Multilevel (Hierachical) Models","uri":"/2020-06-13-stats-hierachial-models/"},{"categories":["Statistic"],"content":"How do multilevel models differ from regression models? $$ H_{ij} = (\\beta_0 + \\mu_{0j}) + \\beta_1 A_{ij} + \\epsilon_{ij} = \\beta_0 + \\beta_1 A_{ij} + \\mu_{0j} + \\epsilon_{ij} $$ The feature that distinguishes this model from an ordinary regression model is the presence of two random variables the measurement level random variable $\\epsilon_{ij}$ the subject level random variable $\\mu_{0j}$ Because multilevel models contain a mix of fixed effects and random effects, they are sometimes known as mixed-effects models. ","date":"2020-06-13","objectID":"/2020-06-13-stats-hierachial-models/:3:0","tags":null,"title":"Multilevel (Hierachical) Models","uri":"/2020-06-13-stats-hierachial-models/"},{"categories":["Statistic"],"content":"Benefits of multilevel modelling Generalize to a wider population Fewer parameters are needed Information can be shared between groups ","date":"2020-06-13","objectID":"/2020-06-13-stats-hierachial-models/:4:0","tags":null,"title":"Multilevel (Hierachical) Models","uri":"/2020-06-13-stats-hierachial-models/"},{"categories":["Statistic"],"content":"How to deal with hierachical structures Hierarchical structures : model all levels simultaneously Non- Hierarchical structures cross-classified structure multiple membership with weights ‚Ä¶ ÂèÇËÄÉÔºö https://en.wikipedia.org/wiki/Multilevel_model http://www.statstutor.ac.uk/resources/uploaded/multilevelmodelling.pdf https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/hierarchical-models.pdf ","date":"2020-06-13","objectID":"/2020-06-13-stats-hierachial-models/:5:0","tags":null,"title":"Multilevel (Hierachical) Models","uri":"/2020-06-13-stats-hierachial-models/"},{"categories":["Machine Learning"],"content":"ËíôÁâπÂç°ÁΩóÊñπÊ≥ïÔºåÂèàÁß∞ÁªüËÆ°Ê®°ÊãüÊñπÊ≥ï(statistical simulation method), ÈÄöËøáÊ¶ÇÁéáÊ®°ÂûãÁöÑÈöèÊú∫ÊäΩÊ†∑ËøõË°åËøõË°åËøë‰ººÊï∞ÂÄºËÆ°ÁÆóÁöÑÊñπÊ≥ï„ÄÇ È©¨ÂèØÂ§´ËíôÁâπÂç°ÁΩóÊ≥ïÔºàMarkov Chain Monte Carlo, MCMCÔºâÂàôÊòØ‰ª•È©¨ÂèØÂ§´Èìæ‰∏∫Ê¶ÇÁéáÊ®°ÂûãÁöÑËíôÁâπÂç°ÁΩóÊñπÊ≥ï„ÄÇ Metropolis-HastingsÁÆóÊ≥ïÊòØÊúÄÂü∫Êú¨ÁöÑMCMC„ÄÇ Gibbs samplingÊòØÊõ¥ÁÆÄÂçï„ÄÅ‰ΩøÁî®Êõ¥ÂπøÊ≥õÁöÑMCMC„ÄÇ Markov Chain Monte Carlo (MCMC) ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:0:0","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"ËíôÁâπÂç°ÁΩóÊ≥ïÔºàMonte CarloÔºâ ËíôÁâπÂç°ÁΩóÊ≥ïË¶ÅËß£ÂÜ≥ÁöÑÈóÆÈ¢òÊòØÔºåÂÅáËÆæÊ¶ÇÁéáÂàÜÂ∏ÉÁöÑÂÆö‰πâÂ∑±Áü•ÔºåÈÄöËøáÊäΩÊ†∑Ëé∑ÂæóÊ¶ÇÁéáÂàÜÂ∏ÉÁöÑÈöèÊú∫Ê†∑Êú¨ÔºåÂπ∂ÈÄöËøáÂæóÂà∞ÁöÑÈöèÊú∫Ê†∑Êú¨ÂØπÊ¶ÇÁéáÂàÜÂ∏ÉÁöÑÁâπÂæÅËøõË°åÂàÜÊûê ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:1:0","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"1. ËíôÁâπÂç°ÁΩóÊñπÊ≥ïÁöÑÊ†∏ÂøÉ ËíôÁâπÂç°ÁΩóÊñπÊ≥ïÁöÑÊ†∏ÂøÉÊòØÈöèÊú∫ÊäΩÊ†∑(random sampling) Áõ¥Êé•ÊäΩÊ†∑ Êé•Âèó-ÊãíÁªùÊäΩÊ†∑Ôºö ÈÄÇÁî®‰∫éÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞Â§çÊùÇÔºå‰∏çËÉΩÁõ¥Êé•ÊäΩÊ†∑ÁöÑÊÉÖÂÜµ ÈáçË¶ÅÊÄßÊäΩÊ†∑Ôºö ÈÄÇÁî®‰∫éÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞Â§çÊùÇÔºå‰∏çËÉΩÁõ¥Êé•ÊäΩÊ†∑ÁöÑÊÉÖÂÜµ Êé•Âèó-ÊãíÁªùÊäΩÊ†∑ÊÄùÊÉ≥ÔºöÊâæ‰∏Ä‰∏™ÂèØ‰ª•Áõ¥Êé•ÊäΩÊ†∑ÁöÑÂª∫ËÆÆÂàÜÂ∏ÉÔºàproposal distributionÔºâÔºåÂÖ∂Ê¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞‰∏∫$q(x)$, Âπ∂‰∏î$q(x)$ÁöÑ$c$ÂÄç‰∏ÄÂÆöÂ§ß‰∫é$p(x)$Ôºå ÂÖ∂‰∏≠$c \u003e 0$,ÊåâÁÖß$q(x)$ËøõË°åÊäΩÊ†∑ÔºåÂÅáËÆæÂæóÂà∞ÁªìÊûú$x^\\ast$Ôºå ÂÜçÊåâÁÖß$\\frac{p(x^\\ast)}{cq(x^\\ast )}$ÁöÑÊØî‰æãÈöèÊú∫ÂÜ≥ÂÆöÊòØÂê¶Êé•Âèó$x^\\ast$„ÄÇËêΩÂà∞$p(x)$ËåÉÂõ¥ÂÜÖÁöÑÂ∞±Êé•ÂèóÔºåËêΩÂà∞$p(x)$ËåÉÂõ¥Â§ñÁöÑÂ∞±ÊãíÁªù‚ùå„ÄÇ Ëøô‰∫õÊäΩÊ†∑ÊñπÊ≥ïÁöÑÁº∫ÁÇπÔºö ÊäΩÊ†∑ÊïàÁéá‰ΩéÔºå ÊØîÂ¶Ç $p(x)$ Âç† $cq(x)$ Ê∂µÁõñ‰ΩìÁßØÊØî‰æãÂæà‰Ωé ÂΩìx‰∏∫È´òÁª¥Êï∞ÊçÆÊó∂ÔºåÂæàÈöæÂØªÊâæÂêàÈÄÇÁöÑÂª∫ËÆÆÂàÜÂ∏É ‰∏Ä‰∏™Ëß£ÂÜ≥ÂäûÊ≥ïÂ∞±ÊòØMCMC. ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:1:1","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"2. Êï∞Â≠¶ÊúüÊúõ‰º∞ËÆ°(Estimation of mathematical expectation) ÊåâÁÖßÊ¶ÇÁéáÂàÜÂ∏É $p(x)$ Áã¨Á´ãÊäΩÂèñn‰∏™Ê†∑Êú¨ÂêéËÆ°ÁÆóÂáΩÊï∞ÁöÑÊ†∑Êú¨ÂùáÂÄº $$ \\hat f_{n}=\\frac{1}{n} \\sum_{i=1}^{n} f\\left(x_{i}\\right) $$ ‰Ωú‰∏∫Êï∞Â≠¶ÊúüÊúõÁöÑËøë‰ººÂÄº„ÄÇ Ê†πÊçÆÂ§ßÊï∞ÂÆöÂæãÂèØÁü•ÔºåÂΩìÊ†∑Êú¨ÂÆπÈáèÂ¢ûÂ§ßÊòØÔºåÊ†∑Êú¨ÂùáÂÄº‰ª•Ê¶ÇÁéá1Êî∂ÊïõÊÄß‰∫éÊï∞Â≠¶ÊúüÊúõ $$ \\hat f_{n} \\rightarrow E_{p(x)}[f(x)], \\quad n \\rightarrow \\infty $$ ‰∫éÊòØÔºåÂæóÂà∞Êï∞Â≠¶ÊúüÊúõÁöÑËøë‰ººËÆ°ÁÆóÊñπÊ≥ï $$ E_{p(x)}[f(x)] \\approx \\frac{1}{n} \\sum_{i=1}^{n} f\\left(x_{i}\\right) $$ ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:1:2","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"3. ËíôÁâπÂç°ÁΩóÁßØÂàÜÔºàMonte carlo intergrationÔºâ ËÆ°ÁÆóÂáΩÊï∞ $h(x)$ ÁßØÂàÜ $$ \\int_{\\mathcal{X}} h(x) \\mathrm{d} x $$ Â∞Ü $h(x)$ ÂàÜËß£Êàê $f(x)$ ÂíåÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ $p(x)$ ÁöÑ‰πòÁßØÔºåÂç≥ÂáΩÊï∞ $h(x)$ ÁöÑÁßØÂàÜÂèØ‰ª•Ë°®Á§∫‰∏∫ÂáΩÊï∞ $f(x)$ ÂÖ≥‰∫éÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ $p(x)$ ÁöÑÊï∞Â≠¶ÊúüÊúõÔºö $$ \\int_{\\mathcal{X}} h(x) \\mathrm{d} x=\\int_{\\mathcal{X}} f(x) p(x) \\mathrm{d} x=E_{p(x)}[f(x)] $$ Âõ†Ê≠§ÔºåÂèØÂà©Áî®Ê†∑Êú¨ÂùáÂÄºËÆ°ÁÆóËøë‰ººÁßØÂàÜÔºö $$ \\int_{\\mathcal{X}} h(x) \\mathrm{d} x=E_{p(x)}[f(x)] \\approx \\frac{1}{n} \\sum_{i=1}^{n} f\\left(x_{i}\\right) $$ Êõ¥Ëøõ‰∏ÄÊ≠• $$ \\begin{aligned} E_{p(z)}[f(z)] \u0026= \\int f(z) p(z) dz \\cr \u0026= \\int \\underbrace{f(z) \\frac{p(z)}{q(z)}}_{new \\tilde{f} (z)} q(z) dz \\cr \u0026 \\approx \\frac{1}{N} \\sum_{n=1}^{N} f(z^{i}) \\frac{p(z^{i})}{q(z^{i})} \\end{aligned} $$ ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:1:3","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"Markov Chain ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:2:0","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"ÂÆö‰πâ È©¨ÂèØÂ§´ÊÄßÔºö ÈöèÊú∫ÂèòÈáè$X_t$Âè™‰æùËµñ$X_{t-1}$ÔºåËÄå‰∏ç‰æùËµñËøáÂéªÁöÑÈöèÊú∫ÂèòÈáè $\\lbrace X_{0}, X_{1}, \\cdots, X_{t-2} \\rbrace$„ÄÇÂç≥ $$ P\\left(X_{t} | X_{0}, X_{1}, \\cdots, X_{t-1}\\right)=P\\left(X_{t} | X_{t-1}\\right), \\quad t=1,2, \\cdots $$ È©¨ÂèØÂ§´ÈìæÊàñÈ©¨ÂèØÂ§´ËøáÁ®ãÔºàmarkov processÔºâÊåáÔºö ÂÖ∑ÊúâÈ©¨ÂèØÂ§´ÊÄßÁöÑÈöèÊú∫Â∫èÂàó $X=\\lbrace X_{0}, X_{1}, \\cdots, X_{t}, \\cdots \\rbrace$„ÄÇ È©¨ÂèØÂ§´ÈìæÁöÑËΩ¨ÁßªÊù°‰ª∂Ê¶ÇÁéáÂàÜÂ∏É‰∏∫ $P(X_t | X_{t-1})$ „ÄÇËΩ¨ÁßªÊ¶ÇÁéáÂàÜÂ∏ÉÂÜ≥ÂÆöÈ©¨ÂèØÂ§´ÈìæÁöÑÁâπÊÄß„ÄÇ Êó∂Èó¥ÈΩêÊ¨°È©¨ÂèØÂ§´ÈìæÔºàtime homogenous Markov ChainÔºâÊòØÊåáËΩ¨ÁßªÁä∂ÊÄÅÂàÜÂ∏É‰∫étÊó†ÂÖ≥ÁöÑÈ©¨ÂèØÂ§´Èìæ Á¶ªÊï£Áä∂ÊÄÅÈ©¨ÂèØÂ§´Èìæ Áä∂ÊÄÅËΩ¨ÁßªÁü©Èòµ Âπ≥Á®≥ÂàÜÂ∏ÉÔºö È©¨ÂèØÂ§´Èìæ $X$Ôºå ÂÖ∂Áä∂ÊÄÅÁ©∫Èó¥‰∏∫$\\mathcal{S}$Ôºå ËΩ¨ÁßªÁü©Èòµ‰∏∫ $P = (p_{ij})$Ôºå Â¶ÇÊûúÂ≠òÂú®Áä∂ÊÄÅÁ©∫Èó¥ $\\mathcal{S}$ ‰∏äÁöÑ‰∏Ä‰∏™ÂàÜÂ∏É $$ \\pi = \\left[\\begin{array}{c} \\pi_1 \\cr \\pi_2 \\cr \\vdots \\end{array}\\right] $$ ‰ΩøÂæó $\\pi=P\\pi$, ÂàôÁß∞$\\pi$‰∏∫È©¨ÂèØÂ§´Èìæ$X = {X_0, X_1, \\cdots, X_t, \\cdots }$ ÁöÑÂπ≥Á®≥ÂàÜÂ∏É ËøûÁª≠Áä∂ÊÄÅÈ©¨ÂèØÂ§´Èìæ ÂÆö‰πâÂú®ËøûÁª≠Áä∂ÊÄÅÁ©∫Èó¥ÔºåËΩ¨ÁßªÊ¶ÇÁéáÂàÜÂ∏ÉÊúâÊ¶ÇÁéáËΩ¨ÁßªÊ†∏Ôºàtrainsition kernelÔºâË°®Á§∫ $$ P(x, A) = \\int_{A} p(x, y) dy $$ ËΩ¨ÁßªÊ†∏$P(x, A)$Ë°®Á§∫ËΩ¨ÁßªÊ¶ÇÁéá $$ P (X_t = A | X_{t-1} = x) = P (x, A) $$ È©¨ÂèØÂ§´ÈìæÁöÑÊÄßË¥® ‰∏çÂèØÁ∫¶ (irreducible): Êó∂Âàª 0 ‰ªéÁä∂ÊÄÅ $j$ Âá∫ÂèëÔºåÊó∂Âàª $t$ Âà∞ËææÁä∂ÊÄÅ $i$ ÁöÑÊ¶ÇÁéáÂ§ß‰∫é 0 ÔºåÂàôÁß∞Ê≠§È©¨Â∞îÂèØÂ§´Èìæ $X$ ÊòØ‰∏çÂèØÁ∫¶ÁöÑ $$ P(X_t = i | X_0 = j) \u003e 0 $$ ÈùûÂë®ÊúüÔºö‰∏çÁ∫ØÂú®‰∏Ä‰∏™Áä∂ÊÄÅÔºå‰ΩøÂæóÂÜçËøîÂõûÂà∞Ëøô‰∏™Áä∂ÊÄÅÊâÄÁªèÂéÜÁöÑÊó∂Èó¥ÈïøÂëàÂë®ÊúüÊÄß Ê≠£Â∏∏Ëøî(positive recurrent): ‰ªªÊÑè‰∏Ä‰∏™Áä∂ÊÄÅ$i$Ôºå‰ªéÂÖ∂‰ªñ‰ªªÊÑèÁä∂ÊÄÅ $j$ Âá∫ÂèëÔºåÂΩìÊó∂Èó¥Ë∂ãËøëÊó†Á©∑Êó∂ÔºåÈ¶ñÊ¨°ËΩ¨ÁßªÂà∞Ëøô‰∏™Áä∂ÊÄÅ$i$ÁöÑÊ¶ÇÁéá $p^t_{ij}$ ‰∏ç‰∏∫0 $$ \\lim_{t \\rightarrow \\infty} p^t_{ij} \u003e 0 $$ ÈÅçÂéÜÂÆöÁêÜÔºöÊª°Ë∂≥Áõ∏Â∫îÊù°‰ª∂ÁöÑÈ©¨Â∞îÂèØÂ§´ÈìæÔºåÂΩìÊó∂Èó¥Ë∂ã‰∫éÊó†Á©∑Êó∂ÔºåÈ©¨Â∞îÂèØ Â§´ÈìæÁöÑÁä∂ÊÄÅÂàÜÂ∏ÉË∂ãËøë‰∫éÂπ≥Á®≥ÂàÜÂ∏ÉÔºåÈöèÊú∫ÂèòÈáèÁöÑÂáΩÊï∞ÁöÑÊ†∑Êú¨ÂùáÂÄº‰ª•Ê¶ÇÁéá 1 Êî∂Êïõ‰∫éËØ•ÂáΩÊï∞ ÁöÑÊï∞Â≠¶ÊúüÊúõ È©¨ÂèØÂ§´Èìæ $X$Ôºå ÂÖ∂Áä∂ÊÄÅÁ©∫Èó¥‰∏∫$\\mathcal{S}$Ôºå Ëã•È©¨ÂèØÂ§´Èìæ $X$ ‰∏çÂèØÁ∫¶„ÄÅÈùûÂë®Êúü‰∏îÊ≠£Â∏∏ËøîÔºå ÂàôÈ©¨ÂèØÂ§´ÈìæÊúâÂîØ‰∏ÄÁöÑÂπ≥Á®≥ÂàÜÂ∏É $\\pi = (\\pi_1, \\pi_2, \\cdots)^T$Ôºå Âπ∂‰∏îËΩ¨ÁßªÊ¶ÇÁéáÁöÑÊûÅÈôêÂàÜÂ∏ÉÊòØÈ©¨ÂèØÂ§´ÈìæÁöÑÂπ≥Á®≥ÂàÜÂ∏É $$ \\lim_{t \\rightarrow \\infty} P(X_t = i | X_0 = j) = \\pi_i, i = 1,2, \\cdots ; j = 1,2,\\cdots $$ Ëã• $f(X)$ÊòØÂÆö‰πâÂú®Áä∂ÊÄÅÁ©∫Èó¥‰∏äÁöÑÂáΩÊï∞ $E_{pi}[ | f(X) | ] \u003c \\infty$, Âàô $$ P { \\hat{f_t} \\rightarrow E_{pi}[ f(X) ] } = 1 $$ ËøôÈáåÔºå $$ \\hat{f_t} = \\frac{1}{t} \\sum^t_{s=1} f(x_s) $$ ÂÖ≥‰∫éÂπ≥Á®≥ÂàÜÂ∏É $\\pi = (\\pi_1, \\pi_2, \\cdots)^T$ ÁöÑÊï∞Â≠¶ÊúüÊúõ $E_{pi}[f(X)] = \\sum f(i)\\pi_i$, Êúâ $$ \\hat{f_t} \\rightarrow E_{pi}[ f(X) ], t \\rightarrow \\infty $$ Â§ÑÂ§ÑÊàêÁ´ãÊàñ‰ª•Ê¶ÇÁéá1ÊàêÁ´ã„ÄÇ ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:2:1","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"Markov Chain Monte Carlo È©¨ÂèØÂ§´ËíôÁâπÂç°ÁΩóÊ≥ïÊõ¥ÈÄÇÂêàÈöèÊú∫ÂèòÈáèÊòØÂ§öÂÖÉÁöÑ„ÄÅÂØÜÂ∫¶ÂáΩÊï∞ÊòØÈùûÊ†áÂáÜÂΩ¢ÂºèÁöÑ„ÄÅÈöèÊú∫ÂèòÈáèÂêÑÂàÜÈáè‰∏çÁã¨Á´ãÁ≠âÊÉÖÂÜµ Âü∫Êú¨ÊÄùÊÉ≥Ôºö Âú®ÈöèÊú∫ÂèòÈáè$x$ÁöÑÁä∂ÊÄÅÁ©∫Èó¥$\\mathcal{S}$‰∏äÂÆö‰∏Ä‰∏™Êª°Ë∂≥ÈÅçÂéÜÂÆöÁêÜÁöÑÈ©¨ÂèØÂ§´ÈìæÔºå‰ΩøÂÖ∂Âπ≥Á®≥ÂàÜÂ∏ÉÂ∞±ÊòØÊäΩÊ†∑ÁöÑÁõÆÊ†áÂàÜÂ∏É $p(x)$Ôºå ÁÑ∂ÂêéÂú®Ëøô‰∏™È©¨ÂèØÂ§´Èìæ‰∏äËøõË°åÈöèÊú∫Ê∏∏Ëµ∞ÔºåÊØè‰∏™Êó∂ÂàªÂæóÂà∞‰∏Ä‰∏™Ê†∑Êú¨„ÄÇÊ†πÊçÆÈÅçÂéÜÂÆöÁêÜÔºåÂΩìÊó∂Èó¥Ë∂ã‰∫éÊó†Á©∑ÊòØÔºåÊ†∑Êú¨ÁöÑÂàÜÂ∏ÉË∂ãËøëÂπ≥Á®≥ÂàÜÂ∏ÉÔºåÊ†∑Êú¨ÂáΩÊï∞ÂùáÂÄºË∂ãËøëÂáΩÊï∞ÁöÑÊï∞Â≠¶ÊúüÊúõ $$ \\hat{E}f = \\frac{1}{n-m} \\sum^{n}_{i=m+1} f(x_i) $$ È©¨Â∞îÂèØÂ§´ÈìæËíôÁâπÂç°ÁΩóÊ≥ïÁöÑÂÖ≥ÈîÆÊòØÂ¶Ç‰ΩïÊûÑÂª∫ËΩ¨ÁßªÊ†∏ÂáΩÊï∞ÊàñËΩ¨ÁßªÁü©ÈòµÔºå ÂåÖÊã¨Ôºö Metropolis-Hastings Âíå Gibbs sampling„ÄÇ È©¨Â∞îÂèØÂ§´ÈìæËíôÁâπÂç°ÁΩóÊ≥ï‰∏≠ÂæóÂà∞ÁöÑÊ†∑Êú¨Â∫èÂàóÔºåÁõ∏ÈÇªÁöÑÊ†∑Êú¨ÁÇπÊòØÁõ∏ÂÖ≥ÁöÑÔºåËÄå‰∏çÊòØÁã¨Á´ãÁöÑ È©¨Â∞îÂèØÂ§´ÈìæËíôÁâπÂç°ÁΩóÊ≥ïÁöÑÊî∂ÊïõÊÄßÁöÑÂà§Êñ≠ÈÄöÂ∏∏ÊòØÁªèÈ™åÊÄßÁöÑ Âü∫Êú¨Ê≠•È™§Ôºö Âú®ÈöèÊú∫ÂèòÈáè$x$ÁöÑÁä∂ÊÄÅÁ©∫Èó¥$\\mathcal{S}$‰∏äÊûÑÂª∫‰∏Ä‰∏™Êª°Ë∂≥ÈÅçÂéÜÂÆöÁêÜÁöÑÈ©¨ÂèØÂ§´ÈìæÔºå‰ΩøÂÖ∂Âπ≥Á®≥ÂàÜÂ∏É‰∏∫ÁõÆÊ†áÂàÜÂ∏É $p(x)$ ‰ªéÁä∂ÊÄÅÁ©∫Èó¥ÁöÑÊüê‰∏ÄÁÇπ $X_0$ Âá∫ÂèëÔºåÁî®ÊûÑÈÄ†ÁöÑÈ©¨ÂèØÂ§´ÈìæËøõË°åÈöèÊú∫Ê∏∏Ëµ∞Ôºå‰∫ßÁîüÊ†∑Êú¨Â∫èÂàó ${x_0, x_1, \\cdots, x_t, \\cdots }$„ÄÇ Â∫îÁî®È©¨ÂèØÂ§´ÈìæÁöÑÈÅçÂéÜÂÆöÁêÜÔºå Á°ÆÂÆöÊ≠£Êï¥Êï∞ $m$ Âíå $n$Ôºå $m \u003c n$Ôºå ÂæóÂà∞Ê†∑Êú¨ÈõÜÂêà ${x_{m+1}, x_{m+2}Ôºå \\cdots, x_n }$ Ê±ÇÂæóÂáΩÊï∞fÁöÑÔºàÈÅçÂéÜÔºâÂùáÂÄº $$ \\hat{E}f = \\frac{1}{n-m} \\sum^{n}_{i=m+1} f(x_i) $$ Âá†‰∏™ÈáçË¶ÅÈóÆÈ¢òÈúÄË¶ÅÊ≥®ÊÑèÔºö Â¶Ç‰ΩïÂÆö‰πâÈ©¨ÂèØÂ§´ÈìæÔºå‰øùËØÅMCMCÊàêÁ´ã Â¶Ç‰ΩïÁ°ÆÂÆöÊî∂ÊïõÊ≠•Êï∞mÔºå‰øùËØÅÊäΩÊ†∑Êó†ÂÅèÊÄß Â¶Ç‰ΩïÁ°ÆÂÆöËø≠‰ª£Ê≠•Êï∞nÔºå ‰øùËØÅÈÅçÂéÜÂùáÂÄºÁöÑËÆ°ÁÆóÁ≤æÂ∫¶ ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:3:0","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"Metropolis-Hastings Metropolis-HastingÊòØÈ©¨Â∞îÂèØÂ§´ÈìæËíôÁâπÂç°ÁΩóÊ≥ïÁöÑ‰ª£Ë°®ÁÆóÊ≥ï ÂèØ‰ª•ÂØπÂ§öÂÖÉÂèòÈáèÁöÑÊØè‰∏ÄÂèòÈáèÁöÑÊù°‰ª∂ÂàÜÂ∏É‰æùÊ¨°ÂàÜÂà´ËøõË°åÊäΩÊ†∑Ôºå ‰ªéËÄåÂÆûÁé∞ÂØπÊï¥‰∏™Â§öÂÖÉÂèòÈáèÁöÑ‰∏ÄÊ¨°ÊäΩÊ†∑ÔºåËøôÂ∞±ÊòØÂçïÂàÜÈáè Metropolis- Hastings (singlecomponent Metropolis- Hastings) ÁÆóÊ≥ï„ÄÇ ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:3:1","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"Gibbs Sampling ÂêâÂ∏ÉÊñØÊäΩÊ†∑ÔºåÂèØ‰ª•ËÆ§‰∏∫ÊòØ Metropolis-Hastings ÁÆóÊ≥ïÁöÑÁâπÊÆäÊÉÖÂÜµÔºå‰ΩÜÊòØÊõ¥ÂÆπÊòìÂÆûÁé∞ÔºåÂõ†ËÄåË¢´ÂπøÊ≥õ‰ΩøÁî®„ÄÇ ÂêâÂ∏ÉÊñØÊäΩÊ†∑Áî®‰∫éÂ§öÂÖÉÂèòÈáèËÅîÂêàÂàÜÂ∏ÉÁöÑÊäΩÊ†∑Âíå‰º∞ËÆ°„ÄÇ ÂÖ∂Âü∫Êú¨ÂÅöÊ≥ïÊòØÔºå‰ªéËÅîÂêàÊ¶ÇÁéáÂàÜÂ∏ÉÂÆö‰πâÊª°Êù°‰ª∂Ê¶ÇÁéáÂàÜÂ∏ÉÔºå‰æùÊ¨°ÂØπÊª°Êù°‰ª∂Ê¶ÇÁéáÂàÜÂ∏ÉËøõË°åÊäΩÊ†∑ÔºåÂæóÂà∞Ê†∑Êú¨ÁöÑÂ∫èÂàó„ÄÇ ÂêâÂ∏ÉÊñØÊäΩÊ†∑ÈÄÇÂêà‰∫éÊª°Êù°‰ª∂Ê¶ÇÁéáÂàÜÂ∏É ÂÆπÊòìÊäΩÊ†∑ ÁöÑÊÉÖÂÜµÔºåËÄåÂçïÂàÜÈáèMetropolisHastings ÁÆóÊ≥ïÈÄÇÂêà‰∫éÊª°Êù°‰ª∂Ê¶ÇÁéáÂàÜÂ∏É‰∏çÂÆπÊòìÊäΩÊ†∑ÁöÑÊÉÖÂÜµÔºåËøôÊó∂‰ΩøÁî®ÂÆπÊòìÊäΩÊ†∑ÁöÑÊù°‰ª∂ÂàÜÂ∏É‰ΩúÂª∫ËÆÆÂàÜÂ∏É„ÄÇ ÂèÇËÄÉÔºö ÊùéËà™„ÄäÁªüËÆ°Â≠¶‰π†ÊñπÊ≥ï„Äã ","date":"2020-06-06","objectID":"/2020-01-26-ml-montecarlo/:3:2","tags":["Monte Carlo","Statistical Learning"],"title":"Markov Chain Mento Carlo (MCMC)","uri":"/2020-01-26-ml-montecarlo/"},{"categories":["Machine Learning"],"content":"Â•áÂºÇÂÄºÂàÜËß£(SVD)ÊòØ‰∏ÄÁßçÁü©ÈòµÂõ†Â≠êÂàÜËß£ÊñπÊ≥ïÔºåÂú®Á∫øÊÄß‰ª£Êï∞‰∏≠ÔºåË¢´ÂπøÊ≥õÂ∫îÁî®„ÄÇ Â•áÂºÇÂÄºÂàÜËß£‰πüÊòØ‰∏ÄÁßçÁü©ÈòµËøë‰ººÁöÑÊñπÊ≥ïÔºåËøô‰∏™Ëøë‰ººÊòØÂú®ÂºóÁΩóË¥ùÂ∞º‰πåÊñØËåÉÊï∞ÔºàFrobenius norm) ÊÑè‰πâ‰∏ãÁöÑËøë‰ºº„ÄÇ Â•áÂºÇÂÄºÂàÜËß£ÊòØÂú®Âπ≥ÊñπÊçüÂ§±(ÂºóÁΩóË¥ùÂ∞º‰πåÊñØËåÉÊï∞)ÊÑè‰πâ‰∏ãÂØπÁü©ÈòµÁöÑÊúÄ‰ºòËøë‰ººÔºåÂç≥Êï∞ÊçÆÂéãÁº©„ÄÇ ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:0:0","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"ÂÆö‰πâ ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:1:0","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"1. Â•áÂºÇÂÄºÂàÜËß£ Â∞Ü‰∏Ä‰∏™ $m \\times n$ ÁöÑÂÆûÁü©Èòµ$A$Ôºå$A \\in \\mathbf{R}^{m \\times n}$ Ë°®Á§∫‰∏∫‰ª•‰∏ã‰∏â‰∏™ÂÆûÁü©Èòµ‰πòÁßØÂΩ¢ÂºèÁöÑËøêÁÆóÔºåÂç≥Áü©ÈòµÂõ†Â≠êÂàÜËß£Ôºö $$ A=U \\Sigma V^{\\mathrm{T}} $$ ÂÖ∂‰∏≠Ôºå $U$ÊòØ$m$Èò∂Ê≠£‰∫§Áü©Èòµ: $UU^T = I$ $V$‰∏∫$n$Èò∂Ê≠£‰∫§Áü©Èòµ: $VV^T = I$ $\\Sigma$ÊòØÁî±ÈôçÂ∫èÊéíÂàóÁöÑÈùûË¥üÁöÑÂØπËßíÁ∫øÂÖÉÁ¥†ÁªÑÊàêÁöÑ$m \\times n$ÁöÑÂØπËßíÁü©Èòµ: $\\Sigma = diag(\\sigma_1, \\sigma_2, \\cdots, \\sigma_p)$ $\\sigma_{1} \\geqslant \\sigma_{2} \\geqslant \\cdots \\geqslant \\sigma_{p} \\geqslant 0$ $p = \\min(m, n)$ ÈÇ£‰πàÔºåÁß∞ $U \\Sigma V^{\\mathrm{T}}$ : Áü©ÈòµAÁöÑÂ•áÂºÇÂÄºÂàÜËß£ $\\sigma_i$ ‰∏∫AÁöÑÂ•áÂºÇÂÄº(singluar value) $U$ ÁöÑÂàóÂêëÈáè‰∏∫Â∑¶Â•áÂºÇÂêëÈáè(left singular vector) $V$ ÁöÑÂàóÂêëÈáè‰∏∫Âè≥Â•áÂºÇÂêëÈáè(right singular vector) ‚ö†Ô∏èÊ≥®ÊÑèÔºåÁü©ÈòµÁöÑÂ•áÂºÇÂÄºÂàÜËß£‰∏çÂîØ‰∏Ä„ÄÇ ÂÆûÈôÖÂ∏∏Áî®ÁöÑÊòØÔºö ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:1:1","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"2. Á¥ßÂ•áÂºÇÂÄºÂàÜËß£(compact singular value decomposition)Ôºö Êó†ÊçüÂéãÁº© ËÆæ$m \\times n$ÁöÑÂÆûÁü©Èòµ$A$ÔºåÂÖ∂Áß©‰∏∫$\\operatorname{rank}(A)=r$, $r \\leqslant \\min (m, n)$, ÂàôÁ¥ßÂ•áÂºÇÂÄºÂàÜËß£‰∏∫Ôºö $$ A=U_r \\Sigma_r V^{\\mathrm{T}}_r $$ ÂÖ∂‰∏≠Ôºå $U_r$ ÊòØ $m \\times r$ Áü©Èòµ $V_r$ ÊòØ $n \\times r$ Áü©Èòµ $\\Sigma_r$ ÊòØ $r$ Èò∂ÂØπËßíÈòµ $r = \\operatorname{rank}(A)$ ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:1:2","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"3. Êà™Êñ≠Â•áÂºÇÂÄºÂàÜËß£(truncated singular value decomposition)Ôºö ÊúâÊçüÂéãÁº© ‰∏ÄËà¨ËÆ≤Â•áÂºÇÂÄºÂàÜËß£ÔºåÂÆûÈôÖ‰∏äÂ§öÊåáÊà™Êñ≠Â•áÂºÇÂÄºÂàÜËß£ Âú®Â•áÂºÇÂÄºÂàÜËß£‰∏≠Ôºå Âè™ÂèñÊúÄÂ§ßÁöÑ$k$‰∏™Â•áÂºÇÂÄº($k \u003c r, r= \\operatorname{rank}(A)$ )ÂØπÂ∫îÁöÑÈÉ®ÂàÜÔºåÂ∞±ÂæóÂà∞Êà™Êñ≠Â•áÂºÇÂÄºÂàÜËß£ ËÆæ $m \\times n$ ÁöÑÂÆûÁü©Èòµ $A$ÔºåÂÖ∂Áß©‰∏∫ $\\operatorname{rank}(A)=r$, ‰∏î $0 \u003c k \u003c r$, ÂàôÊà™Êñ≠Â•áÂºÇÂÄºÂàÜËß£‰∏∫Ôºö $$ A \\approx U_{k} \\Sigma_{k} V_{k}^{\\mathrm{T}} $$ ÂÖ∂‰∏≠Ôºå $U_k$ ÊòØ$m \\times k$ Áü©Èòµ(Ââç $k$Âàó) $V_k$ ÊòØ$n \\times k$ Áü©Èòµ(Ââç $k$Âàó) $\\Sigma_k$ ÊòØ $k$ Èò∂ÂØπËßíÈòµ(Ââç $k$‰∏™) $r = \\operatorname{rank}(A)$ ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:1:3","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"Âá†‰ΩïËß£Èáä ‰ªéÁ∫øÊÄßÂèòÊç¢ÁöÑËßíÂ∫¶ÁêÜËß£Â•áÂºÇÂÄºÂàÜËß£Ôºö Â∞Ü $m \\times n$ ÁöÑÂÆûÁü©Èòµ $A$Ë°®Á§∫‰∏∫‰ªé $n$ Áª¥Á©∫Èó¥ $R_n$ Âà∞ $m$ Áª¥Á©∫Èó¥ $R_m$ÁöÑ‰∏Ä‰∏™Á∫øÊÄßÂèòÊç¢Ôºö $$ T: x \\rightarrow A x $$ $x$, $Ax$‰∏∫ÂêÑËá™Á©∫Èó¥ÁöÑÂêëÈáè„ÄÇ ÈÇ£‰πàÁ∫øÊÄßÂèòÊç¢ÂèØ‰ª•ÁêÜËß£‰∏∫Ôºö ‰∏Ä‰∏™ÂùêÊ†áÁ≥ªÁöÑÊóãËΩ¨ÊàñÂèçÂ∞ÑÂèòÊç¢ ‰∏Ä‰∏™ÂùêÊ†áËΩ¥ÁöÑÁº©ÊîæÂèòÊç¢ Âè¶‰∏Ä‰∏™ÂùêÊ†áÁ≥ªÁöÑÊóãËΩ¨ÊàñÂèçÂ∞ÑÂèòÊç¢ ÂØπAËøõË°åÂ•áÂºÇÂÄºÂàÜËß£ÔºåUÂíåVÈÉΩÊòØÊ≠£‰∫§Áü©Èòµ VÁöÑÂàóÂêëÈáèÊûÑÊàêRnÁ©∫Èó¥ÁöÑ‰∏ÄÁªÑÊ†áÂáÜÊ≠£‰∫§Âü∫ÔºåË°®Á§∫RnÁ©∫Èó¥‰∏≠Ê≠£‰∫§ÂùêÊ†áÁ≥ªÁöÑÊóãËΩ¨ÊàñÂèçÂ∞ÑÂèòÊç¢ UÁöÑÂàóÂêëÈáèÈÉΩÊàêRmÁ©∫Èó¥ÁöÑ‰∏ÄÁªÑÊ†áÂáÜÊ≠£‰∫§Âü∫ÔºåË°®Á§∫RmÁ©∫Èó¥‰∏≠Ê≠£‰∫§ÂùêÊ†áÁ≥ªÁöÑÊóãËΩ¨ÊàñÂèçÂ∞ÑÂèòÊç¢ $\\Sigma$ÁöÑÂØπËßíÂÖÉÁ¥†ÊòØ‰∏ÄÁªÑÈùûË¥üÂÆûÊï∞ÔºåË°®Á§∫Rn‰∏≠ÁöÑÂéüÂßãÊ≠£‰∫§ÂùêÊ†áÁ≥ªÂùêÊ†áËΩ¥ÁöÑÁº©ÊîæÂèòÊç¢ ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:2:0","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"Â•áÂºÇÂÄºËÆ°ÁÆó Áü©Èòµ$A$ÁöÑÂ•áÂºÇÂÄºÂàÜËß£ÂèØ‰ª•ÈÄöËøáÊ±ÇÂØπÈòµÁü©Èòµ$A^TA$ÁöÑÁâπÂæÅÂÄºÂíåÁâπÂæÅÂêëÈáèÂæóÂà∞„ÄÇ $A^TA$ÁöÑÁâπÂæÅÂêëÈáèÊûÑÊàêÊ≠£‰∫§Áü©Èòµ$V$ÁöÑÂàó $A^TA$ÁöÑÁâπÂæÅÂÄº$\\lambda_j$ÁöÑÂπ≥ÊñπÊ†π‰∏∫Â•áÂºÇÂÄº$\\sigma_i$ÔºåÂç≥ $$ \\sigma_{j}=\\sqrt{\\lambda_{j}}, \\quad j=1,2, \\cdots, n $$ ÂØπ$\\sigma_i$‰ªéÂ§ßÂà∞Â∞èÊéíÂàóÔºåÂæóÂà∞ÂØπËßíÁü©Èòµ $\\Sigma$ Ê±ÇÊ≠£Â•áÂºÇÂÄºÂØπÂ∫îÁöÑÂ∑¶Â•áÂºÇÂêëÈáèÔºåÂÜçÊâ©ÂÖÖÁöÑ $A^T$ ÁöÑÊ†áÂáÜÊ≠£‰∫§Âü∫ÔºåÊûÑÊàêÊ≠£‰∫§Áü©Èòµ $U$ ÁöÑÂàó ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:3:0","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"Ê±ÇÂÄºËøáÁ®ã Ê±ÇÂØπÈòµÁü©Èòµ$A^TA$ÁöÑÁâπÂæÅÂÄºÂíåÁâπÂæÅÂêëÈáè ËÆ°ÁÆóÂØπÁß∞Áü©Èòµ $W= A^TA$ Ê±ÇËß£ÁâπÂæÅÊñπÁ®ã $(W - \\lambda I)x = 0$ ÂæóÂà∞ÁâπÂæÅÂÄº$\\lambda_i$ÔºåÂπ∂Â∞Ü‰πãÈôçÂ∫èÊéíÂàó $$ \\lambda_{1} \\geqslant \\lambda_{2} \\geqslant \\cdots \\geqslant \\lambda_{n} \\geqslant 0 $$ Â∞ÜÁâπÂæÅÂÄº $\\lambda_i$ ‰ª£ÂÖ•ÁâπÂæÅÊñπÁ®ãÊ±ÇÁöÑÂØπÂ∫îÁâπÂæÅÂêëÈáè Ê±Ç $n$ Èò∂Ê≠£‰∫§Áü©Èòµ $V$ Â∞ÜÁâπÂæÅÂêëÈáèÂçï‰ΩçÂåñÔºå ÂæóÂà∞Âçï‰ΩçÁâπÂæÅÂêëÈáèÊûÑÊàê $n$ Èò∂Ê≠£‰∫§Áü©ÈòµV $$ V=\\left[\\begin{array}{llll} v_{1} \u0026 v_{2} \u0026 \\cdots \u0026 v_{n} \\end{array}\\right] $$ Ê±Ç $m \\times n$ ÂØπËßíÁü©Èòµ $\\Sigma$ ËÆ°ÁÆóAÁöÑÂ•áÂºÇÂÄº $$ \\sigma_{j}=\\sqrt{\\lambda_{j}}, \\quad j=1,2, \\cdots, n $$ ÊûÑÈÄ† $m \\times n$ Áü©ÈòµÂØπËßíÁü©Èòµ $\\Sigma$Ôºå ‰∏ªÂØπËßíÁ∫øÂÖÉÁ¥†ÊòØÂ•áÂºÇÂÄºÔºå ÂÖ∂‰ΩôÂÖÉÁ¥†‰∏∫ 0 $$ \\Sigma=\\operatorname{diag}\\left(\\sigma_{1}, \\sigma_{2}, \\cdots, \\sigma_{n}\\right) $$ Ê±Ç $m$ Èò∂Ê≠£‰∫§Áü©Èòµ $U$ ÂØπ $A$ ÁöÑÂâç $r$‰∏™Ê≠£Â•áÂºÇÂÄºÔºå ‰ª§ $$ u_{j}=\\frac{1}{\\sigma_{j}} A v_{j}, \\quad j=1,2, \\cdots, r $$ ÂæóÂà∞ $$ U_{1}=\\left[\\begin{array}{llll} u_{1} \u0026 u_{2} \u0026 \\cdots \u0026 u_{r} \\end{array}\\right] $$ Ê±Ç $A^T$ ÁöÑÈõ∂Á©∫Èó¥ÁöÑ‰∏ÄÁªÑÊ†áÂáÜÊ≠£‰∫§Âü∫ $$ \\lbrace u_{r+1}, u_{r+2}, \\cdots, u_{m} \\rbrace $$ ‰ª§ $$ U_{2}=\\left[\\begin{array}{llll} u_{r+1} \u0026 u_{r+2} \u0026 \\cdots \u0026 u_{m} \\end{array}\\right] $$ ‰∏î‰ª§ $$ U=\\left[\\begin{array}{ll} U_{1} \u0026 U_{2} \\end{array}\\right] $$ ÂæóÂà∞Â•áÂºÇÂÄºÂàÜËß£ $$ A=U \\Sigma V^{\\mathrm{T}} $$ ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:3:1","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"‰∏æ‰æã Ê±Ç: Áü©Èòµ $A$ ÁöÑÂ•áÂºÇÂÄºÂàÜËß£ $$ A=\\left[\\begin{array}{ll} 1 \u0026 1 \\cr 2 \u0026 2 \\cr 0 \u0026 0 \\end{array}\\right] $$ Ëß£Ôºö Ê±Ç $A^TA$ ÁöÑÁâπÂæÅÂÄºÂíåÁâπÂæÅÂêëÈáè $$ A^{\\mathrm{T}} A=\\left[\\begin{array}{lll} 1 \u0026 2 \u0026 0 \\cr 1 \u0026 2 \u0026 0 \\end{array}\\right]\\left[\\begin{array}{ll} 1 \u0026 1 \\cr 2 \u0026 2 \\cr 0 \u0026 0 \\end{array}\\right]=\\left[\\begin{array}{ll} 5 \u0026 5 \\cr 5 \u0026 5 \\end{array}\\right] $$ Êª°Ë∂≥ÁâπÂæÅÊñπÁ®ã $$ \\left(A^{\\mathrm{T}} A-\\lambda I\\right) x=0 $$ ÂæóÂà∞ÈΩêÊ¨°Á∫øÊÄßÊñπÁ®ãÁªÑ $$ \\begin{cases} (5-\\lambda) x_{1} + \u0026 5 x_{2}=0 \\cr 5 x_{1} + \u0026 (5-\\lambda) x_{2}=0 \\end{cases} $$ ËØ•ÊñπÁ®ãÁªÑÊúâÈùûÈõ∂Ëß£ÁöÑÂÖÖË¶ÅÊù°‰ª∂ÊòØ $$ \\left|\\begin{array}{cc} 5-\\lambda \u0026 5 \\cr 5 \u0026 5-\\lambda \\end{array}\\right|=0 $$ Âç≥ $$ \\lambda^{2}-10 \\lambda=0 $$ ÂæóÂà∞ $\\lambda_1 = 10 $, $\\lambda_2 = 0$. ÁâπÂæÅÂÄº‰ª£ÂÖ•Á∫øÊÄßÊñπÁ®ãÁªÑÔºåÂàÜÂà´ÂæóÂà∞ÂØπÂ∫îÂçï‰ΩçÁâπÂæÅÂêëÈáè $$ v_{1}=\\left[\\begin{array}{c} \\frac{1}{\\sqrt{2}} \\cr \\frac{1}{\\sqrt{2}} \\end{array}\\right], v_{2}=\\left[\\begin{array}{c} \\frac{1}{\\sqrt{2}} \\cr -\\frac{1}{\\sqrt{2}} \\end{array}\\right] $$ Ê±ÇÊ≠£‰∫§Áü©Èòµ $V$ ÊûÑÈÄ†Ê≠£‰∫§Áü©Èòµ $$ V=\\left[\\begin{array}{cc} \\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{2}} \\cr \\frac{1}{\\sqrt{2}} \u0026 -\\frac{1}{\\sqrt{2}} \\end{array}\\right] $$ Ê±ÇÂØπËßíÁü©Èòµ $\\Sigma$ Â•áÂºÇÂÄº‰∏∫ $\\sigma_1 = \\sqrt{\\lambda_1} = \\sqrt{10}$, $\\sigma_2 = 0$, ÈÇ£‰πà $$ \\Sigma=\\left[\\begin{array}{cc} \\sqrt{10} \u0026 0 \\cr 0 \u0026 0 \\cr 0 \u0026 0 \\end{array}\\right] $$ ‚ö†Ô∏èÊ≥®ÊÑèÔºö ‰∏∫‰∫Ü $\\Sigma$ ËÉΩ‰∏é$U$Âíå$V$ËøõË°åÁü©Èòµ‰πòÊ≥ïÔºå $\\Sigma$Ë¶ÅÂä†‰∏äÈõ∂Ë°åÂêëÈáè Ê±ÇÊ≠£‰∫§Áü©Èòµ $U$ Âü∫‰∫é $A$ ÁöÑÂ•áÂºÇÂÄºËÆ°ÁÆóÂæóÂà∞ÂàóÂêëÈáè $u_1$ $$ u_{1}=\\frac{1}{\\sigma_{1}} A v_{1}=\\frac{1}{\\sqrt{10}}\\left[\\begin{array}{cc} 1 \u0026 1 \\cr 2 \u0026 2 \\cr 0 \u0026 0 \\end{array}\\right]\\left[\\begin{array}{c} \\frac{1}{\\sqrt{2}} \\cr \\frac{1}{\\sqrt{2}} \\end{array}\\right]=\\left[\\begin{array}{c} \\frac{1}{\\sqrt{5}} \\cr \\frac{2}{\\sqrt{5}} \\cr 0 \\end{array}\\right] $$ ÂàóÂêëÈáè$u_2$Ôºå $u_3$ÊòØ $A^T$ÁöÑÈõ∂Á©∫Èó¥ $N(A^T)$ ÁöÑ‰∏ÄÁªÑÊ†áÂáÜÊ≠£‰∫§Âü∫ÔºåÊïÖËÄåÊ±ÇËß£‰ª•‰∏ãÁ∫øÊÄßÊñπÁ®ãÁªÑ $$ A^{\\mathrm{T}} x=\\left[\\begin{array}{lll} 1 \u0026 2 \u0026 0 \\cr 1 \u0026 2 \u0026 0 \\end{array}\\right]\\left[\\begin{array}{l} x_{1} \\cr x_{2} \\cr x_{3} \\end{array}\\right]=\\left[\\begin{array}{l} 0 \\cr 0 \\end{array}\\right] $$ Âç≥ $$ \\begin{array}{c} x_{1}+2 x_{2}+0 x_{3}=0 \\cr x_{1}=-2 x_{2}+0 x_{3} \\end{array} $$ ÂàÜÂà´Âèñ $x_2$Ôºå$x_3$ ‰∏∫ $(1,0)$ Âíå $(0,1)$ ÂæóÂà∞ $N(A^T)$ÁöÑÂü∫ $$ (-2,1,0)^{\\mathrm{T}}, \\quad(0,0,1)^{\\mathrm{T}} $$ Âæó $N(A^T)$ ÁöÑ‰∏ÄÁªÑÊ†áÂáÜÊ≠£‰∫§Âü∫ÊòØ $$ u_{2}=\\left(-\\frac{2}{\\sqrt{5}}, \\frac{1}{\\sqrt{5}}, 0\\right)^{\\mathrm{T}}, \\quad u_{3}=(0,0,1)^{\\mathrm{T}} $$ ÊúÄÂêéÔºåÊûÑÈÄ†Ê≠£‰∫§Áü©Èòµ $U$ $$ U=\\left[\\begin{array}{ccc} \\frac{1}{\\sqrt{5}} \u0026 -\\frac{2}{\\sqrt{5}} \u0026 0 \\cr \\frac{2}{\\sqrt{5}} \u0026 \\frac{1}{\\sqrt{5}} \u0026 0 \\cr 0 \u0026 0 \u0026 1 \\end{array}\\right] $$ Áü©Èòµ $A$ ÁöÑÂ•áÂºÇÂÄºÂàÜËß£ $$ A=U \\Sigma V^{\\mathrm{T}}=\\left[\\begin{array}{ccc} \\frac{1}{\\sqrt{5}} \u0026 -\\frac{2}{\\sqrt{5}} \u0026 0 \\cr \\frac{2}{\\sqrt{5}} \u0026 \\frac{1}{\\sqrt{5}} \u0026 0 \\cr 0 \u0026 0 \u0026 1 \\end{array}\\right]\\left[\\begin{array}{cc} \\sqrt{10} \u0026 0 \\cr 0 \u0026 0 \\cr 0 \u0026 0 \\end{array}\\right]\\left[\\begin{array}{cc} \\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{2}} \\cr \\frac{1}{\\sqrt{2}} \u0026 -\\frac{1}{\\sqrt{2}} \\end{array}\\right] $$ ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:3:2","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"Áü©ÈòµÁöÑÂ§ñÁßØÂ±ïÂºÄÂºèË°®Á§∫ Â∞ÜAÁöÑÂ•áÂºÇÂÄºÂàÜËß£ÁúãÊàêÁü©Èòµ $U\\Sigma$ Âíå $V^T$ ÁöÑ‰πòÁßØÔºå Â∞Ü $U\\Sigma$ ÊåâÂàóÂêëÈáèÂàÜÂùóÔºå $$ U \\Sigma=\\left[\\begin{array}{llll} \\sigma_{1} u_{1} \u0026 \\sigma_{2} u_{2} \u0026 \\cdots \u0026 \\sigma_{n} u_{n} \\end{array}\\right] $$ $V^T$ ÊåâË°åÂêëÈáèÂàÜÂùó $$ V^{\\mathrm{T}}=\\left[\\begin{array}{c} v_{1}^{\\mathrm{T}} \\cr v_{2}^{\\mathrm{T}} \\cr \\vdots \\cr v_{n}^{\\mathrm{T}} \\end{array}\\right] $$ ÈÇ£‰πàÂ§ñÁßØÂ±ïÂºÄÂºè‰∏∫ $$ A=\\sigma_{1} u_{1} v_{1}^{\\mathrm{T}}+\\sigma_{2} u_{2} v_{2}^{\\mathrm{T}}+\\cdots+\\sigma_{n} u_{n} v_{n}^{\\mathrm{T}} $$ ÊàñËÄÖ $$ A=\\sum_{k=1}^{n} A_{k}=\\sum_{k=1}^{n} \\sigma_{k} u_{k} v_{k}^{\\mathrm{T}} $$ ÂÖ∂‰∏≠ $A_{k}=\\sigma_{k} u_{k} v_{k}^{\\mathrm{T}}$ ÊòØ $m \\times n$ Áü©Èòµ ËÄå $$ u_{i} v_{j}^{\\mathrm{T}}=\\left[\\begin{array}{c} u_{1 i} \\cr u_{2 i} \\cr \\vdots \\cr u_{m i} \\end{array}\\right]\\left[\\begin{array}{cccc} v_{1 j} \u0026 v_{2 j} \u0026 \\cdots \u0026 v_{n j} \\end{array}\\right] = \\left[\\begin{array}{cccc} u_{1 i} v_{1 j} \u0026 u_{1 i} v_{2 j} \u0026 \\cdots \u0026 u_{1 i} v_{n j} \\cr u_{2 i} v_{1 j} \u0026 u_{2 i} v_{2 j} \u0026 \\cdots \u0026 u_{2 i} v_{n j} \\cr \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\cr u_{m i} v_{1 j} \u0026 u_{m i} v_{2 j} \u0026 \\cdots \u0026 u_{m i} v_{n j} \\end{array}\\right] $$ ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:4:0","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"Ë°•ÂÖÖÔºö ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:5:0","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"Determinant Ë°åÂàóÂºè Determinant: The determinant of a square matrix is a scalar that provides information about the matrix. e.g. Invertibility Geometrically, it can be viewed as the volume scaling factor of the linear transformation described by the matrix. ‰∏Ä‰∏™Áü©ÈòµÁöÑË°åÂàóÂºèÂ∞±ÊòØ‰∏Ä‰∏™Ë∂ÖÂπ≥Ë°åÂ§öÈù¢‰ΩìÁöÑÔºàÊúâÂêëÁöÑÔºâÈù¢ÁßØ/‰ΩìÁßØÔºåËøô‰∏™Â§öÈù¢‰ΩìÁöÑÊØèÊù°ËæπÂØπÂ∫îÁùÄÂØπÂ∫îÁü©ÈòµÁöÑÂàóÔºõ Áü©Èòµ $A$ ÁöÑË°åÂàóÂºè $det(A)$ Â∞±ÊòØÁ∫øÊÄßÂèòÊç¢ $A$ ‰∏ãÁöÑÂõæÂΩ¢Èù¢ÁßØÊàñ‰ΩìÁßØÁöÑ‰º∏Áº©Âõ†Â≠ê„ÄÇ Áü©ÈòµÁöÑË°åÂàóÂºèÁöÑÂá†‰ΩïÊÑè‰πâÊòØÁü©ÈòµÂØπÂ∫îÁöÑÁ∫øÊÄßÂèòÊç¢ÂâçÂêéÁöÑÈù¢ÁßØÊØî Property: $Det(I) = 1$ Exchanging rows only reverse the sign of det Determinant is ‚Äúlinear‚Äù for each row $det(A) \\neq 0$, $A$ is invertible Cramer‚Äôs rule: $A^{-1} = \\frac{1}{det(A)}C^T$ $det(A)$: scalar $C$: cofactors of $A$ (C has the same size as $A$) $C^T$ is adjugate of $A$ (‰º¥ÈöèÁü©Èòµ) ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:5:1","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Machine Learning"],"content":"Eigenvalue and Eigenvector Eigen (German word): ‚Äúunique to‚Äù or ‚Äúbelonging to‚Äù if $Av = \\lambda v$ ($v$ is a vector , $\\lambda$ is a scalar) $A$ must be square $v$ is an eigenvector of $A$, exluding zero vector $\\lambda$ is an eigenvalue of $A$ that correponds to $v$ $T$ is a linear operator if $T(v) = \\lambda v$ ( $v$ is a vector, $\\lambda$ is a scalar) $v$ is an eigenvector of $T$, exluding zero vector $\\lambda$ is an eigenvalue of $T$ that correponds to $v$ An eigenvector of A corresponds to a unique eigenvalue. An eigenvalue of A has infinitely many eigenvectors. =\u003e how to find eigenvalues t : $$ det(A - tI_n) = 0 $$ ÂèÇËÄÉÔºö ÊùéËà™„ÄäÁªüËÆ°Â≠¶‰π†ÊñπÊ≥ï„Äã ","date":"2020-06-06","objectID":"/2020-06-06-ml-svd/:5:2","tags":["SVD","Statistical Learning"],"title":"Singular Value Decomposition (SVD)","uri":"/2020-06-06-ml-svd/"},{"categories":["Coding"],"content":"Get answers for Swift within 10s","date":"2020-05-30","objectID":"/2020-06-03-swift-unsafe-pointer/","tags":["Swift"],"title":"Pointer in UnSafe Swift","uri":"/2020-06-03-swift-unsafe-pointer/"},{"categories":["Coding"],"content":"Pointers Unsafe Swift pointers use a predictable naming scheme: Unsafe [Mutable][Raw][Buffer]Pointer[\u003cT\u003e] Explain: Pointers are just memory addresses. Direct memory access is Unsafe. Mutable means you can write to it. Raw means it points to a blob of bytes. Buffer means that is works like a collection. Generic [\u003cT\u003e] pointers are typed. ","date":"2020-05-30","objectID":"/2020-06-03-swift-unsafe-pointer/:1:0","tags":["Swift"],"title":"Pointer in UnSafe Swift","uri":"/2020-06-03-swift-unsafe-pointer/"},{"categories":["Coding"],"content":"Working with Pointers C Pointer Swift Type int * UnsaftMutablePointer const int * UnsafePointer NSDate ** AutoreleasingUnsafeMutablePointer struct UnknowType * OpaquePointer void * UnsafeMutableRawPointer const void * UnsafeRawPointer Explain: see here ","date":"2020-05-30","objectID":"/2020-06-03-swift-unsafe-pointer/:2:0","tags":["Swift"],"title":"Pointer in UnSafe Swift","uri":"/2020-06-03-swift-unsafe-pointer/"},{"categories":["Coding"],"content":"Usage see here and here ","date":"2020-05-30","objectID":"/2020-06-03-swift-unsafe-pointer/:3:0","tags":["Swift"],"title":"Pointer in UnSafe Swift","uri":"/2020-06-03-swift-unsafe-pointer/"},{"categories":["Coding"],"content":"Get answers for Swift within 10s","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Swift Cheat Sheet Stolen from iwasrobbed. I simplify it and add some more. It‚Äôs a high level and a quick reference to Swift. The purpose of this cheat sheet is to teach myself and get answers within 10s. ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:0:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Table of Contents Code Document Data Types Operators Operator Overloading Declaring Classes Declarations Lazy Property Property Observer Literals Functions Constants and Variables Naming Conventions Closures Generics Control Statements Extension Protocol Protocol Extension Error Handling Passing Information User Defaults Common Patterns Unicode Support File IO ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:0:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Code Documentation Two ways of commenting: // /* ‚Ä¶ */ Two ways of documenting with markdown (Reconigzed by xcode): /// /** ‚Ä¶ */ ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:1:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Markdown a few keywords that xcode can recognized automatically, with the format like - . The most common: Prameters, Throws, Returns /** - Prameters: - argument1: This is arg1 - argument2: This is arg2 - Returns: The results string. - Throws: `Error` if nil */ Other keywords /** - Precondition: - Postcondition: - Requires: All the information in the object should be sorted - Invariant: The object will maintained sorted - Complexity: O(n^2) - Important: - Warning: Very computation consuming - Attention: Same as Warning - Note: something to keep in mind - Remark: Same as note */ Metadata /** - Author: - Authors: - Copyright: - Date: - Since: - Version: */ Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:1:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"MARK Using MARK to organize your code: // MARK:- Use mark to logically organize your code // Declare some functions or variables here // MARK:- They also show up nicely in the properties/functions list in Xcode // Declare some more functions or variables here ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:1:2","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"FIXME Using FIXME to remember to fix your code: // Some broken code might be here // FIXME:Use fixme to create a reminder to fix broken code later FIXME works a lot like MARK because it makes organizing code easier, but it‚Äôs used exclusively when you need to remember to fix something. ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:1:3","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"TODO Using TODO to remember to add, delete, or generally refactor your code: // Some incomplete code might be here // TODO:Use todo to create a reminder to finish things up later TODO is very similar to FIXME and MARK, but it‚Äôs used exclusively when you need to remember to add, delete, or change your code later. Auto-generating method documentation: In a method‚Äôs preceding line, press ‚å• Option + ‚åò Command + / to automatically generate a documentation stub for your method. Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:1:4","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Data Types ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:2:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Size Permissible sizes of data types are determined by how many bytes of memory are allocated for that specific type and whether it‚Äôs a 32-bit or 64-bit environment. In a 32-bit environment, long is given 4 bytes, which equates to a total range of 2^(4*8) (with 8 bits in a byte) or 4294967295. In a 64-bit environment, long is given 8 bytes, which equates to 2^(8*8) or 1.84467440737096e19. For a complete guide to 64-bit changes, please see the transition document. ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:2:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"C Primitives Unless you have a good reason to use C primitives, you should just use the Swift types to ensure compability going foward. In fact, Swift just aliases C types to a Swift equivalent: // C char is aliased as an Int8 and unsigned as UInt8 let aChar = CChar() let anUnsignedChar = CUnsignedChar() print(\"C char size: \\(MemoryLayout.size(ofValue: aChar))with min: \\(Int8.min)and max: \\(Int8.max)\") // C char size: 1 with min: -128 and max: 127 print(\"C unsigned char size: \\(MemoryLayout.size(ofValue: anUnsignedChar))with min: \\(UInt8.min)and max: \\(UInt8.max)\") // C unsigned char size: 1 with min: 0 and max: 255 // C short is aliased as an Int16 and unsigned as UInt16 let aShort = CShort() let unsignedShort = CUnsignedShort() print(\"C short size: \\(MemoryLayout.size(ofValue: aShort))with min: \\(Int16.min)and max: \\(Int16.max)\") // C short size: 2 with min: -32768 and max: 32767 print(\"C unsigned short size: \\(MemoryLayout.size(ofValue: unsignedShort))with min: \\(UInt16.min)and max: \\(UInt16.max)\") // C unsigned short size: 2 with min: 0 and max: 65535 // C int is aliased as an Int32 and unsigned as UInt32 let anInt = CInt() let unsignedInt = CUnsignedInt() print(\"C int size: \\(MemoryLayout.size(ofValue: anInt))with min: \\(Int32.min)and max: \\(Int32.max)\") // C int size: 4 with min: -2147483648 and max: 2147483647 print(\"C unsigned int size: \\(MemoryLayout.size(ofValue: unsignedInt))with min: \\(UInt32.min)and max: \\(UInt32.max)\") // C unsigned int size: 4 with min: 0 and max: 4294967295 // C long is aliased as an Int and unsigned as UInt let aLong = CLong() let unsignedLong = CUnsignedLong() print(\"C long size: \\(MemoryLayout.size(ofValue: aLong))with min: \\(Int.min)and max: \\(Int.max)\") // C long size: 8 with min: -9223372036854775808 and max: 9223372036854775807 print(\"C unsigned long size: \\(MemoryLayout.size(ofValue: unsignedLong))with min: \\(UInt.min)and max: \\(UInt.max)\") // C unsigned long size: 8 with min: 0 and max: 18446744073709551615 // C long long is aliased as an Int64 and unsigned as UInt64 let aLongLong = CLongLong() let unsignedLongLong = CUnsignedLongLong() print(\"C long long size: \\(MemoryLayout.size(ofValue: aLongLong))with min: \\(Int64.min)and max: \\(Int64.max)\") // C long long size: 8 with min: -9223372036854775808 and max: 9223372036854775807 print(\"C unsigned long long size: \\(MemoryLayout.size(ofValue: unsignedLongLong))with min: \\(UInt64.min)and max: \\(UInt64.max)\") // C unsigned long long size: 8 with min: 0 and max: 18446744073709551615 From the docs: C Type Swift Type bool CBool char, signed char CChar unsigned char CUnsignedChar short CShort unsigned short CUnsignedShort int CInt unsigned int CUnsignedInt long CLong unsigned long CUnsignedLong long long CLongLong unsigned long long CUnsignedLongLong wchar_t CWideChar char16_t CChar16 char32_t CChar32 float CFloat double CDouble Integers Integers can be signed or unsigned. When signed, they can be either positive or negative and when unsigned, they can only be positive. Apple states: Unless you need to work with a specific size of integer, always use Int for integer values in your code. This aids code consistency and interoperability. Even on 32-bit platforms, Int [‚Ä¶] is large enough for many integer ranges. Fixed width integer types with their accompanying byte sizes as the variable names: // Exact integer types let aOneByteInt: Int8 = 127 let aOneByteUnsignedInt: UInt8 = 255 let aTwoByteInt: Int16 = 32767 let aTwoByteUnsignedInt: UInt16 = 65535 let aFourByteInt: Int32 = 2147483647 let aFourByteUnsignedInt: UInt32 = 4294967295 let anEightByteInt: Int64 = 9223372036854775807 let anEightByteUnsignedInt: UInt64 = 18446744073709551615 // Minimum integer types let aTinyInt: Int8 = 127 let aTinyUnsignedInt: UInt8 = 255 let aMediumInt: Int16 = 32767 let aMediumUnsignedInt: UInt16 = 65535 let aNormalInt: Int32 = 2147483647 let aNormalUnsignedInt: UInt32 = 4294967295 let aBigInt: Int64 = 9223372036854775807 let aBigUnsignedInt: UInt64 = 1844674407370955","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:2:2","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Enum \u0026 Bitmask Types Enumeration types can be defined as follows: // Specifying a typed enum with a name (recommended way) enum UITableViewCellStyle: Int { case default, valueOne, valueTwo, subtitle } // Accessing it: let cellStyle: UITableViewCellStyle = .default As of Swift 3, all enum options should be named in lowerCamelCased. Working with Bitmasks Newer Swift versions have a nice substitute for the old NS_OPTIONS macro for creating bitmasks to compare to. An example for posterity: struct Options: OptionSet { let rawValue: Int init(rawValue: Int) { self.rawValue = rawValue } init(number: Int) { self.init(rawValue: 1 \u003c\u003c number) } static let OptionOne = Options(number: 0) static let OptionTwo = Options(number: 1) static let OptionThree = Options(number: 2) } let options: Options = [.OptionOne, .OptionTwo] options.contains(.OptionOne) // true options.contains(.OptionThree) // false ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:2:3","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Type Casting Sometimes it is necessary to cast an object into a specific class or data type. Examples of this would be casting from a Float to an Int or from a UITableViewCell to a subclass such as RPTableViewCell. Checking Types Swift uses is and as both for checking object types as well as conformance to a given protocol. Operator: is Checking object type using is: if item is Movie { movieCount += 1 print(\"It is a movie.\") } else if item is Song { songCount += 1 print(\"It is a song.\") } The is operator returns true if an instance is of that object type, or conforms to the specified protocol, and returns false if it does not. Operators: as? and as! If you want to be able to easily access the data during one of these checks, you can use as? to optionally (or as! to force) unwrap the object when necessary: for item in library { if let movie = item as? Movie { print(\"Director: \\(movie.director)\") } else if let song = item as? Song { print(\"Artist: \\(song.artist)\") } } The as? version of the downcast operator returns an optional value of the object or protocol‚Äôs type, and this value is nil if the downcast fails or this instance does not conform to the specified protocol. The as! version of the downcast operator forces the downcast to the specified object or protocol type and triggers a runtime error if the downcast does not succeed. Casting from Generic Types If you‚Äôre working with AnyObject objects given from the Cocoa API, you can use: for movie in someObjects as! [Movie] { // do stuff } If given an array with Any objects, you can use a switch statement with the type defined for each case: var things = [Any]() for thing in things { switch thing { case 0 as Int: print(\"Zero as an Int\") case let someString as! String: print(\"S string value of \\\"\\(someString)\\\"\") case let (x, y) as! (Double, Double): print(\"An (x, y) point at \\(x), \\(y)\") case let movie as! Movie: print(\"A movie called '\\(movie.name)' by director \\(movie.director)\") default: print(\"Didn't match any of the cases specified\") } } Basic Casting Swift also offers some simple methods of casting between it‚Äôs given data types. // Example 1: let aDifferentDataType: Float = 3.14 let anInt: Int = Int(aDifferentDataType) // Example 2: let aString: String = String(anInt) Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:2:4","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Operators Swift supports most standard C operators and improves several capabilities to eliminate common coding errors. The assignment operator = does not return a value, to prevent it from being mistakenly used when the equal to operator == is intended. Arithmetic operators (+, -, *, /, %) detect and disallow value overflow, to avoid unexpected results when working with numbers that become larger or smaller than the allowed value range of the type that stores them. Arithmetic Operators Operator Purpose + Addition - Subtraction * Multiplication / Division % Remainder Comparative Operators Operator Purpose == Equal to === Identical to != Not equal to !== Not identical to ~= Pattern match \u003e Greater than \u003c Less than \u003e= Greater than or equal to \u003c= Less than or equal to Assignment Operators Operator Purpose = Assign += Addition -= Subtraction *= Multiplication /= Division %= Remainder \u0026= Bitwise AND |= Bitwise Inclusive OR ^= Exclusive OR ¬´= Shift Left ¬ª= Shift Right Logical Operators Operator Purpose ! NOT \u0026\u0026 Logical AND || Logical OR Range Operators Operator Purpose ..\u003c Half-open range ‚Ä¶ Closed range Bitwise Operators Operator Purpose \u0026 Bitwise AND | Bitwise Inclusive OR ^ Exclusive OR ~ Unary complement (bit inversion) ¬´ Shift Left ¬ª Shift Right Overflow and Underflow Operators Typically, assigning or incrementing an integer, float, or double past it‚Äôs range would result in a runtime error. However, if you‚Äôd instead prefer to safely truncate the number of available bits, you can opt-in to have the variable overflow or underflow using the following operators: Operator Purpose \u0026+ Addition \u0026- Subtraction \u0026* Multiplication Example for unsigned integers (works similarly for signed): var willOverflow = UInt8.max // willOverflow equals 255, which is the largest value a UInt8 can hold willOverflow = willOverflow \u0026+ 1 // willOverflow is now equal to 0 var willUnderflow = UInt8.min // willUnderflow equals 0, which is the smallest value a UInt8 can hold willUnderflow = willUnderflow \u0026- 1 // willUnderflow is now equal to 255 Other Operators Operator Purpose ?? Nil coalescing ?: Ternary conditional ! Force unwrap object value ? Safely unwrap object value Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:3:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Operator Overloading Swift allows you to overwrite existing operators or define new operators for existing or custom types. For example, this is why in Swift you can join strings using the + operator, even though it is typically used for math. Operator overloading is limited to the following symbols, / = - + * % \u003c \u003e ! \u0026 | ^ . ~, however you cannot overload the = operator by itself (it must be combined with another symbol). Operators can be specified as: prefix: goes before an object such as -negativeNumber infix: goes between two objects, such as a + b postfix: goes after an object, such as unwrapMe! ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:4:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Custom operators associativity: defines how operators of the same precedence are grouped together (left, right) precedence: gives some operators higher priority than others; these operators are applied first. Refer Operator Declarations to see full details about operator associativity and precedence. Example: DefaultPrecedence group // declare first and set rules with a precedence group infix operator ** // use DefaultPrecedence group Custom Precedence group // define a custom precedence group precedencegroup ExponentiationPrecedence { higherThan: MultiplicationPrecedence associativity: right // none, left, right //assignment: false } // now, replace original declaration of ** with infix operator **: ExponentiationPrecedence That‚Äôs it. // impelment infix func ** (x: Double, p: Double) -\u003e Double { return pow(x, p) } 2**3 // 8 2**3**2 // 512 1+2**3**2 // 513 5*2**3**2 // 2560 see also docs Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:4:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Declaring Classes Classes are typically declared using separate .swift files, but multiple classes can also be created within the same file if you‚Äôd like to organize it that way. Unlike Objective-C, there‚Äôs no need for an interface file (.h) in Swift. The implementation file should contain (in this order): Any needed import statements A class declaration which contains any constants or variables necessary for the class All public and private functions Example: MyClass.swift import UIKit class MyClass { // Declare any constants or variables at the top let kRPErrorDomain = \"com.myIncredibleApp.errors\" var x: Int, y: Int // MARK:- Class Methods, e.g. MyClass.functionName() class func alert() { print(\"This is a class function.\") } // MARK:- Instance Methods, e.g. myClass.functionName() init(x: Int, y: Int) { self.x = x self.y = y } // MARK:- Private Methods private func pointLocation() -\u003e String { return \"x: \\(x), y: \\(y)\" } } Instantiation When you want to create a new instance of a class, you use the syntax: let myClass = MyClass(x: 1, y: 2) where x and y are variables that are passed in at the time of instantiation. Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:5:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Declarations More info here in the docs. Preprocessor Swift doesn‚Äôt come with a preprocessor so it only supports a limited number of statements for build time. Things like #define have been replaced with global constants defined outside of a class. Directive Purpose #if An if conditional statement #elif An else if conditional statement #else An else conditional statement #endif An end if conditional statement Imports Directive Purpose import Imports a framework Constants \u0026 Variables Directive Purpose let Declares local or global constant var Declares a local or global variable class Declares a class-level constant or variable static Declares a static type Classes, Structure, Functions and Protocols Directive Purpose typealias Introduces a named alias of an existing type enum Introduces a named enumeration struct Introduces a named structure class Begins the declaration of a class init Introduces an initializer for a class, struct or enum init? Produces an optional instance or an implicitly unwrapped optional instance; can return nil deinit Declares a function called automatically when there are no longer any references to a class object, just before the class object is deallocated func Begins the declaration of a function protocol Begins the declaration of a formal protocol static Defines as type-level within struct or enum convenience Delegate the init process to another initializer or to one of the class‚Äôs designated initializers extension Extend the behavior of class, struct, or enum subscript Adds subscripting support for objects of a particular type, normally for providing a convenient syntax for accessing elements in a collective, list or sequence override Marks overriden initializers Operators Directive Purpose operator Introduces a new infix, prefix, or postfix operator Declaration Modifiers Directive Purpose dynamic Marks a member declaration so that access is always dynamically dispatched using the Objective-C runtime and never inlined or devirtualized by the compiler final Specifies that a class can‚Äôt be subclassed, or that a property, function, or subscript of a class can‚Äôt be overridden in any subclass lazy Indicates that the property‚Äôs initial value is calculated and stored at most once, when the property is first accessed optional Specifies that a protocol‚Äôs property, function, or subscript isn‚Äôt required to be implemented by conforming members required Marks the initializer so that every subclass must implement it weak Indicates that the variable or property has a weak reference to the object stored as its value Access Control Directive Purpose open Can be subclassed outside of its own module and its methods overridden as well; truly open to modification by others and useful for framework builders public Can only be subclassed by its own module or have its methods overridden by others within the same module internal (Default) Indicates the entities are only available to the entire module that includes the definition, e.g. an app or framework target fileprivate Indicates the entities are available only from within the source file where they are defined private Indicates the entities are available only from within the declaring scope within the file where they are defined (e.g. within the { } brackets only) public class AccessLevelsShowCase { // Property accessible for other modules public var somePublicProperty = 0 // Property accessible from the module var someInternelProperty = 1 // Property accessible from its own defining source file fileprivate func someFilePrivateMethod() {} // Property accessible fro its enclosing declaration private func somePrivateMethod() {} } Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:6:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Literals Literals are compiler directives which provide a shorthand notation for creating common objects. Syntax What it does \"string\" Returns a String object 28 Returns an Int 3.14, 0xFp2, 1.25e2 Returns a Double object true, false Returns a Bool object [] Returns an Array object [keyName:value] Returns a Dictionary object 0b Returns a binary digit 0o Returns an octal digit 0x Returns a hexadecimal digit Strings Special characters can be included: Null Character: \\0 Backslash: \\\\ (can be used to escape a double quote) Horizontal Tab: \\t Line Feed: \\n Carriage Return: \\r Double Quote: \\\" Single Quote: \\' Unicode scalar: \\u{n} where n is between one and eight hexadecimal digits Multiline string literal let json = \"\"\" { \"username\": \"David\", \"loginCount\": 2} \"\"\" Array Access Syntax let example = [ \"hi\", \"there\", 23, true ] print(\"item at index 0: \\(example[0])\") Dictionary Access Syntax let example = [ \"hi\" : \"there\", \"iOS\" : \"people\" ] if let value = example[\"hi\"] { print(\"hi \\(value)\") } Mutability For mutable literals, declare it with var; immutable with let. Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:7:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Functions Declaration Syntax Functions without a return type use this format: // Does not return anything or take any arguments func doWork() { // Code } class precedes declarations of class functions: // Call on a class, e.g. MyClass.someClassFunction() class func someClassFunction() { // Code } static is similar to class functions where you don‚Äôt need an instance of the class or struct in order to call a method on it: // Call on a class/struct, e.g. MyStruct.someStaticFunction() static func someStaticFunction() { // Code } Declare instance functions: // Called on an instance of a class, e.g. myClass.someInstanceFunction() func doMoreWork() { // Code } Function arguments are declared within the parentheses: // Draws a point func draw(point: CGPoint) Return types are declared as follows: // Returns a String object for the given String argument func sayHelloToMyLilFriend(lilFriendsName: String) -\u003e String { return \"Oh hello, \\(lilFriendsName). Cup of tea?\" } You can have multiple return values, referred to as a tuple: // Returns multiple objects func sayHelloToMyLilFriend(lilFriendsName: String) -\u003e (msg: String, nameLength: Int) { return (\"Oh hello, \\(lilFriendsName). Cup of tea?\", countElements(lilFriendsName)) } var hello = sayHelloToMyLilFriend(\"Rob\") print(hello.msg) // \"Oh hello, Rob. Cup of tea?\" print(hello.nameLength) // 3 And those multiple return values can be optional: func sayHelloToMyLilFriend(lilFriendsName: String) -\u003e (msg: String, nameLength: Int)? By default, external parameter names are given when you call the function, but you can specify that one or more are not shown in the method signature by putting a _ symbol in front of the parameter name: func sayHelloToMyLilFriend(_ lilFriendsName: String) { // Code } sayHelloToMyLilFriend(\"Rob\") or you can rename the variable once within the method scope: func sayHelloToMyLilFriend(friendsName lilFriendsName: String) { // Code } sayHelloToMyLilFriend(friendsName: \"Rob\") // and local variable is `lilFriendsName` You can also specify default values for the parameters: func sayHelloToMyLilFriend(_ lilFriendsName: String = \"Rob\") { // Code } sayHelloToMyLilFriend() // \"Oh hello, Rob. Cup of tea?\" sayHelloToMyLilFriend(\"Jimbob\") // \"Oh hello, Jimbob. Cup of tea?\" Swift also supports variadic parameters so you can have an open-ended number of parameters passed in: func sayHelloToMyLilFriends(_ lilFriendsName: String...) { // Code } sayHelloToMyLilFriends(\"Rob\", \"Jimbob\", \"Cletus\") // \"Oh hello, Rob, Jimbob and Cletus. Cup of tea?\" And lastly, you can also use a prefix to declare input parameters as inout. An in-out parameter has a value that is passed in to the function, is modified by the function, and is passed back out of the function to replace the original value. You may remember inout parameters from Objective-C where you had to sometimes pass in an \u0026error parameter to certain methods, where the \u0026 symbol specifies that you‚Äôre actually passing in a pointer to the object instead of the object itself. The same applies to Swift‚Äôs inout parameters now as well. Calling Functions Functions are called using dot syntax: myClass.doWork() or self.sayHelloToMyLilFriend(\"Rob Phillips\") self is a reference to the function‚Äôs containing class. At times, it is necessary to call a function in the superclass using super.someMethod(). Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:8:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Constants and Variables Declaring a constant or variable allows you to maintain a reference to an object within a class or to pass objects between classes. Constants are defined with let and variables with var. By nature, constants are obviously immutable (i.e. cannot be changed once they are instantiated) and variables are mutable. class MyClass { let text = \"Hello\" // Constant var isComplete: Bool // Variable } There are many ways to declare properties in Swift, so here are a few examples: var myInt = 1 // inferred type var myExplicitInt: Int = 1 // explicit type var x = 1, y = 2, z = 3 // declare multiple variables let (a,b) = (1,2) // declare multiple constants Getters and Setters In Objective-C, variables were backed by getters, setters, and private instance variables created at build time. However, in Swift getters and setters are only used for computed properties and constants actually don‚Äôt have a getter or setter at all. The getter is used to read the value, and the setter is used to write the value. The setter clause is optional, and when only a getter is needed, you can omit both clauses and simply return the requested value directly. However, if you provide a setter clause, you must also provide a getter clause. You can overrride the getter and setter of a property to create the illusion of the Objective-C property behavior, but you‚Äôd need to store them as a private property with a different name (not recommended for most scenarios): private var _x: Int = 0 var x: Int { get { print(\"Accessing x...\") return _x } set { print(\"Setting x...\") _x = newValue } } Property Observer Swift also has callbacks for when a property will be or was set using willSet and didSet shown below: willset: before assignment didSet: after assignment class LightBulb { static let maxCurrent = 30 var current = 0 { willSet(newCurrent) { // do something before value assignment // newValue -\u003e newCurrent print(\"Current value changed, the change is \\(abs(current- newCurrent))\") } didSet { // do somthing afther value assignment if current == LightBulb.maxCurrent { print(\"current get to maximum point\") } // oldValue } } } let bulb = LightBulb() bulb.current = 20 bulb.current = 30 bulb.current = 40 Back to top Lazy Property lazy: only compute once and remember the value, won‚Äôt re-compute if called again. class ClosedRange { let start: Int let end: Int var width: Int { return end - start +1 } // note the = lazy var sum: Int = { var res = 0 print(\"run\") for i in self.start...self.end{ res += 1 } return }() // don't forget () init?(start: Int, end: Int){ if start \u003e end { return nil } self.start = start self.end = end } } // example if let range = ClosedRange(start: 0, end: 10_000) { range.width //1001 range.sum // will print out \"run\" range.sum // range.sum // } Back to top Accessing Local Variables Local variables and constants only exist within the scope of a function. func doWork() { let localStringVariable = \"Some local string variable.\" self.doSomething(string: localStringVariable) } Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:9:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Naming Conventions The general rule of thumb: Clarity and brevity are both important, but clarity should never be sacrificed for brevity. Functions and Properties These both use camelCase where the first letter of the first word is lowercase and the first letter of each additional word is capitalized. Class names and Protocols These both use CapitalCase where the first letter of every word is capitalized. ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:10:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Enums The options in an enum should be lowerCamelCased Functions These should use verbs if they perform some action (e.g. performInBackground). You should be able to infer what is happening, what arguments a function takes, or what is being returned just by reading a function signature. Example: // Correct func move(from start: Point, to end: Point) {} // Incorrect (likely too expressive, but arguable) func moveBetweenPoints(from start: Point, to end: Point) {} // Incorrect (not expressive enough and lacking argument clarity) func move(x: Point, y: Point) {} Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:10:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Closures Closures in Swift are similar to blocks in Objective-C and are essentially chunks of code, typically organized within a {} clause, that are passed between functions or to execute code as a callback within a function. Swift‚Äôs func functions are actually just a special case of a closure in use. Syntax { (params) -\u003e returnType in statements } Examples // Map just iterates over the array and performs whatever is in the closure on each item let people = [\"Rob\", \"Jimbob\", \"Cletus\"] people.map({ (person: String) -\u003e String in \"Oh hai, \\(person)...\" }) // Oh hai, Rob // Oh hai, Jimbob // Oh hai, Cletus // Closure for alphabetically reversing an array of names, where sorted is a Swift library function let names = [\"Francesca\", \"Joe\", \"Bill\", \"Sally\", ] var reversed = names.sorted { (s1: String, s2: String) -\u003e Bool in return s1 \u003e s2 } // Or on a single line: reversed = names.sorted{ (s1: String, s2: String) -\u003e Bool in return s1 \u003e s2 } // Or because Swift can infer the Bool type: reversed = names.sorted { s1, s2 in return s1 \u003e s2 } // Or because the return statement is implied: reversed = names.sorted { s1, s2 in s1 \u003e s2 } // Or even shorter using shorthand argument names, such as $0, $1, $2, etc.: reversed = names.sorted { $0 \u003e $1 } // Or just ridiculously short because Swift's String greater-than operator implementation exactly matches this function definition: reversed = names.sorted(by: \u003e) If the closure is the last parameter to the function, you can also use the trailing closure pattern. This is especially useful when the closure code is especially long and you‚Äôd like some extra space to organize it: func someFunctionThatTakesAClosure(closure: () -\u003e ()) { // function body goes here } // Instead of calling like this: someFunctionThatTakesAClosure({ // closure's body goes here }) // You can use trailing closure like this: someFunctionThatTakesAClosure() { // trailing closure's body goes here } Capturing Values A closure can capture constants and variables from the surrounding context in which it is defined. The closure can then refer to and modify the values of those constants and variables from within its body, even if the original scope that defined the constants and variables no longer exists. In Swift, the simplest form of a closure that can capture values is a nested function, written within the body of another function. A nested function can capture any of its outer function‚Äôs arguments and can also capture any constants and variables defined within the outer function. func makeIncrementor(forIncrement amount: Int) -\u003e () -\u003e Int { var runningTotal = 0 func incrementor() -\u003e Int { runningTotal += amount return runningTotal } return incrementor } Swift determines what should be captured by reference and what should be copied by value. You don‚Äôt need to annotate a variable to say that they can be used within the nested function. Swift also handles all memory management involved in disposing of variables when they are no longer needed by the function. Capturing Self If you create a closure that references self.* it will capture self and retain a strong reference to it. This is sometimes the intended behavior, but often could lead to retain cycles where both objects won‚Äôt get deallocated at the end of their lifecycles. The two best options are to use unowned or weak. This might look a bit messy, but saves a lot of headache. Use unowned when you know the closure will only be called if self still exists, but you don‚Äôt want to create a strong (retain) reference. Use weak if there is a chance that self will not exist, or if the closure is not dependent upon self and will run without it. If you do use weak also remember that self will be an optional variable and should be checked for existence. typealias SomeClosureType = (_ value: String) -\u003e () class SomeClass { fileprivate var currentValue = \"\" init() { someMethod { (value) in // Retained self self.currentValue = value } someMethod { [unowned self] (value) in // Not retained, but ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:11:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Generics Coming soon‚Ä¶ Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:12:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Control Statements Swift uses all of the same control statements that other languages have: If-Else If-Else if someTestCondition { // Code to execute if the condition is true } else if someOtherTestCondition { // Code to execute if the other test condition is true } else { // Code to execute if the prior conditions are false } As you can see, parentheses are optional. Ternary Operators The shorthand notation for an if-else statement is a ternary operator of the form: someTestCondition ? doIfTrue : doIfFalse Example: func stringForTrueOrFalse(trueOrFalse: Bool) -\u003e String { return trueOrFalse ? \"True\" : \"False\" } Nil Coalescing Operators In Swift, we need to consider the use of optional values. One very basic way to handle nil cases is with an if-else statement: func stringForOptionalExistence(optionalValue: String?) -\u003e String { if optionalValue != nil { return optionalValue } else { return \"Empty\" } } In this particular case, we are returning optionalValue if it is not nil, and \"Empty\" if optionalValue is nil. The shorthand notation for this type of if(!=nil)-else statement is a nil coalescing operator of the form: optionalValue ?? nonOptionalValue Example: func stringForOptionalExistence(optionalValue: String?) -\u003e String { return optionalValue ?? \"Empty\" } For Loops Swift enables you to use ranges inside of for loops now: for index in 1...5 { print(\"\\(index)times 5 is \\(index * 5)\") } // Or if you don't need the value of the index let base = 3, power = 10 var answer = 1 for _ in 1...power { answer *= base } print(\"\\(base)to the power of \\(power)is \\(answer)\") // prints \"3 to the power of 10 is 59049\" Enumerating arrays \u0026 dictionaries // We explicitly cast to the Movie class from AnyObject class for movie in someObjects as [Movie] { // Code to execute each time } // Enumerating simple array let names = [\"Anna\", \"Alex\", \"Brian\", \"Jack\"] for name in names { print(\"Hello, \\(name)!\") } // Enumerating simple dictionary let numberOfLegs = [\"spider\": 8, \"ant\": 6, \"cat\": 4] for (animalName, legCount) in numberOfLegs { print(\"\\(animalName)s have \\(legCount)legs\") } If you need to cast to a certain object type, see the earlier discussion about the as! and as? keywords. While Loop while someTestCondition { // Code to execute while the condition is true } Repeat While Loop repeat { // Code to execute while the condition is true } while someTestCondition Switch Switch statements are often used in place of if statements if there is a need to test if a certain variable matches the value of another constant or variable. For example, you may want to test if an error code integer you received matches an existing constant value or if it‚Äôs a new error code. switch errorStatusCode { case .network: // Code to execute if it matches case .wifi: // Code to execute if it matches default: // Code to execute if nothing else matched } Switch statements in Swift do not fall through the bottom of each case and into the next one by default. Instead, the entire switch statement finishes its execution as soon as the first matching switch case is completed, without requiring an explicit break statement. This makes the switch statement safer and easier to use than in C, and avoids executing more than one switch case by mistake. Exiting Loops Although break is not required in Swift, you can still use a break statement to match and ignore a particular case, or to break out of a matched case before that case has completed its execution. return : Stops execution and returns to the calling function. It can also be used to return a value from a function. break : Used to stop the execution of a loop. Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:13:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Extension Extensions add new functionality to an existing class, structure, enumeration or protocol type extension String { // Extending String type to calculate if a String instance is truthy of falsy var boolValue:Bool { if self == \"1\" return true } return false } let isTrue = \"0\".boolValue Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:14:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Protocol Define protocol Codable { // definitions var description: String var mustBeSettable: Int { get set } var doesNotNeedToBeSettable: Int { get } func dance () -\u003e Double static func someTypeMethod() mutating func toggle() // modify (or mutate) the instance it belongs to init(someParameter: Int) // require specific initializers } Usage import Foundation struct UserInfo: Codable { let username: String let loginCount: Int } extension UserInfo: CustomStringConvertible { var description: String { return \"\\(username)has tried to login \\(loginCount)time(s)\" } } ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:15:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Protocol Extension Protocols can be extended to provide method, initializer, subscript, and computed property implementations to conforming types. Very Important and Useful: Implementation to any method or computed property requirement of that protocol can only be in extension extension RandomNumberGenerator { func randomBool() -\u003e Bool { return random() \u003e 0.5 } } By creating an extension on the protocol, all conforming types automatically gain this method implementation without any additional modification. You can use protocol extensions to provide a default implementation to any method or computed property requirement of that protocol. extension PrettyTextRepresentable { var prettyTextualDescription: String { return textualDescription } } Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:15:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Error Handling Representing an Error enum BeverageMachineError: Error { case invalidSelection case insufficientFunds case outOfStock } func selectBeverage (_ selection: Int) throws -\u003e String { // do something return \"Waiting for beverage...\" } // us do...catch to handle error throwed by func let message:String do { message = try selectBeverage(20) } catch BeverageMachineError.invalidSelection { print(\"Invalid selection\") } catch BeverageMachineError.insufficientFunds { print(\"Insufficient Funds\") } catch BeverageMachineError.outOfStock { print(\"Out of Stock\") } catch { print (\"Generic error\") } // if throw error, return nil let message = try? selectBeverage(10) // if throw error, get a runtime error let message = try! selectBeverage(10) Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:16:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Passing Information Coming soon‚Ä¶ Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:17:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"User Defaults User defaults are basically a way of storing simple preference values which can be saved and restored across app launches. It is not meant to be used as a data storage layer, like Core Data or sqlite. ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:18:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Storing Values let userDefaults = UserDefaults.standard userDefaults.setValue(\"Some Value\", forKey: \"RPSomeUserPreference\") ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:18:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Retrieving Values let userDefaults = UserDefaults.standard let someValue = userDefaults.value(forKey: \"RPSomeUserPreference\") as AnyObject? There are also other convenience functions on UserDefaults instances such as bool(forKey:...), string(forKey:...), etc. Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:18:2","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Common Patterns For a comprehensive list of design patterns, as established by the Gang of Four, look here: Design Patterns in Swift ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:19:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Singletons Singleton‚Äôs are a special kind of class where only one instance of the class exists for the current process. They are a convenient way to share data between different parts of an app without creating global variables or having to pass the data around manually, but they should be used sparingly since they often create tighter coupling between classes. To turn a class into a singleton, you use the following implementation where the function name is prefixed with shared plus another word which best describes your class. For example, if the class is a network or location manager, you would name the function sharedManager instead of sharedInstance. class MyClass { // MARK:- Instantiation // Naming convention: // sharedInstance, sharedManager, sharedController, etc. // depending on the class type static let sharedInstance = MyClass() // This prevents others from using the default '()' initializer for this class. private init() {} var isReady = true // More class code here } Explanation: The static constant sharedInstance is run as dispatch_once the first time that variable is accessed to make sure the initialization is atomic. This ensures it is thread safe, fast, lazy, and also bridged to ObjC for free. More from here. Usage: You would get a reference to that singleton class in another class with the following code: // Now you could do let myClass = MyClass.sharedInstance let answer = myClass.isReady ? \"Yep!\" : \"Nope!\" print(\"Are you ready to rock and roll? \\(answer)\") Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:19:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Unicode Support Although I don‚Äôt recommend this, Swift will compile even if you use emoji‚Äôs in your code since it offers Unicode support. More info from Apple here Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:20:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"FileIO ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:21:0","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"C style FileIO let fd = fopen(\"aFile.txt\", \"w\") fwrite(\"Hello Swift!\", 12, 1, fd) let res = fclose(file) if res != 0 { print(strerror(errno)) } let fd = fopen(\"aFile.txt\", \"r\") var array = [Int8](count: 13, repeatedValue: 0) fread(\u0026array, 12, 1, fd) fclose(fd) let str = String.fromCString(array) print(str) // Hello Swift! ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:21:1","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Coding"],"content":"Swift Style FileIO let path = Bundle.main.path(forResource:\"test\", ofType: \"txt\") // read let lines = try? String(contentsOfFile: path!) .split{$0 == \"\\n\"} .map(String.init) // write do { let lines = self._outlines.joined(separator: \"\\n\") try lines.write(to: url, atomically: false, encoding: .utf8) } catch{} Back to top ","date":"2020-05-30","objectID":"/2020-05-30-swift-cheat-sheet/:21:2","tags":["Swift"],"title":"Swift Cheat Sheet","uri":"/2020-05-30-swift-cheat-sheet/"},{"categories":["Machine Learning"],"content":"ÊèêÂçáÔºàBoostingÔºâÊñπÊ≥ïÔºö ÈÄöËøáÊîπÂèòËÆ≠ÁªÉÊ†∑Êú¨ÁöÑÊùÉÈáçÔºàÊ¶ÇÁéáÂàÜÂ∏ÉÔºâÔºåÂ≠¶‰π†n‰∏™ÂàÜÁ±ªÂô®ÔºåÂπ∂Â∞ÜËøô‰∫õÂàÜÁ±ªÂô®Á∫øÊÄßÁªÑÂêàÔºåÊèêÈ´òÂàÜÁ±ªÊÄßËÉΩ„ÄÇ ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:0:0","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"1. AdaBoost AdaBoostÈÄöËøáÊèêÈ´òË¢´Ââç‰∏ÄËΩÆÂº±ÂàÜÁ±ªÂô®ÈîôËØØÂàÜÁ±ªÊ†∑Êú¨ÁöÑÊùÉÂÄºÔºå‰ªéËÄåÈôç‰ΩéË¢´Ê≠£Á°ÆÂàÜÁ±ªÊ†∑Êú¨ÁöÑÊùÉÂÄºÔºåÂπ∂ÈááÂèñÂä†ÊùÉÂ§öÊï∞Ë°®ÂÜ≥ÁöÑÊñπÊ≥ïËææÂà∞ÂàÜÁ±ªÁõÆÁöÑ„ÄÇ ËæìÂÖ•ÔºöËÆ≠ÁªÉÊï∞ÊçÆÈõÜ$T={(x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)}$, $\\mathcal{Y} = {-1,+1}$; ËæìÂá∫ÔºöÂàÜÁ±ªÂô®$G(x)$ 1). ÂàùÂßãÂåñËÆ≠ÁªÉÊï∞ÊçÆÊùÉÂÄºÂàÜÂ∏É $$D_1 = (w_{11}, \\cdots, w_{1i}, \\cdots, w_{1N}), w_{1i} = \\frac{1}{N}, i = 1,2,\\cdots,N$$ 2). ÂØπ $m = 1Ôºå2Ôºå\\cdots, M$ a.ÂØπÊùÉÂÄºÂàÜÂ∏É$D_m$ÁöÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÂ≠¶‰π†ÔºåÂæóÂà∞Âü∫Êú¨ÂàÜÁ±ªÂô® $$ G_{m}(x): \\mathcal{X} \\rightarrow{-1,+1} $$ b.ËÆ°ÁÆó$G(x)$Âú®ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰∏äÁöÑÂàÜÁ±ªËØØÂ∑ÆÁéá $$ e_{m}=\\sum_{i=1}^{N} P\\left(G_{m}\\left(x_{i}\\right) \\neq y_{i}\\right)=\\sum_{i=1}^{N} w_{m i} I\\left(G_{m}\\left(x_{i}\\right) \\neq y_{i}\\right) $$ c. ËÆ°ÁÆó$G(x)$ÁöÑÁ≥ªÊï∞ $$ \\alpha_{m}=\\frac{1}{2} \\log \\frac{1-e_{m}}{e_{m}} $$ d. Êõ¥Êñ∞ËÆ≠ÁªÉÊï∞ÊçÆÁöÑÊùÉÂÄºÂàÜÂ∏É $$ D_{m+1}=\\left(w_{m+1,1}, \\cdots, w_{m+1, i}, \\cdots, w_{m+1, N}\\right) $$ $$ w_{m+1, i} = \\frac{w_{m i}}{Z_{m}} \\exp \\left(-\\alpha_{m} y_{i} G_{m}\\left(x_{i}\\right)\\right), \\quad i=1,2, \\cdots, N $$ ÂÖ∂‰∏≠Ôºå $$ Z_{m}=\\sum_{i=1}^{N} w_{m i} \\exp \\left(-\\alpha_{m} y_{i} G_{m}\\left(x_{i}\\right)\\right) $$ 3ÔºâÊûÑÂª∫Âü∫Êú¨Á∫øÊÄßÂàÜÁ±ªÂô®ÁªÑÂêà $$ f(x)=\\sum_{m=1}^{M} \\alpha_{m} G_{m}(x) $$ ÂæóÂà∞ÊúÄÁªàÂàÜÁ±ªÂô® $$ \\begin{aligned} G(x) \u0026=\\operatorname{sign}(f(x)) \\cr \u0026=\\operatorname{sign}\\left(\\sum_{m=1}^{M} \\alpha_{m} G_{m}(x)\\right) \\end{aligned} $$ ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:1:0","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"1.1 AdaBoostÁÆóÊ≥ïËØØÂ∑Æ AdaBoostÁÆóÊ≥ïÊúÄÁªàÂàÜÁ±ªÂô®ËÆ≠ÁªÉËØØÂ∑ÆÁïå‰∏∫ $$ \\frac{1}{N} \\sum_{i=1}^{N} I\\left(G\\left(x_{i}\\right) \\neq y_{i}\\right) \\leqslant \\frac{1}{N} \\sum_{i} \\exp \\left(-y_{i} f\\left(x_{i}\\right)\\right)=\\prod_{m} Z_{m} $$ Ëøô‰∏ÄÂÆöÁêÜËØ¥ÊòéÔºåÊØè‰∏ÄËΩÆÈÄâÂèñÈÄÇÂΩìÁöÑ$G_m$‰Ωø$Z_m$ÊúÄÂ∞èÔºå‰ªéËÄå‰ΩøËÆ≠ÁªÉËØØÂ∑Æ‰∏ãÈôçÊúÄÂø´„ÄÇ ÂØπ‰∫é‰∫åÂàÜÁ±ªÈóÆÈ¢òÔºö $$ \\begin{aligned} \\prod_{m=1}^{M} Z_{m} \u0026=\\prod_{m=1}^{M}[2 \\sqrt{e_{m}\\left(1-e_{m}\\right)}] \\cr \u0026=\\prod_{m=1}^{M} \\sqrt{\\left(1-4 \\gamma_{m}^{2}\\right)} \\cr \u0026 \\leqslant \\exp \\left(-2 \\sum_{m=1}^{M} \\gamma_{m}^{2}\\right) \\end{aligned} $$ ÂÖ∂‰∏≠Ôºå $\\gamma_{m}=\\frac{1}{2}-e_{m}$ ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:1:1","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"1.2 AdaBoostÁÆóÊ≥ïËß£Èáä AdaBooostÂèØ‰ª•ËÆ§‰∏∫ÔºöÊ®°Âûã‰∏∫Âä†Ê≥ïÊ®°ÂûãÔºåÊçüÂ§±ÂáΩÊï∞‰∏∫ÊåáÊï∞ÂáΩÊï∞ÔºåÂ≠¶‰π†ÁÆóÊ≥ï‰∏∫ÂâçÂêëÂàÜÂ∏ÉÁÆóÊ≥ïÁöÑ‰∫åÂàÜÁ±ªÂ≠¶‰π†ÊñπÊ≥ï 1.2.1 ÂâçÂêëÂàÜÊ≠•ÁÆóÊ≥ï ËÄÉËôëÂä†Ê≥ïÊ®°ÂûãÔºàadditive modelÔºâ $$ f(x)=\\sum_{m=1}^{M} \\beta_{m} b\\left(x ; \\gamma_{m}\\right) $$ ÂÖ∂‰∏≠Ôºå$b(x; \\gamma_m)$‰∏∫Âü∫ÂáΩÊï∞Ôºå$gamma_m$‰∏∫ÂèÇÊï∞Ôºå $\\beta_m$‰∏∫Á≥ªÊï∞„ÄÇ Âú®ÁªôÂÆöËÆ≠ÁªÉÈõÜÂíåÊçüÂ§±ÂáΩÊï∞$L(y,f(x))$ÁöÑÊù°‰ª∂‰∏ãÔºåÂ≠¶‰π†Âä†Ê≥ïÊ®°Âûã$f(x)$Êàê‰∏∫ÁªèÈ™åÈ£éÈô©ÊûÅÂ∞èÂåñÔºàÊçüÂ§±ÂáΩÊï∞ÊûÅÂ∞èÂåñÔºâÈóÆÈ¢òÔºö $$ \\min_{\\beta_{m}, \\gamma_{m}} \\sum_{i=1}^{N} L\\left(y_{i}, \\sum_{m=1}^{M} \\beta_{m} b\\left(x_{i} ; \\gamma_{m}\\right)\\right) $$ ÂâçÂêëÂàÜÂ∏ÉÁÆóÊ≥ïÊÄùÊÉ≥ÊòØÔºö ‰ªéÂâçÂêëÂêéÔºåÊØè‰∏ÄÊ≠•Âè™Â≠¶‰∏Ä‰∏™Âü∫ÂáΩÊï∞ÂèäÂÖ∂Á≥ªÊï∞ÔºåÈÄêÊ≠•ÈÄºËøë‰ºòÂåñÁõÆÊ†áÂáΩÊï∞ÔºåËææÂà∞‰ºòÂåñÊ≠•È™§ÁÆÄÂåñÁöÑÁõÆÁöÑ„ÄÇ Âõ†Ê≠§ÔºåÊØè‰∏ÄÊ≠•Âè™ÈúÄ‰ºòÂåñÂ¶Ç‰∏ãÊçüÂ§±ÂáΩÊï∞Ôºö $$ \\min_{\\beta, \\gamma} \\sum_{i=1}^{N} L\\left(y_{i}, \\beta b\\left(x_{i} ; \\gamma\\right)\\right) $$ ÁÆóÊ≥ïÊ≠•È™§ ËæìÂÖ•ÔºöËÆ≠ÁªÉÊï∞ÊçÆÈõÜ$T=\\lbrace (x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)\\rbrace$, ÊçüÂ§±ÂáΩÊï∞$L(y,f(x))$;Âü∫ÂáΩÊï∞ÈõÜ$\\lbrace b(x;\\gamma) \\rbrace$; ËæìÂá∫ÔºöÂä†Ê≥ïÊ®°Âûã$f(x)$ 1ÔºâÂàùÂßãÂåñ$f_0(x) = 0$ 2) ÂØπ$m = 1,2,\\cdots, M$ a.ÊûÅÂ∞èÂåñÊçüÂ§±ÂáΩÊï∞ $$ \\left(\\beta_{m}, \\gamma_{m}\\right)=\\arg \\min _{\\beta, \\gamma} \\sum_{i=1}^{N} L\\left(y_{i}, f_{m-1}\\left(x_{i}\\right)+\\beta b\\left(x_{i} ; \\gamma\\right)\\right) $$ ÂæóÂà∞ÂèÇÊï∞$\\beta_m$, $\\gamma_m$„ÄÇ b.Êõ¥Êñ∞ $$ f_{m}(x)=f_{m-1}(x)+\\beta_{m} b\\left(x ; \\gamma_{m}\\right) $$ 3ÔºâÂæóÂà∞Âä†Ê≥ïÊ®°Âûã $$ f(x)=f_{M}(x)=\\sum_{m=1}^{M} \\beta_{m} b\\left(x ; \\gamma_{m}\\right) $$ ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:1:2","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"2. Boosting Tree ÊèêÂçáÊ†ëüå≤ÊòØ‰ª•ÂÜ≥Á≠ñÊ†ë‰∏∫Âü∫Êú¨ÂàÜÁ±ªÂô®ÁöÑÊèêÂçáÊñπÊ≥ï ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:2:0","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"2.1 ÊèêÂçáÊ†ëÊ®°Âûã ÈááÁî®Âä†Ê≥ïÊ®°ÂûãÔºàÂü∫ÂáΩÊï∞ÁöÑÁ∫øÊÄßÁªÑÂêàÔºâ‰∏éÂâçÂêëÂàÜÂ∏ÉÁÆóÊ≥ïÔºö $$ f_{M}(x)=\\sum_{m=1}^{M} T\\left(x ; \\Theta_{m}\\right) $$ ÂÖ∂‰∏≠ $T\\left(x ; \\Theta_{m}\\right)$Ë°®Á§∫ÂÜ≥Á≠ñÊ†ëÔºå$\\Theta_{m}$ÂÜ≥Á≠ñÊ†ëÂèÇÊï∞Ôºå $M$‰∏∫Ê†ëÁöÑ‰∏™Êï∞ ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:2:1","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"2.2 ÊèêÂçáÊ†ëÁÆóÊ≥ï ÈááÁî®Âä†Ê≥ïÊ®°ÂûãÂíåÂâçÂêëÂàÜÂ∏ÉÁÆóÊ≥ïÂÆûÁé∞Â≠¶‰π†‰ºòÂåñÁöÑËøáÁ®ã„ÄÇ È¶ñÂÖàÁ°ÆÂÆöÊèêÂçáÊ†ë$f_{0}(x)=0$Ôºå Á¨¨$m$Ê≠•ÁöÑÊ®°ÂûãÊòØ $$ f_{m}(x)=f_{m-1}(x)+T\\left(x ; \\Theta_{m}\\right) $$ ÂÖ∂‰∏≠Ôºå $f_{m-1}(x)$‰∏∫ÂΩìÂâçÊ®°ÂûãÔºåÈÄöËøáÁªèÈ™åÈ£éÈô©ÊûÅÂ∞èÂåñÁ°ÆÂÆö‰∏ã‰∏ÄÂàªÂÜ≥Á≠ñÊ†ëÁöÑÂèÇÊï∞$\\Theta_{m}$Ôºö $$ \\hat \\Theta_m = \\arg \\min_{\\Theta_{m}} \\sum_{i=1}^{N} L(y_{i}, f_{m-1} (x_{i})+T (x_{i} ; \\Theta_{m} )) $$ 2.2.1 ÂõûÂΩíÈóÆÈ¢òÊèêÂçáÊ†ë ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ: $T=\\lbrace (x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)\\rbrace$, $x_{i} \\in \\mathcal{X} \\subseteq \\mathbf{R}^{n}$, $\\mathcal{X}$‰∏∫ËæìÂÖ•Á©∫Èó¥Ôºå $\\mathcal{Y} \\subseteq \\mathbf{R}$; Â∞ÜËæìÂÖ•Á©∫Èó¥ÂàíÂàÜ‰∏∫$J$‰∏™‰∫í‰∏çÁõ∏‰∫§ÁöÑÂå∫Âüü$R1ÔºåR2, \\cdots, R_J$Ôºå Âπ∂‰∏îÊØè‰∏™Âå∫Âüü‰∏äÁ°ÆÂÆöËæìÂá∫ÁöÑÂ∏∏Èáè$c_j$ÔºåÈÇ£‰πàÊ†ëÂèØ‰ª•Ë°®Á§∫‰∏∫Ôºö $$ T(x ; \\Theta)=\\sum_{j=1}^{J} c_{j} I\\left(x \\in R_{j}\\right) $$ ÂÖ∂‰∏≠Ôºå $$ \\Theta=\\lbrace \\left(R_{1}, c_{1}\\right),\\left(R_{2}, c_{2}\\right), \\cdots,\\left(R_{J}, c_{J}\\right)\\rbrace $$ Ë°®Á§∫Ê†ëÁöÑÂç¥‰∏éÂàíÂàÜÂíåÂêÑ‰∏™ÂèñÊÇ¶ÊòØÈÇ£‰∏™ÁöÑÂ∏∏Êï∞„ÄÇ ÈááÁî®‰∏Ä‰∏ãÂâçÂêëÂàÜÂ∏ÉÁÆóÊ≥ï $$ \\begin{aligned} \u0026f_{0}(x)=0\\cr \u0026\\begin{array}{l} f_{m}(x)=f_{m-1}(x)+T\\left(x ; \\Theta_{m}\\right), \\quad m=1,2, \\cdots, M \\cr f_{M}(x)=\\sum_{m=1}^{M} T\\left(x ; \\Theta_{m}\\right) \\end{array} \\end{aligned} $$ Ê±ÇËß£$\\hat \\Theta_{m}$Ôºå Ëã•Áî®Âπ≥ÊñπËØØÂ∑ÆÊçüÂ§±ÂáΩÊï∞Ôºö $$ L(y, f(x))=(y-f(x))^{2} $$ ÂàôÊçüÂ§±ÂáΩÊï∞‰∏∫Ôºö $$ \\begin{aligned} L\\left(y, f_{m-1}(x)+T\\left(x ; \\Theta_{m}\\right)\\right) \u0026=\\left[y-f_{m-1}(x)-T\\left(x ; \\Theta_{m}\\right)\\right]^{2} \\cr \u0026=\\left[r-T\\left(x ; \\Theta_{m}\\right)\\right]^{2} \\end{aligned} $$ ËøôÈáåÔºå $$ r=y-f_{m-1}(x) $$ ÊòØÂΩìÂâçÊ®°ÂûãÊãüÂêàÊï∞ÊçÆÁöÑÊÆãÂ∑ÆÔºàresidualÔºâ„ÄÇÂõ†Ê≠§ÂØπ‰∫éÂõûÂΩíÈóÆÈ¢òÊèêÂçáÊ†ëÔºåÂè™ÈúÄÊãüÂêàÂΩìÂâçÊ®°ÂûãÊÆãÂ∑Æ„ÄÇÂæóÂà∞$T\\left(x ; \\Theta_{m}\\right)$ÔºåÊõ¥Êñ∞Ê®°ÂûãÔºåÂæóÂà∞$f_m(x)$„ÄÇ ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:2:2","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"3. Ê¢ØÂ∫¶ÊèêÂçá ÂΩìÊçüÂ§±ÂáΩÊï∞‰∏çÊòØÁÆÄÂçïÁöÑÂπ≥ÊñπÊçüÂ§±„ÄÅÊåáÊï∞ÊçüÂ§±Êó∂ÔºåÊèêÂçáÊ†ëÁöÑ‰ºòÂåñÂ∞±ÂæàÈöæ„ÄÇÊ¢ØÂ∫¶ÊèêÂçáÁÆóÊ≥ïÂà©Áî®ÊúÄÈÄü‰∏ãÈôçÊ≥ïÁöÑËøë‰ººÊñπÊ≥ïÔºåËÆ°ÁÆóÊçüÂ§±ÂáΩÊï∞ÁöÑË¥üÊ¢ØÂ∫¶Âú®ÂΩìÂâçÊ®°ÂûãÁöÑÂÄº $$ -\\left[\\frac{\\partial L\\left(y, f\\left(x_{i}\\right)\\right)}{\\partial f\\left(x_{i}\\right)}\\right]_{f(x)=f_{m-1}(x)} $$ Âπ∂Â∞ÜÂÖ∂‰Ωú‰∏∫ÂõûÂΩíÈóÆÈ¢òÊèêÂçáÊ†ëÁÆóÊ≥ï‰∏≠ÁöÑÊÆãÂ∑ÆËøë‰ººÂÄºÔºåÊãüÂêà‰∏Ä‰∏™ÂõûÂΩíÊ†ë„ÄÇ ËæìÂÖ•Ôºö ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ$T=\\lbrace (x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)\\rbrace$, $x_{i} \\in \\mathcal{X} \\subseteq \\mathbf{R}^{n}$,$\\mathcal{X}$‰∏∫ËæìÂÖ•Á©∫Èó¥Ôºå $\\mathcal{Y} \\subseteq \\mathbf{R}$; ÊçüÂ§±ÂáΩÊï∞$L(y,f(x))$ ËæìÂá∫Ôºö ÂõûÂΩíÊ†ë$\\hat f(x)$ ÂàùÂßãÂåñ $$ f_{0}(x)=\\arg \\min _{c} \\sum_{i=1}^{N} L\\left(y_{i}, c\\right) $$ ÂØπ $m=1Ôºå2Ôºå\\cdots, M$ (1) ÂØπ $i=1Ôºå2Ôºå\\cdots, N$ËÆ°ÁÆó $$ r_{m i}=-\\left[\\frac{\\partial L\\left(y_{i}, f\\left(x_{i}\\right)\\right)}{\\partial f\\left(x_{i}\\right)}\\right]_{f(x)=f_{m-1}(x)} $$ (2) ÂØπ$r_{mi}$ÊãüÂêà‰∏Ä‰∏™ÂõûÂΩíÊ†ëÔºåÂæóÂà∞Á¨¨$m$È¢óÊ†ëÁöÑËäÇÁÇπÂå∫Âüü$R_{mj}$ (3) ÂØπ$j=1,2,\\cdots, J$, ËÆ°ÁÆó $$ c_{m j}=\\arg \\min _{c} \\sum_{x_{i} \\in R_{m j}} L\\left(y_{i}, f_{m-1}\\left(x_{i}\\right)+c\\right) $$ (4)Êõ¥Êñ∞ $$ f_{m}(x)=f_{m-1}(x)+\\sum_{j=1}^{J} c_{m j} I\\left(x \\in R_{m j}\\right) $$ ÂæóÂà∞ÂõûÂΩíÊ†ë $$ \\hat{f}(x)=f_{M}(x)=\\sum_{m=1}^{M} \\sum_{j=1}^{J} c_{m j} I\\left(x \\in R_{m j}\\right) $$ ÂèÇËÄÉÔºö ÊùéËà™„ÄäÁªüËÆ°Â≠¶‰π†ÊñπÊ≥ï„Äã ","date":"2020-05-05","objectID":"/2020-05-05-ml-boosting/:3:0","tags":["Boosting","Statistical Learning"],"title":"Boosting","uri":"/2020-05-05-ml-boosting/"},{"categories":["Machine Learning"],"content":"ÈöêÈ©¨ÂèØÂ§´Ê®°ÂûãÔºàHMMÔºâÊèèËø∞ÈöêËóèÁöÑÈ©¨ÂèØÂ§´ÈìæÈöèÊú∫ÁîüÊàêËßÇÊµãÂ∫èÂàóÁöÑËøáÁ®ãÔºåÂ±û‰∫éÁîüÊàêÊ®°Âûã„ÄÇ HMMÂú®ËØ≠Èü≥ËØÜÂà´„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÁîüÁâ©‰ø°ÊÅØ„ÄÅÊ®°ÂºèËØÜÂà´Á≠âÈ¢ÜÂüüÁî±ÂπøÊ≥õÂ∫îÁî®„ÄÇ ","date":"2020-05-03","objectID":"/2020-05-03-ml-hmm/:0:0","tags":["Hidden Markov Model","Expectation Maximization","Probabilistic Graphical Model","Statistical Learning"],"title":"Hidden Markov Model (HMM)","uri":"/2020-05-03-ml-hmm/"},{"categories":["Machine Learning"],"content":"1. HMMÁöÑÂÆö‰πâ ÈöêÈ©¨ÂèØÂ§´Ê®°ÂûãÊòØÂÖ≥‰∫éÊó∂Â∫èÁöÑÊ¶ÇÁéáÊ®°ÂûãÔºå ÊèèËø∞Áî±‰∏Ä‰∏™ÈöêËóèÁöÑÈ©¨ÂèØÂ§´ÈìæÈöèÊú∫ÁîüÊàê‰∏çÂèØËßÇÊµãÁöÑÁä∂ÊÄÅÔºåÂÜçÁî±ÂêÑ‰∏™Áä∂ÊÄÅÁîüÊàê‰∏Ä‰∏™ËßÇÊµãÔºå‰ªéËÄå‰∫ßÁîüËßÇÊµãÈöèÊú∫Â∫èÂàóÁöÑËøáÁ®ã„ÄÇ ÁÆÄËÄåË®Ä‰πãÔºåÈöêÈ©¨ÂèØÂ§´ÈìæÈöèÊú∫ÊàêÁä∂ÊÄÅÂ∫èÂàóÔºàstate sequenceÔºâÔºåËÄåÊØè‰∏™Áä∂ÊÄÅÁîüÊàêËßÇÊµãÔºå‰∫ßÁîüËßÇÊµãÂ∫èÂàóÔºàobservation sequenceÔºâ„ÄÇÂ∫èÂàóÁöÑ‰∏Ä‰∏™‰ΩçÁΩÆÂèØ‰ª•Áúã‰Ωú‰∏Ä‰∏™Êó∂Âàª„ÄÇ ‰ª§$Q$ Ë°®Á§∫ÊâÄÊúâÂèØËÉΩÁä∂ÊÄÅÁöÑÈõÜÂêàÔºö$Q = { q_1, q_2, \\cdots, q_N }$; ‰ª§$V$ Ë°®Á§∫ÊâÄÊúâÂèØËÉΩÁöÑËßÇÊµãÈõÜÂêàÔºö$V = {v_1, v_2, \\cdots, v_M }$; ‰ª§$I$ Ë°®Á§∫ÈïøÂ∫¶‰∏∫TÁöÑÁä∂ÊÄÅÂ∫èÂàóÔºö $I = (i_1, i_2, \\cdots, i_T)$; ‰ª§$O$ Ë°®Á§∫ÂØπÂ∫îÁöÑÊòØËßÇÊµãÂ∫èÂàóÔºö $O = (o_1, o_2, \\cdots, o_T)$. ‰ª§$A$ÊòØËΩ¨ÁßªÊ¶ÇÁéáÁü©ÈòµÔºö $$A = [a_{ij}]_{N \\times N}$$ ÂÖ∂‰∏≠Ôºå $$a_{ij} = P(i_{t+1} = q_j | i_t = q_j), i=1,2, \\cdots, N; j = 1,2, \\cdots, N$$ ÊòØÂú®Êó∂Âàª$t$Â§Ñ‰∫éÁä∂ÊÄÅ$q_i$ÁöÑÊù°‰ª∂‰∏ãÁîüÊàêËßÇÊµã$t +1$ËΩ¨ÁßªÂà∞Áä∂ÊÄÅ$q_j$ÁöÑÊ¶ÇÁéá„ÄÇ ‰ª§$B$ÊòØËßÇÊµãÊ¶ÇÁéáÁü©ÈòµÔºö $$B = [b_j(k)]_{N \\times M}$$ ÂÖ∂‰∏≠Ôºå $$b_j(k) = P(o_t = v_k | i_t = q_j), k=1,2,\\cdots, M; j=1,2,\\cdots, N$$ ÊòØÂú®Êó∂Âàª$t$Â§Ñ‰∫éÁä∂ÊÄÅ$q_j$ÁöÑÊù°‰ª∂‰∏ãÁîüÊàêËßÇÊµã$v_k$ ÁöÑÊ¶ÇÁéá„ÄÇ ‰ª§$\\pi$ÊòØÂàùÂßãÁä∂ÊÄÅÊ¶ÇÁéáÂêëÈáèÔºö $$\\pi = (\\pi_i)$$ ÂÖ∂‰∏≠Ôºå $$\\pi_{i} = P(i_1 = q_i),i=1,2,\\cdots, N$$ ÊòØÊó∂Âàªt=1Â§Ñ‰∫éÁä∂ÊÄÅ$q_i$ÁöÑÊ¶ÇÁéá. ÈöêÈ©¨ÂèØÂ§´Ê®°Âûã$\\lambda$Áî±$\\pi$Ôºå $A$Ôºå$B$ÂÜ≥ÂÆö„ÄÇ $$\\lambda = (A, B, \\pi)$$ ÂÖ∂‰∏≠Ôºå$\\pi$Âíå$A$ÂÜ≥ÂÆöÁä∂ÊÄÅÂ∫èÂàóÔºå$B$ÂÜ≥ÂÆöËßÇÊµãÂ∫èÂàó„ÄÇ ÈöêÈ©¨ÂèØÂ§´Ê®°ÂûãÁöÑ‰∏§‰∏™Âü∫Êú¨ÂÅáËÆæ ÈΩêÊ¨°È©¨ÂèØÂ§´ÊÄß ÈöêÈ©¨ÂèØÂ§´ÈìæÂú®‰ªªÊÑèÊó∂ÂàªtÁöÑÁä∂ÊÄÅÂâç‰∏ÄÊó∂ÂàªÁä∂ÊÄÅÔºå‰∏éÂÖ∂‰ªñÊó∂ÂàªÁöÑÈöêÁä∂ÊÄÅÂíåËßÇÊµãÊó†ÂÖ≥Ôºå ‰πü‰∏éÊó∂ÂàªtÊó†ÂÖ≥Ôºö $$P(i_t | i_{t-1}, O_{t-1}, \\cdots, i_1, o_1) = P(i_t | i_{t-1}), t = 1,2,\\cdots,T$$ ËßÇÊµãÁã¨Á´ãÊÄß ‰ªªÊÑèÊó∂ÂàªÁöÑËßÇÊµãÂè™‰æùËµñÊîπÊó∂ÂàªÁöÑÈ©¨ÂèØÂ§´ÈìæÁä∂ÊÄÅÔºå‰∏éÂÖ∂‰ªñËßÇÊµãÂíåÁä∂ÊÄÅÊó†ÂÖ≥: $$P(o_t | i_{T}, O_{T}, i_{T-1}, o_{T-1}\\cdots, i_{t+1}, O_{t+1}, i_{t-1}, O_{t-1}, i_1, o_1) = P(o_t | i_{t})$$ ","date":"2020-05-03","objectID":"/2020-05-03-ml-hmm/:1:0","tags":["Hidden Markov Model","Expectation Maximization","Probabilistic Graphical Model","Statistical Learning"],"title":"Hidden Markov Model (HMM)","uri":"/2020-05-03-ml-hmm/"},{"categories":["Machine Learning"],"content":"2. HMMÁöÑ3‰∏™Âü∫Êú¨ÈóÆÈ¢ò Ê¶ÇÁéáËÆ°ÁÆóÔºöÁªôÂÆöÊ®°Âûã$\\lambda = (A, B, \\pi)$ÂíåËßÇÊµãÂ∫èÂàó $O = (o_1, o_2, \\cdots, o_T)$Ôºå Ê±ÇÊ¶ÇÁéá$P(O | \\lambda)$ Â≠¶‰π†: Â∑≤Áü•ËßÇÊµãÂ∫èÂàó$O = (o_1, o_2, \\cdots, o_T)$Ôºå‰º∞ËÆ°Ê®°ÂûãÂèÇÊï∞$\\lambda = (A, B, \\pi)$Ôºå ‰ΩøÊ¶ÇÁéá$P(O \\vert \\lambda)$ÊúÄÂ§ßÔºàÁî®ÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°Ôºâ„ÄÇ È¢ÑÊµãÔºöÁªôÂÆöÊ®°Âûã$\\lambda = (A, B, \\pi)$ÂíåËßÇÊµãÂ∫èÂàó $O = (o_1, o_2, \\cdots, o_T)$ÔºåÊ±ÇÊù°‰ª∂Ê¶ÇÁéá$P(I | O)$ÊúÄÂ§ßÁöÑÁä∂ÊÄÅÂ∫èÂàó $I = (i_1, i_2, \\cdots, i_T)$. ","date":"2020-05-03","objectID":"/2020-05-03-ml-hmm/:2:0","tags":["Hidden Markov Model","Expectation Maximization","Probabilistic Graphical Model","Statistical Learning"],"title":"Hidden Markov Model (HMM)","uri":"/2020-05-03-ml-hmm/"},{"categories":["Machine Learning"],"content":"2.1 Ê¶ÇÁéáËÆ°ÁÆóÂâçÂêëÔºàforwardÔºâÂíåÂêéÂêëÔºàbackwardÔºâÁÆóÊ≥ï 2.1.1 ÂâçÂêëÁÆóÊ≥ï ÁªôÂÆöÊ®°Âûã$\\lambda$ÔºåÂΩìÊó∂Âàª$t$Êó∂ÔºåÁä∂ÊÄÅ‰∏∫$q_i$ÔºåÈÉ®ÂàÜËßÇÊµãÂ∫èÂàó‰∏∫$o_1, o_2, \\cdots, o_t$ÔºåËÆ∞Ôºö $$\\alpha_{t}(i) = P(o_1, o_2, \\cdots, o_t, i_t = q_i | \\lambda)$$ ËæìÂÖ•Ôºö ÈöêÈ©¨ÂèØÂ§´Ê®°Âûã $\\lambda$Ôºå ËßÇÊµãÂ∫èÂàó$O$; ËæìÂá∫Ôºö ËßÇÊµãÂ∫èÂàóÊ¶ÇÁéá$P(O | \\lambda)$ Ôºà1ÔºâÂàùÂÄº $$ \\alpha_{1}(i)=\\pi_{i} b_{i}\\left(o_{1}\\right), \\quad i=1,2, \\cdots, N $$ Ôºà2ÔºâÈÄíÊé® ÂØπ $t = 1,2, \\cdots, T-1,$ $$ \\alpha_{t+1}(i)=\\left[\\sum_{j=1}^{N} \\alpha_{t}(j) a_{j i}\\right] b_{i}\\left(o_{t+1}\\right), \\quad i=1,2, \\cdots, N $$ Ôºà3ÔºâÁªàÊ≠¢ $$ P(O | \\lambda)=\\sum_{i=1}^{N} \\alpha_{T}(i) $$ 2.1.2 ÂêéÂêëÁÆóÊ≥ï ÁªôÂÆöÊ®°Âûã$\\lambda$ÔºåÂΩìÊó∂Âàª$t$Êó∂ÔºåÁä∂ÊÄÅ‰∏∫$q_i$ÔºåÈÉ®ÂàÜËßÇÊµãÂ∫èÂàó‰∏∫$o_1, o_2, \\cdots, o_t$ÔºåËÆ∞Ôºö $$\\beta_{t}(i) = P(o_{t+1}, o_{t+2}, \\cdots, o_T | i_t = q_i, \\lambda)$$ ËæìÂÖ•Ôºö ÈöêÈ©¨ÂèØÂ§´Ê®°Âûã $\\lambda$Ôºå ËßÇÊµãÂ∫èÂàó$O$; ËæìÂá∫Ôºö ËßÇÊµãÂ∫èÂàóÊ¶ÇÁéá$P(O | \\lambda)$ Ôºà1ÔºâÂàùÂßã ‰ª§ÊúÄÁªàÊó∂ÂàªÊâÄÊúâÁä∂ÊÄÅ$q_i$ $$\\beta_T(i) = 1, i=1,2,\\cdots, N$$ Ôºà2ÔºâÈÄíÊé® ÂØπ$t=T-1, T-2, \\cdots, 1$ $$ \\beta_{t}(i)=\\sum_{j=1}^{N} a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j), \\quad i=1,2, \\cdots, N $$ Ôºà3ÔºâÁªàÊ≠¢ $$ P(O | \\lambda)=\\sum_{i=1}^{N} \\pi_{i} b_{i}\\left(o_{1}\\right) \\beta_{1}(i) $$ Âà©Áî®ÂâçÂêéÂêëÊ¶ÇÁéáÂÆö‰πâÔºåÂèØ‰ª•Â∞ÜËßÇÊµãÂ∫èÂàóÊ¶ÇÁéá$P(O \\vert \\lambda)$Áªü‰∏ÄÂÜôÊàê $$ P(O | \\lambda)=\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{t}(i) a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j), \\quad t=1,2, \\cdots, T-1 $$ ","date":"2020-05-03","objectID":"/2020-05-03-ml-hmm/:2:1","tags":["Hidden Markov Model","Expectation Maximization","Probabilistic Graphical Model","Statistical Learning"],"title":"Hidden Markov Model (HMM)","uri":"/2020-05-03-ml-hmm/"},{"categories":["Machine Learning"],"content":"2.2 Â≠¶‰π†ÈóÆÈ¢òÁöÑEMÁÆóÊ≥ï ÂÆûË¥®‰∏äÊ±Ç‰∏Ä‰∏™ÈöêÂèòÈáèÁöÑÊ¶ÇÁéáÊ®°ÂûãÁöÑÂèÇÊï∞‰º∞ËÆ°Ôºö $$ P(O | \\lambda)=\\sum_{I} P(O | I, \\lambda) P(I | \\lambda) $$ ÂèÇÊï∞‰º∞ËÆ°Áî±EMÁÆóÊ≥ïÂÆûÁé∞: (ÂæÖÁª≠) ËæìÂÖ•ÔºöËßÇÊµãÊï∞ÊçÆ$O = (o_1, o_2, \\cdots, o_T)$; ËæìÂá∫ÔºöÈöêÈ©¨ÂèØÂ§´Ê®°ÂûãÂèÇÊï∞ Ôºà1ÔºâÂàùÂßãÂåñ ÂØπ n=0Ôºå ÈÄâÂèñ$a_{ij}^{(0)}$, $b_{j}(k)^{(0)}$, $\\pi_{i}^{(0)}$, ÂæóÂà∞Ê®°Âûã$\\lambda = (A^{(0)}, B^{(0)}, \\pi^{(0)})$. Ôºà2ÔºâÈÄíÊé® ÂØπ$n=1,2, \\cdots,$, Êúâ $$ a_{i j}^{(n+1)}=\\frac{\\sum_{t=1}^{T-1} \\xi_{t}(i, j)}{\\sum_{t=1}^{T-1} \\gamma_{t}(i)} $$ Âè¶Ôºå $$ b_{j}(k)^{(n+1)}=\\frac{\\sum_{t=1, o_{t}=v_{k}}^{T} \\gamma_{t}(j)}{\\sum_{t=1}^{T} \\gamma_{t}(j)} $$ $$ \\pi_{i}^{(n+1)}=\\gamma_{1}(i) $$ ÂÖ∂‰∏≠ÔºåÊó∂Âàª$t$Â§Ñ‰∫é$q_i$Ôºå‰∏îÊó∂Âàª$t+1$Â§Ñ‰∫éÁä∂ÊÄÅ$q_j$ÁöÑÊ¶ÇÁéá, ËÆ∞ $$ \\xi_{t}(i, j)=P\\left(i_{t}=q_{i}, i_{t+1}=q_{j} | O, \\lambda\\right) $$ ÈÇ£‰πà $$ \\xi_{t}(i, j)=\\frac{P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O | \\lambda\\right)}{P(O | \\lambda)}=\\frac{P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O | \\lambda\\right)}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O | \\lambda\\right)} $$ ÂíåÊó∂Âàª$t$Â§Ñ‰∫é$q_i$ÁöÑÊ¶ÇÁéáÔºå Êúâ $$ \\gamma_{t}(i)=P\\left(i_{t}=q_{i} | O, \\lambda\\right)=\\frac{P\\left(i_{t}=q_{i}, O | \\lambda\\right)}{P(O | \\lambda)} $$ Ôºà3ÔºâÁªàÊ≠¢ ÂæóÂà∞Ê®°ÂûãÂèÇÊï∞$\\lambda^{(n+1)} = (A^{(n+1)}, B^{(n+1)}, \\pi^{(n+1)})$ ","date":"2020-05-03","objectID":"/2020-05-03-ml-hmm/:2:2","tags":["Hidden Markov Model","Expectation Maximization","Probabilistic Graphical Model","Statistical Learning"],"title":"Hidden Markov Model (HMM)","uri":"/2020-05-03-ml-hmm/"},{"categories":["Machine Learning"],"content":"2.3 È¢ÑÊµãÁÆóÊ≥ï ÂåÖÊã¨Ëøë‰ººÁÆóÊ≥ïÂíåÁª¥ÁâπÊØîÁÆóÊ≥ïÔºàViterbi algorithmÔºâ 2.3.1 Ëøë‰ººÁÆóÊ≥ï Âú®ÊØè‰∏™Êó∂Âàª$t$Ôºå ÈÄâÊã©Âú®ËØ•Êó∂ÂàªÊúÄÂèØËÉΩÂá∫Áé∞ÁöÑÁä∂ÊÄÅ $i^*_t$‰ªéËÄåÂæóÂà∞‰∏Ä‰∏™Áä∂ÊÄÅÂ∫èÂàó $I^* = (i^*_i, i^*_i, \\cdots, i^*_T)$ÔºåÂ∞ÜÂÆÉÊúÄ‰∏∫È¢ÑÊµãÁªìÊûú„ÄÇ ÁªôÂÆöÊ®°Âûã$\\lambda$ÂíåËßÇÊµãÂ∫èÂàó$O$Ôºå Âú®Êó∂Âàª$t$Â§Ñ‰∫éÁä∂ÊÄÅ$q_i$ÁöÑÊ¶ÇÁéá$\\gamma_t(i)$ÊòØ $$ \\gamma_{t}(i)=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{P(O | \\lambda)}=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{\\sum_{j=1}^{N} \\alpha_{t}(j) \\beta_{t}(j)} $$ ËÄåÊØè‰∏ÄÊó∂Âàª$t$ÊúÄÊúâÂèØËÉΩÁöÑÁä∂ÊÄÅ$i_{t}^{*}$ÊòØ $$ i_{t}^{*}=\\arg \\max _{1 \\leqslant i \\leqslant N}\\left[\\gamma_{t}(i)\\right], \\quad t=1,2, \\cdots, T $$ ‰ªéËÄåÂæóÂà∞Áä∂ÊÄÅÂ∫èÂàó $$I^* = (i^*_i, i^*_i, \\cdots, i^*_T)$$ Áº∫ÁÇπÔºö ‰∏çËÉΩ‰øùËØÅÈ¢ÑÊµãÁä∂ÊÄÅÂ∫èÂàóÊï¥‰ΩìÊòØÊúÄÊúâÂèØËÉΩÁöÑÁä∂ÊÄÅÂ∫èÂàóÔºåÂõ†‰∏∫È¢ÑÊµãÁöÑÁä∂ÊÄÅÂ∫èÂàóÂÆûÈôÖÂèØËÉΩÁî±‰∏çÂèëÁîüÁöÑÈÉ®ÂàÜ„ÄÇ 2.3.2 Áª¥ÁâπÊØîÁÆóÊ≥ï ÂÆûË¥®ÊòØËøêÁî®Âä®ÊÄÅËßÑÂàíÊ±ÇÊ¶ÇÁéáÊúÄÂ§ßË∑ØÂæÑÔºå‰ªéËÄåËß£ÂÜ≥HMMÁöÑÈ¢ÑÊµãÈóÆÈ¢ò Áª¥ÁâπÊØîÁÆóÊ≥ï: Âè™ÈúÄ‰ªéÊó∂Âàª$t=1$ÂºÄÂßãÔºåÈÄíÊé®Âú∞ËÆ°ÁÆóÂú®Êó∂Âàª$t$Áä∂ÊÄÅ‰∏∫$q_i$ÁöÑÂêÑÊù°ÈÉ®ÂàÜË∑ØÂæÑÁöÑÊúÄÂ§ßÊ¶ÇÁéáÔºåÁõ¥Ëá≥ÂæóÂà∞Êó∂Âàª$t = T$Áä∂ÊÄÅ‰∏∫$i$ÁöÑÂêÑÊù°Ë∑ØÂæÑÁöÑÊúÄÂ§ßÊ¶ÇÁéá„ÄÇÊó∂Âàª $t = T$ ÁöÑÊúÄÂ§ßÊ¶ÇÁéáÂç≥‰∏∫ÊúÄ‰ºòË∑ØÂæÑÁöÑÊ¶ÇÁéá $P^\\ast$, ÊúÄ‰ºòË∑ØÂæÑÁöÑÁªàÁªìÁÇπ$i^*_T$ ‰πüÂêåÊó∂ÂæóÂà∞„ÄÇ‰πãÂêéÔºå‰∏∫‰∫ÜÊâæÂá∫ÊúÄ‰ºòË∑ØÂæÑÁöÑÂêÑ‰∏™ÁªìÁÇπÔºå‰ªéÁªàÁªìÁÇπ$i^*_T$ÂºÄÂßãÔºåÁî±ÂêéÂêëÂâçÈÄêÊ≠•Ê±ÇÂæóÁªìÁÇπ $i^*_{T-1}, \\cdots, i^*_1$ÔºåÂæóÂà∞ÊúÄ‰ºòË∑ØÂæÑ$I^* = (i^*_i, i^*_i, \\cdots, i^*_T)$„ÄÇ ÂÆö‰πâÂú®Êó∂Âàª$t$Áä∂ÊÄÅ$i$ÁöÑÊâÄÊúâÂçï‰∏™Ë∑ØÂæÑ‰∏≠Ê¶ÇÁéáÊúÄÂ§ßÂÄº‰∏∫ $$ \\delta_{t}(i)=\\max_{i_{1}, i_{2}, \\cdots, i_{t-1}} P\\left(i_{t}=i, i_{t-1}, \\cdots, i_{1}, o_{t}, \\cdots, o_{1} | \\lambda\\right), \\quad i=1,2, \\cdots, N $$ Âõ†Ê≠§ $$ \\begin{aligned} \\delta_{t+1}(i) \u0026=\\max _{i_{1}, i_{2}, \\cdots, i_{t}} P\\left(i_{t+1}=i, i_{t}, \\cdots, i_{1}, o_{t+1}, \\cdots, o_{1} | \\lambda\\right) \\cr \u0026= \\max _{1 \\leqslant j \\leqslant N}\\left[\\delta_{t}(j) a_{j i}\\right] b_{i}\\left(o_{t+1}\\right), \\quad i=1,2, \\cdots, N ; \\quad t=1,2, \\cdots, T-1 \\end{aligned} $$ ÂÆö‰πâÂú®Êó∂ÂàªtÁä∂ÊÄÅiÁöÑÊâÄÊúâÂçï‰∏™Ë∑ØÂæÑ‰∏≠Ê¶ÇÁéáÊúÄÂ§ßË∑ØÂæÑÁöÑÁ¨¨$t-1$‰∏™ËäÇÁÇπ‰∏∫ $$ \\Psi_{t}(i)=\\arg \\max _{1 \\leqslant j \\leqslant N}\\left[\\delta_{t-1}(j) a_{j i}\\right], \\quad i=1,2, \\cdots, N $$ ËæìÂÖ•Ôºö ÈöêÈ©¨ÂèØÂ§´Ê®°Âûã $\\lambda$Ôºå ËßÇÊµãÂ∫èÂàó$O$; ËæìÂá∫Ôºö ÊúÄ‰ºòË∑ØÂæÑ$$I^* = (i^*_i, i^*_i, \\cdots, i^*_T)$$ (1) ÂàùÂßãÂåñ: $$ \\begin{array}{c} \\delta_{1}(i)=\\pi_{i} b_{i}\\left(o_{1}\\right), \\quad i=1,2, \\cdots, N \\cr \\Psi_{1}(i)=0, \\quad i=1,2, \\cdots, N \\end{array} $$ (2) ÈÄíÊé®: $$ \\begin{array}{c} \\delta_{t}(i)=\\max _{1 \\leqslant j \\leqslant N}\\left[\\delta_{t-1}(j) a_{j i}\\right] b_{i}\\left(o_{t}\\right), \\quad i=1,2, \\cdots, N \\cr \\Psi_{t}(i)=\\arg \\max _{1 \\leqslant j \\leqslant N}\\left[\\delta_{t-1}(j) a_{j i}\\right], \\quad i=1,2, \\cdots, N \\end{array} $$ (3) ÁªàÊ≠¢ $$ \\begin{array}{c} P^* = \\max _{1 \\leqslant i \\leqslant N} \\delta_T(i) \\cr i^*_T = \\arg \\max _{1 \\leqslant i \\leqslant N} [ \\delta_T(i)] \\end{array} $$ (4) ÊúÄ‰ºòË∑ØÂæÑÂõûÊ∫Ø ÂØπ$t=T-1, T-2, \\cdots, 1$, $$i^*_t = \\Psi_{t+1}(i^*_{t+1})$$ ÂèÇËÄÉÔºö ÊùéËà™„ÄäÁªüËÆ°Â≠¶‰π†ÊñπÊ≥ï„Äã ","date":"2020-05-03","objectID":"/2020-05-03-ml-hmm/:2:3","tags":["Hidden Markov Model","Expectation Maximization","Probabilistic Graphical Model","Statistical Learning"],"title":"Hidden Markov Model (HMM)","uri":"/2020-05-03-ml-hmm/"},{"categories":["Machine Learning"],"content":"ÊΩúÂú®ËØ≠‰πâÂàÜÊûêÔºàLSAÔºâÊòØ‰∏ÄÁßçÈùûÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïÔºåÁî®‰∫éÊñáÊú¨ËØùÈ¢òÂàÜÊûê„ÄÇÂÖ∂ÁâπÁÇπÊòØÈÄöËøáÁü©ÈòµÂàÜËß£ÂèëÁé∞ÊñáÊú¨‰∫éÂçïËØç‰πãÈó¥ÁöÑÂü∫‰∫éËØùÈ¢òÁöÑËØ≠‰πâÂÖ≥Á≥ª„ÄÇ ÊΩúÂú®ËØ≠‰πâÂàÜÊûêÊòØ‰∏ÄÁßçÈùûÊ¶ÇÁéáËØùÈ¢òÂàÜÊûêÊ®°Âûã„ÄÇÊ≠•È™§‰∏∫Ôºö ÂÖàÂ∞ÜÊñáÊú¨ÈõÜÂêàË°®Á§∫‰∏∫ÂçïËØç-ÊñáÊú¨Áü©Èòµ ÂØπÂçïËØç-ÊñáÊú¨Áü©ÈòµËøõË°åÂ•áÂºÇÂÄºÂàÜËß£ÔºàSVDÔºâÊàñÈùûË¥üÁü©ÈòµÂàÜËß£ÔºàNMFÔºâÔºå‰ªéËÄåÂæóÂà∞ËØùÈ¢òÂêëÈáèÁ©∫Èó¥Ôºå‰ª•ÂèäÊñáÊú¨Âú®ËØùÈ¢òÂêëÈáèÁ©∫Èó¥ÁöÑË°®Á§∫ ","date":"2020-04-30","objectID":"/2020-04-30-ml-lsa/:0:0","tags":["TF-IDF","Latent Semantic Analysis","Statistical Learning"],"title":"Latent semantic analysis (LSA)","uri":"/2020-04-30-ml-lsa/"},{"categories":["Machine Learning"],"content":"1. ÂçïËØçÂêëÈáèÁ©∫Èó¥ ÁªôÂÆöÁî±n‰∏™ÊñáÊú¨ÁöÑÈõÜÂêà $D = \\lbrace d_1, d_2, \\cdots, d_n \\rbrace$ ,‰ª•ÂèäÊâÄÊúâÊñáÊú¨‰∏≠ÂçïËØçÈõÜ $W = \\lbrace w_1, w_2, \\cdots, w_m \\rbrace$, ÂàôÂçïËØçÂú®ÊñáÊú¨‰∏≠Âá∫Áé∞ÁöÑÊï∞ÊçÆÁî®ÂçïËØç-ÊñáÊú¨Áü©Èòµ(word-document matrix)Ë°®Á§∫, ËÆ∞‰∏∫Ôºö $$ X=\\left[\\begin{array}{cccc} x_{11} \u0026 x_{12} \u0026 \\cdots \u0026 x_{1 n} \\cr x_{21} \u0026 x_{22} \u0026 \\cdots \u0026 x_{2 n} \\cr \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\cr x_{m 1} \u0026 x_{m 2} \u0026 \\cdots \u0026 x_{m n} \\end{array}\\right] $$ ÂÖ∂‰∏≠Ôºå $x_{ij}$Ë°®Á§∫ÂçïËØç$w_i$Âú®ÊñáÊú¨$d_j$‰∏≠Âá∫Áé∞ÁöÑÈ¢ëÊï∞ÊàñÊùÉÂÄº„ÄÇÁî±‰∫éÂçïËØçÁßçÁ±ªÂ§öÔºåËÄåÊØè‰∏™ÊñáÊú¨‰∏≠ÂçïËØçÁßçÁ±ªÈÄöÂ∏∏Â∞ëÔºåÊâÄ‰ª•‰∏∫ÂçïËØç-ÊñáÊú¨Áü©ÈòµÊòØÁ®ÄÁñèÁü©Èòµ„ÄÇ ÊùÉÂÄºÂ∏∏Áî®ÂçïËØçÈ¢ëÁéá-ÈÄÜÊñáÊú¨È¢ëÁéá(term frequeny-inverse document frequency, TF-IDF)Ë°®Á§∫: $$ \\mathrm{TFIDF_{ij}}=\\frac{\\mathrm {tf_{ij}}}{\\mathrm {tf_{\\cdot j}}} \\log \\frac{\\mathrm{df}}{\\mathrm {df_i}}, \\quad i=1,2, \\cdots, m ; \\quad j=1,2, \\cdots, n $$ ÂÖ∂‰∏≠Ôºå $\\mathrm{tf}_{ij}$ ÊòØÂçïËØç $w_i$ Âú®ÊñáÊú¨ $d_j$ ‰∏≠ÁöÑÈ¢ëÊï∞; $\\mathrm{tf}_{\\cdot j}$ ÊòØ $d_j$ ‰∏≠Âá∫Áé∞ÁöÑÊâÄÊúâÂçïËØçÈ¢ëÊï∞‰πãÂíå; $\\mathrm{df}_{i}$ ÊòØÂê´ÊúâÂçïËØç$w_i$ÁöÑÊñáÊú¨Êï∞; $\\mathrm{df}$ ÊòØÊñáÊú¨ÈõÜÂêà$D$ÁöÑÂÖ®ÈÉ®ÊñáÊú¨Êï∞„ÄÇ ÂçïËØç-ÊñáÊú¨Áü©ÈòµÁöÑÁ¨¨jÂàóÂêëÈáè$x_j$Ë°®Á§∫ÊñáÊú¨$d_j$Ôºö $$ x_{j}=\\left[\\begin{array}{c} x_{1 j} \\cr x_{2 j} \\cr \\vdots \\cr x_{m j} \\end{array}\\right], \\quad j=1,2, \\cdots, n $$ ‰∏§‰∏™ÂçïËØçÂêëÈáèÂÜÖÁßØÊàñËÄÖÊ†áÂáÜÂåñÂÜÖÁßØË°®Á§∫ÂØπÂ∫îÊñáÊú¨‰πãÈó¥ÁöÑËØ≠‰πâÁõ∏‰ººÂ∫¶Ôºå Âõ†Ê≠§ÊñáÊú¨$d_i$‰∏é$d_j$ÁöÑ‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶Ôºö $$ x_{i} \\cdot x_{j}, \\quad \\frac{x_{i} \\cdot x_{j}}{\\left|x_{i}\\right|\\left|x_{j}\\right|} $$ ÂçïËØçÂêëÈáèÁ©∫Èó¥Ê®°ÂûãÁöÑ‰ºòÁº∫ÁÇπ ‰ºòÁÇπÔºö ÁÆÄÂçïÔºåËÆ°ÁÆóÈ´òÊïà Áº∫ÁÇπÔºö ‰∏ÄËØçÂ§ö‰πâ(polysemy)ÂíåÂ§öËØç‰∏Ä‰πâ(synonymy)‰∏çËÉΩÂæàÂ•ΩÂ§ÑÁêÜÔºå Â≠òÂú®Áõ∏‰ººÂ∫¶ËÆ°ÁÆó‰∏çÂáÜÁ°ÆÁöÑÈóÆÈ¢ò ","date":"2020-04-30","objectID":"/2020-04-30-ml-lsa/:0:1","tags":["TF-IDF","Latent Semantic Analysis","Statistical Learning"],"title":"Latent semantic analysis (LSA)","uri":"/2020-04-30-ml-lsa/"},{"categories":["Machine Learning"],"content":"2. ËØùÈ¢òÔºàtopicÔºâÂêëÈáèÁ©∫Èó¥ ËØùÈ¢òÊ≤°Êúâ‰∏•Ê†ºÁöÑÂÆö‰πâÔºåÊòØÊåáÊñáÊú¨ËÆ®ËÆ∫ÁöÑÂÜÖÂÆπÊàñ‰∏ªÈ¢ò„ÄÇËÄåÂü∫‰∫éËØùÈ¢òÁöÑÊ®°ÂûãÊòØ‰∏∫‰∫ÜËß£ÂÜ≥ÂçïËØçÂêëÈáè‰∏çËÉΩÂæàÂ•ΩÂ§ÑÁêÜ‰∏ÄËØçÂ§ö‰πâ(polysemy)ÂíåÂ§öËØç‰∏Ä‰πâ(synonymy)ÁöÑÈóÆÈ¢ò„ÄÇ ÁªôÂÆöÁî±n‰∏™ÊñáÊú¨ÁöÑÈõÜÂêà $D = \\lbrace d_1, d_2, \\cdots, d_n \\rbrace$, ‰ª•ÂèäÊâÄÊúâÊñáÊú¨‰∏≠ÂçïËØçÈõÜ $W = \\lbrace w_1, w_2, \\cdots, w_m \\rbrace$, ÂàôÂçïËØçÂú®ÊñáÊú¨‰∏≠Âá∫Áé∞ÁöÑÊï∞ÊçÆÁî®ÂçïËØç-ÊñáÊú¨Áü©Èòµ(word-document matrix)Ë°®Á§∫, ËÆ∞‰∏∫Ôºö $$ X=\\left[\\begin{array}{cccc} x_{11} \u0026 x_{12} \u0026 \\cdots \u0026 x_{1 n} \\cr x_{21} \u0026 x_{22} \u0026 \\cdots \u0026 x_{2 n} \\cr \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\cr x_{m 1} \u0026 x_{m 2} \u0026 \\cdots \u0026 x_{m n} \\end{array}\\right] $$ ÂÅáËÆæÊâÄÊúâÊñáÊú¨Âê´Êúâk‰∏™ËØùÈ¢òÔºå ÊØè‰∏™ËØùÈ¢òlÁî±‰∏Ä‰∏™ÂÆö‰πâÂú®ÂçïËØçÈõÜÂêàWÂÇªÂßëÂ®òÁöÑmÁª¥ÂêëÈáèÔºö $$ t_{l}=\\left[\\begin{array}{c} t_{1 l} \\cr t_{2 l} \\cr \\vdots \\cr t_{m l} \\end{array}\\right], \\quad l=1,2, \\cdots, k $$ ÂàôÊúâËØùÈ¢òÂêëÈáèÁü©ÈòµTÔºö $$ T=\\left[\\begin{array}{cccc} t_{11} \u0026 t_{12} \u0026 \\cdots \u0026 t_{1 k} \\cr t_{21} \u0026 t_{22} \u0026 \\cdots \u0026 t_{2 k} \\cr \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\cr t_{m 1} \u0026 t_{m 2} \u0026 \\cdots \u0026 t_{m k} \\end{array}\\right] $$ ","date":"2020-04-30","objectID":"/2020-04-30-ml-lsa/:0:2","tags":["TF-IDF","Latent Semantic Analysis","Statistical Learning"],"title":"Latent semantic analysis (LSA)","uri":"/2020-04-30-ml-lsa/"},{"categories":["Machine Learning"],"content":"3. ÊñáÊú¨Âú®ËØùÈ¢òÂêëÈáèÁ©∫Èó¥ÁöÑË°®Á§∫ ÊñáÊú¨$d_j$Âú®ÂçïËØçÂêëÈáèÁ©∫Èó¥Áî®$x_j$Ë°®Á§∫ÔºåÂ∞Ü$x_j$ÊäïÂΩ±Âà∞ËØùÈ¢òÂêëÈáèÁ©∫Èó¥$T$ÔºåÂæóÂà∞ËØùÈ¢òÂêëÈáèÁ©∫Èó¥$y_j$Ôºå$y_j$ÊòØ‰∏™kÁª¥ÂêëÈáè: $$ y_{j}=\\left[\\begin{array}{c} y_{1 j} \\cr y_{2 j} \\cr \\vdots \\cr y_{k j} \\end{array}\\right], \\quad j=1,2, \\cdots, n $$ Âõ†Ê≠§ÔºåËØùÈ¢ò-ÊñáÊú¨Áü©Èòµ$Y$Ôºö $$ Y=\\left[\\begin{array}{cccc} y_{11} \u0026 y_{12} \u0026 \\cdots \u0026 y_{1 n} \\cr y_{21} \u0026 y_{22} \u0026 \\cdots \u0026 y_{2 n} \\cr \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\cr y_{k 1} \u0026 y_{k 2} \u0026 \\cdots \u0026 y_{k n} \\end{array}\\right] $$ ","date":"2020-04-30","objectID":"/2020-04-30-ml-lsa/:0:3","tags":["TF-IDF","Latent Semantic Analysis","Statistical Learning"],"title":"Latent semantic analysis (LSA)","uri":"/2020-04-30-ml-lsa/"},{"categories":["Machine Learning"],"content":"4. ÂçïËØçÂêëÈáèÁ©∫Èó¥Âà∞ËØùÈ¢òÂêëÈáèÁ©∫Èó¥ËΩ¨Êç¢ Âõ†Ê≠§ÔºåÂçïËØçÂêëÈáèÁ©∫Èó¥ÁöÑÊñáÊú¨ÂêëÈáè$x_j$ÔºåÂèØ‰ª•Áî®ËØùÈ¢òÂêëÈáèÁ©∫Èó¥$y_j$Ëøë‰ººË°®Á§∫ÔºåÂç≥‰ª•k‰∏™ËØùÈ¢òÂêëÈáè‰ª•$y_j$‰∏∫Á≥ªÊï∞ÁöÑÁ∫øÊÄßÁªÑÂêàÔºö $$ x_{j} \\approx y_{1 j} t_{1}+y_{2 j} t_{2}+\\cdots+y_{k j} t_{k}, \\quad j=1,2, \\cdots, n $$ Áü©ÈòµÂΩ¢ÂºèÔºö $$ X \\approx TY $$ ËøôÂ∞±ÊòØÊΩúÂú®ËØ≠‰πâÂàÜÊûê„ÄÇ ÂèÇËÄÉÔºö ÊùéËà™„ÄäÁªüËÆ°Â≠¶‰π†ÊñπÊ≥ï„Äã ","date":"2020-04-30","objectID":"/2020-04-30-ml-lsa/:0:4","tags":["TF-IDF","Latent Semantic Analysis","Statistical Learning"],"title":"Latent semantic analysis (LSA)","uri":"/2020-04-30-ml-lsa/"},{"categories":["Machine Learning"],"content":"CRFÊù°‰ª∂ÈöèÊú∫Âú∫ÔºåÂèØÂ∫îÁî®‰∫éÊ†áÊ≥®ÈóÆÈ¢ò Ê¶ÇÁéáÊó†ÂêëÂõæÊ®°ÂûãProbabilistic undirected graphical model(Markov random field) ÊòØ‰∏Ä‰∏™ÂèØ‰ª•Áî±Êó†ÂêëÂõæË°®Á§∫ÁöÑËÅîÂêàÊ¶ÇÁéáÂàÜÂ∏É ","date":"2020-04-29","objectID":"/2020-04-29-ml-crf/:0:0","tags":["Probabilistic Graphical Model","Statistical Learning"],"title":"Conditional random field (CRF)","uri":"/2020-04-29-ml-crf/"},{"categories":["Machine Learning"],"content":"0. Ê¶ÇÁéáÂõæÊ®°ÂûãÂàÜÁ±ª Ê¶ÇÁéáÂõæÊ®°ÂûãÔºàprobabilistic graphical model, PGMÔºâÔºåÊòØ‰∏ÄÁßçÂ≠¶‰π†‰ªªÂä°ÁöÑÊ°ÜÊû∂ÊèèËø∞ÔºåÂÆÉÂ∞ÜÂ≠¶‰π†‰ªªÂä°ÂΩíÁªì‰∏∫ËÆ°ÁÆóÂèòÈáèÁöÑÊ¶ÇÁéáÂàÜÂ∏É„ÄÇ ÊåâÁÖßÊ¶ÇÁéáÂõæ‰∏≠ÂèòÈáèÂÖ≥Á≥ªÁöÑ‰∏çÂêåÔºåÊ¶ÇÁéáÂõæÊ®°ÂûãÂèØ‰ª•Â§ßËá¥ÂàÜ‰∏∫‰∏§Á±ªÔºö Ë¥ùÂè∂ÊñØÁΩëÁªúÔºöÊúâÂêëÂõæÊ®°ÂûãÔºå‰ΩøÁî®ÊúâÂêëÊó†ÁéØÂõæË°®ËææÂÖ≥Á≥ªÔºàÈÄöÂ∏∏ÔºåÂèòÈáèÈó¥Â≠òÂú®ÊòæÂºèÁöÑÂõ†ÊûúÂÖ≥Á≥ªÔºâ È©¨Â∞îÁßëÂ§´ÁΩëÁªúÔºöÊó†ÂêëÂõæÊ®°ÂûãÔºå‰ΩøÁî®Êó†ÂõæË°®ËææÂÖ≥Á≥ªÔºàÈÄöÂ∏∏ÔºåÂèòÈáèÈó¥Â≠òÊúâÂÖ≥Á≥ªÔºå‰ΩÜÊòØÈöæ‰ª•ÊòæÂºèË°®ËææÔºâ ÂêåÊó∂Â≠òÊúâÊúâÂêëËæπÂíåÊó†ÂêëËæπÁöÑÊ®°ÂûãÔºåÂ¶ÇÊù°‰ª∂ÈöèÊú∫Âú∫Ôºàconditional random fieldÔºâÂíåÈìæÂõæÔºàchain graphÔºâÔºåÂçïÁã¨ÁúãÂÅö‰∏ÄÁ±ªÂ±ÄÈÉ®ÊúâÂêëÊ®°Âûã„ÄÇ Ë¥ùÂè∂ÊñØÁΩëÁªú ÂèØ‰ª•ÂàÜ‰∏∫ÈùôÊÄÅË¥ùÂè∂ÊñØÁΩëÁªúÂíåÂä®ÊÄÅË¥ùÂè∂ÊñØÁΩëÁªú„ÄÇÁõ∏ÊØî‰∫éÈùôÊÄÅË¥ùÂè∂ÊñØÁΩëÁªúÔºåÂä®ÊÄÅÔºàdynamicÔºâË¥ùÂè∂ÊñØÁΩëÁªú‰∏ªË¶ÅÁî®‰∫éÊó∂Â∫èÊï∞ÊçÆÂª∫Ê®°ÔºàÂ¶ÇËØ≠Èü≥ËØÜÂà´„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅËΩ®ËøπÊï∞ÊçÆÊåñÊéòÁ≠âÔºâ„ÄÇÂÖ∂‰∏≠Ôºå‰∏ÄÁßçÁªìÊûÑÊúÄÁÆÄÂçïÁöÑÂä®ÊÄÅË¥ùÂè∂ÊñØÁΩëÁªúÂ∞±ÊòØÈöêÈ©¨Â∞îÂèØÂ§´Ê®°ÂûãÔºàhidden markov model, HMMÔºâ„ÄÇ‰∏ÄËà¨Êù•ËØ¥ÔºåË¥ùÂè∂ÊñØÁΩëÁªú‰∏≠ÊØè‰∏Ä‰∏™ÁªìÁÇπÈÉΩÂØπÂ∫î‰∫é‰∏Ä‰∏™ÂÖàÈ™åÊ¶ÇÁéáÂàÜÂ∏ÉÊàñËÄÖÊù°‰ª∂Ê¶ÇÁéáÂàÜÂ∏ÉÔºåÂõ†Ê≠§Êï¥‰ΩìÁöÑËÅîÂêàÂàÜÂ∏ÉÂèØ‰ª•Áõ¥Êé•ÂàÜËß£‰∏∫ÊâÄÊúâÂçï‰∏™ÁªìÁÇπÊâÄÂØπÂ∫îÁöÑÂàÜÂ∏ÉÁöÑ‰πòÁßØ„ÄÇ È©¨Â∞îÂèØÂ§´ÁΩë Áî±‰∫éÂèòÈáè‰πãÈó¥Ê≤°ÊúâÊòéÁ°ÆÁöÑÂõ†ÊûúÂÖ≥Á≥ªÔºåÂÆÉÁöÑËÅîÂêàÊ¶ÇÁéáÂàÜÂ∏ÉÈÄöÂ∏∏‰ºöË°®Ëææ‰∏∫‰∏ÄÁ≥ªÂàóÂäøÂáΩÊï∞Ôºàpotential functionÔºâÁöÑ‰πòÁßØ„ÄÇÈÄöÂ∏∏ÊÉÖÂÜµ‰∏ãÔºåËøô‰∫õ‰πòÁßØÁöÑÁßØÂàÜÂπ∂‰∏çÁ≠â‰∫é1ÔºåÂõ†Ê≠§ÔºåËøòË¶ÅÂØπÂÖ∂ËøõË°åÂΩí‰∏ÄÂåñÊâçËÉΩÂΩ¢Êàê‰∏Ä‰∏™ÊúâÊïàÁöÑÊ¶ÇÁéáÂàÜÂ∏É‚Äî‚ÄîËøô‰∏ÄÁÇπÂæÄÂæÄÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁªôÂèÇÊï∞‰º∞ËÆ°ÈÄ†ÊàêÈùûÂ∏∏Â§ßÁöÑÂõ∞Èöæ„ÄÇ ÊåâÁÖßË°®Á§∫ÁöÑÊäΩË±°Á∫ßÂà´‰∏çÂêåÔºåÊ¶ÇÁéáÂõæÊ®°ÂûãÂèØ‰ª•ÂàÜ‰∏∫Ôºö Âü∫‰∫éÈöèÊú∫ÂèòÈáèÁöÑÊ¶ÇÁéáÂõæÊ®°ÂûãÔºåÂ¶ÇË¥ùÂè∂ÊñØÁΩë„ÄÅÈ©¨Â∞îÂèØÂ§´ÁΩë„ÄÅÊù°‰ª∂ÈöèÊú∫Âú∫ÂíåÈìæÂõæÁ≠â Âü∫‰∫éÊ®°ÊùøÁöÑÊ¶ÇÁéáÂõæÊ®°ÂûãÔºéËøôÁ±ªÊ®°ÂûãÊ†πÊçÆÂ∫îÁî®Âú∫ÊôØ‰∏çÂêåÂèàÂèØÂàÜ‰∏∫‰∏§ÁßçÔºö ÊöÇÊÄÅÊ®°ÂûãÔºåÂåÖÊã¨Âä®ÊÄÅË¥ùÂè∂ÊñØÁΩëÔºàDynamic Bayesian Network, DBNÔºâÂíåÁä∂ÊÄÅËßÇÊµãÊ®°ÂûãÔºåÂÖ∂‰∏≠Áä∂ÊÄÅËßÇÊµãÊ®°ÂûãÂèàÂåÖÊã¨Á∫øÊÄßÂä®ÊÄÅÁ≥ªÁªüÔºàLinear Dynamic System, LDSÔºâÂ¶ÇÂç°Â∞îÊõºÊª§Ê≥¢Âô®ÔºåËøòÊúâÈöêÈ©¨Â∞îÂèØÂ§´Ê®°ÂûãÔºàHidden Markov Model, HMMÔºâÔºõ ÂØπË±°ÂÖ≥Á≥ªÈ¢ÜÂüüÁöÑÊ¶ÇÁéáÂõæÊ®°ÂûãÔºåÂåÖÊã¨ÁõòÊ®°ÂûãÔºàPlate ModelÔºåPMÔºâ„ÄÅÊ¶ÇÁéáÂÖ≥Á≥ªÊ®°ÂûãÔºàProbabilistic Relational Model, PRMÔºâÂíåÂÖ≥Á≥ªÈ©¨Â∞îÂèØÂ§´ÁΩëÔºàRelational Markov Network, RMNÔºâ„ÄÇ ÂèÇËÄÉÔºöÊ¶ÇÁéáÂõæÊ®°ÂûãÊÄªËßà ","date":"2020-04-29","objectID":"/2020-04-29-ml-crf/:0:1","tags":["Probabilistic Graphical Model","Statistical Learning"],"title":"Conditional random field (CRF)","uri":"/2020-04-29-ml-crf/"},{"categories":["Machine Learning"],"content":"1. Ê®°ÂûãÂÆö‰πâ Ê¶ÇÁéáÂõæÊ®°ÂûãÔºöÁî±ÂõæÔºàGraphÔºâË°®Á§∫ÁöÑÊ¶ÇÁéáÂàÜÂ∏É„ÄÇ ‰ª§Êó†ÂêëÂõæ G = (V, E) Ë°®Á§∫ËÅîÂêàÊ¶ÇÁéáÂàÜÂ∏ÉP(Y)ÔºåÂç≥G‰∏≠Ôºå ËäÇÁÇπ$v \\in V$ Ë°®Á§∫ÈöèÊú∫ÂèòÈáè$Y_{v}, Y=\\left(Y_{v}\\right)_{v \\in V}$Ôºõ Ëæπ$e \\in E$Ë°®Á§∫ÈöèÊú∫ÂèòÈáè‰πãÈó¥ÁöÑÊ¶ÇÁéá‰æùËµñÂÖ≥Á≥ª Êó†ÂêëÂõæË°®Á§∫ÁöÑÈöèÊú∫ÂèòÈáèÂ≠òÂú®: ÊàêÂØπÈ©¨ÂèØÂ§´ÊÄß pariwise Markov property Êåá‰ªªÊÑè‰∏§‰∏™Ê≤°ÊúâËæπËøûÊé•ÁöÑËäÇÁÇπÔºåÂú®ÁªôÂÆöÈöèÊú∫ÂèòÈáèÁªÑÔºàÂÖ∂‰ªñÊâÄÊúâËäÇÁÇπÔºâÊù°‰ª∂‰∏ãÔºåËØ•‰∏§ËäÇÁÇπÊòØÊù°‰ª∂Áã¨Á´ã Â±ÄÈÉ®È©¨ÂèØÂ§´ÊÄß local Markov property ÂÖ®Â±ÄÈ©¨ÂèØÂ§´ÊÄß global Markov property Ê¶ÇÁéáÊó†ÂêëÂõæÊ®°ÂûãÔºöÊó†ÂêëÂõæ $G = (V, E)$ Ë°®Á§∫ËÅîÂêàÊ¶ÇÁéáÂàÜÂ∏É $P(Y)$ÔºåÂ¶ÇÊûúËÅîÂêàÊ¶ÇÁéáÂàÜÂ∏É $P(Y)$ Êª°Ë∂≥ÊàêÂØπ„ÄÅÂ±ÄÈÉ®ÊàñÂÖ®Â±ÄÈ©¨ÂèØÂ§´ÊÄßÔºåÂ∞±Áß∞Ê≠§ËÅîÂêàÊ¶ÇÁéáÂàÜÂ∏É $P(Y)$ ‰∏∫Ê¶ÇÁéáÊó†ÂêëÂõæÊ®°ÂûãÔºåÊàñÈ©¨ÂèØÂ§´ÈöèÊú∫Âú∫ Âõ¢ÔºàcliqueÔºâÔºöÂõæG‰∏≠‰ªª‰Ωï‰∏§‰∏™ËäÇÁÇπÂùáÊúâËæπËøûÊé•ÁöÑËäÇÁÇπÂ≠êÈõÜ ÊúÄÂ§ßÂõ¢Ôºàmaximal cliqueÔºâÔºöÂõ¢C‰∏≠‰∏çËÉΩÂÜçÂä†‰ªª‰Ωï‰∏Ä‰∏™ËäÇÁÇπ‰ΩøÂÆÉÊàê‰∏∫Êõ¥Â§ßÁöÑÂõ¢ÔºåÂàôÁß∞ÊúÄÂ§ßÂõ¢ ","date":"2020-04-29","objectID":"/2020-04-29-ml-crf/:0:2","tags":["Probabilistic Graphical Model","Statistical Learning"],"title":"Conditional random field (CRF)","uri":"/2020-04-29-ml-crf/"},{"categories":["Machine Learning"],"content":"2. Êù°‰ª∂ÈöèÊú∫Âú∫ Êù°‰ª∂ÈöèÊú∫Âú∫ÊåáÁªôÂÆöÈöèÊú∫ÂèòÈáèXÊù°‰ª∂‰∏ãÔºå ÈöèÊú∫ÂèòÈáèYÁöÑÈ©¨ÂèØÂ§´ÈöèÊú∫Âú∫„ÄÇ 2.1 Êù°‰ª∂ÈöèÊú∫Âú∫Ôºö Ëã•ÈöèÊú∫ÂèòÈáè$Y$ÊûÑÊàê‰∏Ä‰∏™Áî±Êó†ÂêëÂõæ$G = (V, E)$Ë°®Á§∫ÁöÑÈ©¨ÂèØÂ§´ÈöèÊú∫Âú∫ÔºåÂç≥ $$ P\\left(Y_{v} | X, Y_{w}, w \\neq v\\right)=P\\left(Y_{v} | X, Y_{w}, w \\sim v\\right) $$ ÂØπ‰∫é‰ªªÊÑèËäÇÁÇπ$v$ÊàêÁ´ãÔºå ÂàôÁß∞Êù°‰ª∂Ê¶ÇÁéáÂàÜÂ∏É$P(Y\\vert X)$‰∏∫Êù°‰ª∂ÈöèÊú∫Âú∫„ÄÇÂÖ∂‰∏≠$w \\sim v$Ë°®Á§∫Âú®Âõæ$G = (V, E)$‰∏≠‰∏éËäÇÁÇπ$v$ÊúâËæπËøûÊé•ÁöÑÊâÄÊúâËäÇÁÇπ$w$Ôºå $w \\neq v$Ë°®Á§∫ËäÇÁÇπv‰ª•Â§ñÁöÑÊâÄÊúâËäÇÁÇπ„ÄÇ 2.2 Á∫øÊÄßÈìæÊù°‰ª∂ÈöèÊú∫Âú∫Ôºà linear chain conditional random fieldÔºâ Á∫øÊÄßÈìæÊù°‰ª∂ÈöèÊú∫Âú∫‰πüÊòØÂØπÊï∞Á∫øÊÄßÊ®°Âûã(log linear model)ÔºåÂÆö‰πâ‰∏∫Ôºö $$ P\\left(Y_{i} | X, Y_{1}, \\cdots, Y_{i-1}, Y_{i+1}, \\cdots, Y_{n}\\right)=P\\left(Y_{i} | X, Y_{i-1}, Y_{i+1}\\right) $$ Âú®Êù°‰ª∂Ê¶ÇÁéáÊ®°Âûã$P(Y | X)$‰∏≠Ôºå $Y$ÊòØËæìÂá∫ÂèòÈáèÔºåË°®Á§∫Ê†áËÆ∞Â∫èÂàóÔºàÁä∂ÊÄÅÂ∫èÂàóÔºåÂèÇËßÅHMMÔºâÔºõ$X$‰ΩøËæìÂÖ•ÂèòÈáèÔºåË°®Á§∫ÈúÄË¶ÅÊ†áÊ≥®ÁöÑËßÇÊµãÂ∫èÂàó„ÄÇÂà©Áî®ËÆ≠ÁªÉÈõÜÔºåÈÄöËøáÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÊàñÊ≠£ÂàôÂåñÁöÑÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÂæóÂà∞Êù°‰ª∂Ê¶ÇÁéáÊ®°Âûã$\\hat{P}(Y | X)$;È¢ÑÊµãÊó∂ÔºåÂØπ‰∫éÁªôÂÆöËæìÂÖ•Â∫èÂàó$x$ÔºåÊ±ÇÊù°‰ª∂Ê¶ÇÁéá$\\hat{P}(Y | X)$ÊúÄÂ§ßÁöÑËæìÂá∫Â∫èÂàó$\\hat{y}$„ÄÇ 2.3 Êù°‰ª∂ÈöèÊú∫Âú∫ÁöÑÂèÇÊï∞ÂåñÂΩ¢Âºè ËÆæ$P(Y\\vert X)$‰∏∫Á∫øÊÄßÈìæÊù°‰ª∂ÈöèÊú∫Âú∫ÔºåXÂèñÂÄº‰∏∫xÔºå YÂèñÂÄº‰∏∫yÁöÑÊù°‰ª∂Ê¶ÇÁéáÂÖ∑ÊúâÂ¶Ç‰∏ãÂΩ¢ÂºèÔºö $$ P(y | x)=\\frac{1}{Z(x)} \\exp \\left(\\sum_{i, k} \\lambda_{k} t_{k}\\left(y_{i-1}, y_{i}, x, i\\right)+\\sum_{i, l} \\mu_{l} s_{l}\\left(y_{i}, x, i\\right)\\right) $$ ÂÖ∂‰∏≠Ôºå $$ Z(x)=\\sum_{y} \\exp \\left(\\sum_{i, k} \\lambda_{k} t_{k}\\left(y_{i-1}, y_{i}, x, i\\right)+\\sum_{i, l} \\mu_{l} s_{l}\\left(y_{i}, x, i\\right)\\right) $$ Âºè‰∏≠Ôºå$t_{k}$Âíå$s_{l}$ÊòØÁâπÂæÅÂáΩÊï∞, $\\lambda_{k}$Âíå$\\mu_{l}$ÊòØÂØπÂ∫îÁöÑÊùÉÂÄº„ÄÇ $Z(x)$ÊòØËßÑËåÉÂåñÂõ†Â≠ê„ÄÇÂú®ÊâÄÊúâÂèØËÉΩËæìÂá∫ÁöÑÂ∫èÂàó‰∏äËøõË°åÊ±ÇÂíåÊìç‰Ωú„ÄÇ ÂÖ≥‰∫éÁâπÂæÅÂáΩÊï∞Ôºö ‰ª§$t_{k}$ÊòØÂÆö‰πâÂú®Ëæπ‰∏äÁöÑÁâπÂæÅÂáΩÊï∞ÔºåÁß∞‰∏∫ËΩ¨ÁßªÁâπÂæÅÔºå‰æùËµñÂΩìÂâçÂíåÂâç‰∏Ä‰∏™‰ΩçÁΩÆ ‰ª§$s_{l}$ÊòØÂÆö‰πâÂú®ËäÇÁÇπ‰∏äÁöÑÁâπÂæÅÂáΩÊï∞ÔºåÁß∞‰∏∫Áä∂ÊÄÅÁâπÂæÅÔºå‰æùËµñÂΩìÂâç‰ΩçÁΩÆ ÁâπÂæÅÂáΩÊï∞$t_{k}$Âíå$s_{l}$ÂèñÂÄº0Êàñ1ÔºõÊª°Ë∂≥Êù°‰ª∂Âèñ1ÔºåÂèç‰πã0 Êù°‰ª∂ÈöèÊú∫ÈïøÂÆåÂÖ®Áî±ÁâπÂæÅÂáΩÊï∞$t_{k}$Âíå$s_{l}$Ôºå ÂíåÂØπÂ∫îÁöÑÊùÉÂÄº$\\lambda_{k}$Âíå$\\mu_{l}$Á°ÆÂÆö„ÄÇ 2.4 Êù°‰ª∂ÈöèÊú∫Âú∫ÁöÑÁü©ÈòµÂΩ¢Âºè ÂØπ‰∫éËßÇÊµãÂ∫èÂàóxÁöÑÊØè‰∏™‰ΩçÁΩÆÔºåyÂú®m‰∏™Ê†áËÆ∞‰∏≠ÂèñÂÄºÔºåÂèØ‰ª•ÂÆö‰πâ‰∏Ä‰∏™mÈò∂ÁöÑÁü©ÈòµÈöèÊú∫ÂèòÈáèÔºö $$ M_{i}(x) = [ M_{i}(y_{i-1}, y_{i}|x) ] $$ Áü©ÈòµÈöèÊú∫ÂèòÈáèÂÖÉÁ¥†‰∏∫ $$ \\begin{aligned} \u0026M_{i}\\left(y_{i-1}, y_{i} | x\\right)=\\exp \\left(W_{i}\\left(y_{i-1}, y_{i} | x\\right)\\right)\\cr \u0026W_{i}\\left(y_{i-1}, y_{i} | x\\right)=\\sum_{k=1}^{K} w_{k} f_{k}\\left(y_{i-1}, y_{i}, x, i\\right) \\end{aligned} $$ ËøôÈáå$w_k$‰∏∫ $$ w_{k}=\\begin{cases} \\lambda_{k}, \u0026 k=1,2, \\cdots, K_{1} \\cr \\mu_{l}, \u0026 k=K_{1}+l ; l=1,2, \\cdots, K_{2} \\end{cases} $$ Âíå$f_k$‰∏∫ $$ f_{k}\\left(y_{i-1}, y_{i}, x, i\\right)=\\begin{cases} t_{k}\\left(y_{i-1}, y_{i}, x, i\\right), \u0026 k=1,2, \\cdots, K_{1} \\cr s_{l}\\left(y_{i}, x, i\\right), \u0026 k=K_{1}+l ; l=1,2, \\cdots, K_{2} \\end{cases} $$ ‰∫éÊòØÔºåÊù°‰ª∂Ê¶ÇÁéá$P_{w}(y \\vert x)$: $$ P_{w}(y | x)=\\frac{1}{Z_{w}(x)} \\prod_{i=1}^{n+1} M_{i}\\left(y_{i-1}, y_{i} | x\\right) $$ ÂÖ∂‰∏≠Ôºå $$ Z_{w}(x)=\\left[M_{1}(x) M_{2}(x) \\cdots M_{n+1}(x)\\right]_{\\mathrm{start}, \\mathrm{stop}} $$ Ê≥®Ôºå $y_{0} = \\mathrm{start}$ÔºåË°®Á§∫ÂºÄÂßãÁä∂ÊÄÅÔºõ $y_{n+1} = \\mathrm{stop}$Ôºå Ë°®Á§∫ÁªàÊ≠¢Áä∂ÊÄÅ ÂèÇËÄÉÔºö ÊùéËà™„ÄäÁªüËÆ°Â≠¶‰π†ÊñπÊ≥ï„Äã ","date":"2020-04-29","objectID":"/2020-04-29-ml-crf/:0:3","tags":["Probabilistic Graphical Model","Statistical Learning"],"title":"Conditional random field (CRF)","uri":"/2020-04-29-ml-crf/"},{"categories":["Statistic"],"content":"Probability, P-value, Likelihood ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:0:0","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"Probability and likelihood likehood \u0026 maximum likehood Âú®ÈùûÊ≠£ÂºèÂú∫Âêà‰ººÁÑ∂ÔºàlikelihoodÔºâÂíåÊ¶ÇÁéáÔºàProbabilityÔºâÂá†‰πéÊòØ‰∏ÄÂØπÂêå‰πâËØçÔºå‰ΩÜÊòØÂú®ÁªüËÆ°Â≠¶‰∏≠‰ººÁÑ∂ÂíåÊ¶ÇÁéáÂç¥ÊòØ‰∏§‰∏™‰∏çÂêåÁöÑÊ¶ÇÂøµ„ÄÇ Ê¶ÇÁéá: Âú®ÁâπÂÆöÁéØÂ¢É‰∏ãÊüê‰ª∂‰∫ãÊÉÖÂèëÁîüÁöÑÂèØËÉΩÊÄßÔºå‰πüÂ∞±ÊòØÁªìÊûúÊ≤°Êúâ‰∫ßÁîü‰πãÂâç‰æùÊçÆÁéØÂ¢ÉÊâÄÂØπÂ∫îÁöÑÂèÇÊï∞Êù•È¢ÑÊµãÊüê‰ª∂‰∫ãÊÉÖÂèëÁîüÁöÑÂèØËÉΩÊÄß„ÄÇ ÊØîÂ¶ÇÊäõÁ°¨Â∏ÅÔºåÊäõ‰πãÂâçÊàë‰ª¨‰∏çÁü•ÈÅìÊúÄÂêéÊòØÂì™‰∏ÄÈù¢Êúù‰∏äÔºå‰ΩÜÊòØÊ†πÊçÆÁ°¨Â∏ÅÁöÑÊÄßË¥®Êàë‰ª¨ÂèØ‰ª•Êé®Êµã‰ªª‰Ωï‰∏ÄÈù¢Êúù‰∏äÁöÑÂèØËÉΩÊÄßÂùá‰∏∫50%ÔºåËøô‰∏™Ê¶ÇÁéáÂè™ÊúâÂú®ÊäõÁ°¨Â∏Å‰πãÂâçÊâçÊòØÊúâÊÑè‰πâÁöÑÔºåÊäõÂÆåÁ°¨Â∏ÅÂêéÁöÑÁªìÊûú‰æøÊòØÁ°ÆÂÆöÁöÑÔºõ ‰ººÁÑ∂: ÂàöÂ•ΩÁõ∏ÂèçÔºåÊòØÂú®Á°ÆÂÆöÁöÑÁªìÊûú‰∏ãÂéªÊé®Êµã‰∫ßÁîüËøô‰∏™ÁªìÊûúÁöÑÂèØËÉΩÁéØÂ¢ÉÔºàÂèÇÊï∞Ôºâ„ÄÇ ÂÅáËÆæÈöèÊú∫ÊäõÊé∑‰∏ÄÊûöÁ°¨Â∏Å1,000Ê¨°ÔºåÁªìÊûú500Ê¨°‰∫∫Â§¥Êúù‰∏äÔºå500Ê¨°Êï∞Â≠óÊúù‰∏äÔºåÈÇ£‰πà‰∏§Èù¢Êúù‰∏äÁöÑÊ¶ÇÁéáÂùá‰∏∫50%„ÄÇËøêÁî®Âá∫Áé∞ÁöÑÁªìÊûúÊù•Âà§Êñ≠Ëøô‰∏™‰∫ãÊÉÖÊú¨Ë∫´ÁöÑÊÄßË¥®ÔºàÂèÇÊï∞ÔºâÔºå‰πüÂ∞±ÊòØ‰ººÁÑ∂„ÄÇ ÂΩìÁªìÊûúÂíåÂèÇÊï∞Áõ∏‰∫íÂØπÂ∫îÔºå‰ººÁÑ∂ÂíåÊ¶ÇÁéáÂú®Êï∞ÂÄº‰∏äÁõ∏Á≠â„ÄÇ Áî® Œ∏ Ë°®Á§∫ÁéØÂ¢ÉÂØπÂ∫îÁöÑÂèÇÊï∞Ôºåx Ë°®Á§∫ÁªìÊûúÔºåÈÇ£‰πàÊ¶ÇÁéáÂèØ‰ª•Ë°®Á§∫‰∏∫Ôºö $$P(x | \\theta )$$ $p(x \\vert Œ∏)$ ÊòØÊù°‰ª∂Ê¶ÇÁéáÁöÑË°®Á§∫ÊñπÊ≥ï„ÄÇŒ∏ ÊòØÂâçÁΩÆÊù°‰ª∂ÔºåÁêÜËß£‰∏∫Âú® Œ∏ ÁöÑÂâçÊèê‰∏ãÔºå‰∫ã‰ª∂ x ÂèëÁîüÁöÑÊ¶ÇÁéáÔºåÁõ∏ÂØπÂ∫îÁöÑ‰ººÁÑ∂ÂèØ‰ª•Ë°®Á§∫‰∏∫: $$\\mathcal{L}(\\theta | x)$$ ÂèØ‰ª•ÁêÜËß£‰∏∫Â∑≤Áü•ÁªìÊûú‰∏∫ x ÔºåÂèÇÊï∞‰∏∫ Œ∏ (‰ººÁÑ∂ÂáΩÊï∞Èáå Œ∏ ÊòØÂèòÈáèÔºåËøôÈáåËØ¥ÁöÑÂèÇÊï∞ÂíåÂèòÈáèÊòØÁõ∏ÂØπ‰∏éÊ¶ÇÁéáËÄåË®ÄÁöÑ)ÂØπÂ∫îÁöÑÊ¶ÇÁéáÔºåÂç≥Ôºö $$\\mathcal{L}(\\theta | x)=P(x | \\theta)$$ ‰∏§ËÄÖÂú®Êï∞ÂÄº‰∏äÁõ∏Á≠âÔºå‰ΩÜÊòØÊÑè‰πâÂπ∂‰∏çÁõ∏Âêå, $\\mathcal{L}$ ÊòØÂÖ≥‰∫é Œ∏ ÁöÑÂáΩÊï∞ÔºåËÄå P ÂàôÊòØÂÖ≥‰∫é x ÁöÑÂáΩÊï∞„ÄÇ ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:1:0","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"Probability and P-value A p-value is the probability that random chance generated the data, or something else that is equal or rarer. A p-value is composed of three parts: The probability random chance would result in the observation. The probability of observing something else that is equally rare. The probability of observing something rarer or more extreme. But probability $$ \\text{Probalibility} = \\frac{ \\text{Number of outcomes of interest}} { \\text{The total number of outcomes}}$$ In hypothesis testing, p-values are numbers, between 0 and 1, that, how small does a p-value have to be before we are confident that interested A is different from B. Statquest: P Values, clearly explained ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:2:0","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"PDF (probability density function) PDFÔºöÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞Ôºàprobability density functionÔºâ, ËøûÁª≠ÂûãÈöèÊú∫ÂèòÈáèÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ÊòØ‰∏Ä‰∏™ÊèèËø∞Êüê‰∏™Á°ÆÂÆöÁöÑÂèñÂÄºÁÇπÈôÑËøëÁöÑÂèØËÉΩÊÄßÁöÑÂáΩÊï∞„ÄÇ Êï∞Â≠¶Ë°®Á§∫ÔºöÁî®PDFÂú®Êüê‰∏ÄÂå∫Èó¥‰∏äÁöÑÁßØÂàÜÊù•ÂàªÁîªÈöèÊú∫ÂèòÈáèËêΩÂú®Ëøô‰∏™Âå∫Èó¥‰∏≠ÁöÑÊ¶ÇÁéá $$ \\operatorname{Pr}(a \\leq X \\leq b)=\\int_{a}^{b} f_{X}(x) d x $$ ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:2:1","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"PMF (probability mass function) PMF : Ê¶ÇÁéáË¥®ÈáèÂáΩÊï∞Ôºàprobability mass function), Âú®Ê¶ÇÁéáËÆ∫‰∏≠ÔºåÊ¶ÇÁéáË¥®ÈáèÂáΩÊï∞ÊòØÁ¶ªÊï£ÈöèÊú∫ÂèòÈáèÂú®ÂêÑÁâπÂÆöÂèñÂÄº‰∏äÁöÑÊ¶ÇÁéá„ÄÇ Êï∞Â≠¶Ë°®Á§∫Ôºö PMFÂÖ∂ÂÆûÂ∞±ÊòØÈ´ò‰∏≠ÊâÄÂ≠¶ÁöÑÁ¶ªÊï£ÂûãÈöèÊú∫ÂèòÈáèÁöÑÂàÜÂ∏ÉÂæã„ÄÇ $$ f_{X}(x)=\\operatorname{Pr}(X=x) $$ ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:2:2","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"CDF (cumulative distribution function) CDF : Á¥ØÁßØÂàÜÂ∏ÉÂáΩÊï∞ (cumulative distribution function)ÔºåÊòØÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ÁöÑÁßØÂàÜÔºåËÉΩÂÆåÊï¥ÊèèËø∞‰∏Ä‰∏™ÂÆûÈöèÊú∫ÂèòÈáèXÁöÑÊ¶ÇÁéáÂàÜÂ∏É„ÄÇ CDFÊòØPDFÁöÑÔºà‰ªéË¥üÊó†Á©∑$-\\infty$Âà∞ÂΩìÂâçÂÄºÁöÑÔºâÁßØÂàÜÔºåPDFÊòØCDFÁöÑÂØºÊï∞ÔºéÔºà‰∏∫‰∫Ü‰æø‰∫éÊ¶ÇÁéáÁöÑËÆ°ÁÆóÔºåÂºïÂÖ•CDFÁöÑÊ¶ÇÂøµÔºâ CDFÁõ∏ÂΩì‰∫éÂÖ∂Â∑¶‰æßÁöÑÈù¢ÁßØÔºå‰πüÁõ∏ÂΩì‰∫éÂ∞è‰∫éËØ•ÂÄºÁöÑÊ¶ÇÁéáÔºåË¥üÊó†Á©∑ÁöÑCDFÂÄº‰∏∫ÔºêÔºåÊ≠£Êó†Á©∑ÁöÑCDFÂÄºÊÄª‰∏∫ÔºëÔºé ÂØπ‰∫éËøûÁª≠ÂèòÈáèÔºåÊúâ $$ F_{X}(x)=\\operatorname{Pr}(X \\leq x)=\\int_{-\\infty}^{x} f_{X}(t) dt $$ ÂØπ‰∫éÁ¶ªÊï£ÂûãÂèòÈáèÔºåÊúâÂ¶Ç $$ F_{X}(x)=\\operatorname{Pr}(X \\leq x)= \\begin{cases} 0 \\text { if } x\u003c0 \\cr \\frac{1}{2} \\text { if } 0 \\leq x\u003c1 \\cr 1 \\text { if } x \\geq 1 \\end{cases} $$ ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:2:3","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"Central Limit Theorem ‰∏≠ÂøÉÊûÅÈôêÂÆöÁêÜÔºàCentral Limit TheoremÔºâ ÁªôÂÆö‰∏Ä‰∏™‰ªªÊÑèÂàÜÂ∏ÉÁöÑÊÄª‰ΩìÔºå ÊØèÊ¨°‰ªéËøô‰∫õÊÄª‰Ωì‰∏≠ÈöèÊú∫ÊäΩÂèñ n ‰∏™ÊäΩÊ†∑Ôºå‰∏ÄÂÖ±ÊäΩ m Ê¨°Ôºå ÁÑ∂ÂêéÊääËøô m ÁªÑÊäΩÊ†∑ÂàÜÂà´Ê±ÇÂá∫Âπ≥ÂùáÂÄºÔºå ÂΩìmË∂≥Â§üÂ§ßÊó∂ÔºåËøômÊ¨°ÁöÑÂπ≥ÂùáÂÄºÁöÑÂàÜÂ∏ÉÔºàÁß∞‰∏∫ÊäΩÊ†∑ÂàÜÂ∏ÉÔºâÊé•ËøëÊ≠£ÊÄÅÂàÜÂ∏É„ÄÇ Áã¨Á´ãÂêåÂàÜÂ∏ÉÁöÑ‰∏≠ÂøÉÊûÅÈôêÂÆöÁêÜ $$ \\lim_{n \\rightarrow \\infty} F_{\\mathcal{X}}(x) = \\lim_{n \\rightarrow \\infty} P \\Bigg\\lbrace \\frac{\\sum_{k=1}^{n}X_k - n\\mu}{\\sqrt{n}\\sigma} \\leq x \\Bigg\\rbrace = \\int_{-\\infty}^{x} \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^2}{2}}dt $$ Âæ∑Ëé´‰ΩõÔºçÊãâÊôÆÊãâÊñØÂÆöÁêÜ: ËÆæÈöèÊú∫ÂèòÈáèÂ∫èÂàó$\\lbrace \\eta_1, \\eta_2,\\cdots, \\eta_n \\rbrace$ Êúç‰ªéÂèÇÊï∞‰∏∫$n, p (0 \u003c p \u003c 1)$ ÁöÑ‰∫åÈ°πÂàÜÂ∏É $$ \\lim_{n \\rightarrow +\\infty} P \\Bigg\\lbrace \\frac{\\eta_n - np }{\\sqrt{np(1-p)}} \\leq x \\Bigg\\rbrace = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^2}{2}}dt $$ ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:2:4","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"Law of Large Numbers ÂΩìÊ†∑Êú¨Êï∞ÊçÆÊó†ÈôêÂ§ßÊó∂ÔºåÊ†∑Êú¨ÂùáÂÄºË∂ã‰∫éÊÄª‰ΩìÂùáÂÄº $$ \\bar{X} = \\frac{1}{n} \\sum_{k=1}^n X_k \\xrightarrow{p} \\mu $$ Â§ßÊï∞ÂÆöÂæãÂëäËØâÊàë‰ª¨ËÉΩÁî®È¢ëÁéáËøë‰ºº‰ª£ÊõøÊ¶ÇÁéáÔºõËÉΩÁî®Ê†∑Êú¨ÂùáÂÄºËøë‰ºº‰ª£ÊõøÊÄª‰ΩìÂùáÂÄº„ÄÇ ËæõÈí¶Â§ßÊï∞ÂÆöÂæã: ËÆæ$X_1, X_2, \\cdots, X_n$ÊòØÁõ∏‰∫íÁã¨Á´ã‰∏îÊúç‰ªéÂêåÂàÜÂ∏ÉÁöÑÈöèÊú∫ÂèòÈáèÂ∫èÂàóÔºå ÂÖ∑ÊúâÊï∞Â≠¶ÊúüÊúõ$E(X_k) = \\mu$, $k=1,2,3,\\cdots$„ÄÇÂØπ‰∫é‰ªªÊÑè $\\epsilon \u003e 0$, Êúâ $$ \\lim_{n \\rightarrow +\\infty} P \\bigg\\lbrace | \\frac{1}{n} \\sum_{k=1}^n X_k - \\mu | \u003c \\epsilon \\bigg\\rbrace = 1 $$ ÂàáÊØîÈõ™Â§´Â§ßÊï∞ÂÆöÂæã: ÈöèÊú∫ÂèòÈáèÂ∫èÂàó$X$ÂÖ∑ÊúâÁõ∏ÂêåÊúüÊúõÂíåÊñπÂ∑Æ, Ê†∑Êú¨ÂùáÂÄº‰æùÊ¶ÇÁéá$p$Êî∂Êïõ‰∫é $\\mu$ $$ \\frac{1}{n} \\sum_{k=1}^n X_k \\xrightarrow{p} \\mu $$ ‰ºØÂä™Âà©Â§ßÊï∞ÂÆöÂæã: $n_A$ÊòØnÊ¨°Áã¨Á´ãÈáçÂ§çËØïÈ™å‰∏≠‰∫ã‰ª∂AÂèëÁîüÁöÑÊ¨°Êï∞Ôºå pÊòØ‰∫ã‰ª∂AÂú®ÊØèÊ¨°ËØïÈ™å‰∏≠ÂèëÁîüÁöÑÊ¶ÇÁéáÔºå‰ªªÊÑè$\\epsilon \u003e 0$ $$ \\lim_{n \\rightarrow +\\infty} P \\bigg\\lbrace | \\frac{n_A}{n} - p | \u003c \\epsilon \\bigg\\rbrace = 1 $$ ÊØîËæÉ ÂÆöÂæã ÂàÜÂ∏É ÊúüÊúõ ÊñπÂ∑Æ ÁªìËÆ∫ ËæõÈí¶Â§ßÊï∞ÂÆöÂæã Áõ∏‰∫íÁã¨Á´ã‰∏îÂêåÂàÜÂ∏É Â≠òÂú® ‰º∞ÁÆóÊúüÊúõ ÂàáÊØîÈõ™Â§´Â§ßÊï∞ÂÆöÂæã Áõ∏‰∫íÁã¨Á´ã Áõ∏Âêå Áõ∏Âêå ‰º∞ÁÆóÊúüÊúõ ‰ºØÂä™Âà©Â§ßÊï∞ÂÆöÂæã ‰∫åÈ°πÂàÜÂ∏É Áõ∏Âêå Áõ∏Âêå È¢ëÁéá=Ê¶ÇÁéá Áõ∏ÂêåÁÇπÔºö$n \\rightarrow +\\infty$, ‰æùÊ¶ÇÁéáË∂ãËøë Êù°‰ª∂ÁªÑ‰ª∂ÂèòÂæó‰∏•Ê†º ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:2:5","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"Confidence interval ÁΩÆ‰ø°Âå∫Èó¥Ôºàconfidence intervalÔºâ ÁΩÆ‰ø°Âå∫Èó¥ÊòØÊåáÁî±Ê†∑Êú¨ÁªüËÆ°ÈáèÊâÄÊûÑÈÄ†ÁöÑÊÄª‰ΩìÂèÇÊï∞ÁöÑ‰º∞ËÆ°Âå∫Èó¥„ÄÇ ÁΩÆ‰ø°Âå∫Èó¥Â±ïÁé∞ÁöÑÊòØËøô‰∏™ÂèÇÊï∞ÁöÑÁúüÂÆûÂÄºËêΩÂú®ÊµãÈáèÂÄºÔºàÊé®ÊµãÂÄºÔºâÁöÑÂë®Âõ¥ÁöÑÂèØ‰ø°Á®ãÂ∫¶„ÄÇ ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:2:6","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Statistic"],"content":"Additional: StatQuest How to calculate P-value ","date":"2020-04-29","objectID":"/2020-04-29-stats-proba-pvalue/:3:0","tags":["P-Value"],"title":"Probability, P-value, Likelihood","uri":"/2020-04-29-stats-proba-pvalue/"},{"categories":["Make bioinfo uncool again"],"content":"The correct way to convert seurat Robj to Scanpy h5ad","date":"2020-04-28","objectID":"/2020-04-28-seurat2scanpy/","tags":["scRNA-seq","Scanpy","Seurat"],"title":"Convert Seurat Robj to Scanpy h5ad","uri":"/2020-04-28-seurat2scanpy/"},{"categories":["Make bioinfo uncool again"],"content":"It costed me a lot of time to convert seurat objects to scanpy. It‚Äôs not a pleasant experience. Finally, I solved it. ","date":"2020-04-28","objectID":"/2020-04-28-seurat2scanpy/:0:0","tags":["scRNA-seq","Scanpy","Seurat"],"title":"Convert Seurat Robj to Scanpy h5ad","uri":"/2020-04-28-seurat2scanpy/"},{"categories":["Make bioinfo uncool again"],"content":"1. Install Seurat v3.0.2, or python kernel will always died!!! Don‚Äôt know why latest seurat not work. ","date":"2020-04-28","objectID":"/2020-04-28-seurat2scanpy/:0:1","tags":["scRNA-seq","Scanpy","Seurat"],"title":"Convert Seurat Robj to Scanpy h5ad","uri":"/2020-04-28-seurat2scanpy/"},{"categories":["Make bioinfo uncool again"],"content":"2. Set the R version for rpy2 # user defined R installation import os # path to your libR.so, only Seurat v3.0.2 works! # create a conda R env for seurat 3.0.2 first os.environ['R_HOME'] = '/home/fangzq/miniconda/envs/seurat/lib/R' # path depends on where you installed Python. os.environ['R_USER'] = '/home/fangzq/miniconda/lib/python3.7/site-packages/rpy2' ","date":"2020-04-28","objectID":"/2020-04-28-seurat2scanpy/:0:2","tags":["scRNA-seq","Scanpy","Seurat"],"title":"Convert Seurat Robj to Scanpy h5ad","uri":"/2020-04-28-seurat2scanpy/"},{"categories":["Make bioinfo uncool again"],"content":"3. Now, you‚Äôer good to go import scanpy as sc import glob Install anndata2ri first import anndata2ri from rpy2.robjects import r from rpy2.robjects.conversion import localconverter # activate rpy2 env anndata2ri.activate() robjs = glob.glob(\"data/*Robj\") Convert to h5ad r('library(Seurat)') for robj in robjs: r(f'x\u003c-load(\"{robj}\")') r('y=get(x)') r('rm(x)') r('DefaultAssay(y) \u003c- \"RNA\"') # get raw count matrix to save # seurat2 object # adata = r('as.SingleCellExperiment(UpdateSeuratObject(y))') adata = r('as.SingleCellExperiment(y)') adata.write_h5ad(filename=robj.replace(\"Robj\",\"h5ad\")) ","date":"2020-04-28","objectID":"/2020-04-28-seurat2scanpy/:0:3","tags":["scRNA-seq","Scanpy","Seurat"],"title":"Convert Seurat Robj to Scanpy h5ad","uri":"/2020-04-28-seurat2scanpy/"},{"categories":["Make bioinfo uncool again"],"content":"4. other way seurat -\u003e loom -\u003e scanpy It‚Äôs much easier, but I did not test. save to loom format fist. pbmc.loom \u003c- as.loom(pbmc.seurat, filename = \"../output/pbmc3k.loom\", verbose = FALSE) pbmc.loom read into scanpy pbmc3k = sc.read_loom(\"../output/pbmc3k.loom\") use sceasy to save h5ad. ","date":"2020-04-28","objectID":"/2020-04-28-seurat2scanpy/:0:4","tags":["scRNA-seq","Scanpy","Seurat"],"title":"Convert Seurat Robj to Scanpy h5ad","uri":"/2020-04-28-seurat2scanpy/"},{"categories":["Make bioinfo uncool again"],"content":"GATK is design for human genetics, it works not so good on homogeneous inbred mouse. But we could use bcftools instead. One of my colleague who studies mouse genetics, said, I tried the haplotype caller from GATK. But it seems that the haplotype caller is designed for heterogeneous genome like human than for mice. Therefore, the result coming out of HC is worse than samtools, as I manually inspected a few regions that HC calls didn‚Äôt make sense. In addition, in one of their mouse genomic paper that we reviewed, they even skipped the second recalibration step. We asked them why and they said it was because of the same reason: good for human but not that good for the homogeneous inbred mouse. But we still could collect the resource bundle for mouse. I found a workflow here. However, the script is out of date. Also, see discussion here For GATK4, we have ","date":"2020-03-10","objectID":"/2020-03-10-gatk4-mm10-bundle/:0:0","tags":["GATK","Genetics"],"title":"GATK for inbred mouse is not good","uri":"/2020-03-10-gatk4-mm10-bundle/"},{"categories":["Make bioinfo uncool again"],"content":"1. Genome Download from NCBI (mm10) or Sanger Mouse Genetics Programme # NCBI wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/635/GCF_000001635.26_GRCm38.p6/GCF_000001635.26_GRCm38.p6_genomic.fna.gz -O GRCm38_68.fa.gz # or Sanger MGP wget ftp://ftp-mouse.sanger.ac.uk/ref/GRCm38_68.fa -O GRCm38_68.fa ","date":"2020-03-10","objectID":"/2020-03-10-gatk4-mm10-bundle/:0:1","tags":["GATK","Genetics"],"title":"GATK for inbred mouse is not good","uri":"/2020-03-10-gatk4-mm10-bundle/"},{"categories":["Make bioinfo uncool again"],"content":"2. dbSNP Depends on your study design. Download All in one vcf file from NCBI wget ftp://ftp.ncbi.nih.gov/snp/organisms/archive/mouse_10090/VCF/00-All.vcf.gz \\ -O mouse.dbsnp.vcf.gz Download from the Sanger Mouse Genetics Programme (Sanger MGP) wget ftp://ftp-mouse.sanger.ac.uk/REL-1505-SNPs_Indels/mgp.v5.merged.snps_all.dbSNP142.vcf.gz ","date":"2020-03-10","objectID":"/2020-03-10-gatk4-mm10-bundle/:0:2","tags":["GATK","Genetics"],"title":"GATK for inbred mouse is not good","uri":"/2020-03-10-gatk4-mm10-bundle/"},{"categories":["Make bioinfo uncool again"],"content":"3. Known Indels For mouse indels, the Sanger Mouse Genetics Programme (Sanger MGP) is probably the best resource. Download all MGP indels (5/2015 release): wget ftp://ftp-mouse.sanger.ac.uk/REL-1505-SNPs_Indels/mgp.v5.merged.indels.dbSNP142.normed.vcf.gz \\ -O mgp.v5.indels.vcf.gz Filter for passing variants # take header first zcat mgp.v5.indels.vcf.gz | head -1000 | grep \"^#\" | cut -f 1-8 \\ \u003e mgp.v5.indels.pass.chr.vcf # keep only passing and append zcat mgp.v5.indels.vcf.gz | grep -v \"^#\" | cut -f 1-8 \\ | grep -w \"PASS\" \u003e\u003e mgp.v5.indels.pass.chr.vcf Sort VCF (automatically generated index has to be deleted due to a known bug -\u003e No anymore): gatk SortVcf -SD GRCm38_68.dict -I mgp.v5.indels.pass.chr.vcf -O mgp.v5.indels.pass.chr.sort.vcf # rm .idx # rm mgp.v5.indels.pass.chr.sort.vcf.idx ","date":"2020-03-10","objectID":"/2020-03-10-gatk4-mm10-bundle/:0:3","tags":["GATK","Genetics"],"title":"GATK for inbred mouse is not good","uri":"/2020-03-10-gatk4-mm10-bundle/"},{"categories":["Coding"],"content":"Get answers for C/C++ within ? s","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"Just some advanced C/C++ code snippets to keep in mind. ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:0:0","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"Header naming Never used some header file name with std. Sometimes, compiler could not find the std headers.!!! ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:1:0","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"FileIO The simplest example #include \u003ciostream\u003e#include \u003cfstream\u003e // output file std::ofstream output; output.open(\"test.compact.txt\"); // read input file string line; std::ifstream input(\"test.chrX.vcf\"); if (input.is_open()) { while (getline(input, line)) output \u003c\u003c line \u003c\u003c'\\n'; } input.close(); output.close(); ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:2:0","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"Strings: split and strip split string by delimiter std::vector\u003cstd::string\u003e split(const std::string\u0026 s, char delimiter) { std::vector\u003cstd::string\u003e tokens; std::string token; std::istringstream tokenStream(s); while (std::getline(tokenStream, token, delimiter)) { tokens.push_back(token); } return tokens; } strip strings std::string trim(const std::string\u0026 str, const std::string delimiter = \" \\n\\r\\t\") { // std::string s; // s.erase(s.find_last_not_of(\" \\n\\r\\t\")+1); size_t first = str.find_first_not_of(delimiter); if (std::string::npos == first) { return str; } size_t last = str.find_last_not_of(delimiter); return str.substr(first, (last - first + 1)); } ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:3:0","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"Containor ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:4:0","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"remove duplicated elements: O(nlogn) sort(nums.begin(),nums.end()); // inplace // unique do not change vector size, only put dup elements to end of containor // and return a iter which points to the first non-uniqdup element vector\u003cint\u003e::iterator iter = unique(nums.begin(), nums.end()); nums.erase(iter, nums.end()); //remove duplciates inplace // nums.resize( std::distance(nums.begin(),iter) ); ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:4:1","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"Remove elements Vector or Deque: algorithm remove() followed by erase() vector\u003cint\u003e vec = {1,1,2,3,4,4,6}; auto itr = remove(vec.begin(), vec.end(), 4); vec.erase(iter, vec.end()); vec.shrink_to_fit(); // reduce capacity List: member function .remove() Associative Container or Unordered Container: .erase() ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:4:2","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"Lambda Syntax auto basicLambda = [] { cout \u003c\u003c \"Hello, world!\" \u003c\u003c endl; }; basicLambda(); // Hello, world! // return type auto add = [](int a, int b) -\u003e int { return a + b; }; // inference return type auto multiply = [](int a, int b) { return a * b; }; Capture parameters int x = 10; auto add_x = [x](int a) { return a + x; }; // copy capture x auto multiply_x = [\u0026x](int a) { return a * x; }; // ref capture x []ÔºöÈªòËÆ§‰∏çÊçïËé∑‰ªª‰ΩïÂèòÈáèÔºõ [=]ÔºöÈªòËÆ§‰ª•ÂÄºÊçïËé∑ÊâÄÊúâÂèòÈáèÔºõ [\u0026]ÔºöÈªòËÆ§‰ª•ÂºïÁî®ÊçïËé∑ÊâÄÊúâÂèòÈáèÔºõ [ x ]Ôºö‰ªÖ‰ª•ÂÄºÊçïËé∑xÔºåÂÖ∂ÂÆÉÂèòÈáè‰∏çÊçïËé∑Ôºõ [\u0026x]Ôºö‰ªÖ‰ª•ÂºïÁî®ÊçïËé∑xÔºåÂÖ∂ÂÆÉÂèòÈáè‰∏çÊçïËé∑Ôºõ [=, \u0026x]ÔºöÈªòËÆ§‰ª•ÂÄºÊçïËé∑ÊâÄÊúâÂèòÈáèÔºå‰ΩÜÊòØxÊòØ‰æãÂ§ñÔºåÈÄöËøáÂºïÁî®ÊçïËé∑Ôºõ [\u0026, x]ÔºöÈªòËÆ§‰ª•ÂºïÁî®ÊçïËé∑ÊâÄÊúâÂèòÈáèÔºå‰ΩÜÊòØxÊòØ‰æãÂ§ñÔºåÈÄöËøáÂÄºÊçïËé∑Ôºõ [this]ÔºöÈÄöËøáÂºïÁî®ÊçïËé∑ÂΩìÂâçÂØπË±°ÔºàÂÖ∂ÂÆûÊòØÂ§çÂà∂ÊåáÈíàÔºâÔºõ [*this]ÔºöÈÄöËøá‰º†ÂÄºÊñπÂºèÊçïËé∑ÂΩìÂâçÂØπË±°Ôºõ capture expression // capture by expression int x = 4; auto y = [\u0026r = x, x = x + 1] { r += 2; return x * x; }(); // x = 6Ôºåy = 25 // initialize directly auto z = [str = \"string\"]{ return str; }(); // z: const char * generic: auto auto add = [](auto x, auto y) { return x + y; }; int x = add(2, 3); // 5 double y = add(2.5, 3.5); // 6.0 ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:5:0","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"Design Pattern ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:6:0","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"Singleton Define class Singleton { private: /* Here will be the instance stored. */ static Singleton* instance; /* Private constructor to prevent instancing. */ Singleton() {}; public: /* Static access method. */ static Singleton* getInstance() { if (instance == 0) instance = new Singleton(); return instance; } }; /* NULL, because instance will be initialized on demand. */ Singleton* Singleton::instance = 0; Usage #include \u003ciostream\u003eint main() { //new Singleton(); // Won't work Singleton* s = Singleton::getInstance(); // Ok Singleton* r = Singleton::getInstance(); /* The addresses will be the same. */ std::cout \u003c\u003c s \u003c\u003c std::endl; std::cout \u003c\u003c r \u003c\u003c std::endl; } ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:6:1","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"Delegate Don‚Äôt confuse with delegate constructor!!! A delegate is a class that wraps a pointer or reference to an object instance, a member method of that object‚Äôs class to be called on that object instance, and provides a method to trigger that call. Example 1 #include \u003ciostream\u003eusing namespace std; class RealPrinter { public: void print() { std::cout \u003c\u003c \"real-printer\" \u003c\u003c std::endl; } }; class Printer { public: Printer() : p(RealPrinter()) {} void print() { p.print(); } private: RealPrinter p; }; int main() { Printer* printer = new Printer(); printer-\u003eprint(); } Example 2: #include \u003ciostream\u003eclass I //interface { public: virtual void f() = 0; virtual void g() = 0; }; class A : public I { public: void f(){std::cout \u003c\u003c \"A::f()\" \u003c\u003c std::endl;} void g(){std::cout \u003c\u003c \"A::g()\" \u003c\u003c std::endl;} }; class B : public I { public: void f(){std::cout \u003c\u003c \"B::f()\" \u003c\u003c std::endl;} void g(){std::cout \u003c\u003c \"B::g()\" \u003c\u003c std::endl;} }; class C : public I { public: C() { m_i = new A();/*delegation*/ } void f(){ m_i-\u003ef(); } void g(){ m_i-\u003eg(); } // normal attributes void toA(){ m_i = new A(); } void toB(){ m_i = new B(); } private: I* m_i; } int main() { C cc = C(); cc.f(); // output: A::f() cc.g(); // output: A::g() cc.toB(); cc.f(); // output: B::f() cc.g(); // output: B::g() } ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:6:2","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"Composite Composite is a structural design pattern that allows composing objects into a tree-like structure and work with the it as if it was a singular object. ","date":"2020-02-10","objectID":"/2020-02-10-cpp-stl/:6:3","tags":["C++"],"title":"C++ Notes:  STL","uri":"/2020-02-10-cpp-stl/"},{"categories":["Coding"],"content":"Get answers for C/C++ within ? s","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"C++ Notes Just some C/C++ code snippets to keep in mind. C/C++ is tremendous complicated, but it‚Äôs still the most powerful programming language. ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:0:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Table of Contents char to Int Pointer Pointer and Smart Pointer Array as Argument Operator that can‚Äôt be overloaded Object Instantization Object Relationship Virtual Function and Ploymorphism Friend Const Constexpr Extern and static ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:1:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Char to Int ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:2:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"1. Char sotred as ASCII C store Char as ASCII (Int) by default. So, Char is equal to ASCII code. char a = 'A'; // 65 int c = a; //c = 65 ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:2:1","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"2. Char to Int, to String, and vice versa char and int char c='5' int res = c -'0' ; // 5 int i=5; char res = I + '0'; // '5' char*,int and string // char * to string const char * str_c = \"hello\"; std::string str = str_c; // string to char* str.c_str(); // return const char* // int to string std::to_string() // string to int std::stoi() ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:2:2","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"3. Char as subscript of array: legal! This is useful when create hashmap. e.g. counting chars int test[200] = {0}; test['A'] = 1; // legal test['b'] = 2; // legal ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:2:3","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Operator Could not be Overloaded operator memo . Member access or dot operator .* Pointer-to-member Operator :: Scope Resolution operator ? : conditional operator, ternary operator sizeof object size operator, built-in operations typeid object type operator, built-in operations OK, What‚Äôs .* ? //we have a class struct X { void f() {} void g() {} }; typedef void (X::*pointer)(); //ok, let's take a pointer and assign f to it. pointer somePointer = \u0026X::f; //now I want to call somePointer. But for that, I need an object X x; //now I call the member function on x like this (x.*somePointer)(); //will call x.f() //now, suppose x is not an object but a pointer to object X* px = new X; //I want to call the memfun pointer on px. I use -\u003e* (px -\u003e* somePointer)(); //will call px-\u003ef(); Now, you can‚Äôt use x.somePointer(), or px-\u003esomePointer() because there is no such member in class X. see here ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:3:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Pointer Pointer syntax Rule: read from right to left int a; // an int int *a; // a pointer point to int int **a; // secondary int pointer, point to another int pointer int a[10]; // int array int *a[10]; // a poiter array, point to int int (*a)[10]; // a int pointer point to an int array int (*a)(int); // a pointer point to a function, will return an int int (*a[10])(int); // a poiter array, point to a functionÔºåwill return an int Declare two pointers int* a, b; // equal to int* a; int b; int *a, *b; // correct way ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:4:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Pointer and Smart Pointer #include \u003cmemory\u003e // smart pointer header // [pomter to smart pointer struct Base {}; struct Derived: Base {}; // pointer to smart Base *p1 = new Derived(); // upcast std::shared_ptr\u003cBase\u003e sp(p1); // a polymorphic type Base *p = new Derived(); // upcast, dynamic_cast is unnecessary Derived* dp = dynamic_cast\u003cDerived*\u003e (p); // downcast // smart pointer convert to pointer std::shared_ptr\u003cBase\u003e smart = std::make_shared\u003cDerived\u003e(); Base* p2 = smart.get(); // .get() // smart pointer cast // downcast std::shared_ptr\u003cDerived\u003e dsmart = std::dynamic_pointer_cast\u003cDerived\u003e(smart); // upcast // case 1 std::shared_ptr\u003cBase\u003e foo(new Derived()); // case 2 std::shared_ptr\u003cDerived\u003e bar = std::make_shared\u003cBase\u003e(); std::shared_ptr\u003cBase\u003e foo = std::dynamic_pointer_cast\u003cA\u003e(bar); ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:5:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Array ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:6:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Array as formal arguments An Array could not copy to anther Array (Copy pointer is not allowed!), so call-by-value is not allowed. So, use array pointer: //these are same void print(const int*); void print(const int[]); void print(const int[5]); multi-dimension array void print(const int(*p)[3], int rowsize); void print(const int p[][5], int rowsize); When use pointer to an Array, the dimension is unknown. So, need an extra argument to specify it explicitly. Example: void print1(int (*p)[3]) { cout\u003c\u003cp[1][1]\u003c\u003cendl; } void print2(int p[][3]) { cout\u003c\u003cp[0][0]\u003c\u003cendl; } int a[2][3]={ {1,2},{3,4} }; print1(a); // 4 print2(a); // 1 int b[2][4]={ {1,2,5,6},{3,4,7,8} }; print1(b); // error ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:6:1","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Object Instantization ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:7:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"1. without new stack ClassName object(param); // A a(1);¬†ClassName object2 = ClassName(param); // A b = A(1); ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:7:1","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"2. with new heap ClassName *object = new¬†ClassName(param);//A *a = new A(); delete object; ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:7:2","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"3. copy constructor // ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:7:3","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"4. Smart Pointer std::unique_ptr\u003cClassName\u003e object (new ClassName(param)); // recommend this way of instantization std::unique_ptr\u003cClassName\u003e object = std::make_unique\u003cClassName\u003e(param); ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:7:4","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Friend The friend declaration appears in a class body and grants a function or another class access to private and protected members of the class where the friend declaration appears. ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:8:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"1. friend function Declare anywhere inside a class, but define outside // function friend \u003ctype\u003e \u003cName\u003e(\u003carguments\u003e); Example: class A { public: A(int _a):a(_a){}; // non-member function friend int getA_a(A \u0026_classA); private: int a; }; // without the friend keyword int getA_a(A \u0026_classA) { //access member by formal arguments return _classA.a; } A _classA(3); std::cout\u003c\u003cgetA_a(_classA); // 3 ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:8:1","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"2. friend class Delare inside class, define outside // class friend class \u003cName\u003e; Note: friend class X {}; is an error Example: class B { public: B(int _b):b(_b){}; friend class C; // friend class private: int b; }; class C { public: int getB_b(B _classB){ //access member by formal arguments return _classB.b; }; }; B _classB(3); C _classC; // an instance of a friend class _classC.getB_b(_classB); ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:8:2","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"3. Others: friend ostream, friend template ‚Ä¶ class Y { int data; // the non-member function operator\u003c\u003c will have access to Y's private members friend std::ostream\u0026 operator\u003c\u003c(std::ostream\u0026 out, const Y\u0026 o); friend char* X::foo(int); // members of other classes can be friends too friend X::X(char), X::~X(); // constructors and destructors can be friends }; // this operator\u003c\u003c still needs to be defined, as a non-member std::ostream\u0026 operator\u003c\u003c(std::ostream\u0026 out, const Y\u0026 y) { // can access private member Y::data return out \u003c\u003c y.data; } Back to top ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:8:3","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Object Relationships relation types ‚Äúis-a‚Äù ‚Äúhas-a‚Äù ‚Äúuses-a‚Äù ‚Äúdepends-on‚Äù Property Composition Aggregation Association Relationship type Whole/part Whole/part Otherwise unrelated Members can belong to multiple classes No Yes Yes Members existence managed by class Yes No No Directionality Unidirectional Unidirectional Unidirectional or bidirectional Relationship verb Part-of Has-a Uses-a ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:9:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Composition: has a data member Building complex objects from simpler ones is called object composition . object composition models a ‚Äúhas-a‚Äù relationship between two objects. In C++, It means structs and classes can have data members of various types. class AÔºõ class B { public: B(){} ~B(){} private: A a; int b; }Ôºõ Summary: Typically use normal member variables Can use pointer members if the class handles object allocation/deallocation itself Responsible for creation/destruction of parts ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:9:1","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Aggregation: ‚Äúhas a‚Äù Unlike a composition, parts can belong to more than one object at a time, and the whole object is not responsible for the existence and lifespan of the parts. Summary: Typically use pointer or reference members that point to or reference objects that live outside the scope of the aggregate class Not responsible for creating/destroying parts ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:9:2","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Association: ‚Äúuses a‚Äù Association models as ‚Äúuses-a‚Äù relationship. The doctor ‚Äúuses‚Äù the patient (to earn income). The patient uses the doctor (for whatever health purposes they need). ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:9:3","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Delegate: ‚Äúhas a‚Äù or called pImpl(Pointer to IMPLementation) Delegate: Composition by reference has a pointer of another object class AÔºõ class B { public: B(){} ~B(){} private: A *a; int b; }Ôºõ ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:9:4","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Inheritance: ‚Äúis a‚Äù public, protected, private class A { public: A(){} virtual ~A(){} } class B : public A { }; ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:9:5","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Virtual Functions and Runtime Polymorphism Declare: vitrual keyword class TestA { public: virtual void func() { cout \u003c\u003c \"virtual function\" \u003c\u003c endl; } }; class Test : public TestA { public: virtual void func() { // virtual could be omited cout \u003c\u003c \"Test virtual function\" \u003c\u003c endl; } ~Test() { } }; TestA* t = new Test; // parent pointer point to child (Ploymorphism) t-\u003efunc(); // ResultsÔºöTest virtual function delete t; Member Could not be virtual inline function constructor non-member function static function: only one copy of all objects. friend function: it‚Äôs non-member function member function template ! Pure virtual function declare virtual void fun() = 0; class with pure virtual functoin could not be instantized! a derived class of virtual class has to define pure virtual function. then the derived class could be instantized. abstract class: class with pure virtual function virtual deconstrutor A parent pointer point to it‚Äôs child. When delete the parent pointer, only parent constuctor is called. if declared a virtual deconstuctor, child‚Äôs deconstuctor is called first, then the parent deconsturctor. virtural keyword could be omited if a parent deconstructor is declared. delete a pointer will only called object‚Äôs deconstructor where the pointer point to. see also Pointer and smart pointer cast ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:10:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Const ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:11:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"1. const before or behind type/class, the syntax semantic are same // they are same const int x; // (int x) is const/inmutable int const x; // (const x) has type int ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:11:1","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"2. Pointer with const: const¬†int* p,¬†int¬†const* p and int¬†*const p Dirty trick: use * as a separator, const restrict the type according to the side where it belong to point to const: These two expression are same // -\u003e (const int) | p; p : a mutable pointer points to a const/immutable int const int * p; // -\u003e (int const) | p; p2: a mutable pointer points a const which has type int int const * p2; const pointer: But these two not the same // -\u003e int | (const p); p3: a const pointer, point to an mutable int int * const p3; // -\u003e (const int) | const p; p4: a const pointer, pointing to an immutable/const int const int * const p4; ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:11:2","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"3. class member func with const: () const a. const object could not change class variable could not call non-const function class Number { public: void set(int num) { number = num; } int get() { return number; } int get2() const {return number;} int number = 0; }; // Example const Number n; n.number = 1; // Error, n is const n.set(1); // Error, n is const, non-const `set()` n.get(); // Error, non-const `get()` n.get2(); // OK b. () const could not change class variable, except static could get variable class Number { private: int a; static int b; const int c = 20; public: void set() { a = 10; // error when `this` argument has type 'const' void set2() const { b = 20; // OK } int get() const { // OK return a; // did not change a } }; const Number n; n.set(); // Error n.set2(); // OK n.get(); // OK Easy to understand, when pointer this is const void Number::set(const Number *const this, int num) { number = num; } // illegal -\u003e const this c. () const overloading ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:11:3","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"constexpr The constexpr specifier declares that it is possible to evaluate the value of the function or variable at compile time. ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:12:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"default and delete in class special class member: default constructor deconstructor copy constructor operater = when use default and delete default class X { public: X()=default; // with this, you could declare like this: X x; X(int){}; }; X x; // works delete: prohibit func call marked by delete class X { public: X(); X(const X\u0026) = delete; X\u0026 operator = (const X \u0026) = delete; }; // example X x1; X x2=x1; // Error, copy constructor is prohibited ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:13:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"extern, static global variable defined outside all functions and available to all functions. unaffected by scopes and are always available (exists until the program ends) extern declare a global variable (exists on the whole project): variable could be used in multi- .cpp files extern \"C\" {/* c code */}: compile c code. static declare a local global variable (file scope): only be accessed in its translation unit or .o file, that‚Äôs, in the file where it is created. declare a static class member (class scope): initialization should be outside class body static data member static function: no this pointer: only access to other static member/function could declare as private ","date":"2020-02-10","objectID":"/2020-02-10-cpp-basics/:14:0","tags":["C++"],"title":"C++ Notes: Basics","uri":"/2020-02-10-cpp-basics/"},{"categories":["Coding"],"content":"Get answers for C/C++ within ? s","date":"2020-02-10","objectID":"/2020-02-10-cpp-pointer/","tags":["C++"],"title":"C++ Notes: Return Pointer or Reference","uri":"/2020-02-10-cpp-pointer/"},{"categories":["Coding"],"content":"Explaination A pointer or reference could not be return if they point/refer to a local variable stored in stack inside a function (local variable stored in stack will be destoried automatically when return, and the pointer become wild) Situations when a function could return pointer or reference variable defined outside a function scope global variable local static variable local variable stored in heap ( new opterator, malloc()) Other process could not access the memory of variable stored in heap until it is released. That‚Äôs why. ","date":"2020-02-10","objectID":"/2020-02-10-cpp-pointer/:1:0","tags":["C++"],"title":"C++ Notes: Return Pointer or Reference","uri":"/2020-02-10-cpp-pointer/"},{"categories":["Coding"],"content":"Example #include \u003ciostream\u003e#include \u003cstring.h\u003e#include \u003cstdlib.h\u003e using namespace std; string\u0026 f1(const string \u0026s) { static string result = s; return result; } string \u0026f2(const string \u0026s) { string *p = new string; *p = s; return *p; } int *f3() { int *a = (int *)malloc(sizeof(int) * 10); *a = 10; *(a + 1) = 11; return a; } int \u0026f4() { int *a = (int *)malloc(sizeof(int) * 10); *a = 10; *(a + 1) = 11; return *a; } int main() { int *a = \u0026f7(); cout\u003c\u003c(*(a + 1))\u003c\u003cendl; free(a); // free the memory when done. return 0; } Return *this, or alrealy exist objets // ref A\u0026 A::operator++() { count++; return *this; // already existed object, created outside } ","date":"2020-02-10","objectID":"/2020-02-10-cpp-pointer/:2:0","tags":["C++"],"title":"C++ Notes: Return Pointer or Reference","uri":"/2020-02-10-cpp-pointer/"},{"categories":["Coding"],"content":"What‚Äôs the defference between new and malloc() malloc() is a function that takes a number (of bytes) as its argument; it __returns a void*__ pointing to unitialized storage. new is an operator that **takes a type** and (optionally) a set of initializers for that type as its arguments; it **returns a pointer to an** (optionally) initialized **object of its type**. The difference is most obvious when you want to allocate an object of a user-defined type with non-trivial initialization semantics ","date":"2020-02-10","objectID":"/2020-02-10-cpp-pointer/:3:0","tags":["C++"],"title":"C++ Notes: Return Pointer or Reference","uri":"/2020-02-10-cpp-pointer/"},{"categories":["Machine Learning"],"content":"A recipe for interactive computing using custom Jupyter kernels on Stanford's Sherlock.","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"A recipe for interactive computing using custom Jupyter kernels on Stanford‚Äôs Sherlock. ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:0:0","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"Setting up custom conda environment on Sherlock‚Äôs login node ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:0","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"1. Download and install Miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh # install bash Miniconda3-latest-Linux-x86_64.sh conda config --set always_yes yes ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:1","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"2. Install jupyter notebook/lab and secure your notebooks with a password # install the default py3 kernel for jupyter notebook conda install ipython jupyter notebook jupyterlab # add password jupyter notebook password ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:2","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"3. (Optional) Add custom conda environment. i.e. fastai conda create -n fastai ipython ipykernel # add the custom to Jupyter notebook conda activate fastai python -m ipykernel install --user --name fastai --display-name FastAI you could also add R, Julia etc kernel. ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:3","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"4. Install pytorch/tensorflow You should select the existed cuda version which installed in Sherlock conda install -c pytorch pytorch torchvision cudatoolkit=10.1 tensorflow conda install tensorflow-gpu cudatoolkit=10.1 ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:4","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"5. Load gpu modules. Select the corresponding cuda version you‚Äôve just installed # this is my version module load cuda/10.1.168 module load cudnn/7.6.4 module load nccl ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:5","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"6. now, open ipython, run import torch print(torch.cuda.is_avilable()) if print out is True, then you‚Äôer OK to use GPUs. Follow these steps on your local machine see details here. ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:6","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"7. Download the forward repo git clone https://github.com/vsoch/forward cd forward ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:7","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"8. Generate your parameters bash setup.sh Select Sherlock partition: gpu ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:8","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"9. SSH Credentials bash hosts/sherlock_ssh.sh \u003e\u003e ~/.ssh/config ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:9","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"10. create a sbatch script in forward/sbatches/sherlock and save as jupyter-gpu.sbatch #!/bin/bash PORT=$1 NOTEBOOK_DIR=$2 if [ -z \"$NOTEBOOK_DIR\" ]; then cd $SCRATCH else cd $NOTEBOOK_DIR fi ## to compile libtorch C++ code, load these modules # module load gcc/7.3.0 # module load gdb # module load cmake # export CC=$(which gcc) # export CXX=$(which g++) # select cuda version you need module load cuda/10.1.168 module load cudnn/7.6.4 module load nccl # activate fastai env source activate fastai jupyter lab --no-browser --port=$PORT ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:10","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"11. Start a session The default working directory is $SCRATCH bash start.sh jupyter-gpu change the working directory bash start.sh jupyter /path/to/dir ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:11","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"12. open your browser in local machine and type if your port is 51888, then http://localhost:51888/ here is my jupyter lab computing environment. Have fun! fastai kernel Test GPUs ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:12","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"13. Resume a session bash resume.sh jupyter-gpu # or bash resume.sh jupyter-gpu /path/to/dir ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:13","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Machine Learning"],"content":"14. Stop a session bash end.sh jupyter-gpu # or bash end.sh jupyter-gpu /path/to/dir ","date":"2020-02-10","objectID":"/2020-02-10-ml-sherlock/:1:14","tags":["Jupyter","Sherlock"],"title":"How to do deep learning using custom Jupyter kernels on Sherlock","uri":"/2020-02-10-ml-sherlock/"},{"categories":["Algorithm and data structure"],"content":"Graphs ","date":"2020-02-09","objectID":"/2020-02-09-cs-graph/:0:0","tags":["C++"],"title":"Graph","uri":"/2020-02-09-cs-graph/"},{"categories":["Algorithm and data structure"],"content":"Some data structures to keep in my mind. BinaryHeap: Complete binary tree MaxHeap: Parent \u003e Both Children IndexMaxHeap MinHeap: Parent \u003c Both Children IndexMinHeap Priority queue (MaxHeap) BinarySearchTree Not always complete binary tree Value: leftChild \u003c Parent \u003c rightChild DenseGraph SparseGraph Code snippets take from Play with Algorithm ","date":"2020-02-09","objectID":"/2020-02-09-cs-graph/:0:1","tags":["C++"],"title":"Graph","uri":"/2020-02-09-cs-graph/"},{"categories":["Algorithm and data structure"],"content":"1. Dense Graph #include \u003ciostream\u003e#include \u003cvector\u003e#include \u003ccassert\u003e using namespace std; // Á®†ÂØÜÂõæ - ÈÇªÊé•Áü©Èòµ class DenseGraph{ private: int n, m; bool directed; vector\u003cvector\u003cbool\u003e\u003e g; public: DenseGraph( int n , bool directed ){ this-\u003en = n; this-\u003em = 0; this-\u003edirected = directed; for( int i = 0 ; i \u003c n ; i ++ ) g.push_back( vector\u003cbool\u003e(n, false) ); } ~DenseGraph(){ } int V(){ return n;} int E(){ return m;} void addEdge( int v , int w ){ assert( v \u003e= 0 \u0026\u0026 v \u003c n ); assert( w \u003e= 0 \u0026\u0026 w \u003c n ); if( hasEdge( v , w ) ) return; g[v][w] = true; if( !directed ) g[w][v] = true; m ++; } bool hasEdge( int v , int w ){ assert( v \u003e= 0 \u0026\u0026 v \u003c n ); assert( w \u003e= 0 \u0026\u0026 w \u003c n ); return g[v][w]; } void show(){ for( int i = 0 ; i \u003c n ; i ++ ){ for( int j = 0 ; j \u003c n ; j ++ ) cout\u003c\u003cg[i][j]\u003c\u003c\"\\t\"; cout\u003c\u003cendl; } } class adjIterator{ private: DenseGraph \u0026G; int v; int index; public: adjIterator(DenseGraph \u0026graph, int v): G(graph){ this-\u003ev = v; this-\u003eindex = -1; } int begin(){ index = -1; return next(); } int next(){ for( index += 1 ; index \u003c G.V() ; index ++ ) if( G.g[v][index] ) return index; return -1; } bool end(){ return index \u003e= G.V(); } }; }; ","date":"2020-02-09","objectID":"/2020-02-09-cs-graph/:0:2","tags":["C++"],"title":"Graph","uri":"/2020-02-09-cs-graph/"},{"categories":["Algorithm and data structure"],"content":"2. Sparse Graph #include \u003ciostream\u003e#include \u003cvector\u003e#include \u003ccassert\u003e using namespace std; // Á®ÄÁñèÂõæ - ÈÇªÊé•Ë°® class SparseGraph{ private: int n, m; bool directed; vector\u003cvector\u003cint\u003e\u003e g; public: SparseGraph( int n , bool directed ){ this-\u003en = n; this-\u003em = 0; this-\u003edirected = directed; for( int i = 0 ; i \u003c n ; i ++ ) g.push_back( vector\u003cint\u003e() ); } ~SparseGraph(){ } int V(){ return n;} int E(){ return m;} void addEdge( int v, int w ){ assert( v \u003e= 0 \u0026\u0026 v \u003c n ); assert( w \u003e= 0 \u0026\u0026 w \u003c n ); g[v].push_back(w); if( v != w \u0026\u0026 !directed ) g[w].push_back(v); m ++; } bool hasEdge( int v , int w ){ assert( v \u003e= 0 \u0026\u0026 v \u003c n ); assert( w \u003e= 0 \u0026\u0026 w \u003c n ); for( int i = 0 ; i \u003c g[v].size() ; i ++ ) if( g[v][i] == w ) return true; return false; } void show(){ for( int i = 0 ; i \u003c n ; i ++ ){ cout\u003c\u003c\"vertex \"\u003c\u003ci\u003c\u003c\":\\t\"; for( int j = 0 ; j \u003c g[i].size() ; j ++ ) cout\u003c\u003cg[i][j]\u003c\u003c\"\\t\"; cout\u003c\u003cendl; } } class adjIterator{ private: SparseGraph \u0026G; int v; int index; public: adjIterator(SparseGraph \u0026graph, int v): G(graph){ this-\u003ev = v; this-\u003eindex = 0; } int begin(){ index = 0; if( G.g[v].size() ) return G.g[v][index]; return -1; } int next(){ index ++; if( index \u003c G.g[v].size() ) return G.g[v][index]; return -1; } bool end(){ return index \u003e= G.g[v].size(); } }; }; ","date":"2020-02-09","objectID":"/2020-02-09-cs-graph/:0:3","tags":["C++"],"title":"Graph","uri":"/2020-02-09-cs-graph/"},{"categories":["Algorithm and data structure"],"content":"Binary trees Difference ","date":"2020-02-08","objectID":"/2020-02-08-cs-binarytree/:0:0","tags":["C++"],"title":"Binary tree","uri":"/2020-02-08-cs-binarytree/"},{"categories":["Algorithm and data structure"],"content":"Some data structures to keep in my mind. BinaryHeap: Complete binary tree MaxHeap: Parent \u003e Both Children IndexMaxHeap MinHeap: Parent \u003c Both Children IndexMinHeap Priority queue (MaxHeap) BinarySearchTree Not always complete binary tree Value: leftChild \u003c Parent \u003c rightChild DenseGraph SparseGraph Code snippets take from Play with Algorithm ","date":"2020-02-08","objectID":"/2020-02-08-cs-binarytree/:0:1","tags":["C++"],"title":"Binary tree","uri":"/2020-02-08-cs-binarytree/"},{"categories":["Algorithm and data structure"],"content":"1. MaxHeap #include \u003calgorithm\u003e#include \u003ccassert\u003e using namespace std; template\u003ctypename Item\u003e class MaxHeap{ private: Item *data; int count; int capacity; void shiftUp(int k){ while( k \u003e 1 \u0026\u0026 data[k/2] \u003c data[k] ){ swap( data[k/2], data[k] ); k /= 2; } } void shiftDown(int k){ while( 2*k \u003c= count ){ int j = 2*k; // which child is larger, then swap if( j+1 \u003c= count \u0026\u0026 data[j+1] \u003e data[j] ) j ++; if( data[k] \u003e= data[j] ) break; swap( data[k] , data[j] ); k = j; } } public: MaxHeap(int capacity){ data = new Item[capacity+1]; count = 0; this-\u003ecapacity = capacity; } MaxHeap(Item arr[], int n){ data = new Item[n+1]; capacity = n; for( int i = 0 ; i \u003c n ; i ++ ) data[i+1] = arr[i]; count = n; for( int i = count/2 ; i \u003e= 1 ; i -- ) shiftDown(i); } ~MaxHeap(){ delete[] data; } int size(){ return count; } bool isEmpty(){ return count == 0; } void insert(Item item){ assert( count + 1 \u003c= capacity ); data[count+1] = item; shiftUp(count+1); count ++; } Item extractMax(){ assert( count \u003e 0 ); Item ret = data[1]; swap( data[1] , data[count] ); count --; shiftDown(1); return ret; } Item getMax(){ assert( count \u003e 0 ); return data[1]; } }; ","date":"2020-02-08","objectID":"/2020-02-08-cs-binarytree/:0:2","tags":["C++"],"title":"Binary tree","uri":"/2020-02-08-cs-binarytree/"},{"categories":["Algorithm and data structure"],"content":"2. IndexMaxHeap Need 3 vector: data, indexes, reverse Code #include \u003calgorithm\u003e#include \u003ccassert\u003e using namespace std; template\u003ctypename Item\u003e class IndexMaxHeap{ private: Item *data; int *indexes; int *reverse; int count; int capacity; void shiftUp( int k ){ while( k \u003e 1 \u0026\u0026 data[indexes[k/2]] \u003c data[indexes[k]] ){ swap( indexes[k/2] , indexes[k] ); reverse[indexes[k/2]] = k/2; reverse[indexes[k]] = k; k /= 2; } } void shiftDown( int k ){ while( 2*k \u003c= count ){ int j = 2*k; if( j + 1 \u003c= count \u0026\u0026 data[indexes[j+1]] \u003e data[indexes[j]] ) j += 1; if( data[indexes[k]] \u003e= data[indexes[j]] ) break; swap( indexes[k] , indexes[j] ); reverse[indexes[k]] = k; reverse[indexes[j]] = j; k = j; } } public: IndexMaxHeap(int capacity){ data = new Item[capacity+1]; indexes = new int[capacity+1]; reverse = new int[capacity+1]; for( int i = 0 ; i \u003c= capacity ; i ++ ) reverse[i] = 0; count = 0; this-\u003ecapacity = capacity; } ~IndexMaxHeap(){ delete[] data; delete[] indexes; delete[] reverse; } int size(){ return count; } bool isEmpty(){ return count == 0; } // ‰º†ÂÖ•ÁöÑiÂØπÁî®Êà∑ËÄåË®Ä,ÊòØ‰ªé0Á¥¢ÂºïÁöÑ void insert(int i, Item item){ assert( count + 1 \u003c= capacity ); assert( i + 1 \u003e= 1 \u0026\u0026 i + 1 \u003c= capacity ); i += 1; data[i] = item; indexes[count+1] = i; reverse[i] = count+1; count++; shiftUp(count); } Item extractMax(){ assert( count \u003e 0 ); Item ret = data[indexes[1]]; swap( indexes[1] , indexes[count] ); reverse[indexes[count]] = 0; reverse[indexes[1]] = 1; count--; shiftDown(1); return ret; } int extractMaxIndex(){ assert( count \u003e 0 ); int ret = indexes[1] - 1; swap( indexes[1] , indexes[count] ); reverse[indexes[count]] = 0; reverse[indexes[1]] = 1; count--; shiftDown(1); return ret; } Item getMax(){ assert( count \u003e 0 ); return data[indexes[1]]; } int getMaxIndex(){ assert( count \u003e 0 ); return indexes[1]-1; } bool contain( int i ){ assert( i + 1 \u003e= 1 \u0026\u0026 i + 1 \u003c= capacity ); return reverse[i+1] != 0; } Item getItem( int i ){ assert( contain(i) ); return data[i+1]; } void change( int i , Item newItem ){ assert( contain(i) ); i += 1; data[i] = newItem; // ÊâæÂà∞indexes[j] = i, jË°®Á§∫data[i]Âú®Â†Ü‰∏≠ÁöÑ‰ΩçÁΩÆ // ‰πãÂêéshiftUp(j), ÂÜçshiftDown(j) // for( int j = 1 ; j \u003c= count ; j ++ ) // if( indexes[j] == i ){ // shiftUp(j); // shiftDown(j); // return; // } int j = reverse[i]; shiftUp( j ); shiftDown( j ); } // test reverse index bool testReverseIndex(){ int *copyIndexes = new int[count+1]; int *copyReverseIndexes = new int[count+1]; for( int i = 0 ; i \u003c= count ; i ++ ){ copyIndexes[i] = indexes[i]; copyReverseIndexes[i] = reverse[i]; } copyIndexes[0] = copyReverseIndexes[0] = 0; std::sort(copyIndexes, copyIndexes + count + 1); std::sort(copyReverseIndexes, copyReverseIndexes + count + 1); bool res = true; for( int i = 1 ; i \u003c= count ; i ++ ) if( copyIndexes[i-1] + 1 != copyIndexes[i] || copyReverseIndexes[i-1] + 1 != copyReverseIndexes[i] ) res = res || false; delete[] copyIndexes; delete[] copyReverseIndexes; if( !res ){ cout\u003c\u003c\"Error 1\"\u003c\u003cendl; return res; } for( int i = 1 ; i \u003c= count ; i ++ ) if( reverse[ indexes[i] ] != i ){ cout\u003c\u003c\"Error 2\"\u003c\u003cendl; return false; } return true; } }; ","date":"2020-02-08","objectID":"/2020-02-08-cs-binarytree/:0:3","tags":["C++"],"title":"Binary tree","uri":"/2020-02-08-cs-binarytree/"},{"categories":["Algorithm and data structure"],"content":"3. BinarySearchTree #include \u003ciostream\u003e#include \u003cqueue\u003e#include \u003ccassert\u003e using namespace std; template \u003ctypename Key, typename Value\u003e class BST{ private: struct Node{ Key key; Value value; Node *left; Node *right; Node(Key key, Value value){ this-\u003ekey = key; this-\u003evalue = value; this-\u003eleft = this-\u003eright = NULL; } Node(Node *node){ this-\u003ekey = node-\u003ekey; this-\u003evalue = node-\u003evalue; this-\u003eleft = node-\u003eleft; this-\u003eright = node-\u003eright; } }; Node *root; int count; public: BST(){ root = NULL; count = 0; } ~BST(){ destroy( root ); } int size(){ return count; } bool isEmpty(){ return count == 0; } void insert(Key key, Value value){ root = insert(root, key, value); } bool contain(Key key){ return contain(root, key); } Value* search(Key key){ return search( root , key ); } // ÂâçÂ∫èÈÅçÂéÜ void preOrder(){ preOrder(root); } // ‰∏≠Â∫èÈÅçÂéÜ void inOrder(){ inOrder(root); } // ÂêéÂ∫èÈÅçÂéÜ void postOrder(){ postOrder(root); } // Â±ÇÂ∫èÈÅçÂéÜ void levelOrder(){ queue\u003cNode*\u003e q; q.push(root); while( !q.empty() ){ Node *node = q.front(); q.pop(); cout\u003c\u003cnode-\u003ekey\u003c\u003cendl; if( node-\u003eleft ) q.push( node-\u003eleft ); if( node-\u003eright ) q.push( node-\u003eright ); } } // ÂØªÊâæÊúÄÂ∞èÁöÑÈîÆÂÄº Key minimum(){ assert( count != 0 ); Node* minNode = minimum( root ); return minNode-\u003ekey; } // ÂØªÊâæÊúÄÂ§ßÁöÑÈîÆÂÄº Key maximum(){ assert( count != 0 ); Node* maxNode = maximum(root); return maxNode-\u003ekey; } // ‰ªé‰∫åÂèâÊ†ë‰∏≠Âà†Èô§ÊúÄÂ∞èÂÄºÊâÄÂú®ËäÇÁÇπ void removeMin(){ if( root ) root = removeMin( root ); } // ‰ªé‰∫åÂèâÊ†ë‰∏≠Âà†Èô§ÊúÄÂ§ßÂÄºÊâÄÂú®ËäÇÁÇπ void removeMax(){ if( root ) root = removeMax( root ); } // ‰ªé‰∫åÂèâÊ†ë‰∏≠Âà†Èô§ÈîÆÂÄº‰∏∫keyÁöÑËäÇÁÇπ void remove(Key key){ root = remove(root, key); } private: // Âêë‰ª•node‰∏∫Ê†πÁöÑ‰∫åÂèâÊêúÁ¥¢Ê†ë‰∏≠,ÊèíÂÖ•ËäÇÁÇπ(key, value) // ËøîÂõûÊèíÂÖ•Êñ∞ËäÇÁÇπÂêéÁöÑ‰∫åÂèâÊêúÁ¥¢Ê†ëÁöÑÊ†π Node* insert(Node *node, Key key, Value value){ if( node == NULL ){ count ++; return new Node(key, value); } if( key == node-\u003ekey ) node-\u003evalue = value; else if( key \u003c node-\u003ekey ) node-\u003eleft = insert( node-\u003eleft , key, value); else // key \u003e node-\u003ekey node-\u003eright = insert( node-\u003eright, key, value); return node; } // Êü•Áúã‰ª•node‰∏∫Ê†πÁöÑ‰∫åÂèâÊêúÁ¥¢Ê†ë‰∏≠ÊòØÂê¶ÂåÖÂê´ÈîÆÂÄº‰∏∫keyÁöÑËäÇÁÇπ bool contain(Node* node, Key key){ if( node == NULL ) return false; if( key == node-\u003ekey ) return true; else if( key \u003c node-\u003ekey ) return contain( node-\u003eleft , key ); else // key \u003e node-\u003ekey return contain( node-\u003eright , key ); } // Âú®‰ª•node‰∏∫Ê†πÁöÑ‰∫åÂèâÊêúÁ¥¢Ê†ë‰∏≠Êü•ÊâækeyÊâÄÂØπÂ∫îÁöÑvalue Value* search(Node* node, Key key){ if( node == NULL ) return NULL; if( key == node-\u003ekey ) return \u0026(node-\u003evalue); else if( key \u003c node-\u003ekey ) return search( node-\u003eleft , key ); else // key \u003e node-\u003ekey return search( node-\u003eright, key ); } // ÂØπ‰ª•node‰∏∫Ê†πÁöÑ‰∫åÂèâÊêúÁ¥¢Ê†ëËøõË°åÂâçÂ∫èÈÅçÂéÜ void preOrder(Node* node){ if( node != NULL ){ cout\u003c\u003cnode-\u003ekey\u003c\u003cendl; preOrder(node-\u003eleft); preOrder(node-\u003eright); } } // ÂØπ‰ª•node‰∏∫Ê†πÁöÑ‰∫åÂèâÊêúÁ¥¢Ê†ëËøõË°å‰∏≠Â∫èÈÅçÂéÜ void inOrder(Node* node){ if( node != NULL ){ inOrder(node-\u003eleft); cout\u003c\u003cnode-\u003ekey\u003c\u003cendl; inOrder(node-\u003eright); } } // ÂØπ‰ª•node‰∏∫Ê†πÁöÑ‰∫åÂèâÊêúÁ¥¢Ê†ëËøõË°åÂêéÂ∫èÈÅçÂéÜ void postOrder(Node* node){ if( node != NULL ){ postOrder(node-\u003eleft); postOrder(node-\u003eright); cout\u003c\u003cnode-\u003ekey\u003c\u003cendl; } } void destroy(Node* node){ if( node != NULL ){ destroy( node-\u003eleft ); destroy( node-\u003eright ); delete node; count --; } } // Âú®‰ª•node‰∏∫Ê†πÁöÑ‰∫åÂèâÊêúÁ¥¢Ê†ë‰∏≠,ËøîÂõûÊúÄÂ∞èÈîÆÂÄºÁöÑËäÇÁÇπ Node* minimum(Node* node){ if( node-\u003eleft == NULL ) return node; return minimum(node-\u003eleft); } // Âú®‰ª•node‰∏∫Ê†πÁöÑ‰∫åÂèâÊêúÁ¥¢Ê†ë‰∏≠,ËøîÂõûÊúÄÂ§ßÈîÆÂÄºÁöÑËäÇÁÇπ Node* maximum(Node* node){ if( node-\u003eright == NULL ) return node; return maximum(node-\u003eright); } // Âà†Èô§Êéâ‰ª•node‰∏∫Ê†πÁöÑ‰∫åÂàÜÊêúÁ¥¢Ê†ë‰∏≠ÁöÑÊúÄÂ∞èËäÇÁÇπ // ËøîÂõûÂà†Èô§ËäÇÁÇπÂêéÊñ∞ÁöÑ‰∫åÂàÜÊêúÁ¥¢Ê†ëÁöÑÊ†π Node* removeMin(Node* node){ if( node-\u003eleft == NULL ){ Node* rightNode = node-\u003eright; delete node; count --; return rightNode; } node-\u003eleft = removeMin(node-\u003eleft); return node; } // Âà†Èô§Êéâ‰ª•node‰∏∫Ê†πÁöÑ‰∫åÂàÜÊêúÁ¥¢Ê†ë‰∏≠ÁöÑÊúÄÂ§ßËäÇÁÇπ // ËøîÂõûÂà†Èô§ËäÇÁÇπÂêéÊñ∞ÁöÑ‰∫åÂàÜÊêúÁ¥¢Ê†ëÁöÑÊ†π Node* removeMax(Node* node){ if( node-\u003eright == NULL ){ Node* leftNode = node-\u003eleft; delete node; count --; return leftNode; } node-\u003eright = removeMax(node-\u003eright); return node; } // Âà†Èô§Êéâ‰ª•node‰∏∫Ê†πÁöÑ‰∫åÂàÜÊêúÁ¥¢Ê†ë‰∏≠ÈîÆÂÄº‰∏∫keyÁöÑËäÇÁÇπ // ËøîÂõûÂà†Èô§ËäÇÁÇπÂêéÊñ∞ÁöÑ‰∫åÂàÜÊêúÁ¥¢Ê†ëÁöÑÊ†π Node* remove(Node* node, Key key){ if( node == NULL ) return NULL; if( key \u003c node-\u003ekey ){ node-\u003eleft = remove( node-\u003eleft , key ); return node; } else if( key \u003e node-\u003ekey ){ node-\u003eright = remove( node-\u003eright, key ); return node; } else{ // ","date":"2020-02-08","objectID":"/2020-02-08-cs-binarytree/:0:4","tags":["C++"],"title":"Binary tree","uri":"/2020-02-08-cs-binarytree/"},{"categories":["Algorithm and data structure"],"content":"Sort algorithms Code snippets are taken from Play with Algorithm ","date":"2020-02-08","objectID":"/2020-02-08-cs-sort/:0:0","tags":["C++"],"title":"Sort","uri":"/2020-02-08-cs-sort/"},{"categories":["Algorithm and data structure"],"content":"Some algorithm to keep in my mind. selectionSort insertionSort mergeSort quickSort two way three way heapSort ","date":"2020-02-08","objectID":"/2020-02-08-cs-sort/:0:1","tags":["C++"],"title":"Sort","uri":"/2020-02-08-cs-sort/"},{"categories":["Algorithm and data structure"],"content":"1. insertionSort #include \u003ciostream\u003e#include \u003calgorithm\u003e using namespace std; template\u003ctypename T\u003e void insertionSort(T arr[], int n){ for( int i = 1 ; i \u003c n ; i ++ ) { T e = arr[i]; int j; for (j = i; j \u003e 0 \u0026\u0026 arr[j-1] \u003e e; j--) arr[j] = arr[j-1]; arr[j] = e; } return; } // ÂØπarr[l...r]ËåÉÂõ¥ÁöÑÊï∞ÁªÑËøõË°åÊèíÂÖ•ÊéíÂ∫è template\u003ctypename T\u003e void insertionSort(T arr[], int l, int r){ for( int i = l+1 ; i \u003c= r ; i ++ ) { T e = arr[i]; int j; for (j = i; j \u003e l \u0026\u0026 arr[j-1] \u003e e; j--) arr[j] = arr[j-1]; arr[j] = e; } return; } ","date":"2020-02-08","objectID":"/2020-02-08-cs-sort/:0:2","tags":["C++"],"title":"Sort","uri":"/2020-02-08-cs-sort/"},{"categories":["Algorithm and data structure"],"content":"2. mergeSort #include \u003ciostream\u003e#include \u003calgorithm\u003e#include \"InsertionSort.h\" using namespace std; template\u003ctypename T\u003e void __merge(T arr[], int l, int mid, int r){ T aux[r-l+1]; for( int i = l ; i \u003c= r; i ++ ) aux[i-l] = arr[i]; int i = l, j = mid+1; for( int k = l ; k \u003c= r; k ++ ){ if( i \u003e mid ) { arr[k] = aux[j-l]; j ++;} else if( j \u003e r ){ arr[k] = aux[i-l]; i ++;} else if( aux[i-l] \u003c aux[j-l] ){ arr[k] = aux[i-l]; i ++;} else { arr[k] = aux[j-l]; j ++;} } } template\u003ctypename T\u003e void __mergeSort(T arr[], int l, int r){ if( r - l \u003c= 15 ){ insertionSort(arr, l, r); return; } int mid = (l+r)/2; __mergeSort(arr, l, mid); __mergeSort(arr, mid+1, r); if( arr[mid] \u003e arr[mid+1] ) __merge(arr, l, mid, r); } template\u003ctypename T\u003e void mergeSort(T arr[], int n){ __mergeSort( arr , 0 , n-1 ); } template \u003ctypename T\u003e void mergeSortBU(T arr[], int n){ for( int i = 0 ; i \u003c n ; i += 16 ) insertionSort(arr,i,min(i+15,n-1)); for( int sz = 16; sz \u003c= n ; sz += sz ) for( int i = 0 ; i \u003c n - sz ; i += sz+sz ) if( arr[i+sz-1] \u003e arr[i+sz] ) __merge(arr, i, i+sz-1, min(i+sz+sz-1,n-1) ); } ","date":"2020-02-08","objectID":"/2020-02-08-cs-sort/:0:3","tags":["C++"],"title":"Sort","uri":"/2020-02-08-cs-sort/"},{"categories":["Algorithm and data structure"],"content":"3. qucikSort #include \u003ciostream\u003e#include \u003cctime\u003e#include \u003calgorithm\u003e#include \"InsertionSort.h\" using namespace std; template \u003ctypename T\u003e int _partition(T arr[], int l, int r){ // select a rand index in arr, and swap swap( arr[l] , arr[rand()%(r-l+1)+l] ); T v = arr[l]; int i = l+1, j = r; while( true ){ while( i \u003c= r \u0026\u0026 arr[i] \u003c v ) i ++; while( j \u003e= l+1 \u0026\u0026 arr[j] \u003e v ) j --; if( i \u003e j ) break; swap( arr[i] , arr[j] ); i ++; j --; } swap( arr[l] , arr[j]); return j; } template \u003ctypename T\u003e void _quickSort(T arr[], int l, int r){ if( r - l \u003c= 15 ){ insertionSort(arr,l,r); return; } int p = _partition(arr, l, r); _quickSort(arr, l, p-1 ); _quickSort(arr, p+1, r); } template \u003ctypename T\u003e void quickSort(T arr[], int n){ srand(time(NULL)); _quickSort(arr, 0, n-1); } template \u003ctypename T\u003e void __quickSort3Ways(T arr[], int l, int r){ if( r - l \u003c= 15 ){ insertionSort(arr,l,r); return; } swap( arr[l], arr[rand()%(r-l+1)+l ] ); T v = arr[l]; int lt = l; // arr[l+1...lt] \u003c v int gt = r + 1; // arr[gt...r] \u003e v int i = l+1; // arr[lt+1...i) == v while( i \u003c gt ){ if( arr[i] \u003c v ){ swap( arr[i], arr[lt+1]); i ++; lt ++; } else if( arr[i] \u003e v ){ swap( arr[i], arr[gt-1]); gt --; } else{ // arr[i] == v i ++; } } swap( arr[l] , arr[lt] ); __quickSort3Ways(arr, l, lt-1); __quickSort3Ways(arr, gt, r); } template \u003ctypename T\u003e void quickSort3Ways(T arr[], int n){ srand(time(NULL)); __quickSort3Ways( arr, 0, n-1); } ","date":"2020-02-08","objectID":"/2020-02-08-cs-sort/:0:4","tags":["C++"],"title":"Sort","uri":"/2020-02-08-cs-sort/"},{"categories":["Algorithm and data structure"],"content":"4. heapSort #include \"Heap.h\" using namespace std; template\u003ctypename T\u003e void heapSort2(T arr[], int n){ MaxHeap\u003cT\u003e maxheap = MaxHeap\u003cT\u003e(arr,n); for( int i = n-1 ; i \u003e= 0 ; i-- ) arr[i] = maxheap.extractMax(); } template\u003ctypename T\u003e void heapSort1(T arr[], int n){ MaxHeap\u003cT\u003e maxheap = MaxHeap\u003cT\u003e(n); for( int i = 0 ; i \u003c n ; i ++ ) maxheap.insert(arr[i]); for( int i = n-1 ; i \u003e= 0 ; i-- ) arr[i] = maxheap.extractMax(); } ","date":"2020-02-08","objectID":"/2020-02-08-cs-sort/:0:5","tags":["C++"],"title":"Sort","uri":"/2020-02-08-cs-sort/"},{"categories":["Algorithm and data structure"],"content":"What on earth is Big O? Time complexity and space complexity ","date":"2020-02-01","objectID":"/2020-02-01-cs-algo/:0:1","tags":["C++"],"title":"What is Big O","uri":"/2020-02-01-cs-algo/"},{"categories":["Algorithm and data structure"],"content":"Time complexity O(f(n)): number of commands need to execute. proportional to f(n). Ë°®Á§∫ËøêË°åÁÆóÊ≥ïÊâÄÈúÄË¶ÅÊâßË°åÁöÑÊåá‰ª§Êï∞ÔºåÂíåf(n)ÊàêÊ≠£„ÄÇ ‰∏•Ê†ºÊù•ËÆ≤ÔºåO(f(n))Ë°®Á§∫ÁÆóÊ≥ïÊâßË°åÁöÑ‰∏äÁïå„ÄÇ‰∏öÁïåÈªòËÆ§‰∏∫ÁÆóÊ≥ïÊâßË°åÁöÑÊúÄ‰Ωé‰∏äÁïå(ÊúÄÂùèÊÉÖÂÜµÔºâ„ÄÇ n represents the data scale Êï∞ÊçÆËßÑÊ®° when n is a large number, the constant is usually ignored. algorithm n of cmd Binary reserach $O(logn)$ $a*logn$ Max/min in an array $O(n)$ b*n merge sort $O(nlogn)$ $c*nlogn$ select sort $O(n^2)$ $d*n^2$ quick sort $O(nlogn)$ e*nlogn adjacent graph $O(V+E)$ Lazy Prim $O(ElogE)$ Prim $O(ElogV)$ Kruskal $O(ElogE)$ Dijkstra $O(ElogV)$ Bellman-Ford $O(EV)$ minimum span tree Shortest path tree (Single source shortest path) ","date":"2020-02-01","objectID":"/2020-02-01-cs-algo/:0:2","tags":["C++"],"title":"What is Big O","uri":"/2020-02-01-cs-algo/"},{"categories":["Algorithm and data structure"],"content":"Space complexity cmd complexity new an array $O(n)$ new 2d array $O(n^2)$ new an constant space $O(1)$ recursive function: the depth (n) of a recursive function, the extra space need $O(n)$. ","date":"2020-02-01","objectID":"/2020-02-01-cs-algo/:0:3","tags":["C++"],"title":"What is Big O","uri":"/2020-02-01-cs-algo/"},{"categories":["Algorithm and data structure"],"content":"Make sense of n If you want to solve the problem in 1 second, then an algorithm of complexity cmds n $O(n^2)$ could exec cmd n = $10^4$ $O(n)$ could exec cmd n = $10^8$ $O(nlogn)$ could exec cmd n = $10^7$ ","date":"2020-02-01","objectID":"/2020-02-01-cs-algo/:0:4","tags":["C++"],"title":"What is Big O","uri":"/2020-02-01-cs-algo/"},{"categories":["Algorithm and data structure"],"content":"Example binarySearch find from n element find from $n/2$ element find from $n/4$ element ... find from 1 That‚Äôs, need how many steps of search when n = 1? $log_{2}n = O(logn)$. int2string. Set num \u003e 0 string int2string(int num) { string s=\"\"; while(num) { s += '0' + num%10; num /= 10; } reverse(s); // O(n) return s; } That is, how many ‚Äú/10‚Äù steps when num = 0? $log_{10}n = O(logn)$. Case: two nested for loop, not always $O(n^2)$ void hello(int n){ for (int sz =1; sz \u003c n; sz ++ sz) // logn here for( int i=1; i \u003c n;; i++) //n cout\u003c\u003c\"hello, complex\" \u003c\u003cendl; } So, should be $O(nlogn)$ isPrime: $O(\\sqrt{n})$ // set n \u003e 1 bool isPrime(int n){ for( int x =2; x*x \u003c= n; x++){ if( n%x == 0) return false; return true; } } recursive function single recursive function call int binarySearch(int arr[], int l, int r, int target) { if (l\u003er) return -1; int mid = l +(r-l)/2; if (arr[mid] == target) return mid; else if (arr[mid] \u003e target) return binarySearch(arr, ;, mid-1, target); else return binarySearch(arr, mid+1, r, target); } each step need O(1), so overall complexity depend on recursive exec depth. That is, if each function call needs time T, then time complexity: O(T*depth) -\u003e O(n). Another example: recursion depth logn, them time complexity O(logn). double pow( double x, int n){ assert(n \u003e=0); if (n==0) return 1.0; double t = pow(x, n/2); if( n%2) return x*t*t; return t*t; } multi recursive exec how many exec step? int f(int n) { assert(n \u003e=0); if(n == 0) return 1; return f(n-1) + f(n); } that‚Äôs, count how many nodes on a full binary tree. $2^{n+1} -1 = O(2^n)$ how to think about this? void mergeSort(int arr[]. int l. int r){ if (l \u003e=r) return; int mid = (l+r) /2; mergeSort(arr, l, mid); mergeSort(arr, mid+1, r); merge(arr, l, mid, r); } For binary tree, complexity for each level O(n), while tree depth O(logn). Overall, O(nlogn) Amortized time i.e. dynamic vector/stack/deque template\u003cT\u003e class MyVector{ private: T* data; int capacity; int size; //O(n) void resize(int newCapacity){ assert(newCapacity \u003e= size); T* newData = new T[newCapacity]; for(int i = 0; i \u003c size; i++ ){ newData[i] = data[i]; } delete[] data; data = newData; capacity = newCapacity; } public: MyVector() { data = new T[10]; capacity = 10; size = 0; } ~MyVector() { delete[] data; } // Average: O(1) void push_back(T e){ //assert(size \u003c capacity) if (size == capacity) resize (2 *capacity); data[size++] = e; } // Average O(1) T pop_back(){ assert (size \u003e0); T ret = data[size-1]; size --; // note the denominator here. To advoid ossilation of space complexity if(size == capacity / 4) resize(capacity /2); return ret; } }; ","date":"2020-02-01","objectID":"/2020-02-01-cs-algo/:0:5","tags":["C++"],"title":"What is Big O","uri":"/2020-02-01-cs-algo/"},{"categories":["Machine Learning"],"content":"A breif review over the foundations of statistical inference ","date":"2020-01-30","objectID":"/2020-01-30-ml-bayes/:0:0","tags":["Naive Bayes","Bayesian Inference"],"title":"Statistical Modeling and Inference","uri":"/2020-01-30-ml-bayes/"},{"categories":["Machine Learning"],"content":"Statistical Models and Inference statistical inference: a formal approach to characterizing a random phenomenon using observations, either by providing a description of a past phenomenon or by giving some predictions about future phenomenon of similar nature. ","date":"2020-01-30","objectID":"/2020-01-30-ml-bayes/:1:0","tags":["Naive Bayes","Bayesian Inference"],"title":"Statistical Modeling and Inference","uri":"/2020-01-30-ml-bayes/"},{"categories":["Machine Learning"],"content":"1. Statistical Models The first step in statistical inference is to specify a statistical model, under some simplifying assumptions (i.e. independence assumptions). Hierarchical models: the probability distribution of one parameter is dependent on the values of other hierachical paramters (i.e. conditional independent). Steps: Set assumptions (i.e. independent), parameter and model. Make explicit assumptions on the probability distributions. focus on parametric modeling, because of limited data nonparametric not consider here, used for hypothesis testing or when sample size is very large. Once the model is specified, then choose a method of inference, as well as an algorithm to obtain estimates. Most commonly use: Maximum likelihood inference Bayesian inference ","date":"2020-01-30","objectID":"/2020-01-30-ml-bayes/:1:1","tags":["Naive Bayes","Bayesian Inference"],"title":"Statistical Modeling and Inference","uri":"/2020-01-30-ml-bayes/"},{"categories":["Machine Learning"],"content":"2. Maximum likelihood inference Quantifying confidence: the Fisher Information Matrix Newton‚Äôs algorithm Approximate Techniques Monte Carlo Sampling for intractable likelihoods Composite likelihood ","date":"2020-01-30","objectID":"/2020-01-30-ml-bayes/:1:2","tags":["Naive Bayes","Bayesian Inference"],"title":"Statistical Modeling and Inference","uri":"/2020-01-30-ml-bayes/"},{"categories":["Machine Learning"],"content":"3. Bayesian Inference A statistical model describes the uncertainty about how the data was produced. The ultimate aim for statistical inference is to obtain information about the unknown parameter $\\theta$ given the data $\\mathcal{D}$. Frequentist: $\\theta$ is fixed but unknown quantity. Bayesian: use a fully probabilistic model and treat $\\theta$ as a random quantity. To do so, chose an appropriate prior distribution $\\mathbb{P}(\\theta)$, which reflects the knowledge (i.e. uncertainty) about $\\theta$ prior to the experiment the goal is to update the knowledge given the information contained in the data $\\mathcal{D}$. the updated knowledge (i.e. reduced uncertainty) is encapsulated in the posterior distribution $\\mathbb{P}(\\theta \\vert \\mathcal{D})$, which is calculated via Bayes‚Äôtheorem. $$ \\mathbb{P}(\\boldsymbol{\\theta} | \\mathcal{D})=\\frac{\\mathbb{P}(\\mathcal{D} | \\boldsymbol{\\theta}) \\mathbb{P}(\\boldsymbol{\\theta})}{\\mathbb{P}(\\mathcal{D})} $$ The bayesian paradigm boils down to the slogan: posterior $\\propto$ likelihood $\\times$ prior 3.1 Choice of prior distributions Conjugate priors the prior and the posterior lie in the same class of distributions. often chosen, because it leads to a well-known form of the posterior, which simplifies the calculations choose a prior that contains as little information about the parameter as possible at first choice would, of course, be a locally uniform prior. Under a uniform prior we have $\\mathbb{P}(\\boldsymbol{\\theta} \\vert \\boldsymbol{D}) \\propto \\mathcal{L}(\\boldsymbol{\\theta})$. Jeffrey‚Äôs prior, but often hard to come by 3.2 Bayesian point estimates and confidence intervals Bayesian point estimates: the posterior mean, mode and median $$ \\hat{\\theta}=\\mathbb{E}[\\theta | D]=\\int \\theta \\mathbb{P}(\\theta | \\mathcal{D}) \\mathrm{d} \\theta $$ confidence: highest posterior density (HPD) region for a threshold value $\\pi$, the region $\\mathcal{C}_{\\alpha}={\\theta: \\mathbb{P}(\\theta \\vert \\mathcal{D})\u003e\\pi}$, we get $$ \\int_{C_{\\alpha}} \\mathbb{P}(\\theta | \\mathcal{D}) \\mathrm{d} \\theta=1-\\alpha $$ This region $\\mathcal{C}_{\\alpha}$ is the HPD region. 3.3 Markov Chain Monte Carlo A common challenge in Bayesian inference is that the integral $$ \\mathbb{P}(D)=\\int \\mathbb{P}(D | \\theta) \\mathbb{P}(\\theta) d_{\\theta} $$ can‚Äôt be solved analytically. to be continued‚Ä¶ 3.4 Empirical Bayes for Latent Variable Problems The first step is to infer point estimates for the parameters at higher levels by integrating out those at lower levels, and the infer posterior distributions for lower level parameters while setting those at a higher level to their point estimate. 3.5 Approximate Bayesian Computation Approximate Bayesian computation (ABC) is a class of simulation-based techniques to conduct Bayesian inference under models with intractable likelihoods 3.6 Model selection how to compare the several candidate models explaining the data $\\mathcal{D}$? the most commonly used methods: likelihood ratio statistic model posterior probabilities Bayes factors others: cross-validation, Akaike‚Äôs information criterion (AIC), Bayesian information criterion (BIC) ","date":"2020-01-30","objectID":"/2020-01-30-ml-bayes/:1:3","tags":["Naive Bayes","Bayesian Inference"],"title":"Statistical Modeling and Inference","uri":"/2020-01-30-ml-bayes/"},{"categories":["Machine Learning"],"content":"4. Naive Bayes and Bayesian estimation Naive Bayes and Bayesian estimation are two different concepts! Naive Bayes is a statistical learning method. For a give training set, learn the join probability distribution of $P(X,Y)$. Based on this model, for a given input $x$, output a $y$ with maximal posterior probability (Bayes theorem). Set prior prob distribution: $$ P\\left(Y=c_{k}\\right), \\quad k=1,2, \\cdots, K $$ Conditional prob distribution: $$ P\\left(X=x | Y=c_{k}\\right)=P\\left(X^{(1)}=x^{(1)}, \\cdots, X^{(n)}=x^{(n)} | Y=c_{k}\\right), \\quad k=1,2, \\cdots, K $$ Naive Bayes make a strong assumption that conditional prob distribution are all conditional independent, which is: $$ \\begin{aligned} P\\left(X=x | Y=c_{k}\\right) \u0026=P\\left(X^{(1)}=x^{(1)}, \\cdots, X^{(n)}=x^{(n)} | Y=c_{k}\\right) \\cr \u0026=\\prod_{j=1}^{n} P\\left(X^{(j)}=x^{(j)} | Y=c_{k}\\right) \\end{aligned} $$ then, posterior prob is: $$ P\\left(Y=c_{k} | X=x\\right)=\\frac{P\\left(X=x | Y=c_{k}\\right) P\\left(Y=c_{k}\\right)}{\\sum_{k} P\\left(X=x | Y=c_{k}\\right) P\\left(Y=c_{k}\\right)} $$ If conditional probability of each input variable is not independent, then model become Baysian Network! Naive Bayes Classifier is: $$ y=f(x)=\\arg \\max_{c_{k}} \\frac{P\\left(Y=c_{k}\\right) \\prod_{j} P\\left(X^{(j)}=x^{(j)} | Y=c_{k}\\right) }{\\sum_{k} P\\left(Y=c_{k}\\right) \\prod_{j} P\\left(X^{(j)}=x^{(j)} | Y=c_{k}\\right)} $$ and it‚Äôs short form: $$ y=f(x)= \\arg \\max_{c_{k}} \\overbrace{P\\left(Y=c_{k}\\right)}^{\\text{prior}} \\overbrace{ \\prod_{j} P\\left(X^{(j)}=x^{(j)} | Y=c_{k}\\right)}^{\\text{likelihood}} $$ In sentiment analysis of NLP, the naive bayes classifiter make two assumptions. bag of words assumption: position doesn‚Äôt matter. Each feature only encode word identity not position. naive bayes assumption: conditional independence. ","date":"2020-01-30","objectID":"/2020-01-30-ml-bayes/:1:4","tags":["Naive Bayes","Bayesian Inference"],"title":"Statistical Modeling and Inference","uri":"/2020-01-30-ml-bayes/"},{"categories":["Machine Learning"],"content":"Multi-label classification, tasks commonly be seen on health record data (multi symptoms). Loss function design: Multi binary cross-entropy each class has a binary output Label smoothing, another regularization technique It‚Äôs designed to make the model a little bit less certain of it‚Äôs decision by changing a little bit its target: instead of wanting to predict 1 for the correct class and 0 for all the others, we ask it to predict 1-Œµ for the correct class and Œµ for all the others, with Œµ a (small) positive number and N the number of classes. This can be written as: $$ \\text {loss}=(1-\\varepsilon) c e(i)+\\varepsilon \\sum c e(j) / N $$ where ce(x) is cross-entropy of x (i.e. ‚àílog(px)), and i is the correct class. finally, for multi-label loss function: $$ (1-\\epsilon) \\sum_{i}\\left(-\\frac{\\log p_{i}}{n}\\right)+\\frac{\\epsilon}{N} \\sum\\left(-\\log p_{i}\\right) $$ See the fastai implementation here: LabelSmoothingCrossEntropy about line 285: class LabelSmoothingCrossEntropy(Module): y_int = True def __init__(self, eps:float=0.1, reduction='mean'): self.eps,self.reduction = eps,reduction def forward(self, output, target): c = output.size()[-1] log_preds = F.log_softmax(output, dim=-1) if self.reduction=='sum': loss = -log_preds.sum() else: loss = -log_preds.sum(dim=-1) if self.reduction=='mean': loss = loss.mean() return loss*self.eps/c + (1-self.eps) * F.nll_loss(log_preds, target.long(), reduction=self.reduction) def activation(self, out): return F.softmax(out, dim=-1) def decodes(self, out): return out.argmax(dim=-1) ","date":"2020-01-29","objectID":"/2020-01-29-ml-lossfunc/:0:0","tags":["Pytorch"],"title":"Loss function for multi-label classification","uri":"/2020-01-29-ml-lossfunc/"},{"categories":["Machine Learning"],"content":"Maximum Likelihood Estimation Gaussian Mixture Model Expectation Maximization ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:0:0","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"1. Probability and likelihood likehood \u0026 maximum likehood Âú®ÈùûÊ≠£ÂºèÂú∫Âêà‰ººÁÑ∂ÔºàlikelihoodÔºâÂíåÊ¶ÇÁéáÔºàProbabilityÔºâÂá†‰πéÊòØ‰∏ÄÂØπÂêå‰πâËØçÔºå‰ΩÜÊòØÂú®ÁªüËÆ°Â≠¶‰∏≠‰ººÁÑ∂ÂíåÊ¶ÇÁéáÂç¥ÊòØ‰∏§‰∏™‰∏çÂêåÁöÑÊ¶ÇÂøµ„ÄÇ Ê¶ÇÁéá: Âú®ÁâπÂÆöÁéØÂ¢É‰∏ãÊüê‰ª∂‰∫ãÊÉÖÂèëÁîüÁöÑÂèØËÉΩÊÄßÔºå‰πüÂ∞±ÊòØÁªìÊûúÊ≤°Êúâ‰∫ßÁîü‰πãÂâç‰æùÊçÆÁéØÂ¢ÉÊâÄÂØπÂ∫îÁöÑÂèÇÊï∞Êù•È¢ÑÊµãÊüê‰ª∂‰∫ãÊÉÖÂèëÁîüÁöÑÂèØËÉΩÊÄß„ÄÇ ÊØîÂ¶ÇÊäõÁ°¨Â∏ÅÔºåÊäõ‰πãÂâçÊàë‰ª¨‰∏çÁü•ÈÅìÊúÄÂêéÊòØÂì™‰∏ÄÈù¢Êúù‰∏äÔºå‰ΩÜÊòØÊ†πÊçÆÁ°¨Â∏ÅÁöÑÊÄßË¥®Êàë‰ª¨ÂèØ‰ª•Êé®Êµã‰ªª‰Ωï‰∏ÄÈù¢Êúù‰∏äÁöÑÂèØËÉΩÊÄßÂùá‰∏∫50%ÔºåËøô‰∏™Ê¶ÇÁéáÂè™ÊúâÂú®ÊäõÁ°¨Â∏Å‰πãÂâçÊâçÊòØÊúâÊÑè‰πâÁöÑÔºåÊäõÂÆåÁ°¨Â∏ÅÂêéÁöÑÁªìÊûú‰æøÊòØÁ°ÆÂÆöÁöÑÔºõ ‰ººÁÑ∂: ÂàöÂ•ΩÁõ∏ÂèçÔºåÊòØÂú®Á°ÆÂÆöÁöÑÁªìÊûú‰∏ãÂéªÊé®Êµã‰∫ßÁîüËøô‰∏™ÁªìÊûúÁöÑÂèØËÉΩÁéØÂ¢ÉÔºàÂèÇÊï∞Ôºâ„ÄÇ ÂÅáËÆæÈöèÊú∫ÊäõÊé∑‰∏ÄÊûöÁ°¨Â∏Å1,000Ê¨°ÔºåÁªìÊûú500Ê¨°‰∫∫Â§¥Êúù‰∏äÔºå500Ê¨°Êï∞Â≠óÊúù‰∏äÔºåÈÇ£‰πà‰∏§Èù¢Êúù‰∏äÁöÑÊ¶ÇÁéáÂùá‰∏∫50%„ÄÇËøêÁî®Âá∫Áé∞ÁöÑÁªìÊûúÊù•Âà§Êñ≠Ëøô‰∏™‰∫ãÊÉÖÊú¨Ë∫´ÁöÑÊÄßË¥®ÔºàÂèÇÊï∞ÔºâÔºå‰πüÂ∞±ÊòØ‰ººÁÑ∂„ÄÇ ÂΩìÁªìÊûúÂíåÂèÇÊï∞Áõ∏‰∫íÂØπÂ∫îÔºå‰ººÁÑ∂ÂíåÊ¶ÇÁéáÂú®Êï∞ÂÄº‰∏äÁõ∏Á≠â„ÄÇ Áî® Œ∏ Ë°®Á§∫ÁéØÂ¢ÉÂØπÂ∫îÁöÑÂèÇÊï∞Ôºåx Ë°®Á§∫ÁªìÊûúÔºåÈÇ£‰πàÊ¶ÇÁéáÂèØ‰ª•Ë°®Á§∫‰∏∫Ôºö $$P(x | \\theta )$$ $p(x \\vert Œ∏)$ ÊòØÊù°‰ª∂Ê¶ÇÁéáÁöÑË°®Á§∫ÊñπÊ≥ï„ÄÇŒ∏ ÊòØÂâçÁΩÆÊù°‰ª∂ÔºåÁêÜËß£‰∏∫Âú® Œ∏ ÁöÑÂâçÊèê‰∏ãÔºå‰∫ã‰ª∂ x ÂèëÁîüÁöÑÊ¶ÇÁéáÔºåÁõ∏ÂØπÂ∫îÁöÑ‰ººÁÑ∂ÂèØ‰ª•Ë°®Á§∫‰∏∫: $$\\mathcal{L}(\\theta | x)$$ ÂèØ‰ª•ÁêÜËß£‰∏∫Â∑≤Áü•ÁªìÊûú‰∏∫ x ÔºåÂèÇÊï∞‰∏∫ Œ∏ (‰ººÁÑ∂ÂáΩÊï∞Èáå Œ∏ ÊòØÂèòÈáèÔºåËøôÈáåËØ¥ÁöÑÂèÇÊï∞ÂíåÂèòÈáèÊòØÁõ∏ÂØπ‰∏éÊ¶ÇÁéáËÄåË®ÄÁöÑ)ÂØπÂ∫îÁöÑÊ¶ÇÁéáÔºåÂç≥Ôºö $$\\mathcal{L}(\\theta | x)=P(x | \\theta)$$ ‰∏§ËÄÖÂú®Êï∞ÂÄº‰∏äÁõ∏Á≠âÔºå‰ΩÜÊòØÊÑè‰πâÂπ∂‰∏çÁõ∏Âêå, $\\mathcal{L}$ ÊòØÂÖ≥‰∫é Œ∏ ÁöÑÂáΩÊï∞ÔºåËÄå P ÂàôÊòØÂÖ≥‰∫é x ÁöÑÂáΩÊï∞„ÄÇ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:1:0","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"2. Maximum Likelihood Estimation ÂçïÈ´òÊñØÊ®°Âûã $x \\sim \\mathcal{N}(\\mu, \\Sigma)$, $x_{i} \\in \\mathcal{D}$, ÈÇ£‰πàÂØπÂèÇÊï∞ $\\mu$Âíå $\\Sigma$ ËøõË°å‰º∞ËÆ°ÔºåÂè™ÈúÄË¶ÅÊúÄÂ§ßÂåñlog-likelihoodÂáΩÊï∞Ôºö $$ \\begin{aligned} \\log p(X) \u0026=\\sum_{i=1}^{N} \\log \\mathcal{N}\\left(x_{i} | \\mu, \\Sigma\\right) \\cr \u0026=\\sum_{i=1}^{N} \\log \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{\\left(x_{i}-\\mu\\right)^{2}}{2 \\sigma^{2}}} \\cr \u0026=\\sum_{i=1}^{N} \\log \\frac{1}{\\sqrt{2 \\pi} \\sigma}+\\sum_{i=1}^{N}-\\frac{\\left(x_{i}-\\mu\\right)^{2}}{2 \\sigma^{2}} \\cr \u0026=-\\frac{N}{2} \\log 2 \\pi-\\frac{N}{2} \\log \\sigma^{2}-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{N}\\left(x_{i}-\\mu\\right)^{2} \\end{aligned} $$ Ê±ÇÂÅèÂØºÊï∞ÔºåÂæóÂà∞ÂèÇÊï∞‰º∞ËÆ°Ôºö $$ \\begin{aligned} \\frac{\\partial \\log p(X)}{\\partial \\mu} \u0026=\\frac{1}{\\sigma^{2}} \\sum_{i=1}^{N}\\left(x_{i}-\\mu\\right)=0 \\cr \u0026 \\Rightarrow \\mu=\\frac{1}{N} \\sum_{i=1}^{N} x_{i} \\cr \\frac{\\partial \\log p(X)}{\\partial \\sigma^{2}} \u0026=-\\frac{N}{2 \\sigma^{2}}+\\frac{1}{2 \\sigma^{4}} \\sum_{i=1}^{N}\\left(x_{i}-\\mu\\right)^{2}=0 \\cr \u0026 \\Rightarrow \\sigma^{2}=\\frac{1}{N} \\sum_{i=1}^{N}\\left(x_{i}-\\mu\\right)^{2} \\end{aligned} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:2:0","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"3. Gaussian Mixture Model Â¶ÇÊûúÊúâK‰∏™È´òÊñØÁ∫øÊÄßÂè†Âä†: $$ \\begin{aligned} p(x)=\u0026 \\sum_{k=1}^{K} \\pi_{k} \\mathcal{N}\\left(x | \\mu_{k}, \\Sigma_{k}\\right) \\cr \u0026 \\text { s.t. } \\sum_{k=1}^{K} \\pi_{k}=1 \\cr \u0026 0 \\leq \\pi_{k} \\leq 1 \\end{aligned} $$ ÈÇ£‰πàÂØπÊï∞‰ººÁÑ∂ÂáΩÊï∞‰∏∫ $$ \\log p(X)=\\sum_{i=1}^{N} \\log \\lbrace \\sum_{k=1}^{K} \\pi_{k} \\mathcal{N} (x_{i} | \\mu_{k}, \\Sigma_{k}) \\rbrace $$ Âõ†‰∏∫ÂØπÊï∞ÈáåÊúâÊ±ÇÂíåÔºåÂõ†Ê≠§Êó†Ê≥ïÊó†Ê≥ïÁõ¥Êé•ÈÄöËøáÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÊñπÊ≥ïËøõË°åÂèÇÊï∞‰º∞ËÆ°„ÄÇ ÂÖ∂‰∏≠ÔºåÂ¶ÇÊûú$\\pi_{k}$ÊòØÊØè‰∏™È´òÊñØÂá∫Áé∞ÁöÑÊ¶ÇÁéá$p(k)$ÔºåÂàôÈ´òÊñØÊ∑∑ÂêàÊ®°ÂûãÂàÜËß£‰∏∫‰ª•$p(k)$Ëé∑Âæó‰∏Ä‰∏™È´òÊñØÂàÜÂ∏ÉÔºåÁÑ∂ÂêéÂú®ÂàÜÂ∏É‰∏≠Ëé∑Âæó$x$ÔºåÂõ†Ê≠§$x$ËæπÁºòÊ¶ÇÁéáÂàÜÂ∏É‰∏∫Ôºö $$ p(x)=\\sum_{k=1}^{K} p(k) p(x | k) $$ ÂêéÈ™åÊ¶ÇÁéá$p(k\\vert x)$Ë°®Á§∫$x$Â±û‰∫éÊØè‰∏™È´òÊñØÁöÑÊ¶ÇÁéáÔºàÁ¶ªÊï£ÂÄºÔºâ: $$ \\begin{aligned} p(k | x) \u0026=\\frac{p(x | k) p(k)}{\\sum_{l} p(x | l) p(l)} \\cr \u0026=\\frac{\\pi_{k} \\mathcal{N}\\left(x | \\mu_{k}, \\Sigma_{k}\\right)}{\\sum_{l} \\pi_{l} \\mathcal{N}\\left(x | \\mu_{l}, \\Sigma_{l}\\right)} \\end{aligned} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:3:0","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"4. Expectation Maximization ÊÄùÊÉ≥Ôºö ÈÄöËøáÂºïÂÖ•ÈöêÂèòÈáèÔºåËøêÁî®Ëø≠‰ª£ÊñπÊ≥ïÔºåÊ±ÇËß£Ê∑∑ÂêàÈ´òÊñØÊ®°Âûã $$ \\theta^{(t+1)}=\\underset{\\theta}{\\arg \\max } \\mathcal{L}(\\theta ; X) $$ ÂºïÂÖ•ÈöêÂèòÈáèZi(Áä∂ÊÄÅiÔºâÔºå zÊúç‰ªéÂ§öÈ°πÂàÜÂ∏ÉÔºåÈÄâÊã©ziÁöÑÊ¶ÇÁéá‰∏∫p(zi),ÂàôÈ´òÊñØÊ∑∑ÂêàÊ®°Âûã‰∏∫Ôºö $$ \\begin{aligned} z_{i} \u0026 \\sim \\operatorname{Multinoimal}\\left(\\pi_{1}, \\cdots, \\pi_{k}\\right) \\cr x_{i} | z_{i} \u0026 \\sim \\mathcal{N}\\left(\\mu_{z_{i}}, \\Sigma_{z_{i}}\\right) \\end{aligned} $$ Ê≠•È™§Ôºö E-Step: Âú®Áé∞Êúâ$\\theta^{(t)}$‰∏ãÊúÄÂ§ßÂåñ‰ººÁÑ∂‰∏ãÁïå, ËÆ°ÁÆóÈöêÂèòÈáè$z$ÁöÑÊúüÊúõ$Q\\left(z_{i}\\right)=p\\left(z_{i} \\vert x_{i}, \\theta\\right)$ ‰Ωú‰∏∫ÂÖ∂‰∏ãÁïå M-Step: Âú®‰∏äÈù¢$Q(z_{i})$‰∏ãËÆ°ÁÆóÂèÇÊï∞ÂàóË°®$\\theta$Êù•ÊúÄÂ§ßÂåñ‰ººÁÑ∂ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:4:0","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"(0) ÁêÜËß£EMÁöÑÂâçÊèê ÂáπÂá∏ÂáΩÊï∞: $\\forall_{x \\in \\mathbb{R}}, f^{\\prime \\prime}(x) \\geq 0$,Âàô$f$‰∏∫Âá∏ÂáΩÊï∞„ÄÇ ÂΩì$x$‰∏∫ÂêëÈáèÔºåÂ¶ÇÊûúÂÖ∂hessianÁü©Èòµ $H$ ÊòØÂçäÊ≠£ÂÆöÁöÑ($H \\geq 0$),Âàô$f$‰∏∫Âá∏ÂáΩÊï∞ Â¶ÇÊûú$f^{\\prime \\prime}(x)\u003e0$ÊàñËÄÖ$H\u003e0$, $f$ÊòØ‰∏•Ê†ºÂá∏ÂáΩÊï∞„ÄÇ Â¶ÇÊûú$f^{\\prime \\prime}(x)\u003c0$ÊàñËÄÖ$H\u003e0$, $f$ÊòØÂáπÂáΩÊï∞„ÄÇ Jensen ‰∏çÁ≠âÂºè: Â¶ÇÊûú$f$‰∏∫Âá∏ÂáΩÊï∞, Âàô$E[f(X)] \\geq f(E [ X ])$„ÄÇÂΩì‰∏î‰ªÖÂΩì$x$ÊòØÂ∏∏Êï∞Êó∂Ôºå$E[f(x)]=f(E[ x ])$„ÄÇ Â¶ÇÊûú$f$ÊòØÂáπÂáΩÊï∞, Âàô$E[f(X)] \\leq f(E[ X ])$„ÄÇ ÂºïÂÖ•ÈöêÂèòÈáèÂêéÔºåÂèòÊç¢ÂØπÊï∞‰ººÁÑ∂ÂáΩÊï∞: $$ \\begin{aligned} \\mathcal{L}(\\theta ; X) \u0026=\\sum_{i=1}^{N} \\log p\\left(x_{i} | \\theta\\right) \\cr \u0026=\\sum_{i=1}^{N} \\log \\sum_{z_{i}} p\\left(x_{i}, z_{i} | \\theta\\right) \\cr \u0026=\\sum_{i=1}^{N} \\log \\sum_{z_{i}} Q\\left(z_{i}\\right) \\frac{p\\left(x_{i}, z_{i} | \\theta\\right)}{Q\\left(z_{i}\\right)} \\cr \u0026 \\geq \\sum_{i=1}^{N} \\sum_{z_{i}} Q\\left(z_{i}\\right) \\log \\frac{p\\left(x_{i}, z_{i} | \\theta\\right)}{Q\\left(z_{i}\\right)} \\end{aligned} $$ Êé®ÂØº: ÊääÂºè‰∏≠ÁöÑlogÂáΩÊï∞‰ΩìÁúãÊàêÊòØ‰∏Ä‰∏™Êï¥‰ΩìÔºåÁî±‰∫é$\\log (x)$ÁöÑ‰∫åÈò∂ÂØºÊï∞‰∏∫$-\\frac{1}{x^2}$, Â∞è‰∫é0Ôºå‰∏∫ÂáπÂáΩÊï∞„ÄÇÊâÄ‰ª•‰ΩøÁî®Jensen‰∏çÁ≠âÂºèÊó∂ÔºåÂ∫îÁî®Á¨¨‰∫åÊù°ÂáÜÂàôÔºö$f(E [ X ] ) \\geq E[f(x)]$„ÄÇ $$ f\\left(E_{z_{i} \\sim Q}\\left[\\frac{p\\left(x_{i}, z_{i} | \\theta\\right)}{Q\\left(z_{i}\\right)}\\right]\\right) \\geq E_{z_{i} \\sim Q}\\left[f\\left(\\frac{p\\left(x_{i}, z_{i} | \\theta\\right)}{Q\\left(z_{i}\\right)}\\right)\\right] $$ ËøôÈáåÔºå$Q\\left(z_{i}\\right)$ÊòØ$z_{i}$ÁöÑÂáΩÊï∞Ôºå ‰∏î$\\sum_{z_{i}} Q\\left(z_{i}\\right)=1$„ÄÇ Áî±Êï∞Â≠¶ÊúüÊúõ$E_{x \\sim p}[g(X)]=\\sum_{x} g(x) p(x)$Ôºå‰∏äÂºèÂèØ‰ª•ÁêÜËß£‰∏∫: $p(x)$ÂØπÂ∫î$Q\\left(z_{i}\\right)$, g(x)ÂØπÂ∫î$\\log \\frac{p\\left(x_{i}, z_{i} \\vert \\theta\\right)}{Q\\left(z_{i}\\right)}$Ë°®Á§∫$z_{i}$ÁöÑÂáΩÊï∞„ÄÇ ‰ººÁÑ∂ÂáΩÊï∞: $\\mathcal{L}(\\theta) \\geq \\mathcal{J}(z,Q)$Ôºà$z$‰∏∫ÈöêÂê´ÂèòÈáèÔºâÔºåÈÇ£‰πàÊàë‰ª¨ÂèØ‰ª•ÈÄöËøá‰∏çÊñ≠ÁöÑÊúÄÂ§ßÂåñ$\\mathcal{J}$ÁöÑ‰∏ãÁïåÔºåÊù•‰ΩøÂæó$\\mathcal{L}(\\theta)$‰∏çÊñ≠ÊèêÈ´òÔºåÊúÄÁªàËææÂà∞ÂÆÉÁöÑÊúÄÂ§ßÂÄº„ÄÇ ÊúÄÂ§ßÂåñ$\\mathcal{L}(\\theta)$ÂáΩÊï∞ÁöÑ‰∏ãÁïåÔºåÂç≥ËÆ©$g(x)$‰∏∫Â∏∏Êï∞c: $$ \\frac{p\\left(x_{i}, z_{i} | \\theta\\right)}{Q\\left(z_{i}\\right)}=c $$ Jensen‰∏çÁ≠âÂºè‰∏≠ËØ¥Âà∞ÔºåÂΩìËá™ÂèòÈáè$X=E(X)$Êó∂ÔºåÂç≥‰∏∫Â∏∏Êï∞ÁöÑÊó∂ÂÄôÔºåÁ≠âÂºèÊàêÁ´ã! ÂèòÊç¢ÂÖ¨Âºè, ÂØπÊâÄÊúâ$z$Ê±ÇÂíåÂæó: $$ \\begin{aligned} p\\left(x_{i}, z_{i} | \\theta\\right) \u0026=c \\cdot Q\\left(z_{i}\\right) \\cr \\sum_{z_{i}} p\\left(x_{i}, z_{i} | \\theta\\right) \u0026=c \\cdot \\sum_{z_{i}} Q\\left(z_{i}\\right) \\cr c \u0026=\\sum_{z_{i}} p\\left(x_{i}, z_{i} | \\theta\\right) \\end{aligned} $$ ÂÖ∂‰∏≠Ôºå$\\sum_{z_{i}} Q\\left(z_{i}\\right) = 1$, ‰πüÂæóÔºö $$ \\begin{aligned} Q\\left(z_{i}\\right) \u0026=\\frac{p\\left(x_{i}, z_{i} | \\theta\\right)}{\\sum_{z_{i}} p\\left(x_{i}, z_{i} | \\theta\\right)} \\cr \u0026=p\\left(z_{i} | x_{i}, \\theta\\right) \\end{aligned} $$ Ëá≥Ê≠§ÔºåÊàë‰ª¨Êé®Âá∫‰∫ÜÂú®Âõ∫ÂÆöÂèÇÊï∞Œ∏ÂêéÔºå‰Ωø‰∏ãÁïåÊãâÂçáÁöÑ$Q(z)$ÁöÑËÆ°ÁÆóÂÖ¨ÂºèÂ∞±ÊòØÂêéÈ™åÊ¶ÇÁéáÔºàÊù°‰ª∂Ê¶ÇÁéáÔºâÔºå‰∏ÄÂπ∂Ëß£ÂÜ≥‰∫Ü$Q(z)$Â¶Ç‰ΩïÈÄâÊã©ÁöÑÈóÆÈ¢ò„ÄÇÊ≠§Ê≠•Â∞±ÊòØEMÁÆóÊ≥ïÁöÑE-step„ÄÇ ÊâßË°åE-StepÂêé‰∏é‰∏ãÁïåÈáçÂêàÔºåÊ≠§Êó∂‰ººÁÑ∂Âèò‰∏∫Ôºö $$ \\mathcal{L}\\left(\\theta^{(t)} ; X\\right)=\\sum_{i=1}^{N} \\sum_{z_{i}} Q^{(t)}\\left(z_{i}\\right) \\log \\frac{p\\left(x_{i}, z_{i} | \\theta^{(t)}\\right)}{Q^{(t)}\\left(z_{i}\\right)} $$ ËøôÊó∂ÔºåÂØπÂÖ¨ÂºèÊ±ÇÂØº $$ \\theta^{(t+1)}=\\underset{\\theta}{\\arg \\max } \\mathcal{L}(\\theta ; X) $$ ÂæóÂà∞ $t+1$ Ê≠•ÁöÑ‰ººÁÑ∂ÂáΩÊï∞ $\\mathcal{L}\\left(\\theta^{(t+1)} ; X\\right)$„ÄÇ ÈÄöËøá‰∏çÊñ≠ÁöÑËø≠‰ª£ÔºåÂèØ‰ª•ÂæóÂà∞‰Ωø‰ººÁÑ∂ÂáΩÊï∞$\\mathcal{L}(\\theta)$ÊúÄÂ§ßÂåñÁöÑÂèÇÊï∞ $\\theta$ÔºåÁõ¥Ëá≥ÂáΩÊï∞Êî∂Êïõ„ÄÇ Âè™ÈúÄË¶ÅËØÅÊòé$\\mathcal{L}\\left(\\theta^{(t+1)} ; X\\right) \\geq \\mathcal{L}\\left(\\theta^{(t)} ; X\\right)$, ÂàôÂèØËØÅÊòéEMÁöÑÊî∂ÊïõÊÄß: $$ \\begin{aligned} \\mathcal{L}\\left(\\theta^{(t+1)} ; X\\right) \u0026=\\sum_{i=1}^{N} \\log \\sum_{z_{i}} Q^{(t)}\\left(z_{i}\\right) \\frac{p\\left(x_{i}, z_{i} | \\theta^{(t+1)}\\right)}{Q^{(t)}\\left(z_{i}\\right)} \\cr \u0026 \\geq \\sum_{i=1}^{N} \\sum_{z_{i}} Q^{(t)}\\left(z_{i}\\right) \\log \\frac{p\\left(x_{i}, z_{i} | \\theta^{(t+1)}\\right)}{Q^{(t)}\\left(z_{i}\\right)} \\cr \u0026 \\geq \\sum_{i=1}^{N} \\sum_{z_{i}} Q^{(t)}\\left(z_{i}\\right) \\log \\frac{p\\left(x_{i}, z_{i} | \\theta^{(t)}\\right)}{Q^{(t)}\\left(z_{i}\\right)} \\cr \u0026=\\mathcal{L}\\left(\\theta^{(t)} ; X\\right) \\end{aligned} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:4:1","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"5. Ê±ÇËß£GMM ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:5:0","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"(1) GMM E-Step: Â∑≤Áü•$\\theta^{(t)}$, Ê±Ç$Q^{(t+1)}\\left(z_{i}\\right)$: $$ \\begin{aligned} Q^{(t+1)}\\left(z_{i}\\right) \u0026=\\frac{p\\left(x_{i}, z_{i} | \\theta^{(t)}\\right)}{p\\left(x_{i} | \\theta^{(t)}\\right)} \\cr \u0026=\\frac{p\\left(x_{i}, z_{i} | \\theta^{(t)}\\right)}{\\sum_{l \\in z_{i}} p\\left(x_{i}, l | \\theta^{(t)}\\right)} \\cr \u0026=\\frac{p\\left(x_{i} | z_{i}, \\theta^{(t)}\\right) p\\left(z_{i} | \\theta^{(t)}\\right)}{\\sum_{l \\in z_{i}} p\\left(x_{i} | l, \\theta^{(t)}\\right) p\\left(l | \\theta^{(t)}\\right)} \\cr \u0026=\\frac{\\mathcal{N}\\left(\\mu_{z_{i}}, \\Sigma_{z_{i}}\\right) \\pi_{z_{i}}}{\\sum_{l \\in z_{i}} \\mathcal{N}\\left(\\mu_{l}, \\Sigma_{l}\\right) \\pi_{l}} \\end{aligned} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:5:1","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"(2) GMM M-Step: Â∑≤Áü•$Q^{(t+1)}\\left(z_{i}\\right)$, Ê±Ç $\\theta^{(t+1)}$: $$ \\begin{aligned} \\mathcal{L}(\\theta ; X) \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log \\frac{p\\left(x_{i}, l | \\theta\\right)}{Q_{i}(l)} \\cr \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log p\\left(x_{i}, l | \\theta\\right)-\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log Q_{i}(l) \\cr \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log p\\left(x_{i}, l | \\theta\\right)-\\text {Constant } \\cr \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log \\pi_{l} \\mathcal{N}\\left(\\mu_{l}, \\Sigma_{l}\\right)-\\text {Constant } \\cr \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log \\pi_{l}+\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log \\mathcal{N}\\left(\\mu_{l}, \\Sigma_{l}\\right)-\\text {Constant} \\end{aligned} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:5:2","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"(3) Ê±Ç $\\pi$: ‰ª§ $\\forall_{l \\in{1, \\cdots, K}}$ $$ \\begin{aligned} \\frac{\\partial \\mathcal{L}(\\theta ; X)}{\\partial \\pi_{l}} \u0026=0 \\cr \\text { s.t. } \\sum_{l}^{K} \\pi_{l} \u0026= 1 \\end{aligned} $$ ÊãâÊ†ºÊúóÊó•‰πòÊ≥ïÁ∫¶Êùü $$ \\begin{cases}\\begin{aligned} L_{\\pi_{l}} \u0026=\\frac{\\partial \\mathcal{L}(\\theta ; X)}{\\partial \\pi_{l}}+\\lambda(\\sum_{l}^{K} \\pi_{l}-1)=0 \\cr L_{\\lambda} \u0026=\\sum_{l}^{K} \\pi_{l}-1=0 \\end{aligned}\\end{cases} $$ Ê±ÇÂØºÔºö $$ \\begin{cases}\\begin{array}{c} \\frac{1}{\\pi_{1}} \\sum_{i}^{N} Q_{i}(1)-\\lambda=0 \\cr \\vdots \\cr \\frac{1}{\\pi_{l}} \\sum_{i}^{N} Q_{i}(l)-\\lambda=0 \\end{array}\\end{cases} $$ Áõ∏Âä†ÂæóÔºö $$ \\sum_{l}^{K} \\sum_{i}^{N} Q_{i}(l)=\\lambda \\sum_{l}^{K} \\pi_{l}=\\lambda $$ Áî± $Q_{i}(l)=p\\left(l \\vert x_{i}, \\theta\\right)$, Âæó $$ \\begin{aligned} \\sum_{l}^{K} \\sum_{i}^{N} Q_{i}(l) \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\cr \u0026=\\sum_{i}^{N} \\sum_{l}^{K} p\\left(l | x_{i}, \\theta\\right) \\cr \u0026=\\sum_{i}^{N} 1 \\cr \u0026=N \\end{aligned} $$ Âàô $$ \\begin{aligned} \\pi_{l} \u0026=\\frac{1}{\\lambda} \\sum_{i}^{N} Q_{i}(l) \\cr \u0026=\\frac{1}{N} \\sum_{i}^{N} Q_{i}(l) \\cr \u0026=\\frac{1}{N} \\sum_{i}^{N} p\\left(l | x_{i}, \\theta\\right) \\end{aligned} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:5:3","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"(4) ËÆ°ÁÆó$\\mu$ $$ \\begin{aligned} \u0026\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log \\mathcal{N}\\left(\\mu_{l}, \\Sigma_{l}\\right)\\cr \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\log \\frac{1}{\\sqrt{2 \\pi} \\sigma_{l}} e^{-\\frac{\\left(x_{i}-\\mu_{l}\\right)^{2}}{2 \\sigma_{l}^{2}}}\\cr \u0026=\\sum_{i}^{N} \\sum_{l}^{K} Q_{i}(l) \\lbrace -\\frac{1}{2} \\log 2 \\pi-\\frac{1}{2} \\log \\sigma_{l}^{2}-\\frac{\\left(x_{i}-\\mu_{l}\\right)^{2}}{2 \\sigma_{l}^{2}}\\rbrace \\end{aligned} $$ Ê±ÇÂÅèÂØºÔºö $$ \\begin{aligned} \\frac{\\partial \\mathcal{L}(\\theta ; X)}{\\partial \\mu_{l}} \u0026=\\sum_{i}^{N} Q_{i}(l) \\frac{x_{i}-\\mu_{l}}{\\sigma^{2}} \\cr \u0026=0 \\end{aligned} $$ Âæó$\\mu$Ôºö $$ \\mu_{l}=\\frac{\\sum_{i}^{N} Q_{i}(l) x_{i}}{\\sum_{i}^{N} Q_{i}(l)} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:5:4","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"(5) ËÆ°ÁÆó$\\sigma$ $$ \\begin{aligned} \\frac{\\partial \\mathcal{L}(\\theta ; X)}{\\partial \\sigma_{l}^{2}} \u0026=\\sum_{i}^{N} Q_{i}(l) \\bigg\\lbrace -\\frac{1}{2 \\sigma_{l}^{2}}+\\frac{\\left(x_{i}-\\mu_{l}\\right)^{2}}{2 \\sigma_{l}^{4}} \\bigg\\rbrace \\cr \u0026=0 \\end{aligned} $$ ÂæóÂà∞ $$ \\sigma_{l}=\\frac{\\sum_{i}^{N} Q_{i}(l)\\left(x_{i}-\\mu_{l}\\right)^{2}}{\\sum_{i}^{N} Q_{i}(l)} $$ ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:5:5","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Machine Learning"],"content":"6 ‰ªéKLÊï£Â∫¶ËßíÂ∫¶Ëß£ÈáäEM $$ \\begin{aligned} K L(q | p) \u0026=\\sum_{z} q(z) \\log \\frac{q(z)}{p(z | x, \\theta)} \\cr \u0026=\\sum_{z} q(z) \\log \\frac{q(z) p(x | \\theta)}{p(z, x | \\theta)} \\cr \u0026=-\\sum_{z} q(z) \\log \\frac{p(z, x | \\theta)}{q(z)}+\\sum_{z} q(z) \\log p(x | \\theta) \\cr \u0026=-\\sum_{z} q(z) \\log \\frac{p(z, x | \\theta)}{q(z)}+\\log p(x | \\theta) \\sum_{z} q(z) \\cr \u0026=-\\sum_{z} q(z) \\log \\frac{p(z, x | \\theta)}{q(z)}+\\log p(x | \\theta) \\cr \\log p(x | \\theta) \u0026=K L(q | p)+\\sum_{z} q(z) \\log \\frac{p(z, x | \\theta)}{q(z)} \\cr \u0026=K L(q | p)+\\mathcal{L}(q, \\theta) \\end{aligned} $$ ÂèÇËÄÉÔºö Âæê‰∫¶Ëææ-Êú∫Âô®Â≠¶‰π†-EM ","date":"2020-01-26","objectID":"/2020-01-26-ml-em/:6:0","tags":["Expectation Maximization","Statistical Learning"],"title":"Expectation Maximization","uri":"/2020-01-26-ml-em/"},{"categories":["Make bioinfo uncool again"],"content":"Usefull tools","date":"2020-01-20","objectID":"/2020-01-20-begin/","tags":["Bioinformatics","Linux"],"title":"Cheatsheet for command line","uri":"/2020-01-20-begin/"},{"categories":["Make bioinfo uncool again"],"content":"usefull tools for linux command line ","date":"2020-01-20","objectID":"/2020-01-20-begin/:0:0","tags":["Bioinformatics","Linux"],"title":"Cheatsheet for command line","uri":"/2020-01-20-begin/"},{"categories":["Make bioinfo uncool again"],"content":"a. Make terminal cool, install OhMyZsh # install zsh sudo apt-get install zsh # ubuntu # change default shell to zsh chsh -s /usr/bin/zsh # install ohmyzsh sh -c \"$(wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)\" source ~/.zshrc ","date":"2020-01-20","objectID":"/2020-01-20-begin/:0:1","tags":["Bioinformatics","Linux"],"title":"Cheatsheet for command line","uri":"/2020-01-20-begin/"},{"categories":["Make bioinfo uncool again"],"content":"b. Terminal keyboard short cuts Jump to head: Ctrl + a Jump to end: Ctrl + e Delete strings ahead: Ctrl + u Delete strings follow: Ctrl + k ","date":"2020-01-20","objectID":"/2020-01-20-begin/:0:2","tags":["Bioinformatics","Linux"],"title":"Cheatsheet for command line","uri":"/2020-01-20-begin/"},{"categories":["Make bioinfo uncool again"],"content":"c. Program keeps running in the background 1. Run cmd using nohup nohup command [options] \u0026 2. Run cmd using Tmux Outside Tmux: Typically these are run outside, but you can also run them inside an existing session a. Start New Session tmux new -s myname b. Attach To Existing Session tmux attach -t myname #by name tmux attach 4 #by number (in this case 4) c. List Sessions tmux ls d. Kill Session tmux kill-session -t myname Inside Tmux Session: Start each command with CTRL + b, release, then press one of the following: Panes % vertical split \" horizontal split d detach from session (it keeps running in the background) x kill pane Up/Down/Left/Right move between panes PageUP/PageDown CTRL+c to exit the PageUp/Down mode Fn+Up/Down PageUp/Down: Mac keyboard : + resize-pane -D Resizes the current pane down : + resize-pane -U Resizes the current pane upward : + resize-pane -L Resizes the current pane left : + resize-pane -R Resizes the current pane right : + resize-pane -D 20 Resizes the current pane down by 20 cells ","date":"2020-01-20","objectID":"/2020-01-20-begin/:0:3","tags":["Bioinformatics","Linux"],"title":"Cheatsheet for command line","uri":"/2020-01-20-begin/"},{"categories":["Make bioinfo uncool again"],"content":"d. File compression and decompression Decompression File type Cmd e.g. *.tar tar -xvf *.tar.gz or *.tgz tar -xvzf *bz2 bzip2 -d or bunzip2 *.tar.bz2 tar -xjf *.Z uncompress *.tar.Z tar -xZf *.rar unrar e or rar x unrar e file.rar *.zip unzip *.gz gunzip Compression File type Cmd e.g. *.tar tar -cvf *.tar.gz or *.tgz tar -cvzf *bz2 bzip2 -z *.tar.bz2 tar -cjf *.Z compress *.tar.Z tar -cZf *.rar rar a rar a -ep1 newname /home/user/cpp *.zip zip *.gz gzip For rar installation sudo apt-get install rar ","date":"2020-01-20","objectID":"/2020-01-20-begin/:0:4","tags":["Bioinformatics","Linux"],"title":"Cheatsheet for command line","uri":"/2020-01-20-begin/"},{"categories":["Make bioinfo uncool again"],"content":"e. Handy tricks for handling filepath very useful to strip file sufix, path et.al. # e.g. var=./home/fastq/filename_R1.fq.gz # extract filename ${var#*/} # -\u003e home/fastq/filename_R1.fq.gz var1=${var##*/} # -\u003e filename_R1.fq.gz # remove file suffix ${var1%.*} # -\u003e filename_R1.fq ${var1%%.*} # -\u003e filename_R1 # get basebame var2=$(basename \"${var}\" .fq.gz) #-\u003e filename_R1 ","date":"2020-01-20","objectID":"/2020-01-20-begin/:0:5","tags":["Bioinformatics","Linux"],"title":"Cheatsheet for command line","uri":"/2020-01-20-begin/"},{"categories":["Make bioinfo uncool again"],"content":"hisat2-htseq-deseq2","date":"2020-01-20","objectID":"/2020-01-20-rnaseq/","tags":["RNA-seq"],"title":"Shortcut to bulk RNA-seq analysis","uri":"/2020-01-20-rnaseq/"},{"categories":["Make bioinfo uncool again"],"content":"hisat2-htseq-deseq2 ","date":"2020-01-20","objectID":"/2020-01-20-rnaseq/:0:0","tags":["RNA-seq"],"title":"Shortcut to bulk RNA-seq analysis","uri":"/2020-01-20-rnaseq/"},{"categories":["Make bioinfo uncool again"],"content":"3.1 transcriptom mapping step 0: install tools conda install htseq hisat2 stringtie step 1: build index and extract splice sites build index hisat2-build -p {threads} genome/hg38.fa hisat2_index/hg38 extract known splice sites for alignmnet hisat2_extract_splice_sites.py gencode.gtf \u003e hisat2_index/splicesites.txt hisat2_extract_exons.py gencode.gtf \u003e histat2_index/exon.txt step2: mapping hisat2 --dta --threads ${threads} \\ -x hisat2_index/hg38 \\ --known-splicesite-infile hisat2_index/splicesites.txt \\ -1 R1.fq.gz \\ -2 R2.fq.gz \\ -S output.sam step 3: sam to bam samtools view -Sbh -q 25 \\ -@ ${threads} \\ -o ouput.bam \\ input.sam step 4: bam sort and index samtools sort -@ ${threads} input.bam \u003e output.sorted.bam samtools index input.sorted.bam #generate input.sorted.bam.bai step 5: bam to bigwig bamCoverage -p ${threads} \\ --normalizeUsing RPKM \\ # note: other normalization options -b input.sorted.bam \\ -o output.bw ","date":"2020-01-20","objectID":"/2020-01-20-rnaseq/:0:1","tags":["RNA-seq"],"title":"Shortcut to bulk RNA-seq analysis","uri":"/2020-01-20-rnaseq/"},{"categories":["Make bioinfo uncool again"],"content":"3.2 Differentially expressed genes analysis step 1: count reads htseq-count -r pos -s no \\ --additional-attr gene_name \\ --additional-attr gene_type \\ -f bam input.sorted.bam gencode.gtf \u003e output.count step2: differentially expressed genes analysis (1) construct read count table option 1: HTSeq count file input library(\"DESeq2\") directory \u003c- \"/path/to/your/readCountFiles/\" sampleFiles \u003c- grep(\"count\", list.files(directory), value=TRUE) condition \u003c- factor(c(\"KO\",\"KO\", \"WT\",\"WT\"), levels = c(\"WT\", \"KO\")) # phenotable sampleTable \u003c- data.frame(sampleName = sampleFiles, fileName = sampleFiles, condition = condition) # construct read count table ddsHTSeq \u003c- DESeqDataSetFromHTSeqCount(sampleTable = sampleTable, directory = directory, design= ~ condition) option 2: combined read count file into a single table first, then run library(DESeq2) # read count table database \u003c- read.table(file = \"raw.counts.csv\", sep = \",\", header = TRUE, row.names = 1) database \u003c- round(as.matrix(database)) # set level condition \u003c- factor(c(\"KO\",\"KO\", \"WT\",\"WT\"), levels = c(\"WT\", \"KO\")) # build DESeq object coldata \u003c- data.frame(row.names = colnames(database), condition) dds \u003c- DESeqDataSetFromMatrix(countData=database, colData=coldata, design=~condition + treatmement) (2) run DESeq2 and get output library(DESeq2) dds \u003c- dds[ rowSums(counts(dds)) \u003e 1, ] # run statistical test dds \u003c- DESeq(dds) # get results res \u003c- results(dds) # summary(res) count_r \u003c- counts(dds, normalized=T) #normalized count matrix # export results res \u003c- res[order(res$padj),] diff_gene \u003c- subset(res, padj \u003c 0.05 \u0026 (log2FoldChange \u003e 1 | log2FoldChange \u003c -1)) diff_gene \u003c- row.names(diff_gene) resdata \u003c- merge(as.data.frame(res), as.data.frame(counts(dds, normalized=TRUE)), by=\"row.names\", sort=FALSE) write.csv(resdata, file = \"DEGs.csv\", row.names = FALSE) 3.3 Gene set enrichrment analysis GO clusterprofiler Enrichr (GSEApy) GSEA ","date":"2020-01-20","objectID":"/2020-01-20-rnaseq/:0:2","tags":["RNA-seq"],"title":"Shortcut to bulk RNA-seq analysis","uri":"/2020-01-20-rnaseq/"},{"categories":["Make bioinfo uncool again"],"content":"3.4 Alternative splicing analysis rMATS ","date":"2020-01-20","objectID":"/2020-01-20-rnaseq/:0:3","tags":["RNA-seq"],"title":"Shortcut to bulk RNA-seq analysis","uri":"/2020-01-20-rnaseq/"},{"categories":["Make bioinfo uncool again"],"content":"bowtie2-macs2-deeptools","date":"2020-01-20","objectID":"/2020-01-20-chip/","tags":["ChIP-seq"],"title":"Shortcut to ChIP-seq analysis","uri":"/2020-01-20-chip/"},{"categories":["Make bioinfo uncool again"],"content":"bowtie2-macs2-deeptools ","date":"2020-01-20","objectID":"/2020-01-20-chip/:0:0","tags":["ChIP-seq"],"title":"Shortcut to ChIP-seq analysis","uri":"/2020-01-20-chip/"},{"categories":["Make bioinfo uncool again"],"content":"2.1 Genome mapping Step 0: install software # install miniconda, then call conda conda install -c bioconda bowtie2 hisat2 samtools deeptools step 1: build index bowtie2-build hg38.fa bowtie2_index/hg38 step 2: mapping Unpaired data bowtie2 -p ${threads} -x index/hg38 \\ -U input.fastq.gz \\ -S ouput.sam Paired data bowtie2 -p 4 -x index/hg38 \\ -1 input_R1.fastq.gz \\ -2 input_R2.fastq.gz \\ -S ouput.sam step 3: sam to bam samtools view -Sbh -q 25 \\ -@ ${threads} \\ -o ouput.bam \\ input.sam step 4: bam sort and index samtools sort -@ ${threads} input.bam \u003e output.sorted.bam samtools index input.sorted.bam #generate input.sorted.bam.bai step 5: bam to bigwig bamCoverage -p ${threads} \\ --normalizeUsing RPKM \\ # note: other normalization options --centerReads \\ -e 200 \\ -b input.sorted.bam \\ -o output.bw ","date":"2020-01-20","objectID":"/2020-01-20-chip/:0:1","tags":["ChIP-seq"],"title":"Shortcut to ChIP-seq analysis","uri":"/2020-01-20-chip/"},{"categories":["Make bioinfo uncool again"],"content":"2.2 Peaks analysis note:: macs2 (\u003ev2.2.x) supports python 3. step 0: install tools conda install macs2 bedtools pygenometracks step 1: callpeaks (1) narrow peaks, e.g. TFs, h3k4m3 # bam file input macs2 callpeak -t ChIP.elute.sorted.bam \\ -c ChIP.input.sorted.bam \\ -f BAM \\ -g hs # organism \\ -B -q 0.05 \\ -n ${outFileName}\\ --outdir macs_out (2) Broad peaks, e.g. h3k27me3 # sam file also works fine macs2 callpeak -t ./bowtie_out/WTme2ChIP.sam \\ -c ./bowtie_out/ESCInput.sam \\ -f SAM \\ -g mm \\ -B --SPMR \\ --nomodel --extsize 147 \\ --broad -n WTme2ChIP --outdir macs_out step 2: advanced analysis tools: bedtools, deeptools, pyGenomeTracks, igv genome algebra overlap with other peaks: bedtools visualization heatmap: deeptools signal tracks: pyGenomeTracks, igv ","date":"2020-01-20","objectID":"/2020-01-20-chip/:0:2","tags":["ChIP-seq"],"title":"Shortcut to ChIP-seq analysis","uri":"/2020-01-20-chip/"},{"categories":["Make bioinfo uncool again"],"content":"salmon-tximport-deseq2","date":"2020-01-20","objectID":"/2020-01-23-rnaseqturbo/","tags":["RNA-seq"],"title":"SuperFast RNA-seq","uri":"/2020-01-23-rnaseqturbo/"},{"categories":["Make bioinfo uncool again"],"content":"salmon-tximport-deseq2 ","date":"2020-01-20","objectID":"/2020-01-23-rnaseqturbo/:0:0","tags":["RNA-seq"],"title":"SuperFast RNA-seq","uri":"/2020-01-23-rnaseqturbo/"},{"categories":["Make bioinfo uncool again"],"content":"Step 0: install salmon and download transcriptome cdna from gencode conda install salmon wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_32/gencode.v32.transcripts.fa.gz ","date":"2020-01-20","objectID":"/2020-01-23-rnaseqturbo/:0:1","tags":["RNA-seq"],"title":"SuperFast RNA-seq","uri":"/2020-01-23-rnaseqturbo/"},{"categories":["Make bioinfo uncool again"],"content":"Step 1. build salmon index salmon index -p 8 --gencode -t gencode.v32.transcripts.fa.gz -i salmonIndex_hg38 ","date":"2020-01-20","objectID":"/2020-01-23-rnaseqturbo/:0:2","tags":["RNA-seq"],"title":"SuperFast RNA-seq","uri":"/2020-01-23-rnaseqturbo/"},{"categories":["Make bioinfo uncool again"],"content":"Step 2: quantification salmon quant -i salmonIndex_hg38 -l A \\ -1 ${fn}/${samp}_1.fastq.gz \\ -2 ${fn}/${samp}_2.fastq.gz \\ -p 8 --validateMappings -o quants/${samp}_quant ","date":"2020-01-20","objectID":"/2020-01-23-rnaseqturbo/:0:3","tags":["RNA-seq"],"title":"SuperFast RNA-seq","uri":"/2020-01-23-rnaseqturbo/"},{"categories":["Make bioinfo uncool again"],"content":"Step 3: merge quantification outputs use tximport in R # R code library(tximport) library(readr) suppressMessages(library('EnsDb.Hsapiens.v86')) txdb \u003c- EnsDb.Hsapiens.v86 k \u003c- keys(txdb, keytype = \"GENEID\") df \u003c- select(txdb, keys = k, keytype = \"GENEID\", columns = c(\"TXID\",\"GENEID\")) tx2gene \u003c- df[, 2:1] # tx ID, then gene ID #tx2gene \u003c- read.table(tx2gene, header= T, sep=\"\\t\", stringsAsFactors = F) samples \u003c- unlist(strsplit(sample_ids,\",\")) salmon.files \u003c- file.path('salmon',samples, \"quant.sf\") names(salmon.files) \u003c- samples all(file.exists(salmon.files)) # get transcript level results txi.transcripts \u003c- tximport(salmon.files, type = \"salmon\", txOut = TRUE, tx2gene = tx2gene,) # ignoreTxVersion = TRUE) # get gene level results txi.salmon \u003c- summarizeToGene(txi.transcripts, tx2gene) #save raw counts salmon.counts\u003c- txi.salmon$counts salmon.counts\u003c- as.data.frame(salmon.counts) write.table(salmon.counts, out_counts, sep=\"\\t\", quote=F) #save gene tpms salmon.TPM\u003c- txi.salmon$abundance salmon.TPM\u003c- as.data.frame(salmon.TPM) write.table(salmon.TPM, out_tpm, sep=\"\\t\", quote=F) #save transcripts tpms salmon.trans.TPM\u003c- txi.transcripts$abundance salmon.trans.TPM\u003c- as.data.frame(salmon.trans.TPM) write.table(salmon.trans.TPM, outTrans_tpm, sep=\"\\t\", quote=F) save(txi.salmon, file=\"txi.salmon.RData\") ","date":"2020-01-20","objectID":"/2020-01-23-rnaseqturbo/:0:4","tags":["RNA-seq"],"title":"SuperFast RNA-seq","uri":"/2020-01-23-rnaseqturbo/"},{"categories":["Make bioinfo uncool again"],"content":"Step 4: Differentially expressed gene analysis DESeq2 pipeline demo load(\"txi.salmon.RData\") dds \u003c- DESeqDataSetFromTximport(txi.salmon, sampleTable, ~condition) dds$condition \u003c- relevel(dds$condition, ref=ctrl) dds \u003c- DESeq(dds, parallel=TRUE) res \u003c- results(dds, contrast=c(\"condition\", treat, ctrl)) resOrdered \u003c- res[order(res$padj),] resOrdered = as.data.frame(resOrdered) write.table(resOrdered, file=\"degs.txt\", quote=F, sep=\"\\t\") ","date":"2020-01-20","objectID":"/2020-01-23-rnaseqturbo/:0:5","tags":["RNA-seq"],"title":"SuperFast RNA-seq","uri":"/2020-01-23-rnaseqturbo/"},{"categories":null,"content":"My PhD thesis","date":"2019-12-10","objectID":"/publication/2019-12-10-sox21/","tags":null,"title":"SOX21 Ensures Rostral Forebrain Identity by Suppression of WNT8B during Neural Regionalization of Human Embryonic Stem Cells","uri":"/publication/2019-12-10-sox21/"},{"categories":null,"content":"My PhD thesis. Published in Stem Cell Reports, 2019 Highlights The transcriptomic analysis of rostrocaudal patterning of hESC-derived NPCs SOX21 KO leads to caudalized regional identity in rostral forebrain progenitors SOX21 represses Wnt signaling to ensure the rostral forebrain identity WNT8B is a major downstream target of SOX21 Download here ","date":"2019-12-10","objectID":"/publication/2019-12-10-sox21/:0:0","tags":null,"title":"SOX21 Ensures Rostral Forebrain Identity by Suppression of WNT8B during Neural Regionalization of Human Embryonic Stem Cells","uri":"/publication/2019-12-10-sox21/"},{"categories":null,"content":"Ding J, Fang Z, Liu X, Zhu Z, Wen C, Wang H, et al. CDK11 safeguards the identity of human embryonic stem cells via fine-tuning signaling pathways. J Cell Physiol. 2019 Download here ","date":"2019-10-15","objectID":"/publication/2019-10-15/:0:0","tags":null,"title":"CDK11 safeguards the identity of human embryonic stem cells via fine‚Äêtuning signaling pathways","uri":"/publication/2019-10-15/"},{"categories":null,"content":"Hu J, Li S, Sun X, Fang Z, Wang L, Xiao F, et al. Stk40 deletion elevates c-JUN protein level and impairs mesoderm differentiation. J Biol Chem. 2019;294(25):9959-72. Download here ","date":"2019-06-25","objectID":"/publication/2019-06-21/:0:0","tags":null,"title":"Stk40 deletion elevates c-JUN protein level and impairs mesoderm differentiation","uri":"/publication/2019-06-21/"},{"categories":null,"content":"Xu Y, Luo X, Fang Z, Zheng X, Zeng Y, Zhu C, et al. Transcription coactivator Cited1 acts as an inducer of trophoblast-like state from mouse embryonic stem cells through the activation of BMP signaling. Cell Death Dis. 2018;9(9):924. Download here ","date":"2018-09-11","objectID":"/publication/2018-09-11/:0:0","tags":null,"title":"Transcription coactivator Cited1 acts as an inducer of trophoblast-like state from mouse embryonic stem cells through the activation of BMP signaling","uri":"/publication/2018-09-11/"},{"categories":null,"content":"I‚Äôm honored to be one of the contributors. It is definitely worthy. Thanks for the Bioconda team, so we could install bioinformatic tools so easy! The open source Bioconda project is a milestone in computational biology. It saves lots of time when trying to install bioinfo tools. Download here ","date":"2018-07-02","objectID":"/publication/2018-07-02/:0:0","tags":null,"title":"Bioconda: sustainable and comprehensive software distribution for the life sciences","uri":"/publication/2018-07-02/"},{"categories":null,"content":"Single cell analysis of mouse E5.5, E6.5 embryos. In this work, I dissected the embryos and prepared the single cell cDNA libraries.","date":"2017-06-09","objectID":"/publication/2016-06-09/","tags":null,"title":"Single-cell analysis reveals lineage segregation in early post-implantation mouse embryos","uri":"/publication/2016-06-09/"},{"categories":null,"content":"Wen J, Zeng Y, Fang Z, Gu J, Ge L, et al. Single-cell analysis reveals lineage segregation in early post-implantation mouse embryos. The Journal of biological chemistry 2017;292:9840-54 Download here ","date":"2017-06-09","objectID":"/publication/2016-06-09/:0:0","tags":null,"title":"Single-cell analysis reveals lineage segregation in early post-implantation mouse embryos","uri":"/publication/2016-06-09/"},{"categories":null,"content":"Wang L, Yu H, Cheng H, He K, Fang Z, et al. Deletion of Stk40 impairs definitive erythropoiesis in the mouse fetal liver. Cell Death Dis. 2017;8:e2722 Download here ","date":"2017-03-30","objectID":"/publication/2017-03-30/:0:0","tags":null,"title":"Deletion of Stk40 impairs definitive erythropoiesis in the mouse fetal liver","uri":"/publication/2017-03-30/"},{"categories":null,"content":"Liao B, Zhong XM, Xu HM, Xiao F, Fang ZQ, et al. Itch, an E3 ligase of Oct4, is required for embryonic stem cell self-renewal and pluripotency induction. Journal of Cellular Physiology 2013;228:1443-51 Download here ","date":"2012-12-18","objectID":"/publication/2012-12-18/:0:0","tags":null,"title":"Itch, an E3 ligase of Oct4, is required for embryonic stem cell self-renewal and pluripotency induction","uri":"/publication/2012-12-18/"},{"categories":null,"content":"I would like to use pandas to explore my data, but I did not find a convenient tool to do gene set enrichment analysis in python. I want something like this: Ability to run inside python console without switching to R. User friendly for both wet and dry lab users. Produce or reproduce publishable figures. Perform batch jobs easy. Command line support (Win, Mac, Linux). That‚Äôs why GSEApy comes to play. See code here ","date":"0001-01-01","objectID":"/portfolio/portfolio-1/:0:0","tags":null,"title":"GSEApy","uri":"/portfolio/portfolio-1/"},{"categories":null,"content":"My bioinformatic workflows using snakemake. I would update it if only I have more time. Why snakemake? Good: Community Easy to use and update Reusable workflows Scalable Support conda env Support slurm, qsub ‚Ä¶ python syntax Drawback: Hard to debug workflows. TODO: Integrate snakemake-wrappers to current workflows to make it more reproducible. See code here ","date":"0001-01-01","objectID":"/portfolio/portfolio-3/:0:0","tags":null,"title":"Snakeflow","uri":"/portfolio/portfolio-3/"},{"categories":null,"content":"My journey to object detection began with YOLOv3. I think it‚Äôs really a good starting point for someone like me without computer vison background to understand what‚Äôs going on behind the scence. While learning object detection, I made a simple modified C++ version (with LibTorch) based on others‚Äô work 1 2 3. See code here ","date":"0001-01-01","objectID":"/portfolio/portfolio-2/:0:0","tags":null,"title":"YOLOv3 Inference Framework in C++","uri":"/portfolio/portfolio-2/"},{"categories":null,"content":"Performance test code: yolov3 models/yolov3.cfg models/yolov3.weights images Results: tested with CPU: Core i9 Windows MAC: average time (682 ms/image). tested with GPU: Tesla V100 Linux: average time (22 ms/image). ","date":"0001-01-01","objectID":"/portfolio/portfolio-2/:1:0","tags":null,"title":"YOLOv3 Inference Framework in C++","uri":"/portfolio/portfolio-2/"},{"categories":null,"content":"Features Supports NMS Soft NMS Weighted NMS ","date":"0001-01-01","objectID":"/portfolio/portfolio-2/:2:0","tags":null,"title":"YOLOv3 Inference Framework in C++","uri":"/portfolio/portfolio-2/"},{"categories":null,"content":"TODO Support training ‚Ä¶ ","date":"0001-01-01","objectID":"/portfolio/portfolio-2/:3:0","tags":null,"title":"YOLOv3 Inference Framework in C++","uri":"/portfolio/portfolio-2/"},{"categories":null,"content":"Credits YOLOv3_CPP repo is created based on the implementations below: weixu000. ‚Ü©Ô∏é PyTorch-YOLOv3. ‚Ü©Ô∏é YOLO_v3_tutorial_from_scratch. ‚Ü©Ô∏é ","date":"0001-01-01","objectID":"/portfolio/portfolio-2/:4:0","tags":null,"title":"YOLOv3 Inference Framework in C++","uri":"/portfolio/portfolio-2/"}]