<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>All Posts - My Computational Genomic Playground</title><link>https://zqfang.github.io/posts/</link><description>All Posts | My Computational Genomic Playground</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 06 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://zqfang.github.io/posts/" rel="self" type="application/rss+xml"/><item><title>Graph Embeding: Node2Vec</title><link>https://zqfang.github.io/2020-12-06-ml-node2vec/</link><pubDate>Sun, 06 Dec 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-12-06-ml-node2vec/</guid><description><![CDATA[Node Embedings are learnt in the same way as word2vec (skip-gram model)
However, graphs could be (un)directed, (un)weighted, (a)cyclic and are basically much more complex than the strucure of a sequence&hellip;
So how do we generate &ldquo;corpus&rdquo; from a graph ?
Sampling strategy Node2vec&rsquo;s sampling strategy accepts 4 argument:
 Number of walks: number of random walks to be generated from each node in the graph Walk length: how many nodes are in each random walk P: return hyperparameter Q: Inout hyperparameter (DFS biased or BFS baised control)  Also, the standard skip-gram parameters]]></description></item><item><title>NLP: Word2Vec</title><link>https://zqfang.github.io/2020-12-05-nlp-word2vec/</link><pubDate>Sat, 05 Dec 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-12-05-nlp-word2vec/</guid><description>Word2Vec
CBOW Continuous Bag of Words Model (CBOW)
When trainning, use N-gram language model. That&amp;rsquo;s for a target word, select $m$ (window) words before and after.
Model one-hot encoding get $2m$ vectors: $$X = (x^{c-m}, \cdots, x^{c-1}, x^{c+1}, \cdots, x^{c+m})$$
Embeding Vector $\mathcal{V} \in R^{n \times \mathcal{V}}$,</description></item><item><title>Probabilistic Graphical Model</title><link>https://zqfang.github.io/2020-11-20-ml-pgm/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-11-20-ml-pgm/</guid><description>概率图模型（probabilistic graphical model, PGM），是一种学习任务的框架描述，它将学习任务归结为计算</description></item><item><title>Deploy IGV webapp on linux server</title><link>https://zqfang.github.io/2020-11-16-igvweb/</link><pubDate>Mon, 16 Nov 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-11-16-igvweb/</guid><description>Config IGV on the server.
I have to share the inteactive results with my colleague. But I don&amp;rsquo;t like to install UCSC genomebrower in local. Instead, a light-weight one is what I need.
1. Installation Install nodejs if you have conda, just
1 conda install -c conda-forge nodejs build igv-webapp 1 2 3 4 git clone https://github.</description></item><item><title>Canonical Correlation Analysis (CCA)</title><link>https://zqfang.github.io/2020-08-27-ml-cca/</link><pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-08-27-ml-cca/</guid><description>典型相关分析(CCA) ，一种常用降维算法，也可以用于多个线性空间相关性计算。比如同一对象的多模态数据</description></item><item><title>Complex number for biologist</title><link>https://zqfang.github.io/2020-08-15-ml-complexnumber/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-08-15-ml-complexnumber/</guid><description>A biologist like me might have never had a numerical computing training. I don&amp;rsquo;t even known what a complex number really means. Here are some useful basics to keep in mind.
Complex number complex number $a+bi$ lives in a 2d complex plane, including
real axis: $a$ imagnary axis: $i$ orthognal to real axis $i \rightarrow 90 \degree \text{rotation}$ 2 ways of representation $z = a + bi$ $z = r \cos(\phi) + r \sin(\phi) i = r e^{i \phi}$ 3 Facts about Multiplication $z \cdot 1 = z$ $z \cdot i = \operatorname{Rot90}(z)$ e.</description></item><item><title>Deploy snakemake pipeline on HPC</title><link>https://zqfang.github.io/2020-08-19-hpc-snakemake/</link><pubDate>Wed, 19 Aug 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-08-19-hpc-snakemake/</guid><description>The best part of snakemake is allowed you to run your pipeline on HPC automatically. It save you a lot of time.
How to run snakemake on HPC there are two ways to configure
use --cluster: works on different HPC system, e.g. slurm, SGE. assign resource in params directive explicitly.</description></item><item><title>Fourier transform for biologist</title><link>https://zqfang.github.io/2020-08-01-dl-fourier/</link><pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-08-01-dl-fourier/</guid><description>A biologist&amp;rsquo;s way to learn Fourier transform Visual intuition in 3D This is an awesome introduction Fourier Series Discrete Fourier transform (DFT) A Fourier series is a periodic function composed of harmonically related sinusoids, combined by a weighted summation. 周期性函数可以变换为正</description></item><item><title>NLP: Parse trees</title><link>https://zqfang.github.io/2020-08-01-nlp-constituency-and-depenency-parsing/</link><pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-08-01-nlp-constituency-and-depenency-parsing/</guid><description>Structure of Sentences: Parse trees Shallow parsing identifies phrasal units, the task of identifying the relationship between them is called parsing.
Parse trees indicate how different grammatical units in a sentence are related hierachically. (aslo refer to constituent parse, chart-based )
dependency parsing: directed graph (graph-based)</description></item><item><title>NLP: RNN and Transformers</title><link>https://zqfang.github.io/2020-07-31-nlp-rnn/</link><pubDate>Fri, 31 Jul 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-07-31-nlp-rnn/</guid><description>Backpropagation Through Time Long Short-Term Memory Delete information from the context that is no longer needed: Forget Gate f $$ f_t = \sigma (U_f h_{t-1} + W_f X_t) $$ $$ k_t = c_{t-1} \odot f_t $$ Compute the actual information we need to extract from the previous hidden stat and</description></item></channel></rss>