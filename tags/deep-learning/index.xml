<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Deep Learning - Tag - My Computational Genomic Playground</title><link>https://zqfang.github.io/tags/deep-learning/</link><description>Deep Learning - Tag - My Computational Genomic Playground</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sat, 01 Aug 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://zqfang.github.io/tags/deep-learning/" rel="self" type="application/rss+xml"/><item><title>Fourier transform for biologist</title><link>https://zqfang.github.io/2020-08-01-dl-fourier/</link><pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-08-01-dl-fourier/</guid><description>A biologist&amp;rsquo;s way to learn Fourier transform Visual intuition in 3D This is an awesome introduction Fourier Series Discrete Fourier transform (DFT) A Fourier series is a periodic function composed of harmonically related sinusoids, combined by a weighted summation. 周期性函数可以变换为正</description></item><item><title>NLP: Parse trees</title><link>https://zqfang.github.io/2020-08-01-nlp-constituency-and-depenency-parsing/</link><pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-08-01-nlp-constituency-and-depenency-parsing/</guid><description>Structure of Sentences: Parse trees Shallow parsing identifies phrasal units, the task of identifying the relationship between them is called parsing.
Parse trees indicate how different grammatical units in a sentence are related hierachically. (aslo refer to constituent parse, chart-based )
dependency parsing: directed graph (graph-based)</description></item><item><title>NLP: RNN and Self-attention</title><link>https://zqfang.github.io/2020-07-31-nlp-rnn/</link><pubDate>Fri, 31 Jul 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-07-31-nlp-rnn/</guid><description>Backpropagation Through Time Long Short-Term Memory Delete information from the context that is no longer needed: Forget Gate f $$ f_t = \sigma (U_f h_{t-1} + W_f X_t) $$
$$ k_t = c_{t-1} \odot f_t $$
Compute the actual information we need to extract from the previous hidden stat and current inputs $$ g_t = \tanh (U_g h_{t-1} + W_g x_t) $$</description></item><item><title>Graph Neural Network</title><link>https://zqfang.github.io/2020-07-26-dl-gnn/</link><pubDate>Sun, 26 Jul 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-07-26-dl-gnn/</guid><description>More about Graph Neural Network
Algebra presentation of Graphs 1. Adjacency matrix $$ A_{i j}= \begin{cases} 1 &amp;amp; \text { if }\lbrace v_{i}, v_{j}\rbrace \in E \text { and } i \neq j \cr 0 &amp;amp; \text { otherwise } \end{cases} $$
2. Degree matrix: D is a diagonal matrix, where $$ D_{ii} = d(v_i) $$</description></item><item><title>Geometric Deep Learning</title><link>https://zqfang.github.io/2020-07-25-dl-geometric/</link><pubDate>Sat, 25 Jul 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-07-25-dl-geometric/</guid><description>Introduction of Graph Neural Networks
Data Eculidean Structure Data: image, video, voice &amp;hellip; easy to find adjacent neighbors easy to define distance Non-Eculidean data: Graph, Manifold hard to define adjacent neighbors or the numbers of adjacent nodes varies. means hard to define distance, convolution &amp;hellip; Embed (project) Non-Eculidean Data into Eculidean Space using geometric deep learning</description></item><item><title>NLP: A short guide for biologist</title><link>https://zqfang.github.io/2020-06-20-nlp-basics/</link><pubDate>Sat, 20 Jun 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-06-20-nlp-basics/</guid><description>NLP Basics for the newbies like me
Languwage model Models that assigns probabilities to sequences of words are called languwage models.
Count-based Representation 1. one-hot representation 2. BoW: Bag of words Blow describes the occurrence of words within a document. including
A Vocabulary of known words A measure of the presence of known words, e.</description></item></channel></rss>