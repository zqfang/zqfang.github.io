<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Deep Learning - Tag - My Computational Genomic Playground</title><link>https://zqfang.github.io/tags/deep-learning/</link><description>Deep Learning - Tag - My Computational Genomic Playground</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Mon, 29 Jul 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://zqfang.github.io/tags/deep-learning/" rel="self" type="application/rss+xml"/><item><title>RoPE: Rotary Positional Embeddings</title><link>https://zqfang.github.io/2024-07-29-nlp-rope/</link><pubDate>Mon, 29 Jul 2024 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2024-07-29-nlp-rope/</guid><description>Rotary Positional Embeddings, proposed in 2022, this innovation is swiftly making its way into prominent language models like Google&amp;rsquo;s PaLM and Meta&amp;rsquo;s LLaMa. RoPE is a new type of positional encoding that unifies absolute and relative positional encoding approaches Rotary Positional Encoding is a type of position encoding that encodes</description></item><item><title>SwiGLU</title><link>https://zqfang.github.io/2024-07-29-nlp-swiglu/</link><pubDate>Mon, 29 Jul 2024 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2024-07-29-nlp-swiglu/</guid><description>The most commonly used activation function in LLM.
1 2 3 4 5 6 7 8 9 10 11 12 class SwiGLU(nn.Module): def __init__(self, w1, w2, w3): super.__init__() self.w1 = w1 self.w2 = w2 slef.w3 = w3 def forward(self, x): x1 = F.linear(x, self.w1.weight) x2 = F.linear(x, self.w2.weight) hidden = F.</description></item><item><title>Graph: Augmentation</title><link>https://zqfang.github.io/2025-03-16-graph-augmentation/</link><pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2025-03-16-graph-augmentation/</guid><description>Graph Augmentations using PyG
Graph Structure Agumentation Half-Hop HalfHop adds a “slow node” to all edges with some probability p . Note that these slow nodes have averaged features from the parent nodes, and additionally are undirected.
Virtual Node VirtualNode (Gilmer 2017) appends a virtual node to the given homogeneous graph that is connected to all other nodes.</description></item><item><title>Graph: Train, valid, and test dataset split for link prediction</title><link>https://zqfang.github.io/2021-08-12-graph-linkpredict/</link><pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2021-08-12-graph-linkpredict/</guid><description>Link Prediction Link prediction is a common task in knowledgegraph&amp;rsquo;s link completeion. Link prediction is usually an unsupervised or self-supervised task, which means that sometimes we need to split the dataset and create corresponding labels on our own. How to prepare train, valid, test datasets ? For link prediction, we will split edges twice</description></item><item><title>Graph: Mini-batch sampling in large-scale graphs</title><link>https://zqfang.github.io/2021-08-11-graph-minibatch/</link><pubDate>Wed, 11 Aug 2021 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2021-08-11-graph-minibatch/</guid><description>Mini-batch Sampling Real world graphs can be very large with millions or even billions of nodes and edges. But the naive full-batch implementation of GNN cannot be feasible to these large-scale graphs.
Two frequently used methods are summarized here:
Neighbor Sampling (Hamilton et al. (2017)) torch_geometric.loader.NeighborLoader Cluster-GCN (Chiang et al.</description></item><item><title>Graph: Implement a MessagePassing layer in Pytorch Geometric</title><link>https://zqfang.github.io/2021-08-07-graph-pyg/</link><pubDate>Sat, 07 Aug 2021 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2021-08-07-graph-pyg/</guid><description>How to implement a custom MessagePassing layer in Pytorch Geometric (PyG) ?
Before you start, something you need to know.
special_arguments: e.g. x_j, x_i, edge_index_j, edge_index_i aggregate: scatter_add, scatter_mean, scatter_min, scatter_max PyG MessagePassing framework only works for node_graph. 1 2 3 4 5 x = ... # Node features of shape [num_nodes, num_features] edge_index = .</description></item><item><title>Graph: Concepts</title><link>https://zqfang.github.io/2021-04-19-graph-foundation/</link><pubDate>Mon, 19 Apr 2021 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2021-04-19-graph-foundation/</guid><description>Basics.
Definition Graph: $G(V, E)$ Adjacency Matrix: $A$ Degree: $D$, the number of nodes that are adjacent to $v$. Neighbors: $N$, the number of $N_{v(i)}$ is equal to $D_{v(i)}$. Connectivity Walk A walk on a graph is an alternating sequence of nodes and edges, starting with a node and ending with a node where each edge is incident with the nodes immediately preceding and following it.</description></item><item><title>Graph: GraphRNN</title><link>https://zqfang.github.io/2020-12-12-dl-graph-generation/</link><pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-12-12-dl-graph-generation/</guid><description>Why is it interesting Drug discovery discovery highly drug-like molecules complete an existing molecule to optimize a desired property Discovering novel structures Network science Why is it hard Large and variable output Non-unique representations $n$-node graph can be represented in $n!$ ways Hard to compute/optimize objective functions Complex dependencies edge fprmation has long-range dependencies Graph Generative Model Given: Graphs sampled from $p_{data}(G)$</description></item><item><title>Graph: GCN and GAT</title><link>https://zqfang.github.io/2020-12-12-ml-gcn-gat/</link><pubDate>Fri, 11 Dec 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-12-12-ml-gcn-gat/</guid><description>Graph Convolutional Network and Graph Attention
Why deep graph encoder ? Limitations of Shallow Encoders (e.g. node2vec)
$O( | V | )$ parameters are needed: No sharing of parameters between nodes Every node has its own unique embedding Inherently &amp;ldquo;transductive&amp;rdquo;: Can not generate embeddings for nodes that are not seen during training Do not incorporate node features Many graphs have features that we can and should leverage Graph Convolutional Network Could get embedding for unseen nodes!</description></item><item><title>Graph: Semi-supervised Node Classification</title><link>https://zqfang.github.io/2020-12-12-ml-node-classififcation/</link><pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-12-12-ml-node-classififcation/</guid><description>Problems: Given a network with labels on some nodes, how do we assign labels to all other nodes in the network?
classification label of an object $O$ in network may depend on:
Features of $O$ Labels of the objects in $O$'s neighborhood Features of objects in $O$'s neigborhood Collective classification models Reational clasifiers Iterative classifications Loopy belief propagation Intuition Simultaneous classification of interlinked nodes using correlations</description></item></channel></rss>