<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>NLP - Tag - My Computational Genomic Playground</title><link>https://zqfang.github.io/tags/nlp/</link><description>NLP - Tag - My Computational Genomic Playground</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sat, 05 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://zqfang.github.io/tags/nlp/" rel="self" type="application/rss+xml"/><item><title>NLP: Word2Vec</title><link>https://zqfang.github.io/2020-12-05-nlp-word2vec/</link><pubDate>Sat, 05 Dec 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-12-05-nlp-word2vec/</guid><description>Word2Vec
CBOW Continuous Bag of Words Model (CBOW)
When trainning, use N-gram language model. That&amp;rsquo;s for a target word, select $m$ (window) words before and after.
Model
one-hot encoding get $2m$ vectors: $$X = (x^{c-m}, \cdots, x^{c-1}, x^{c+1}, \cdots, x^{c+m})$$
Embeding Vector $\mathcal{V} \in R^{n \times \mathcal{V}}$,</description></item><item><title>NLP: Parse trees</title><link>https://zqfang.github.io/2020-08-01-nlp-constituency-and-depenency-parsing/</link><pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-08-01-nlp-constituency-and-depenency-parsing/</guid><description>Structure of Sentences: Parse trees Shallow parsing identifies phrasal units, the task of identifying the relationship between them is called parsing.
Parse trees indicate how different grammatical units in a sentence are related hierachically. (aslo refer to constituent parse, chart-based )
dependency parsing: directed graph (graph-based)</description></item><item><title>NLP: RNN and Transformers</title><link>https://zqfang.github.io/2020-07-31-nlp-rnn/</link><pubDate>Fri, 31 Jul 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-07-31-nlp-rnn/</guid><description>Backpropagation Through Time Long Short-Term Memory Delete information from the context that is no longer needed: Forget Gate f $$ f_t = \sigma (U_f h_{t-1} + W_f X_t) $$ $$ k_t = c_{t-1} \odot f_t $$ Compute the actual information we need to extract from the previous hidden stat and</description></item><item><title>NLP: Start point for biologist</title><link>https://zqfang.github.io/2020-06-20-nlp-basics/</link><pubDate>Sat, 20 Jun 2020 00:00:00 +0000</pubDate><author>Author</author><guid>https://zqfang.github.io/2020-06-20-nlp-basics/</guid><description>NLP Basics for the newbies like me
Languwage model Models that assigns probabilities to sequences of words are called languwage models.
Count-based Representation 1. one-hot representation 2. BoW: Bag of words Blow describes the occurrence of words within a document. including
A Vocabulary of known words A measure of the presence of known words, e.</description></item></channel></rss>