<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Pytorch - Tag - My Computational Genomic Playground</title>
        <link>https://zqfang.github.io/tags/pytorch/</link>
        <description>Pytorch - Tag - My Computational Genomic Playground</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 29 Jan 2020 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://zqfang.github.io/tags/pytorch/" rel="self" type="application/rss+xml" /><item>
    <title>Loss function for multi-label classification</title>
    <link>https://zqfang.github.io/2020-01-29-ml-lossfunc/</link>
    <pubDate>Wed, 29 Jan 2020 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://zqfang.github.io/2020-01-29-ml-lossfunc/</guid>
    <description><![CDATA[Multi-label classification, tasks commonly be seen on health record data (multi symptoms).
Loss function design:
  Multi binary cross-entropy
each class has a binary output
  Label smoothing, another regularization technique
  It’s designed to make the model a little bit less certain of it’s decision by changing a little bit its target: instead of wanting to predict 1 for the correct class and 0 for all the others, we ask it to predict 1-ε for the correct class and ε for all the others, with ε a (small) positive number and N the number of classes.]]></description>
</item></channel>
</rss>
